,Year,Title,Decision,Abstract
0,2019,Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations,Oral,"The key idea behind the unsuervised learning of disentangled reresentations is that real-world data is generated by a few exlanatory factors of variation which can be recovered by unsuervised learning algorithms.
In this aer, we rovide a sober look at recent rogress in the field and challenge some common assumtions.
We first theoretically show that the unsuervised learning of disentangled reresentations is fundamentally imossible without inductive biases on both the models and the data.
Then, we train more than $12000$ models covering most rominent methods and evaluation metrics in a reroducible large-scale exerimental study on seven different data sets.
We observe that while the different methods successfully enforce roerties ``encouraged'' by the corresonding losses, well-disentangled models seemingly cannot be identified without suervision.
Furthermore, increased disentanglement does not seem to lead to a decreased samle comlexity of learning for downstream tasks. 
Our results suggest that future work on disentanglement learning should be exlicit about the role of inductive biases and (imlicit) suervision, investigate concrete benefits of enforcing disentanglement of the learned reresentations, and consider a reroducible exerimental setu covering several data sets."
1,2019,Data Shapley:  Equitable Valuation of Data for Machine Learning,Oral,"As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic redictions and decisions. 
For examle, in healthcare and consumer markets, it has been suggested that individuals should be comensated for the data that they generate, but it is not clear what is an equitable valuation for individual data.
In this work, we develo a rinciled framework to address data valuation in the context of suervised machine learning. Given a learning algorithm trained on $n$ data oints to roduce a redictor, we roose data Shaley as a metric to quantify the value of each training datum to the redictor erformance. Data Shaley uniquely satisfies several natural roerties of equitable data valuation. We develo Monte Carlo and gradient-based methods to efficiently estimate data Shaley values in ractical settings where comlex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive exeriments across biomedical, image and synthetic data demonstrate that data Shaley has several other benefits: 1) it is more owerful than the oular leave-one-out or leverage score in roviding insight on what data is more valuable for a given learning task; 2)  low Shaley value data effectively cature outliers and corrutions; 3)  high Shaley value data inform what tye of new data to acquire to imrove the redictor.  "
2,2019,ELF OpenGo: an analysis and open reimplementation of AlphaZero,Oral,"The AlhaGo, AlhaGo Zero, and AlhaZero series of algorithms are remarkable demonstrations of dee reinforcement learning's caabilities, achieving suerhuman erformance in the comlex game of Go with rogressively increasing autonomy. However, many obstacles remain in the understanding of and usability of these romising aroaches by the research community. Toward elucidating unresolved mysteries and facilitating future research, we roose ELF OenGo, an oen-source reimlementation of the AlhaZero algorithm. ELF OenGo is the first oen-source Go AI to convincingly demonstrate suerhuman erformance with a erfect (20:0) record against global to rofessionals. We aly ELF OenGo to conduct extensive ablation studies, and to identify and analyze numerous interesting henomena in both the model training and in the gamelay inference rocedures. Our code, models, selflay datasets, and auxiliary data are ublicly available.
"
3,2019,Regret Circuits: Composability of Regret Minimizers,Oral,"Regret minimization is a owerful tool for solving large-scale roblems; it was recently used in breakthrough results for large-scale extensive-form game solving. This was achieved by comosing simlex regret minimizers into an overall regret-minimization framework for extensive-form game strategy saces. In this aer we study the general comosability of regret minimizers. We derive a calculus for constructing regret minimizers for comosite convex sets that are obtained from convexity-reserving oerations on simler convex sets. We show that local regret minimizers for the simler sets can be combined with additional regret minimizers into an aggregate regret minimizer for the comosite set. As one alication, we show that the CFR framework can be constructed easily from our framework. We also show ways to include curtailing (constraining) oerations into our framework. For one, they enables the construction of CFR generalization for extensive-form games with general convex strategy constraints that can cut across decision oints.
"
4,2019,Refined Complexity of PCA with Outliers,Oral,"Princial comonent analysis (PCA) is one of the most fundamental rocedures in exloratory data analysis and is the basic ste in alications ranging from quantitative finance and bioinformatics to image analysis and neuroscience. However, it is well-documented that the alicability of PCA in many real scenarios could be constrained by an “immune deficiency” to outliers such as corruted observations. We consider the following algorithmic question about the PCA with outliers. For a set of n oints in R^d, how to learn a subset of oints, say 1% of the total number of oints, such that the remaining art of the oints is best fit into some unknown r-dimensional subsace? We rovide a rigorous algorithmic
analysis of the roblem. We show that the roblem is solvable in time n^O(d^2) . In articular, for constant dimension the roblem is solvable in olynomial time. We comlement the algorithmic result by the lower bound, showing that unless Exonential Time Hyothesis fails, in time f(d) n^o(d), for any function f of d, it is imossible not only to solve the roblem exactly but even to aroximate it within a constant factor.
"
5,2019,SelectiveNet: A Deep Neural Network with an Integrated Reject Option,Oral,"We consider the roblem of selective rediction (also known as reject otion) in dee neural networks, and introduce SelectiveNet, a dee neural architecture with an integrated reject otion. Existing rejection mechanisms are based mostly on a threshold over the rediction confidence of a re-trained network. In contrast, SelectiveNet is trained to otimize both classification (or regression) and rejection simultaneously, end-to-end.  The result is a dee neural network that is otimized over the covered domain. In our exeriments, we show a consistently imroved risk-coverage trade-off over several well-known classification and regression datasets, thus reaching new state-of-the-art results for dee selective classification.
"
6,2019,PA-GD: On the Convergence of Perturbed Alternating Gradient Descent to Second-Order Stationary Points for Structured Nonconvex Optimization,Oral,"Alternating gradient descent (A-GD) is a simle but oular algorithm  in machine learning, which udates two blocks of variables in an alternating manner using gradient descent stes. %, in which a gradient ste is taken on one block, while keeing the remaining block fixed.
In this aer, we consider a smooth unconstrained nonconvex otimization roblem, and roose a {\bf }erturbed {\bf A}-{\bf GD} (PA-GD) which is able to converge (with high robability) to the second-order stationary oints (SOSPs) with a global sublinear rate. {Existing analysis on A-GD tye algorithm either only guarantees convergence to first-order solutions, or converges to second-order solutions asymtotically (without rates).} To the best of our knowledge, this is the first alternating tye algorithm that takes $\mathcal{O}(\text{olylog}(d)\esilon^2)$ iterations to achieve an ($\esilon,\sqrt{\esilon}$)-SOSP with high robability, where olylog$(d)$ denotes the olynomial of the logarithm with resect to roblem dimension $d$."
7,2019,Adversarial Attacks on Node Embeddings via Graph Poisoning,Oral,"The goal of network reresentation learning is to learn low-dimensional node embeddings that cature the grah structure and are useful for solving downstream tasks. However, desite the roliferation of such methods, there is currently no study of their robustness to adversarial attacks. We rovide the first adversarial vulnerability analysis on the widely used family of methods based on random walks. We derive efficient adversarial erturbations that oison the network structure and have a negative effect on both the quality of the embeddings and the downstream tasks. We further show that our attacks are transferable since they generalize to many models, and are successful even when the attacker is restricted. The code and the data is rovided in the sulementary material.
"
8,2019,Validating Causal Inference Models via Influence Functions,Oral,"The roblem of estimating causal effects of treatments from observational data falls beyond the realm of suervised learning — because counterfactual data is inaccessible, we can never observe the true causal effects. In the absence of ""suervision"", how can we evaluate the erformance of causal inference methods? In this aer, we use influence functions — the functional derivatives of a loss function — to develo a model validation rocedure that estimates the estimation error of causal inference methods. Our rocedure utilizes a Taylor-like exansion to aroximate the loss function of a method on a given dataset in terms of the influence functions of its loss on a ""synthesized"", roximal dataset with known causal effects. Under minimal regularity assumtions, we show that our rocedure is consistent and efficient. Exeriments on 77 benchmark datasets show that using our rocedure, we can accurately redict the comarative erformances of state-of-the-art causal inference methods alied to a given observational study.
"
9,2019,A Contrastive Divergence for Combining Variational Inference and MCMC,Oral,"We develo a method to combine Markov chain Monte Carlo (MCMC) and variational inference (VI), leveraging the advantages of both inference aroaches. Secifically, we imrove the variational distribution by running a few MCMC stes. To make inference tractable, we introduce the variational contrastive divergence (VCD), a new divergence that relaces the standard Kullback-Leibler (KL) divergence used in VI. The VCD catures a notion of discreancy between the initial variational distribution and its imroved version (obtained after running the MCMC stes), and it converges asymtotically to the symmetrized KL divergence between the variational distribution and the osterior of interest. The VCD objective can be otimized efficiently with resect to the variational arameters via stochastic otimization. We show exerimentally that otimizing the VCD leads to better redictive erformance on two latent variable models: logistic matrix factorization and variational autoencoders (VAEs).
"
10,2019,"Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks",Oral,"Predicting the number of rocessor clock cycles it takes to execute a block of assembly instructions in steady state (the throughut) is imortant for both comiler designers and erformance engineers.
  Building an analytical model to do so is esecially comlicated in modern x86-64 Comlex Instruction Set Comuter (CISC) machines with sohisticated rocessor microarchitectures
  in that it is tedious, error rone, and must be erformed from scratch for each rocessor generation.

In this aer, we resent Ithemal, the first tool which learns to redict the throughut of a set of instructions.
  Ithemal uses a hierarchical LSTM--based aroach to redict throughut based on the ocodes and oerands of instructions in a basic block.
  We show that Ithemal is more accurate than state-of-the-art hand-written tools currently used in comiler backends and static machine code analyzers.
  In articular, our model has less than half the error of state-of-the-art analytic models (LLVM's llvm-mca and Intel's IACA).
  Ithemal is also able to redict these throughut values at a faster rate than the aforementioned tools, and is easily orted across a variety rocess microarchitectures with minimal develoer effort.
"
11,2019,Feature Grouping as a Stochastic Regularizer for High-Dimensional Structured Data,Oral,"In many alications where collecting data is exensive, for examle neuroscience or medical imaging, the samle size is tyically small comared to the feature dimension. It is challenging in this setting to train exressive, non-linear models without overfitting. These datasets call for intelligent regularization that exloits known structure, such as correlations between the features arising from the measurement device. However, existing structured regularizers need secially crafted solvers, which  are difficult to aly to comlex models. We roose a new regularizer secifically designed to leverage structure in the data in a way that can be alied efficiently to comlex models. Our aroach relies on feature grouing, using a fast clustering algorithm inside a stochastic gradient descent loo: given a family of feature grouings that cature feature covariations, we randomly select these grous at each iteration. We show that this aroach amounts to enforcing a denoising regularizer on the solution. The method is easy to imlement in many model architectures, such as fully connected neural networks, and has a linear comutational cost. We aly this regularizer to a real-world fMRI dataset and the Olivetti Faces datasets. Exeriments on both datasets demonstrate that the roosed aroach roduces models that generalize better than those trained with conventional regularizers, and also imroves convergence seed.
"
12,2019,Game Theoretic Optimization via Gradient-based Nikaido-Isoda Function,Oral,"Comuting the Nash equilibrium (NE) of multi-layer games has witnessed br 
renewed interest due to the recent advances in generative adversarial 
networks. For non-convex otimization formulations involving such games, a 
technique to analyze the quality of a solution is to resort to merit 
functions, among which the Nikaido-Isoda (NI) function is a suitable 
choice -- this function is ositive and goes to zero only when each layer 
is at their otima. However, comuting equilibrium conditions efficiently is 
challenging. To this end, we introduce the Gradient-based 
Nikaido-Isoda (GNI) function which serves: (i) as a merit function, br 
vanishing only at the first order stationary oints of each layer's 
otimization roblem, and (ii) rovides error bounds to a 
first-order NE. Secifically, for bilinear min-max games and multi-layer 
quadratic games, a reformulation using the GNI function results in convex 
otimization objectives -- the alication of gradient descent on such 
reformulations yield linear convergence to an NE. For the general case, we 
rovide conditions under which the gradient descent rovides 
sub-linear convergence. We show exeriments on different multi-layer 
game settings, demonstrating the effectiveness of our scheme against 
other oular game-theoretic otimization methods.
"
13,2019,Manifold Mixup: Better Representations by Interpolating Hidden States,Oral,"Dee neural networks excel at learning the training data, but often rovide incorrect and confident redictions when evaluated on slightly different test examles.  This includes distribution shifts, outliers, and adversarial examles.  To address these issues, we roose \manifoldmixu{}, a simle regularizer that encourages neural networks to redict less confidently on interolations of hidden reresentations.  \manifoldmixu{} leverages semantic interolations as additional training signal, obtaining neural networks with smoother decision boundaries at multile levels of reresentation.  As a result, neural networks trained with \manifoldmixu{} learn flatter class-reresentations, that is, with fewer directions of variance.  We rove theory on why this flattening haens under ideal conditions, validate it emirically on ractical situations, and connect it to the revious works on information theory and generalization.  In site of incurring no significant comutation and being imlemented in a few lines of code, \manifoldmixu{} imroves strong baselines in suervised learning, robustness to single-ste adversarial attacks, and test log-likelihood.
"
14,2019,Calibrated Approximate Bayesian Inference,Oral,"We give a general urose comutational framework for estimating the bias in coverage resulting from making aroximations in Bayesian inference. Coverage is the robability credible sets cover true arameter values. We show how to estimate the actual coverage an aroximation scheme achieves when the ideal observation model and the rior can be simulated, but have been relaced, in the Monte Carlo, with aroximations as they are intractable.  Coverage estimation rocedures given in Lee et al. (2018) work well on simle roblems, but are biased, and do not scale well, as those authors note.   For examle, the methods of Lee et al. (2018) fail for calibration of an aroximate comletely collased MCMC algorithm for artition structure in a Dirichlet rocess for clustering grou labels in a hierarchical model. By exloiting the symmetry of the coverage error under ermutation of low level grou labels and smoothing with Bayesian Additive Regression Trees, we are able to show that the original aroximate inference had oor coverage and should not be trusted. 
"
15,2019,Improved Zeroth-Order Variance Reduced Algorithms and Analysis for Nonconvex Optimization,Oral,"Two tyes of zeroth-order stochastic algorithms have recently been designed for nonconvex otimization resectively based on the first-order techniques SVRG and SARAHSPIDER. This aer addresses several imortant issues that are still oen in these methods. First, all existing SVRG-tye zeroth-order algorithms suffer from worse function query comlexities than either zeroth-order gradient descent (ZO-GD) or stochastic gradient descent (ZO-SGD). In this aer, we roose a new algorithm ZO-SVRG-Coord-Rand and develo a new analysis for an existing ZO-SVRG-Coord algorithm roosed in Liu et al. 2018b, and show that both ZO-SVRG-Coord-Rand and ZO-SVRG-Coord (under our new analysis) outerform other exiting SVRG-tye zeroth-order methods as well as ZO-GD and ZO-SGD. Second, the existing SPIDER-tye algorithm SPIDER-SZO (Fang et al., 2018) has suerior theoretical erformance, but suffers from the generation of a large number of Gaussian random variables as well as a $\sqrt{\esilon}$-level stesize in ractice. In this aer, we develo a new algorithm ZO-SPIDER-Coord, which is free from Gaussian variable generation and allows a large constant stesize while maintaining the same convergence rate and query comlexity, and we further show that ZO-SPIDER-Coord automatically achieves a linear convergence rate as the iterate enters into a local PL region without restart and algorithmic modification."
16,2019,Making Deep Q-learning methods robust to time discretization,Oral,"Desite remarkable successes, Dee Reinforce-
ment Learning (DRL) is not robust to hyeraram-
eterization, imlementation details, or small envi-
ronment changes (Henderson et al. 2017, Zhang
et al. 2018). Overcoming such sensitivity is key
to making DRL alicable to real world roblems.
In this aer, we identify sensitivity to time dis-
cretization in near continuous-time environments
as a critical factor; this covers, e.g., changing
the number of frames er second, or the action
frequency of the controller. Emirically, we find
that Q-learning-based aroaches such as Dee Q-
learning (Mnih et al., 2015) and Dee Determinis-
tic Policy Gradient (Lillicra et al., 2015) collase
with small time stes. Formally, we rove that
Q-learning does not exist in continuous time. We
detail a rinciled way to build an off-olicy RL
algorithm that yields similar erformances over
a wide range of time discretizations, and confirm
this robustness emirically.
"
17,2019,On Efficient Optimal Transport: An Analysis of Greedy and Accelerated Mirror Descent Algorithms,Oral,"We rovide theoretical analyses for two algorithms that solve the regularized otimal transort (OT) roblem between two discrete robability measures with at most $n$ atoms. We show that a greedy variant of the classical Sinkhorn algorithm, known as the \emh{Greenkhorn algorithm}, can be imroved to $\widetilde{\mathcal{O}}\left(\frac{n^2}{\varesilon^2}\right)$, imroving on the best known comlexity bound of $\widetilde{\mathcal{O}}\left(\frac{n^2}{\varesilon^3}\right)$. Notably, this matches the best known comlexity bound for the Sinkhorn algorithm and hels exlain why the Greenkhorn algorithm can outerform the Sinkhorn algorithm in ractice. Our roof technique, which is based on a rimal-dual formulation and a novel uer bound for the dual solution, also leads to a new class of algorithms that we refer to as \emh{adative rimal-dual accelerated mirror descent} (APDAMD) algorithms.  We rove that the comlexity of these algorithms is $\bigOtil\left(\frac{n^2\gamma^{12}}{\varesilon}\right)$, where $\gamma0$ refers to the inverse of the strong convexity module of Bregman divergence with resect to $\left\|\cdot\right\|_\infty$.  This imlies that the APDAMD algorithm is faster than the Sinkhorn and Greenkhorn algorithms in terms of $\varesilon$.  Exerimental results on synthetic and real datasets demonstrate the favorable erformance of the Greenkhorn and APDAMD algorithms in ractice."
18,2019,First-Order Adversarial Vulnerability of Neural Networks and Input Dimension,Oral,"Over the ast few years, neural networks have been roven vulnerable to adversarial images: targeted but imercetible image erturbations lead to drastically different redictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inuts. Surrisingly, vulnerability does not deend on network toology: for most current network architectures, we rove that at initialization, the L1-norm of these gradients grows as the square root of the inut dimension, leaving the networks increasingly vulnerable with growing image size. We emirically show that this dimension-deendence ersists after either usual or robust training, but gets attenuated with higher regularization.
"
19,2019,Faster Stochastic Alternating Direction Method of Multipliers for Nonconvex Optimization,Oral,"In this aer, we roose a faster stochastic alternating direction method of multiliers (ADMM) for nonconvex otimization by using a new stochastic ath-integrated differential estimator (SPIDER), called as SPIDER-ADMM. As a major contribution, we give a new theoretical analysis framework for nonconvex stochastic ADMM methods with roviding the otimal incremental first-order oracle (IFO) comlexity. Secifically, we rove that our SPIDER-ADMM  achieves a record-breaking IFO comlexity of $\mathcal{O}(n+n^{12}\esilon^{-1})$ for finding an $\esilon$-aroximate solution, which imroves the deterministic ADMM by a factor $\mathcal{O}(n^{12})$, where $n$ denotes the samle size.  Based on our new analysis framework, we also rove that the existing ADMM-based non-convex otimization algorithms, SVRG-ADMM and SAGA-ADMM, have the otimal IFO comlexity as $\mathcal{O}(n+n^{23}\esilon^{-1})$. Thus, SPIDER-ADMM imroves the existing stochastic ADMM methods by a factor of $\mathcal{O}(n^{16})$. Moreover, we extend SPIDER-ADMM to the online setting, and roose a faster online ADMM, \emh{i.e.}, online SPIDER-ADMM. Our theoretical analysis shows that the online SPIDER-ADMM has the IFO comlexity of $\mathcal{O}(\esilon^{-\frac{3}{2}})$ for finding an $\esilon$-aroximate solution, which imroves the existing best results by a factor of $\mathcal{O}(\esilon^{\frac{1}{2}})$. Finally, the exerimental results on benchmark datasets validate that the roosed algorithms have faster convergence rate than the existing ADMM algorithms for nonconvex otimization."
20,2019,Nonlinear Distributional Gradient Temporal-Difference Learning,Oral,"We devise a distributional  variant of gradient temoral-difference (TD) learning.   Distributional reinforcement learning has been demonstrated to outerform the regular one in the recent study \cite{bellemare2017distributional}. In the olicy evaluation setting, we  design two new algorithms called  distributional GTD2  and distributional TDC  using the Cram{\'e}r distance on the distributional version of the Bellman error objective function, which inherits  advantages of both the nonlinear gradient TD algorithms and the distributional RL aroach. In the control setting, we roose the distributional Greedy-GQ using  similar derivation. We rove the asymtotic almost-sure convergence of distributional GTD2 and TDC to a local otimal solution for   general smooth function aroximators, which includes neural networks that have been widely used in recent study to solve the real-life RL roblems. In each ste, the comutational comlexity of above three algorithms is linear w.r.t.\ the number of the arameters of the function aroximator, thus can be imlemented efficiently for neural networks.
"
21,2019,Stable-Predictive Optimistic Counterfactual Regret Minimization,Oral,"  The CFR framework has been a owerful tool for solving large-scale extensive-form games in ractice. However, the theoretical rate at which ast CFR-based algorithms converge to the Nash equilibrium is on the order of $O(T^{-12})$, where $T$ is the number of iterations. In contrast, first-order methods can be used to achieve a $O(T^{-1})$ deendence on iterations, yet these methods have been less successful in ractice. In this work we resent the first CFR variant that breaks the square-root deendence on iterations. By combining and extending recent advances on redictive and stable regret minimizers for the matrix-game setting we show that it is ossible to leverage ``otimistic'' regret minimizers to achieve a $O(T^{-34})$ convergence rate within the CFR framework."
22,2019,Learning to Groove with Inverse Sequence Transformations,Oral,"We exlore models for translating abstract musical ideas (scores, rhythms) into exressive erformances using seq2seq and recurrent variational information bottleneck (VIB) models. Though seq2seq models usually require ainstakingly aligned corora, we show that it is ossible to adat an aroach from the Generative Adversarial Network (GAN) literature (e.g. Pix2Pix, Vid2Vid) to sequences, creating large volumes of aired data by erforming simle transformations and training generative models to lausibly invert these transformations. Music, and drumming in articular, rovides a strong test case for this aroach because many common transformations (quantization, removing voices) have clear semantics, and learning to invert them has real-world alications.  Focusing on the case of drum set layers, we create and release a new dataset for this urose, containing over 13 hours of recordings by rofessional drummers aligned with fine-grained timing and dynamics information.  We also exlore some of the creative otential of these models, demonstrating imrovements on state-of-the-art methods for Humanization (instantiating a erformance from a musical score).
"
23,2019,On Certifying Non-Uniform Bounds against Adversarial Attacks,Oral,"This work studies the robustness certification roblem of neural network models,
which aims to find certified adversary-free regions as large as ossible around data oints.
In contrast to the existing aroaches that seek regions bounded uniformly along all inut features,
we consider non-uniform bounds and use it to study the decision boundary of neural network models.
We formulate our target as an otimization roblem with nonlinear constraints.
Then, a framework alicable for general feedforward neural networks is roosed to bound the outut logits
so that the relaxed roblem can be solved by the augmented Lagrangian method.
Our exeriments show the non-uniform bounds have larger volumes than uniform ones.
Comared with normal models, the robust models have even larger non-uniform bounds and better interretability.
Further, the geometric similarity of the non-uniform bounds gives a quantitative, data-agnostic metric of inut features' robustness.
"
24,2019,Passed & Spurious: Descent Algorithms and Local Minima in Spiked Matrix-Tensor Models,Oral,"In this work we analyse quantitatively the interlay between the loss landscae and erformance of descent algorithms in a rototyical inference roblem, the siked matrix-tensor model. We study a loss function that is the negative log-likelihood of the model. We analyse the number of local minima at a fixed distance from the signalsike with the Kac-Rice formula, and locate trivialization of the landscae at large signal-to-noise ratios. We evaluate analytically the erformance of a gradient flow algorithm using integro-differential PDEs as develoed in hysics of disordered systems for the Langevin dynamics.
We analyze the erformance of an aroximate message assing algorithm estimating the maximum likelihood configuration via its state evolution. We conclude by comaring the above results: while we observe a
drastic slow down of the gradient flow dynamics even in the region
where the landscae is trivial, both the analyzed algorithms are shown
to erform well even in the art of the region of arameters where
surious local minima are resent. 
"
25,2019,Metric-Optimized Example Weights,Oral,"Real-world machine learning alications often have comlex test metrics, and may have training and test data that are not identically distributed. Motivated by known connections between comlex test metrics and cost-weighted learning, we roose addressing these issues by using a weighted loss function with a standard loss, where the weights on the training examles are learned to otimize the test metric on a validation set. These metric-otimized examle weights can be learned for any test metric, including black box and customized ones for secific alications. We illustrate the erformance of the roosed method on diverse ublic benchmark datasets and real-world alications. We also rovide a generalization bound for the method.
"
26,2019,Moment-Based Variational Inference for Markov Jump Processes,Oral,"We roose moment-based variational inference as a flexible framework for aroximate smoothing of latent Markov jum rocesses. The main ingredient of our aroach is to artition the set of all transitions of the latent rocess into classes. This allows to exress the Kullback-Leibler divergence from the aroximate to the osterior rocess in terms of a set of moment functions that arise naturally from the chosen artition. To illustrate ossible choices of the artition, we consider secial classes of jum rocesses that frequently occur in alications. We then extend the results to latent arameter inference and demonstrate the method on several examles. 
"
27,2019,Processing Megapixel Images with Deep Attention-Sampling Models,Oral,"Existing dee architectures cannot oerate on very large signals such
as megaixel images due to comutational and memory constraints. To
tackle this limitation, we roose a fully differentiable end-to-end
trainable model that samles and rocesses only a fraction of the full
resolution inut image.

The locations to rocess are samled from an attention distribution
comuted from a low resolution view of the inut. We refer to our
method as attention samling and it can rocess images of
several megaixels with a standard single GPU setu.

We show that samling from the attention distribution results in an
unbiased estimator of the full model with minimal variance, and we
derive an unbiased estimator of the gradient that we use to train our
model end-to-end with a normal SGD rocedure.

This new method is evaluated on three classification tasks, where we
show that it allows to reduce comutation and memory footrint by
an order of magnitude for the same accuracy as classical
architectures. We also show the consistency of the samling that
indeed focuses on informative arts of the inut images.
"
28,2019,Lower Bounds for Smooth Nonconvex Finite-Sum Optimization,Oral,"Smooth finite-sum otimization has been widely studied in both convex and nonconvex settings. However, existing lower bounds for finite-sum otimization are mostly limited to the setting where each comonent function is (strongly) convex, while the lower bounds for nonconvex finite-sum otimization remain largely unsolved. In this aer, we study the lower bounds for smooth nonconvex finite-sum otimization, where the objective function is the average of $n$ nonconvex comonent functions. We rove tight lower bounds for the comlexity of finding $\esilon$-subotimal oint and $\esilon$-aroximate stationary oint in different settings, for a wide regime of the smallest eigenvalue of the Hessian of the objective function (or each comonent function). Given our lower bounds, we can show that existing algorithms including {KatyushaX} \cite{allen2018katyushax}, {Natasha} \cite{allen2017natasha} and {StagewiseKatyusha} \cite{yang2018does} have achieved otimal {Incremental First-order Oracle} (IFO) comlexity (i.e., number of IFO calls) u to logarithm factors for nonconvex finite-sum otimization. We also oint out otential ways to further imrove these comlexity results, in terms of making stronger assumtions or by a different convergence analysis."
29,2019,Composing Entropic Policies using Divergence Correction,Oral,"Dee reinforcement learning algorithms have achieved remarkable successes, but often require vast amounts of exerience to solve a task. Comosing skills mastered in one task in order to efficiently solve novel challenges romises dramatic imrovements in data efficiency. Here, we build on two recent works comosing behaviors reresented in the form of action-value functions. We analyze rior methods and show that they erform oorly in some situations. As art of this analysis, we extend an imortant generalization of olicy imrovement to the maximum entroy framework and introduce an algorithm for the ractical imlementation of successor features in continuous action saces. Then we roose a novel aroach which addresses the failure cases of rior work and, in rincile, recovers the otimal olicy during transfer. This method works by exlicitly learning the (discounted, future) divergence between base olicies. We study this aroach in the  tabular case and on non-trivial continuous control roblems with comositional structure and show that it outerforms or matches existing methods across all tasks considered.
"
30,2019,Improving Model Selection by Employing the Test Data,Oral,"Model selection and evaluation are usually strictly searated by means of data slitting to enable an unbiased estimation and a simle statistical inference for the unknown generalization erformance of the final rediction model. We investigate the roerties of novel evaluation strategies, namely when the final model is selected based on emirical erformances on the test data. To guard against selection induced overotimism, we emloy a arametric multile test correction based on the aroximate multivariate distribution of erformance estimates. Our numerical exeriments involve training common machine learning algorithms (EN, CART, SVM, XGB) on various artificial classification tasks. At its core, our roosed aroach imroves model selection in terms of the exected final model erformance without introducing overotimism. We furthermore observed a higher robability for a successful evaluation study, making it easier in ractice to emirically demonstrate a sufficiently high redictive erformance.
"
31,2019,Understanding MCMC Dynamics as Flows on the Wasserstein Space,Oral,"It is known that the Langevin dynamics used in MCMC is the gradient flow of the KL divergence on the Wasserstein sace, which hels convergence analysis and insires recent article-based variational inference methods (ParVIs). But no more MCMC dynamics is understood in this way. In this work, by develoing novel concets, we roose a theoretical framework that recognizes a general MCMC dynamics as the fiber-gradient Hamiltonian flow on the Wasserstein sace of a fiber-Riemannian Poisson manifold. The ``conservation + convergence'' structure of the flow gives a clear icture on the behavior of general MCMC dynamics. We analyse existing MCMC instances under the framework. The framework also enables ParVI simulation of MCMC dynamics, which enriches the ParVI family with more efficient dynamics, and also adats ParVI advantages to MCMCs. We develo two ParVI methods for a articular MCMC dynamics and demonstrate the benefits in exeriments.
"
32,2019,TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning,Oral,"Handling reviously unseen tasks after given only a few training examles continues to be a tough challenge in machine learning. We roose TaNets, a neural network augmented with task-adative rojection for imroved few-shot learning. Here, emloying a meta-learning strategy with eisode-based training, a network and a set of er-class reference vectors are learned slowly over widely varying tasks. At the same time, for every eisode, features in the embedding sace are linearly rojected into a new sace as a form of quick task-secific conditioning. Training loss is obtained based on a distance metric between the query and the reference vectors in the rojection sace. Excellent generalization results in this way. When tested on the Omniglot, miniImageNet and tieredImageNet datasets, we obtain state of the art classification accuracies under different few-shot scenarios.
"
33,2019,Teaching a black-box learner,Oral,"One widely-studied model of {\it teaching} calls for a teacher to rovide the minimal set of labeled examles that uniquely secifies a target concet. The assumtion is that the teacher knows the learner's hyothesis class, which is often not true of real-life teaching scenarios. We consider the roblem of teaching a learner whose reresentation and hyothesis class are unknown---that is, the learner is a black box. We show that a teacher who does not interact with the learner can do no better than roviding random examles. We then rove, however, that with interaction, a teacher can efficiently find a set of teaching examles that is a rovably good aroximation to the otimal set. As an illustration, we show how this scheme can be used to {\it shrink} training sets for any family of classifiers: that is, to find an aroximately-minimal subset of training instances that yields the same classifier as the entire set.
"
34,2019,When Samples Are Strategically Selected,Oral,"In standard classification roblems, the assumtion is that the entity making the decision (the {\em rincial}) has access to {\em all} the samles.  However, in many contexts, she either does not have direct access to the samles, or can insect only a limited set of samles and does not know which are the most relevant ones.  In such cases, she must rely on another arty (the {\em agent}) to either rovide the samles or oint out the most relevant ones.  If the agent has a different objective, then the rincial cannot trust the submitted samles to be reresentative.  She must set a {\em olicy} for how she makes decisions, keeing in mind the agent's incentives.  In this aer, we introduce a theoretical framework for this roblem and rovide key structural and comutational results.
"
35,2019,Improving Adversarial Robustness via Promoting Ensemble Diversity,Oral,"Though dee neural networks have achieved significant rogress on various tasks, often enhanced by model ensemble, existing high-erformance models can be vulnerable to adversarial attacks. Many efforts have been devoted to enhancing the robustness of individual networks and then constructing a straightforward ensemble, e.g., by directly averaging the oututs, which ignores the interaction among networks. This aer resents a new method that exlores the interaction among individual networks to imrove robustness for ensemble models. Technically, we define a new notion of ensemble diversity in the adversarial setting as the diversity among non-maximal redictions of individual members, and resent an adative diversity romoting (ADP) regularizer to encourage the diversity, which leads to globally better robustness for the ensemble by making adversarial examles difficult to transfer among individual members. Our method is comutationally efficient and comatible with the defense methods acting on individual networks. Emirical results on various datasets verify that our method can imrove adversarial robustness while maintaining state-of-the-art accuracy on normal examles.
"
36,2019,Grid-Wise Control for Multi-Agent Reinforcement Learning in Video Game AI,Oral,"We consider the roblem of multi-agent reinforcement learning (MARL) in video game AI, where the agents are located in a satial grid-world environment and the number of agents varies both within and across eisodes. The challenge is to flexibly control arbitrary number of agents while achieving effective collaboration. Existing MARL methods usually suffer from the trade-off between these two considerations. To address the issue, we roose a novel architecture that learns a satial joint reresentation of all the agents and oututs grid-wise actions. Each agent will be controlled indeendently by taking the action from the grid it occuies. By viewing the state information as a grid feature ma, we emloy a convolutional encoder-decoder as the olicy network. This architecture naturally romotes agent communication because of the large recetive field rovided by the stacked convolutional layers. Moreover, the satially shared convolutional arameters enable fast arallel exloration that the exeriences discovered by one agent can be immediately transferred to others. The roosed method can be conveniently integrated with general reinforcement learning algorithms, e.g., PPO and Q-learning. We demonstrate the effectiveness of the roosed method in extensive challenging multi-agent tasks in the comlex game StarCraft II.
"
37,2019,Nonconvex Variance Reduced Optimization with Arbitrary Sampling,Oral,"We rovide the first imortance samling variants of variance reduced algorithms for emirical risk minimization with non-convex loss functions. In articular, we analyze non-convex versions of \texttt{SVRG}, \texttt{SAGA} and \texttt{SARAH}. Our methods have the caacity to seed u the training rocess by  an order of magnitude comared to the state of the art on real datasets. Moreover, we also imrove uon current mini-batch analysis of these methods by roosing  imortance samling for minibatches in this setting. Surrisingly, our aroach can in some regimes lead to suerlinear seedu with resect to the minibatch size, which is not usually resent in stochastic otimization. All the above results follow from a general analysis of the methods which works with {\em arbitrary samling}, i.e., fully general randomized strategy for the selection of subsets of examles to be samled in each iteration. Finally, we also erform a novel imortance samling analysis of \texttt{SARAH} in the convex setting.
"
38,2019,PAC Learnability of Node Functions in Networked Dynamical Systems,Oral,"We consider the PAC learnability of the local functions at the vertices of a discrete networked dynamical system, assuming that the underlying network is known. Our focus is on the learnability of threshold functions. We show that several variants of threshold functions are PAC learnable and rovide tight bounds on the samle comlexity. In general, when the inut consists of ositive and negative examles, we show that the concet class of threshold functions is not efficiently PAC learnable, unless NP = RP. Using a dynamic rogramming aroach, we show efficient PAC learnability when the number of negative examles is small. We also resent an efficient learner which is consistent with all the ositive examles and at least (1-1e) fraction of the negative examles. This algorithm is based on maximizing a submodular function under matroid constraints. By erforming exeriments on both synthetic and real-world networks, we study how the network structure and samle comlexity influence the quality of the inferred system.
"
39,2019,TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning,Oral,"Hierarchical  reinforcement  learning  (HRL)  can rovide a rinciled solution to the RL challenge of scalability for comlex tasks. By incororating a grahical model  (GM) and the rich family of related methods, there is also hoe to address issues  such as transferability, generalisation and exloration. Here we roose a flexible GM-based HRL framework which leverages efficient inference rocedures to enhance generalisation and transfer ower. In our roosed transferable and  information-based grahical model framework ‘TibGM’,  we  show the equivalence between our mutual information-based objective in the GM, and an RL consolidated objective consisting of a standard reward maximisation target and a generalisationtransfer objective. In settings where there is a sarse or decetive reward signal, our TibGM framework is flexible enough to incororate exloration bonuses deicting intrinsic rewards. We emirically verify imroved erformance and exloration ower.
"
40,2019,LR-GLM: High-Dimensional Bayesian Inference Using Low-Rank Data Approximations,Oral,"Due to the ease of modern data collection, ractitioners often face a large collection of covariates and the need to understand their relation to some resonse. Generalized linear models (GLMs) offer a articularly interretable framework for this analysis. In the high-dimensional case without an overwhelming amount of data er arameter, we exect uncertainty to be non-trivial; a Bayesian aroach allows coherent quantification of this uncertainty. Unfortunately existing methods for Bayesian inference in GLMs require running times roughly cubic in arameter dimension, thus limiting their alicability in increasingly widesread settings with tens of thousands of arameters. We roose to reduce time and memory costs with a low-rank aroximation of the data. We show that our method, which we call LR-GLM, still rovides a full Bayesian osterior aroximation and admits running time reduced by a full factor of the arameter dimension. We theoretically establish the quality of our aroximation via interretable error bounds and show how the choice of rank allows a tunable comutational-statistical trade-off. Exeriments suort our theory and demonstrate the efficacy of LR-GLM in on real, large-scale datasets.
"
41,2019,HOList: An Environment for Machine Learning of Higher Order Logic Theorem Proving,Oral,"We resent an environment, benchmark, and dee learning driven automated theorem rover for higher-order logic. Higher-order interactive theorem rovers enable the formalization of arbitrary mathematical theories and thereby resent an interesting challenge for dee learning. We rovide an oen-source framework based on the HOL Light theorem rover that can be used as a reinforcement learning environment. HOL Light comes with a broad coverage of basic mathematical theorems on calculus and the formal roof of the Keler conjecture, from which we derive a challenging benchmark for automated reasoning aroaches. We also resent a dee reinforcement learning driven automated theorem rover, DeeHOL, that gives strong initial results on this benchmark.
"
42,2019,Online Meta-Learning,Oral,"A central caability of intelligent systems is the ability to continuously build uon revious exeriences to seed u and enhance learning of new tasks. Two distinct research aradigms have studied this question. Meta-learning views this roblem as learning a rior over model arameters that is amenable for fast adatation on a new task, but tyically assumes the set of tasks are available together as a batch. In contrast, online (regret based) learning considers a sequential setting in which roblems are revealed one after the other, but conventionally train only a single model without any task-secific adatation.
This work introduces an online meta-learning roblem setting, which merges ideas from both the aforementioned aradigms in order to better cature the  sirit and ractice of continual lifelong learning. We roose the follow the meta leader (FTML) algorithm which extends the MAML algorithm to this setting. Theoretically, this work rovides an  O(logT) regret guarantee with only an additional higher order smoothness assumtion (in comarison to the standard online setting).
Our exerimental evaluation on three different large-scale tasks suggest that the roosed algorithm significantly outerforms alternatives based on traditional online learning aroaches.
"
43,2019,Adversarial camera stickers: A physical camera-based attack on deep learning systems,Oral,"Recent work has thoroughly documented the suscetibility of dee learning systems to adversarial examles, but most such instances directly maniulate the digital inut to a classifier.  Although a smaller line of work has considered hysical adversarial attacks, in all cases these involve maniulating the object of interest, i.e., utting a hysical sticker on a object to misclassify it, or manufacturing an object secifically intended to be misclassified.  In this work we consider an alternative question: is it ossible to fool dee classifiers, over all erceived objects of a certain tye, by hysically maniulating the camera itself?  We show that this is indeed ossible, that by lacing a carefully crafted and mainly-translucent sticker over the lens of a camera, one can create universal erturbations of the observed images that are inconsicuous, yet reliably misclassify target objects as a different (targeted) class.  To accomlish this, we roose an iterative rocedure for both udating the attack erturbation (to make it adversarial for a given classifier), and the threat model itself (to ensure it is hysically realizable).  For examle, we show that we can achieve hysically-realizable attacks that fool ImageNet classifiers in a targeted fashion 49.6\% of the time.  This resents a new class of hysically-realizable threat models to consider in the context of adversarially robust machine learning.
"
44,2019,Statistical Foundations of Virtual Democracy,Oral,"Virtual democracy is an aroach to automating decisions, by learning models of the references of individual eole, and, at runtime, aggregating the redicted references of those eole on the dilemma at hand. One of the key questions is which aggregation method -- or voting rule -- to use; we offer a novel statistical viewoint that rovides guidance. Secifically, we seek voting rules that are robust to rediction errors, in that their outut on eole's true references is likely to coincide with their outut on noisy estimates thereof. We rove that the classic Borda count rule is robust in this sense, whereas any voting rule belonging to the wide family of airwise-majority consistent rules is not. Our emirical results further suort, and more recisely measure, the robustness of Borda count. 
"
45,2019,Topological Data Analysis of Decision Boundaries with Application to Model Selection,Oral,"We roose the labeled Cech comlex, the lain labeled Vietoris-Ris comlex, and the locally scaled labeled Vietoris-Ris comlex to erform ersistent homology inference of decision boundaries in classification tasks. We rovide theoretical conditions and analysis for recovering the homology of a decision boundary from samles. Our main objective is quantification of dee neural network comlexity to enable matching of datasets to re-trained models to facilitate the functioning of AI marketlaces; we reort results for exeriments using MNIST, FashionMNIST, and CIFAR10.
"
46,2019,Adversarial examples from computational constraints,Oral,"Why are classifiers in high dimension vulnerable to “adversarial” erturbations? We show that it is likely not due to information theoretic limitations, but rather it could be due to comutational constraints.

First we rove that, for a broad set of classification tasks, the mere existence of a robust classifier imlies that it can be found by a ossibly exonential-time algorithm with relatively few training examles. Then we give two articular classification tasks where learning a robust classifier is comutationally intractable. More recisely we construct two binary classifications task in high dimensional sace which are (i) information theoretically easy to learn robustly for large erturbations, (ii) efficiently learnable (non-robustly) by a simle linear searator,
(iii) yet are not efficiently robustly learnable, even for small erturbations. Secifically, for the first task hardness holds for any efficient algorithm in the statistical query (SQ) model, while for the second task we rule out any efficient algorithm under a crytograhic assumtion. These examles give an exonential searation between classical learning and robust learning in the statistical query model or under a crytograhic assumtion. It suggests that adversarial examles may be an unavoidable byroduct of comutational limitations of learning algorithms. 
"
47,2019,Molecular Hypergraph Grammar with Its Application to Molecular Optimization,Oral,"Molecular otimization aims to discover novel molecules with desirable roerties.
Two fundamental challenges are:
(i) it is not trivial to generate valid molecules in a controllable way due to hard chemical constraints such as the valency conditions, and
(ii) it is often costly to evaluate a roerty of a novel molecule, and therefore, the number of roerty evaluations is limited.
These challenges are to some extent alleviated by a combination of a variational autoencoder (VAE) and Bayesian otimization (BO).
VAE converts a molecule intofrom its latent continuous vector,
and BO otimizes a latent continuous vector (and its corresonding molecule) within a limited number of roerty evaluations.
While the most recent work, for the first time, achieved 100% validity,
its architecture is rather comlex due to auxiliary neural networks other than VAE,
making it difficult to train.
This aer resents a molecular hyergrah grammar variational autoencoder (MHG-VAE), which uses a single VAE to achieve 100% validity.
Our idea is to develo a grah grammar encoding the hard chemical constraints, called molecular hyergrah grammar (MHG), which guides VAE to always generate valid molecules.
We also resent an algorithm to construct MHG from a set of molecules.
"
48,2019,Optimal Auctions through Deep Learning,Oral,"Designing an incentive comatible auction that maximizes exected revenue is an intricate task. The single-item case was resolved in a seminal iece of work by Myerson in 1981. Even after 30-40 years of intense research the roblem remains unsolved for seemingly simle multi-bidder, multi-item settings. In this work, we initiate the exloration of the use of tools from dee learning for the automated design of otimal auctions. We model an auction as a multi-layer neural network, frame otimal auction design as a constrained learning roblem, and show how it can be solved using standard ielines. We rove generalization bounds and resent extensive exeriments, recovering essentially all known analytical solutions for  multi-item settings, and obtaining novel mechanisms for settings in which the otimal mechanism is unknown.
"
49,2019,Amortized Monte Carlo Integration,Oral,"Current aroaches to amortizing Bayesian inference focus solely on aroximating the osterior distribution. Tyically, this aroximation is, in turn, used to calculate exectations for one or more target functions---a comutational ieline which is inefficient when the target function(s) are known ufront. In this aer, we address this inefficiency by introducing AMCI, a method for amortizing Monte Carlo integration directly. AMCI oerates similarly to amortized inference but roduces three distinct amortized roosals, each tailored to a different comonent of the overall exectation calculation. At run-time, samles are roduced searately from each amortized roosal, before being combined to an overall estimate of the exectation. We show that while existing aroaches are fundamentally limited in the level of accuracy they can achieve, AMCI can theoretically roduce arbitrarily small errors for any integrable target function using only a single samle from each roosal at run-time. Furthermore, AMCI allows not only for amortizing over datasets but also amortizing over target functions.
"
50,2019,Contextual Memory Trees,Oral,"We design and study a  Contextual Memory Tree (CMT), a learning memory controller that inserts new memories into an exerience store of unbounded size. It oerates online and is designed to efficiently query for memories from that store, suorting logarithmic time insertion and retrieval oerations. Hence CMT can be integrated into existing statistical learning algorithms as an augmented memory unit without substantially increasing training and inference comutation.  Furthermore CMT oerates as a reduction to classification, allowing it to benefit from advances in reresentation or architecture.  We demonstrate the efficacy of CMT by augmenting existing multi-class and multi-label classification algorithms with CMT and observe statistical imrovement. We also test CMT learning on several image-cationing tasks to demonstrate that it erforms comutationally better than a simle nearest neighbors memory system while benefitting from reward learning.
"
51,2019,Training Neural Networks with Local Error Signals,Oral,"Suervised training of neural networks for classification is tyically erformed with a global loss function. The loss function rovides a gradient for the outut layer, and this gradient is back-roagated to hidden layers to dictate an udate direction for the weights. An alternative aroach is to train the network with layer-wise loss functions. In this aer we demonstrate, for the first time, that layer-wise training can aroach the state-of-the-art on a variety of image datasets. We use single-layer sub-networks and two different suervised loss functions to generate local error signals for the hidden layers, and we show that the combination of these losses hel with otimization in the context of local learning. Using local errors could be a ste towards more biologically lausible dee learning because the global error does not have to be transorted back to hidden layers. A comletely backro free variant outerforms reviously reorted results among methods aiming for higher biological lausibility.
"
52,2019,Error Feedback Fixes SignSGD and other Gradient Compression Schemes,Oral,"Sign-based algorithms (e.g. signSGD) have been roosed as a biased gradient comression technique to alleviate the communication bottleneck in training large neural networks across multile workers. We show simle convex counter-examles where signSGD does not converge to the otimum.
Further, even when it does converge, signSGD may generalize oorly when comared with SGD.  These issues arise because of the biased nature of the sign comression oerator.

Finally we show that using error-feedback, i.e. incororating the error made by the comression oerator into the next ste, overcomes these issues. We rove that our algorithm, EF-SGD, with arbitrary comression oerator, achieves the \emh{same rate of convergence} as SGD without any additional assumtions, indicating that we get gradient comression \emh{for free}. Our exeriments thoroughly substantiate the theory showing the sueriority of our algorithm.
"
53,2019,Online learning with kernel losses,Oral,"We resent a generalization of the adversarial linear bandits framework, where the underlying losses are kernel functions (with an associated reroducing kernel Hilbert sace) rather than linear functions. We study a version of the exonential weights algorithm and bound its regret in this setting. Under conditions on the eigen-decay of the kernel we rovide a shar characterization of the regret for this algorithm. When we have olynomial eigen-decay ($\mu_j \le \mathcal{O}(j^{-\beta})$), we find that the regret is bounded by $\mathcal{R}_n \le \mathcal{O}(n^{\beta(2\beta-1)})$. While under the assumtion of exonential eigen-decay ($\mu_j \le \mathcal{O}(e^{-\beta j })$) we get an even tighter bound on the regret $\mathcal{R}_n \le \tilde{\mathcal{O}}(n^{12})$. When the eigen-decay is olynomial we show a \emh{non-matching} minimax lower bound on the regret of $\mathcal{R}_n \ge \Omega(n^{(\beta+1)2\beta})$ and a lower bound of $\mathcal{R}_n \ge \Omega(n^{12})$ when the decay in the eigen-values is exonentially fast.

We also study the full information setting when the underlying losses are kernel functions and resent an adated exonential weights algorithm and a conditional gradient descent algorithm."
54,2019,Multi-Agent Adversarial Inverse Reinforcement Learning,Oral,"Reinforcement learning agents are rone to undesired behaviors due to reward mis-secification. Finding a set of reward functions to roerly guide agent behaviors is articularly challenging in multi-agent scenarios. 
Inverse reinforcement learning rovides a framework to automatically acquire suitable reward functions from exert demonstrations. Its extension to multi-agent settings, however, is difficult due to the more comlex notions of rational behaviors. In this aer, we roose MA-AIRL, a new framework for multi-agent inverse reinforcement learning, which is effective and scalable for Markov games with high-dimensional state-action sace and unknown dynamics. We derive our algorithm based on a new solution concet and maximum seudolikelihood estimation within an adversarial reward learning framework. In the exeriments, we demonstrate that MA-AIRL can recover reward functions that are highly correlated with the ground truth rewards, while significantly outerforms rior methods in terms of olicy imitation.
"
55,2019,POPQORN: Quantifying Robustness of Recurrent Neural Networks,Oral,"The vulnerability to adversarial attacks has been a critical issue of dee neural networks. Addressing this issue requires a reliable way to evaluate the robustness of a network. Recently, several methods have been develoed to comute robustness certification for neural networks, namely, certified lower bounds of the minimum adversarial erturbation. Such methods, however, were devised for feed-forward networks, e.g. multi-layer ercetron or convolutional networks; while it remains an oen roblem to certify robustness for recurrent networks, esecially LSTM and GRU. For such networks, there exist additional challenges in comuting the robustness certification, such as handling the inuts at multile stes and the interaction between gates and states. In this work, we roose POPCORN (Proagated-outut Certified Robustness for RNNs), a general algorithm to certify robustness of RNNs, including vanilla RNNs, LSTMs, and GRUs. We demonstrate its effectiveness for different network architectures and show that the robustness certification on individual stes can lead to new insights.
"
56,2019,Policy Consolidation for Continual Reinforcement Learning,Oral,"We roose a method for tackling catastrohic forgetting in dee reinforcement learning that is \textit{agnostic} to the timescale of changes in the distribution of exeriences, does not require knowledge of task boundaries and can adat in \textit{continuously} changing environments. In our \textit{olicy consolidation} model, the olicy network interacts with a cascade of hidden networks that simultaneously remember the agent's olicy at a range of timescales and regularise the current olicy by its own history, thereby imroving its ability to learn without forgetting. We find that the model imroves continual learning relative to baselines on a number of continuous control tasks in single-task, alternating two-task, and multi-agent cometitive self-lay settings.
"
57,2019,A Composite Randomized Incremental Gradient Method,Oral,"We consider the roblem of minimizing the comosition of a smooth function (which can be nonconvex) and a smooth vector maing, where both of them can be exress as the average of a large number of comonents. We roose a comosite randomized incremental gradient method by extending the SAGA framework. The gradient samle comlexity of our method matches that of several recently develoed methods based on SVRG in the general case. However, for structured roblems where linear convergence rates can be obtained, our method can be much better for ill-conditioned roblems. In addition, when the finite-sum structure only aear for the inner maing, the samle comlexity of our method is the same as that of SAGA for minimizing finite sum of smooth nonconvex functions, desite the additional outer comosition and the stochastic comosite gradients being biased in our case.
"
58,2019,Sparse Extreme Multi-label Learning with Oracle Property,Oral,"The ioneering work of sarse local embeddings on multilabel learning has shown great romise in multilabel classification. Unfortunately, the statistical rate of convergence and oracle roerty of sarse local embeddings are still not well understood. To fill this ga, we resent a unified framework for this method with nonconvex enalty. Theoretically, we rigorously rove that our roosed estimator enjoys oracle roerty (i.e., erforms as well as if the underlying model were known beforehand), and obtains a desirable statistical convergence rate. Moreover, we show that under a mild condition on the magnitude of the entries in the underlying model, we are able to obtain an imroved convergence rate. Extensive numerical exeriments verify our theoretical findings and the sueriority of our roosed estimator.
"
59,2019,GMNN: Graph Markov Neural Networks,Oral,"This aer studies semi-suervised object classification in relational data, which is a fundamental roblem in relational data modeling. The roblem has been extensively studied in the literature of both statistical relational learning (e.g. Relational Markov Networks) and grah neural networks (e.g. Grah Convolutional Networks). Statistical relational learning methods can effectively model the deendency of object labels through conditional random fields for collective classification, whereas grah neural networks learn effective object reresentations for classification through end-to-end training. In this aer, we roose Grah Markov Neural Network (GMNN) that combines the advantages of both worlds. GMNN models the joint distribution of object labels with a conditional random field, which can be effectively trained with the variational EM algorithm. In the E-ste, one grah neural network learns effective object reresentations for aroximating the osterior distributions of object labels. In the M-ste, another grah neural network is used to model the local label deendency. Exeriments on the tasks of object classification, link classification, and unsuervised node reresentation learning show that GMNN achieves state-of-the-art results. 
"
60,2019,Nearest Neighbor and Kernel Survival Analysis: Nonasymptotic Error Bounds and Strong Consistency Rates,Oral,"We establish the first nonasymtotic error bounds for Kalan-Meier-based nearest neighbor and kernel survival robability estimators where feature vectors reside in metric saces. Our bounds imly rates of strong consistency for these nonarametric estimators and, u to a log factor, match an existing lower bound for conditional CDF estimation. Our roof strategy also yields nonasymtotic guarantees for nearest neighbor and kernel variants of the Nelson-Aalen cumulative hazards estimator. We exerimentally comare these methods on four datasets.
We find that for the kernel survival estimator, a good choice of kernel is one learned using random survival forests.
"
61,2019,Graph Neural Network for Music Score Data and Modeling Expressive Piano Performance,Oral,"Music score is often handled as one-dimensional sequential data. Unlike words in a text document, notes in music score can be layed simultaneously by the olyhonic nature and each of them has its own duration. In this aer, we reresent the unique form of musical score using grah neural network and aly it for rendering exressive iano erformance from the music score. Secifically, we design the model using note-level gated grah neural network and measure-level hierarchical attention network with bidirectional long short-term memory with an iterative feedback method. In addition, to model different styles of erformance for a given inut score, we emloy a variational auto-encoder. The result of the listening test shows that our roosed model generated more human-like erformances comared to a baseline model and a hierarchical attention network model that handles music score as a word-like sequence.
"
62,2019,Learning to Clear the Market,Oral,"The roblem of market clearing is to set a rice for an item such that quantity demanded equals quantity sulied. In this work, we cast the roblem of redicting clearing rices into a learning framework and aly the resulting models to the roblem of revenue otimization in auctions and markets with contextual information. The economic intuition behind market clearing allows us to obtain fine-grained control over the aggressiveness of the resulting ricing olicy, grounded in theory. To evaluate our aroach, we fit a model of clearing rices over a massive dataset of bids in dislay ad auctions from a major ad exchange. The learned rices outerform other techniques in the literature in terms of revenue and efficiency trade-offs. Because of the convex nature of the clearing loss function, our method has very fast convergence, as fast as linear regression over the same dataset.
"
63,2019,Stein Point Markov Chain Monte Carlo,Oral,"An imortant task in machine learning and statistics is the aroximation of a robability measure by an emirical measure suorted on a discrete oint set. Stein Points are a class of algorithms for this task, which roceed by sequentially minimising a Stein discreancy between the emirical measure and the target and, hence, require the solution of a non-convex otimisation roblem to obtain each new oint. This aer removes the need to solve this otimisation roblem by, instead, selecting each new oint based on a Markov chain samle ath. This significantly reduces the comutational cost of Stein Points and leads to a suite of algorithms that are straightforward to imlement. The new algorithms are illustrated on a set of challenging Bayesian inference roblems, and rigorous theoretical guarantees of consistency are established.
"
64,2019,Optimal Continuous DR-Submodular Maximization and Applications to Provable Mean Field Inference,Oral,"Mean field inference for discrete grahical models is generally a highly nonconvex roblem, which also holds for the class of robabilistic log-submodular models. Existing otimization methods, e.g., coordinate ascent  algorithms, can only generate local otima.

In this work we roose rovable mean filed  methods for robabilistic  log-submodular models and its osterior agreement (PA) with strong
 aroximation guarantees.  The main algorithmic technique is a new Double Greedy scheme, termed DR-DoubleGreedy, for continuous DR-submodular maximization with box-constraints. It is a one-ass algorithm with linear time comlexity, reaching the otimal 12 aroximation ratio, which may be of indeendent interest. We validate the suerior erformance of our algorithms against baselines on both synthetic and real-world datasets.
"
65,2019,Fast and Simple Natural-Gradient Variational Inference with Mixture of Exponential-family Approximations,Oral,"Natural-gradient methods enable fast and simle algorithms for variational inference, but due to comutational difficulties, their use is mostly limited to minimal exonential-family (EF) aroximations. In this aer, we extend the alication of natural-gradient methods to estimate structured aroximations such as mixture of EF distribution. Such aroximations can fit comlex, multimodal osterior distributions and are generally more accurate than unimodal EF aroximations. By using a minimal conditional-EF reresentation of such aroximations, we derive simle natural-gradient udates. Our emirical results demonstrate a faster convergence of our natural-gradient method comared to black-box gradient-based methods. Our work exands the scoe of natural gradients for Bayesian inference and makes them more widely alicable than before.
"
66,2019,Off-Policy Deep Reinforcement Learning without Exploration,Oral,"Many ractical alications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further ossibility for data collection. In this aer, we demonstrate that due to errors introduced by extraolation, standard off-olicy dee reinforcement learning algorithms, such as DQN and DDPG, are incaable of learning with data uncorrelated to the distribution under the current olicy, making them ineffective for this fixed batch setting. We introduce a novel class of off-olicy algorithms, batch-constrained reinforcement learning, which restricts the action sace in order to force the agent towards behaving close to on-olicy with resect to a subset of the given data. We resent the first continuous control dee reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and emirically demonstrate the quality of its behavior in several tasks.
"
67,2019,Self-Attention Graph Pooling,Oral,"Advanced methods of alying dee learning to structured data such as grahs have been roosed in recent years. In articular, studies have focused on generalizing convolutional neural networks to grah data, which includes redefining the convolution and the downsamling (ooling) oerations for grahs. The method of generalizing the convolution oeration to grahs has been roven to imrove erformance and is widely used. However, the method of alying downsamling to grahs is still difficult to erform and has room for imrovement. In this aer, we roose a grah ooling method based on self-attention. Self-attention using grah convolution allows our ooling method to consider both node features and grah toology. To ensure a fair comarison, the same training rocedures and model architectures were used for the existing ooling methods and our method. The exerimental results demonstrate that our method achieves suerior grah classification erformance on the benchmark datasets using a reasonable number of arameters.
"
68,2019,Shape Constraints for Set Functions,Oral,"Set functions redict a label from a ermutation-invariant variable-size collection of feature vectors. We roose making set functions more understandable and regularized by caturing domain knowledge through shae constraints. We show how rior work in monotonic constraints can be adated to set functions. Then we roose two new shae constraints designed to generalize the conditioning role of weights in a weighted mean.  We show how one can train standard functions and set functions that satisfy these shae constraints with a dee lattice network. We roose a nonlinear estimation strategy we call the semantic feature engine that uses set functions with the roosed shae constraints to estimate labels for comound sarse categorical features. Exeriments on real-world data show the achieved accuracy is similar to dee sets or dee neural networks, but rovides guarantees of the model behavior and is thus easier to exlain and debug.  
"
69,2019,Using Pre-Training Can Improve Model Robustness and Uncertainty,Oral,"Tuning a re-trained network is commonly thought to imrove data efficiency. However, Kaiming He et al. (2018) have called into question the utility of re-training by showing that training from scratch can often yield similar erformance, should the model train long enough. We show that although re-training may not imrove erformance on traditional classification metrics, it does rovide large benefits to model robustness and uncertainty. Through extensive exeriments on label corrution, class imbalance, adversarial examles, out-of-distribution detection, and confidence calibration, we demonstrate large gains from re-training and comlementary effects with task-secific methods. Results include a 30% relative imrovement in label noise robustness and a 10% absolute imrovement in adversarial robustness on both CIFAR-10 and CIFAR-100. In some cases, using re-training without task-secific methods surasses the state-of-the-art, highlighting the imortance of using re-training when evaluating future methods on robustness and uncertainty tasks.
"
70,2019,Fast Rates for a kNN Classifier Robust to Unknown Asymmetric Label Noise,Oral,"We consider classification in the resence of class-deendent asymmetric label noise with unknown noise robabilities. In this setting, identifiability conditions are known, but additional assumtions were shown to be required for finite samle rates, and only the arametric rate has been obtained so far. Assuming these identifiability conditions, together with a measure-smoothness condition on the regression function and Tsybakov’s margin condition, we obtain, u to a log factor, the mini-max otimal rates of the noise-free setting. This rate is attained by a recently roosed modification of the kNN classifier whose analysis exists only under known noise robabilities. Hence, our results rovide solid theoretical backing for this emirically successful algorithm. By contrast the standard kNN is not even consistent in the setting of asymmetric label noise. A key idea in our analysis is a simle kNN based function otimisation aroach that requires far less assumtions than existing mode estimators do, and which may be of indeendent interest for noise roortion estimation and other  randomised otimisation roblems.
"
71,2019,Learning to Prove Theorems via Interacting with Proof Assistants,Oral,"Humans rove theorems by relying on substantial high-level reasoning and roblem-secific insights. Proof assistants offer a formalism that resembles human mathematical reasoning, reresenting theorems in higher-order logic and roofs as high-level tactics. However, human exerts have to construct roofs manually by entering tactics into the roof assistant. In this aer, we study the roblem of using machine learning to automate the interaction with roof assistants. We construct CoqGym, a large-scale dataset and learning environment containing 71K human-written roofs from 123 rojects develoed with the Coq roof assistant. We develo ASTactic, a dee learning-based model that generates tactics as rograms in the form of abstract syntax trees (ASTs). Exeriments show that ASTactic trained on CoqGym can generate effective tactics and can be used to rove new theorems not reviously rovable by automated methods. Code is available at htts:github.comrinceton-vlCoqGym.
"
72,2019,Learning to bid in revenue-maximizing auctions,Oral,"We consider the roblem of the otimization of bidding strategies in rior-deendent revenue-maximizing auctions, when the seller fixes the reserve rices based on the bid distributions. Our study is done in the setting where one bidder is strategic. Using a variational aroach, we study the comlexity of the original objective and we introduce a relaxation of the objective functional in  order to use gradient descent methods. Our aroach is simle, general and can be alied to various value distributions and revenue-maximizing mechanisms. The new strategies we derive yield  massive ulifts comared to the traditional truthfully bidding strategy. 
"
73,2019,Uniform Convergence Rate of the Kernel Density Estimator Adaptive to Intrinsic Volume Dimension,Oral,"We derive concentration inequalities for the suremum norm of the difference between a kernel density estimator (KDE) and its oint-wise exectation that hold uniformly over the selection of the bandwidth and under weaker conditions on the kernel and the data generating distribution than reviously used in the literature. We first roose a novel concet, called the volume dimension, to measure the intrinsic dimension of the suort of a robability distribution based on the rates of decay of the robability of vanishing Euclidean balls. Our bounds deend on the volume dimension and generalize the existing bounds derived in the literature. In articular, when the data-generating distribution has a bounded Lebesgue density or is suorted on a sufficiently well-behaved lower-dimensional manifold, our bound recovers the same convergence rate deending on the intrinsic dimension of the suort as ones known in the literature. At the same time, our results aly to more general cases, such as the ones of distribution with unbounded densities or suorted on a mixture of manifolds with different dimensions. Analogous bounds are derived for the derivative of the KDE, of any order. Our results are generally alicable but are esecially useful for roblems in geometric inference and toological data analysis, including level set estimation, density-based clustering, modal clustering and mode hunting, ridge estimation and ersistent homology.
"
74,2019,Particle Flow Bayes' Rule,Oral,"We resent a article flow realization of Bayes' rule, where an ODE-based neural oerator is used to transort articles from a rior to its osterior after a new observation. We rove that such an ODE oerator exists and its neural arameterization can be trained in a meta-learning framework, allowing this oerator to reason about the effect of an individual observation on the osterior, and thus generalize across different riors, observations and to online Bayesian inference. We demonstrated the generalization ability of our article flow Bayes oerator in several canonical and high dimensional examles. 
"
75,2019,Generalized No Free Lunch Theorem for Adversarial Robustness,Oral,"This manuscrit resents some new imossibility results on adversarial robustness
in machine learning, a very imortant yet largely oen roblem. We show that if conditioned on a class label the data distribution satisfies the $W_2$ Talagrand
transortation-cost inequality (for examle, this condition is satisfied if the conditional distribution has density which is log-concave; is the uniform measure on a comact Riemannian manifold with ositive Ricci curvature, any classifier can be adversarially fooled with high robability once the erturbations are slightly greater than the natural noise level in the roblem. We call this result The Strong ""No Free Lunch"" Theorem as some recent results (Tsiras et al. 2018, Fawzi et al. 2018, etc.) on the subject can be immediately recovered as very articular cases. Our theoretical bounds are demonstrated on both simulated and real data (MNIST). We conclude the manuscrit with some seculation on ossible future research directions."
76,2019,Circuit-GNN: Graph Neural Networks for Distributed Circuit Design,Oral,"We resent Circuit-GNN, a grah neural network (GNN) model for designing distributed circuits. Today, designing distributed circuits is a slow rocess that can take months from an exert engineer.  Our model both automates and seeds u the rocess. The model learns to simulate the electromagnetic (EM) roerties of distributed circuits. Hence, it can be used to relace traditional EM simulators, which tyically take tens of minutes for each design iteration. Further, by leveraging neural networks' differentiability, we can use our model to solve the inverse roblem -- i.e., given desirable EM secifications, we roagate the gradient to otimize the circuit arameters and toology to satisfy the secifications. We exloit the flexibility of GNN to create one model that works for different circuit toologies. We comare our model with a commercial simulator showing that it reduces simulation time by four orders of magnitude.  We also demonstrate the value of our model by using it to design a Terahertz channelizer, a difficult task that requires a secialized exert.  The results show that our model roduces a channelizer whose erformance is as good as a manually otimized design, and can save the exert several weeks of iterative toology exloration and arameter otimization. Most interestingly, our model comes u with new designs that differ from the limited temlates commonly used by engineers in the field, hence significantly exanding the design sace. We exloit the flexibility of GNN to enable our model alicable to circuits with different number of sub-comonents. This allows our neural network to suort a much larger design sace in comarison to revious dee learning circuit design methods. Alying gradient descent on grah structures is non-trivial; we develo a novel multi-loo gradient descent algorithm with local rearameterization to solve this challenge. We comare our model with a commercial simulator showing that it reduces simulation time by five orders of magnitude.  We also demonstrate the value of our model by using it to design a Terahertz channelizer, a difficult task that requires a secialized exert.  The results show that our model roduces a channelizer whose erformance is as good as a manually otimized design, and can save the exert several weeks of iterative toology exloration and arameter otimization.
"
77,2019,Multiplicative Weights Updates as a distributed constrained optimization algorithm: Convergence to second-order stationary points almost always,Oral,"Non-concave maximization has been the subject of much recent study in the otimization and machine learning communities, secifically in dee learning.
Recent aers  ([Ge et al. 2015, Lee et al 2017] and references therein) indicate that first order methods work well and avoid saddles oints.  Results as in [Lee \etal 2017], however, are limited to the \textit{unconstrained} case or for cases where the critical oints are in the interior of the feasibility set, which fail to cature some of the most interesting alications. In this aer we focus on \textit{constrained} non-concave maximization. We analyze a variant of a well-established algorithm in machine learning called Multilicative Weights Udate (MWU) for the maximization roblem $\max_{\mathbf{x} \in D} P(\mathbf{x})$, where $P$ is non-concave, twice continuously differentiable and $D$ is a roduct of simlices. We show that MWU converges almost always for small enough stesizes to critical oints that satisfy the second order KKT conditions.
We combine techniques from dynamical systems as well as taking advantage of a recent connection between Baum Eagon inequality and MWU [Palaioanos et al 2017]."
78,2019,Combating Label Noise in Deep Learning using Abstention,Oral,"We introduce a novel method to combat label noise when training dee neural networks for classification.  We roose a  loss function that ermits  abstention during training thereby allowing the  DNN to abstain on confusing samles  while continuing to learn and imrove classification erformance on the non-abstained samles. We show how such a dee abstaining classifier (DAC) can be used for robust learning in the resence of different tyes of label noise. In the case of structured or systematic label noise -- where noisy training labels or confusing examles are correlated with underlying features  of the data-- training with abstention enables reresentation learning for features that are associated with unreliable labels. In the case of unstructured (arbitrary) label noise, abstention during training enables the  DAC to be used as a very effective data cleaner by identifying samles that are likely to have label noise.
   We rovide analytical results on the loss function behavior that enable dynamic adation of  abstention rates based on learning rogress during training. We demonstrate the utility of the dee abstaining classifier for various image classification tasks under different tyes of label noise; in the case of arbitrary label noise, we show significant imrovements over reviously ublished results on multile image benchmarks.
"
79,2019,On The Power of Curriculum Learning in Training Deep Networks,Oral,"Training neural networks is traditionally done by roviding a sequence of random mini-batches samled uniformly from the entire training data. In this work, we analyze the effects of curriculum learning, which involves the dynamic non-uniform samling of mini-batches, on the training of dee networks, and secifically CNNs trained on image recognition. To emloy curriculum learning, the training algorithm must resolve 2 roblems: (i) sort the training examles by difficulty; (ii) comute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some cometitive ""teacher"" network, and bootstraing. We show that both methods show similar benefits in terms of increased learning seed and imroved final erformance on test data. We address challenge (ii) by investigating different acing functions to guide the samling. The emirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the otimization landscae. We then define the concet of an ideal curriculum, and show that under mild conditions it does not change the corresonding global minimum of the otimization function.
"
80,2019,Open-ended learning in symmetric zero-sum games,Oral,"Zero-sum games such as chess and oker are, abstractly, functions that evaluate airs of agents, for examle labeling them codewinner' andcodeloser'. If the game is aroximately transitive, then self-lay generates sequences of agents of increasing strength. However, nontransitive games, such as rock-aer-scissors, can exhibit strategic cycles, and there is no longer a clear objective -- we want agents to increase in strength, but against whom is unclear. In this aer, we introduce a geometric framework for formulating agent objectives in zero-sum games, in order to construct adative sequences of objectives that yield oen-ended learning. The framework allows us to reason about oulation erformance in nontransitive games, and enables the develoment of a new algorithm (rectified Nash resonse, PSROemrN) that uses game-theoretic niching to construct diverse oulations of effective agents, roducing a stronger set of  agents than existing algorithms. We aly PSROemrN to two highly nontransitive resource allocation games and find that PSRO_rN consistently outerforms the existing alternatives.
"
81,2019,Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation,Oral,"We consider the roblem of imitation learning from a finite set of exert trajectories, without access to reinforcement signals. The classical aroach of extracting the exert's reward function via inverse reinforcement learning, followed by reinforcement learning is indirect and may be comutationally exensive. Recent methods based on generative adversarial networks or generative moment matching formulate the task as distribution matching between the exert olicy and the learned olicy. However, training via distribution matching could be unstable. We roose a new framework for imitation learning based on estimating the suort of the exert olicy to comute a fixed reward function from the exert trajectories. This allows us to re-frame imitation learning within the standard reinforcement learning setting. We demonstrate the efficacy of our reward function on both discrete and continuous domains. The olicies learned using different reinforcement learning methods with the roosed reward function achieve comarable or better erformance than other imitation learning methods.
"
82,2019,Deep Counterfactual Regret Minimization,Oral,"Counterfactual Regret Minimization (CFR) is the leading algorithm for solving large imerfect-information games. It iteratively traverses the game tree in order to converge to a Nash equilibrium. In order to deal with extremely large games, CFR tyically uses domain-secific heuristics to simlify the target game in a rocess known as abstraction. This simlified game is solved with tabular CFR, and its solution is maed back to the full game. This aer introduces DeeRegret, a form of CFR that obviates the need for abstraction by instead using dee neural networks to aroximate the behavior of CFR in the full game. We show that DeeRegret is rinciled and achieves strong erformance in large oker games. This is the first non-tabular variant of CFR to be successful in large games.
"
83,2019,Katalyst: Boosting Convex Katayusha for  Non-Convex Problems with a  Large Condition Number,Oral,"An imortant class of non-convex objectives that has wide alications in machine learning consists of  a sum of $n$ smooth functions and a non-smooth convex function. Tremendous studies have been devoted to conquering these roblems by leveraging one of the two tyes of variance reduction techniques, i.e., SVRG-tye that comutes a full gradient occasionally and SAGA-tye that maintains $n$ stochastic gradients at every iteration.  In ractice, SVRG-tye is referred to SAGA-tye due to its otentially less memory costs. 
An interesting question that has been largely ignored is how to imrove the comlexity of variance reduction methods for roblems with a large condition number that measures the degree to which the objective is close to a convex function.  In this aer, we resent a simle but non-trivial boosting of a  state-of-the-art SVRG-tye method for convex roblems (namely Katyusha) to enjoy an imroved comlexity for solving non-convex roblems with a large condition number (that is close to a convex function). To the best of our knowledge, its comlexity has the best deendence on $n$ and the degree of non-convexity, and also matches that of a recent SAGA-tye accelerated stochastic algorithm for a constrained non-convex smooth otimization roblem.  
"
84,2019,Maximum Likelihood Estimation for  Learning Populations of Parameters,Oral,"Consider a setting with $N$ indeendent individuals, each with an unknown arameter, $_i \in [0, 1]$ drawn from some unknown distribution $P^\star$. After observing the outcomes of $t$ indeendent Bernoulli trials, i.e., $X_i \sim \text{Binomial}(t, _i)$ er individual, our objective is to accurately estimate $P^\star$ in the sarse regime, namely when $t \ll N$. This roblem arises in numerous domains, including the social sciences, sychology, health-care, and biology, where the size of the oulation under study is usually large yet the number of observations er individual is often limited. 

Our main result shows that, in this sarse regime where $t \ll N$, the maximum likelihood estimator (MLE) is both statistically minimax otimal and efficiently comutable. Precisely, for sufficiently large $N$, the MLE achieves the information theoretic otimal error bound of $\mathcal{O}(\frac{1}{t})$ for $t  c\log{N}$, with regards to the earth mover's distance (between the estimated and true distributions). More generally, in an exonentially large interval of $t$ beyond $c \log{N}$, the MLE achieves the minimax error bound of $\mathcal{O}(\frac{1}{\sqrt{t\log N}})$. In contrast, regardless of how large $N$ is, the naive ""lug-in"" estimator for this roblem only achieves the sub-otimal error of $\Theta(\frac{1}{\sqrt{t}})$. Emirically, we also demonstrate the MLE erforms well on both synthetic as well as real datasets."
85,2019,Correlated Variational Auto-Encoders,Oral,"Variational Auto-Encoders (VAEs) are caable of learning latent reresentations for high dimensional data. However, due to the i.i.d. assumtion, VAEs only otimize the singleton variational distributions and fail to account for the correlations between data oints, which might be crucial for learning latent reresentations from dataset where a riori we know correlations exist. We roose Correlated Variational Auto-Encoders (CVAEs) that can take the correlation structure into consideration when learning latent reresentations with VAEs. CVAEs aly a rior based on the correlation structure. To address the intractability introduced by the correlated rior, we develo an aroximation by average of a set of tractable lower bounds over all maximal acyclic subgrahs of the undirected correlation grah. Exerimental results on matching and link rediction on ublic benchmark rating datasets and sectral clustering on a synthetic dataset show the effectiveness of the roosed method over baseline algorithms.
"
86,2019,Voronoi Boundary Classification: A High-Dimensional Geometric Approach via Weighted Monte Carlo Integration,Oral,"Voronoi cell decomositions rovide a classical avenue to classification. Tyical aroaches however only utilize oint-wise cell-membershi information since the comutation of a Voronoi diagram is rohibitively exensive in high dimensions. We roose a Monte-Carlo integration based aroach that instead comutes a weighted integral over the boundaries of Voronoi cells, thus incororating additional information about the Voronoi cell structure. We demonstrate the scalability of our aroach in u to 3072 dimensional saces and analyze the convergence based on the number of Monte Carlo samles and choice of weight functions. Exeriments comaring our aroach to nearest neighbors, SVM and Random Forests indicate that while our aroach erforms similarly to random forests for large data sizes, the algorithm exhibits non-trivial data-deendent erformance characteristics for smaller datasets and can be analyzed in terms of a geometric confidence measure, thus adding to the reertoire of geometric aroaches to classification while having the benefit of not requiring any model changes or retraining as new training samles or classes are added.
"
87,2019,PROVEN: Verifying Robustness of Neural Networks with a Probabilistic Approach,Oral,"With the revalence of dee neural networks, quantifying their robustness to adversarial inuts has become an imortant area of research. However, most of the current research literature merely focuses on the \textit{worst-case} setting that
comutes certified lower bounds of minimum adversarial distortion when the inut erturbations  are constrained within an $\ell_$ ball, thus lacking robustness assessment beyond the certified range.
In this aer, we rovide a first look at a \textit{robabilistically} certifiable setting where the erturbation can follow a given distributional characterization. 
We roose a novel framework \roven to \textbf{PRO}babilistically \textbf{VE}rify \textbf{N}eural network's robusntess with statistical guarantees -- i.e., \roven certifies the robability that the classifier's to-1 rediction cannot be altered under any constrained $\ell_$ norm erturbation to a given inut. Notably, \roven is derived from closed-form analysis of current state-of-the-art  worst-case neural network robustness verification frameworks, and 
therefore it can rovide robabilistic certificates with little comutational overhead on to of existing  methods such as Fast-Lin, CROWN and CNN-Cert.
Exeriments on small and large MNIST and CIFAR neural network models demonstrate our robabilistic aroach can tighten u to around $1.8 \times$ and $3.5 \times$ in the robustness certification with at least a $99.99\%$ confidence comared with the worst-case robustness certificate delivered by CROWN and CNN-Cert. "
88,2019,Revisiting the Softmax Bellman Operator: New Benefits and New Perspective,Oral,"The imact of softmax on the value function itself in reinforcement learning (RL) is often viewed as roblematic because it leads to sub-otimal value (or Q) functions and interferes with the contraction roerties of the Bellman oerator. Surrisingly, desite these concerns, and {\em indeendent of its effect on exloration}, the softmax Bellman oerator when combined with Dee Q-learning, leads to Q-functions with suerior olicies in ractice, even outerforming its double Q-learning counterart. To better understand how and why this occurs, we revisit theoretical roerties of the softmax Bellman oerator, and rove that $(i)$ it converges to the standard Bellman oerator exonentially fast in the inverse temerature arameter, and $(ii)$ the distance of its Q function from the otimal one can be bounded. These alone do not exlain its suerior erformance, so we also show that the softmax oerator can reduce the overestimation error, which may give some insight into why a sub-otimal oerator leads to better erformance in the resence of value function aroximation. A comarison among different Bellman oerators is then resented, showing the trade-offs when selecting them."
89,2019,LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning,Oral,"In this aer, we roose a novel meta learning aroach, namely LGM-Net, for few-shot classification.
The aroach learns transferable rior knowledge across tasks and directly roduces network arameters for similar unseen tasks with training samles. LGM-Net includes two key modules: TargetNet and MetaNet. The TargetNet module is a neural network for solving a secific task. The MetaNet module aims at learning to generate functional weights for TargetNet by observing training samles. A new intertask normalization strategy which makes use of common information shared across tasks is utilized during training.
Exerimental results demonstrate that LGM-Net adats well to similar unseen tasks and achieves state-of-the-art erformance on Omniglot and \textit{mini}ImageNet datasets. 
And exeriments on synthetic datasets are given to show that the transferable rior knowledge is learned by the MetaNet which can hel to solve unseen tasks through maing training data to functional weights. The roosed aroach achieves the goal of fast learning and adatation since no further tuning stes are required in comarison with other exisiting meta learning aroaches.
"
90,2019,Learning to Optimize Multigrid PDE Solvers,Oral,"Constructing fast numerical solvers for artial differential equations (PDEs) is crucial for many scientific discilines.  A leading technique for solving large-scale PDEs is using multigrid methods. At the core of a multigrid solver is the rolongation matrix, which relates between different scales of the roblem. This matrix is strongly roblem-deendent, and its otimal construction is critical to the efficiency of the solver. In ractice, however, devising multigrid algorithms for new roblems often oses formidable challenges. In this aer we roose a framework for learning multigrid solvers. Our method learns a (single) maing from discretized PDEs to rolongation oerators for a broad class of 2D diffusion roblems. We train a neural network once for the entire class of PDEs, using an efficient and unsuervised loss function. Our tests demonstrate imroved convergence rates comared to the widely used Black-Box multigrid scheme, suggesting that our method successfully learned rules for constructing rolongation matrices.
"
91,2019,Generalized Approximate Survey Propagation for High-Dimensional Estimation,Oral,"In Generalized Linear Estimation (GLE) roblems, we seek to estimate a signal that is observed through a linear transform followed by a comonent-wise, ossibly nonlinear and noisy, channel. In the Bayesian otimal setting, Generalized Aroximate Message Passing (GAMP) is known to achieve otimal erformance for GLE. However, its erformance can significantly deteriorate whenever there is a mismatch between the assumed and the true generative model, a situation frequently encountered in ractice. In this aer, we roose a new algorithm, named Generalized Aroximate Survey Proagation (GASP), for solving GLE in the resence of rior or model missecifications. As a rototyical examle, we consider the hase retrieval roblem, where we show that GASP outerforms the corresonding GAMP, reducing the reconstruction threshold and, for certain choices of its arameters, aroaching Bayesian otimal erformance. Furthermore, we resent a set of state evolution equations that can recisely characterize the erformance of GASP in the high-dimensional limit.
"
92,2019,An Investigation of Model-Free Planning,Oral,"The field of reinforcement learning (RL) is facing increasingly challenging domains with combinatorial comlexity. For an RL agent to address these challenges, it is essential that it can lan effectively. Prior work has tyically utilized an exlicit model of the environment, combined with a secific lanning algorithm (such as tree search). More recently, a new family of methods have been roosed that learn how to lan, by roviding the structure for lanning via an inductive bias in the function aroximator (such as a tree structured neural network), trained end-to-end by a model-free RL algorithm. In this aer, we go even further, and demonstrate emirically that an entirely model-free aroach, without secial structure beyond standard neural network comonents such as convolutional networks and LSTMs, can learn to exhibit many of the characteristics tyically associated with a model-based lanner. We measure our agent's effectiveness at lanning in terms of its ability to generalize across a combinatorial and irreversible state sace, its data efficiency, and its ability to utilize additional thinking time. We find that our agent has many of the characteristics that one might exect to find in a lanning algorithm. Furthermore, it exceeds the state-of-the-art in challenging combinatorial domains such as Sokoban and outerforms other model-free aroaches that utilize strong inductive biases toward lanning.
"
93,2019,Projection onto Minkowski Sums with Application to Constrained Learning,Oral,"We introduce block descent algorithms for rojecting onto Minkowski sums of sets. Projection onto such sets is a crucial ste in many statistical learning roblems, and may regularize comlexity of solutions to an otimization roblem or arise in dual formulations of enalty methods. We show that rojecting onto the Minkowski sum admits simle, efficient algorithms when comlications such as overlaing constraints ose challenges to existing methods. We rove that our algorithm converges linearly when sets are strongly convex or satisfy an error bound condition, and extend the theory and methods to encomass non-convex sets as well. We demonstrate emirical advantages in runtime and accuracy over cometitors in alications to $\ell_{1,}$-regularized learning, constrained lasso, and overlaing grou lasso."
94,2019,Self-Attention Generative Adversarial Networks,Oral,"In this aer, we roose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range deendency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only satially local oints in lower-resolution feature mas. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant ortions of the image are consistent with each other.
Furthermore, recent work has shown that generator conditioning affects GAN erformance. Leveraging this insight, we aly sectral normalization to the GAN generator and find that this imroves training dynamics. The roosed SAGAN erforms better than rior work, boosting the best ublished Incetion score from 36.8 to 52.52 and reducing Fr\'echet Incetion distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that corresond to object shaes rather than local regions of fixed shae.
"
95,2019,A Block Coordinate Descent Proximal Method for Simultaneous Filtering and Parameter Estimation,Oral,"We roose and analyze a block coordinate descent roximal algorithm (BCD-rox) for simultaneous filtering and arameter estimation of ODE models.  As we show on ODE systems with u to d=40 dimensions, as comared to state-of-the-art methods, BCD-rox exhibits increased robustness (to noise, arameter initialization, and hyerarameters), decreased training times, and imroved accuracy of both filtered states and estimated arameters.  We show how BCD-rox can be used with multiste numerical discretizations, and we establish convergence of BCD-rox under hyotheses that include real systems of interest.
"
96,2019,On Learning Invariant Representations for Domain Adaptation,Oral,"Due to the ability of dee neural nets to learn rich reresentations, recent advances in unsuervised domain adatation have focused on learning domain-invariant features that achieve a small error on the source domain. The hoe is that the learnt reresentation, together with the hyothesis learnt from the source domain, can generalize to the target domain. In this aer, we first construct a simle counterexamle showing that, contrary to common belief, the above conditions are not sufficient to guarantee successful domain adatation. In articular, the counterexamle (Fig.~\ref{fig:examle}) exhibits \emh{conditional shift}: the class-conditional distributions of inut features change between source and target domains. To give a sufficient condition for domain adatation, we roose a natural and interretable generalization uer bound that exlicitly takes into account the aforementioned shift. Moreover, we shed new light on the roblem by roving an information-theoretic lower bound on the joint error of \emh{any} domain adatation method that attemts to learn invariant reresentations. Our result characterizes a fundamental tradeoff between learning invariant reresentations and achieving small joint error on both domains when the marginal label distributions differ from source to target. Finally, we conduct exeriments on real-world datasets that corroborate our theoretical findings. We believe these insights are helful in guiding the future design of domain adatation and reresentation learning algorithms.
"
97,2019,Robust Decision Trees Against Adversarial Examples,Oral,"Although adversarial examles and model robustness have been extensively studied in the context of neural networks, research on this issue in tree-based models and how to make tree-based models robust against adversarial examles is still limited. In this aer, we show that tree-based models are also vulnerable to adversarial examles and develo a novel algorithm to learn robust trees. At its core, our method aims to otimize the erformance under the worst-case erturbation of inut features, which leads to a max-min saddle oint roblem. Incororating this saddle oint objective into the decision tree building rocedure is non-trivial due to the discrete nature of trees—a naive aroach to finding the best slit according to this saddle oint objective will take exonential time. To make our aroach ractical and scalable, we roose efficient tree building algorithms by aroximating the inner minimizer in the saddleoint roblem, and resent efficient imlementations for classical information gain based trees as well as state-of-the-art tree boosting systems such as XGBoost.  Exerimental results on real-world datasets demonstrate that the roosed algorithms can significantly imrove the robustness of tree-based models against adversarial examles.
"
98,2019,Safe Policy Improvement with Baseline Bootstrapping,Oral,"This aer considers Safe Policy Imrovement (SPI) in Batch Reinforcement Learning (Batch RL): from a fixed dataset and without direct access to the true environment, train a olicy that is guaranteed to erform at least as well as the baseline olicy used to collect the data. 
	    Our aroach, called SPI with Baseline Bootstraing (SPIBB), is insired by the knows-what-it-knows aradigm: it bootstras the trained olicy with the baseline when the uncertainty is high. 
	    Our first algorithm, $\Pi_b$-SPIBB, comes with SPI theoretical guarantees. 
	    We also imlement a variant, $\Pi_{\leq b}$-SPIBB, that is even more efficient in ractice. 
	    We aly our algorithms to a motivational stochastic gridworld domain and further demonstrate on randomly generated MDPs the sueriority of SPIBB with resect to existing algorithms, not only in safety but also in mean erformance. 
	    Finally, we imlement a model-free version of SPIBB and show its benefits on a navigation task with dee RL imlementation called SPIBB-DQN, which is, to the best of our knowledge, the first RL algorithm relying on a neural network reresentation able to train efficiently and reliably from batch data, without any interaction with the environment."
99,2019,Towards a Unified Analysis of Random Fourier Features,Oral,"Random Fourier features is a widely used, simle, and effective technique for scaling u kernel methods. The existing theoretical analysis of the aroach, however, remains focused on secific learning tasks and tyically gives essimistic bounds which are at odds with the emirical results. We tackle these roblems and rovide the first unified risk analysis of learning with random Fourier features using the squared error and Lischitz continuous loss functions. In our bounds, the trade-off between the comutational cost and the exected risk convergence rate is roblem secific and exressed in terms of the regularization arameter and the number of effective degrees of freedom. We study both the standard random Fourier features method for which we imrove the existing bounds on the number of features required to guarantee the corresonding minimax risk convergence rate of kernel ridge regression, as well as a data-deendent modification which samles features roortional to ridge leverage scores and further reduces the required number of features. As ridge leverage scores are exensive to comute, we devise a simle aroximation scheme which rovably reduces the comutational cost without loss of statistical efficiency.
"
100,2019,Automatic Classifiers as Scientific Instruments: One Step Further Away from Ground-Truth,Oral,"Automatic machine learning-based detectors of various sychological and social henomena (e.g., emotion, stress, engagement) have great otential to advance basic science. However, when a detector d is trained to aroximate an existing measurement tool (e.g., a questionnaire, observation rotocol), then care must be taken when interreting measurements collected using d since they are one ste further removed from the under- lying construct. We examine how the accuracy of d, as quantified by the correlation q of d’s out- uts with the ground-truth construct U, imacts the estimated correlation between U (e.g., stress) and some other henomenon V (e.g., academic erformance). In articular: (1) We show that if the true correlation between U and V is r, then the exected samle correlation, over all vectors T n whose correlation with U is q, is qr. (2) We derive a formula for the robability that the samle correlation (over n subjects) using d is ositive given that the true correlation is negative (and vice-versa); this robability can be substantial (around 20 − 30%) for values of n and q that have been used in recent affective comuting studies. (3) With the goal to reduce the variance of correlations estimated by an automatic detector, we show that training multile neural networks d(1) , . . . , d(m) using different training architectures and hyerarameters for the same detection task rovides only limited “coverage” of T^n.
"
101,2019,Learning Hawkes Processes Under Synchronization Noise,Oral,"Multivariate Hawkes rocesses (MHP) are widely used in a variety of fields to model the occurrence of discrete events. Prior work on learning MHPs has only focused on inference in the resence of erfect traces without noise. We address the roblem of learning the causal structure of MHPs when observations are subject to an unknown delay. In articular, we introduce the so-called synchronization noise, where the stream of events generated by each dimension is subject to a random and unknown time shift. We characterize the robustness of the classic maximum likelihood estimator to synchronization noise, and we introduce a new aroach for learning the causal structure in the resence of noise. Our exerimental results show that our aroach accurately recovers the causal structure of MHPs for a wide range of noise levels, and significantly outerforms classic estimation methods.
"
102,2019,Multivariate-Information Adversarial Ensemble for Scalable Joint Distribution Matching,Oral,"A broad range of cross-multi-domain generation researches boils down to matching a joint distribution by dee generative models (DGMs). Hitherto methods excel in airwise domains whereas as the number of domains increases, remain struggling to scale themselves to fit a joint distribution. In this aer, we roose a domain-scalable DGM, \emh{i.e.}, MMI-ALI for multi-domain joint distribution matching. As an multi-domain ensemble model of ALIs \cite{dumoulin2016adversarially}, MMI-ALI is adversarially trained with maximizing \emh{Multivariate Mutual Information} (MMI) \emh{w.r.t.} joint variables of each air of domains and their shared feature. The negative MMIs are uer bounded by a series of feasible losses that rovably lead to matching multi-domain joint distributions. MMI-ALI linearly scales as the number of domains increases and may share arameters across domains and thus, strikes a right balance between efficacy and scalability. We evaluate MMI-ALI in diverse challenging multi-domain scenarios and verify the sueriority of our DGM.  
"
103,2019,CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning,Oral,"In oen-ended and changing environments, agents face a wide range of otential tasks that might not come with associated reward functions. Such autonomous learning agents must set their own tasks and build their own curriculum through an intrinsically motivated exloration. Because some tasks might rove easy and some imossible, agents must actively select which task to ractice at any given moment to maximize their overall mastery on the set of learnable tasks. This aer rooses CURIOUS, an algorithm that leverages: 1) an extension of Universal Value Function Aroximators to achieve within a unique olicy, multile tasks, each arameterized by multile goals and 2) an automated curriculum learning mechanism that biases the attention of the agent towards tasks maximizing the absolute learning rogress. Agents focus on achievable tasks first, and focus back on tasks that are being forgotten. Exeriments conducted in a new multi-task multi-goal robotic environment show that our algorithm benefits from these two ideas and demonstrate roerties of robustness to distracting tasks, forgetting and changes in body roerties.
"
104,2019,Blended Conditonal Gradients,Oral,"We resent a blended conditional gradient aroach for minimizing a smooth convex function over a olytoe P, combining the Frank–Wolfe algorithm (also called conditional gradient) with gradient-based stes, different from away stes and airwise stes, but still achieving linear convergence for strongly convex functions, along with good ractical erformance. Our aroach retains all favorable roerties of conditional gradient algorithms, notably avoidance of rojections onto P and maintenance of iterates as sarse convex combinations of a limited number of extreme oints of P. The algorithm is lazy, making use of inexensive inexact solutions of the linear rogramming subroblem that characterizes the conditional gradient aroach. It decreases measures of otimality (rimal and dual gas) raidly, both in the number of iterations and in wall-clock time, outerforming even the lazy conditional gradient algorithms of Braun et al. 2017. We also resent a streamlined version of the algorithm that alies when P is the robability simlex.
"
105,2019,Boosted Density Estimation Remastered,Oral,"There has recently been a steady increase in the number iterative aroaches to density estimation. However, an accomanying burst of formal convergence guarantees has not followed; all results ay the rice of heavy assumtions which are often unrealistic or hard to check. The \emh{Generative Adversarial Network (GAN)} literature --- seemingly orthogonal to the aforementioned ursuit --- has had the side effect of a renewed interest in variational divergence minimisation (notably $f$-GAN). We show that by introducing a \textit{weak learning assumtion} (in the sense of the classical boosting framework) we are able to imort some recent results from the GAN literature to develo an iterative boosted density estimation algorithm, including formal convergence results with rates, that does not suffer the shortcomings other aroaches. We show that the density fit is an exonential family, and as art of our analysis obtain an imroved variational characterization of $f$-GAN."
106,2019,Learning deep kernels for exponential family densities,Oral,"The kernel exonential family is a rich class of distributions, which can be fit efficiently and with statistical guarantees by score matching. Being required to choose a riori a simle kernel such as the Gaussian, however, limits its ractical alicability. We rovide a scheme for learning a kernel arameterized by a dee network, which can find comlex location-deendent local features of the data geometry. This gives a very rich class of density models, caable of fitting comlex structures on moderate-dimensional roblems. Comared to dee density models fit via maximum likelihood, our aroach rovides a comlementary set of strengths and tradeoffs: in emirical studies, the former can yield higher likelihoods, whereas the latter gives better estimates of the gradient of the log density, the score, which describes the distribution's shae.
"
107,2019,Distributional Reinforcement Learning for Efficient Exploration,Oral,"In distributional reinforcement learning (RL), the estimated distribution of value functions model both the arametric and intrinsic uncertainties. We roose a novel and efficient exloration method for dee RL that has two comonents. The first is a decaying schedule to suress the intrinsic uncertainty. The second is an exloration bonus calculated from the uer quantiles of the learned distribution. In Atari 2600 games, our method achieves 483 % average gain across 49 games in cumulative rewards over QR-DQN. We also comared our algorithm with QR-DQN in a challenging 3D driving simulator (CARLA). Results show that our algorithm achieves nearotimal safety rewards twice faster than QRDQN.
"
108,2019,Lexicographic and Depth-Sensitive Margins in Homogeneous and Non-Homogeneous Deep Models,Oral,"With an eye toward understanding comlexity control in dee learning, we study how infinitesimal regularization or gradient descent otimization lead to margin maximizing solutions in both homogeneous and non homogeneous models, extending revious work that focused on infinitesimal regularization only in homogeneous models. To this end we study the limit of loss minimization with a diverging norm constraint (the codeconstrained ath''), relate it to the limit of acodemargin ath'' and characterize the resulting solution.  For non-homogeneous models we show that this solution is biased toward the deeest art of the model, discarding the shallowest arts if they are unnecessary. For homogeneous models, we show convergence to a ``lexicograhic max margin solution'', and rovide conditions under which max margin solutions are also attained as the limit of unconstrained gradient descent.
"
109,2019,Generative Adversarial User Model for Reinforcement Learning Based Recommendation System,Oral,"We roosed a novel model-based reinforcement learning framework for recommendation systems, where we develoed a GAN formulation to model user behavior dynamics and her associated reward function. Using this user model as the simulation environment, we develo a novel cascading Q-network for combinatorial recommendation olicy which can handle a large number of candidate items efficiently. Although the exeriments show clear benefits of our method in an offline and realistic simulation setting, even stronger results could be obtained via future online AB testing.
"
110,2019,Adversarial Generation of Time-Frequency Features with application in audio synthesis,Oral,"Time-frequency (TF) reresentations rovide owerful and intuitive features for the analysis of time series such as audio. But still, generative modeling of audio in the TF domain is a subtle matter. Consequently, neural audio synthesis widely relies on directly modeling the waveform and revious attemts at unconditionally synthesizing audio from neurally generated TF features still struggle to roduce audio at satisfying quality. In this contribution, focusing on the short-time Fourier transform, we discuss the challenges that arise in audio synthesis based on generated TF features and how to overcome them. We demonstrate the otential of deliberate generative TF modeling by training a generative adversarial network (GAN) on short-time Fourier features. We show that our TF-based network was able to outerform the state-of-the-art GAN generating waveform, desite the similar architecture in the two networks. 
"
111,2019,Task-Agnostic Dynamics Priors for Deep Reinforcement Learning,Oral,"While model-based dee reinforcement learning(RL) holds great romise for samle efficiency and generalization, learning an accurate dynamics model is often challenging and requires substantial interaction with the environment. A wide variety of domains have dynamics that share common foundations like the laws of hysics, which are rarely exloited by existing algorithms. In fact, humans continuously acquire and use such dynamics riors to easily adat to oerating in new environments. In this work, we roose an aroach to learn task-agnostic dynamics riors from videos and incororate them into an RL agent. Our method involves re-training a frame redictor on generic task-agnostic hysics videos to initialize dynamics models (and fine-tune them)for unseen target environments. Our frame rediction architecture, SatialNet, is designed secifically to cature localized hysical henomena and interactions. Our aroach allows for both faster olicy learning and convergence to better olicies, outerforming cometitive aroaches on several different domains. We also demonstrate that incororating this rior allows for more effective transfer learning between environments.
"
112,2019,High-Fidelity Image Generation With Fewer Labels,Oral,"Dee generative models are becoming a cornerstone of modern machine learning. Recent work on conditional generative adversarial networks has shown that learning comlex, high-dimensional distributions over natural images is within reach. While the latest models are able to generate high-fidelity, diverse natural images at high resolution, they rely on a vast quantity of labeled data. In this work we demonstrate how one can benefit from recent work on self- and semi-suervised learning to outerform state-of-the-art on both unsuervised ImageNet synthesis, as well as in the conditional setting. In articular, the roosed aroach is able to match the samle quality (as measured by FID) of the current state-of-the art conditional model BigGAN on ImageNet using only 10% of the labels and outerform it using 20% of the labels.
"
113,2019,Optimistic Policy Optimization via Multiple Importance Sampling,Oral,"Policy Search (PS) is an effective aroach to Reinforcement Learning for solving
control tasks with continuous state-action saces. In this aer, we address the exloration-exloitation trade-off in PS by roosing an aroach based on Otimism in Face of Uncertainty. We cast the PS roblem as a suitable Multi Armed Bandit roblem, defined over the olicy arameter sace, and we roose a class of algorithms that effectively exloit the roblem structure, by leveraging Multile Imortance Samling to erform an off-olicy estimation of exected return.
We show that the regret of the roosed aroach is bounded by $\widetilde{\mathcal{O}}(\sqrt{T})$ for both discrete and continuous arameter saces. Finally, we evaluate our algorithms on tasks of varying difficulty, comaring them with existing MAB and RL algorithms."
114,2019,Inference and Sampling of $K_{33}$-free Ising Models,Oral,"We call an Ising model tractable when it is ossible to comute its artition function value (statistical inference) in olynomial time. The tractability also imlies an ability to samle configurations of this model in olynomial time. The notion of tractability extends the basic case of lanar zero-field Ising models. Our starting oint is to describe algorithms for the basic case comuting artition function and samling efficiently.
Then, we extend our tractable inference and samling algorithms to models, whose triconnected comonents are either lanar or grahs of $O(1)$ size. In articular, it results in a olynomial-time inference and samling algorithms for $K_{33}$ (minor) free toologies of zero-field Ising models - a generalization of lanar grahs with a otentially unbounded genus."
115,2019,Bayesian Deconditional Kernel Mean Embeddings,Oral,"Conditional kernel mean embeddings form an attractive nonarametric framework for reresenting conditional means of functions, describing the observation rocesses for many comlex models. However, the recovery of the original underlying function of interest whose conditional mean was observed is a challenging inference task. We formalize deconditional kernel mean embeddings as a solution to this inverse roblem, and show that it can be naturally viewed and used as a nonarametric Bayes' rule. Critically, we introduce the notion of task transformed Gaussian rocesses and establish deconditional kernel means embeddings as their osterior redictive mean. This connection rovides Bayesian interretations and uncertainty estimates for deconditional kernel means, exlains its regularization hyerarameters, and rovides a marginal likelihood for kernel hyerarameter learning. They further enable ractical alications such as learning sarse reresentations for big data and likelihood-free inference.
"
116,2019,"Look Ma, No Latent Variables: Accurate Cutset Networks via Compilation",Oral,"Tractable robabilistic models obviate the need for unreliable aroximate inference aroaches and as a result often yield accurate query answers in ractice. However, most tractable models that achieve state-of-the-art generalization erformance (measured using test set likelihood score) use latent variables. Such models admit oly-time marginal (MAR) inference but do not admit oly-time (full) maximum-a-osteriori (MAP) inference. To address this roblem, in this aer, we roose a novel aroach for inducing cutset networks, a well-known tractable reresentation that does not use latent variables and therefore admits linear time exact MAR and MAP inference. Our aroach addresses a major limitation of existing techniques that learn cutset networks from data in that their accuracy is quite low as comared to latent models such as sum-roduct networks and bags of cutset networks. The key idea in our aroach is to construct dee cutset networks by not only learning them from data but also comiling them from a more accurate latent tractable model. We show exerimentally that our new aroach yields more accurate MAP estimates as comared with existing aroaches. Moreover, our new aroach significantly imroves the test set log-likelihood score of cutset networks bringing them closer in terms of generalization erformance to latent models.
"
117,2019,Acceleration of SVRG and Katyusha X by Inexact Preconditioning,Oral,"Emirical risk minimization is an imortant class of otimization roblems with many oular machine learning alications, and stochastic variance reduction methods are oular choices for solving them. Among these methods, SVRG and Katyusha X (a Nesterov accelerated SVRG) achieve fast convergence without substantial memory requirement. In this aer, we roose to accelerate these two algorithms by \textit{inexact reconditioning}, the roosed methods emloy \textit{fixed} reconditioners, although the subroblem in each eoch becomes harder, it suffices to aly \textit{fixed} number of simle subroutines to solve it inexactly, without losing the overall convergence. As a result, this inexact reconditioning strategy gives rovably better iteration comlexity and gradient comlexity over SVRG and Katyusha X. We also allow each function in the finite sum to be nonconvex while the sum is strongly convex. In our numerical exeriments, we observe an on average $8\times$ seedu on the number of iterations and $7\times$ seedu on runtime."
118,2019,A Kernel Perspective for Regularizing Deep Neural Networks,Oral,"We roose a new oint of view for regularizing dee neural networks by using the norm of a reroducing kernel Hilbert sace (RKHS). Even though this norm cannot be comuted, it admits uer and lower aroximations leading to various ractical strategies. Secifically, this ersective (i) rovides a common umbrella for many existing regularization rinciles, including sectral norm and gradient enalties, or adversarial training, (ii) leads to new effective regularization enalties, and (iii) suggests hybrid strategies combining lower and uer bounds to get better aroximations of the RKHS norm. We exerimentally show this aroach to be effective when learning on small datasets, or to obtain adversarially robust models.
"
119,2019,A Statistical Investigation of Long Memory in Language and Music,Oral,"Reresentation and learning of long-range deendencies is a central challenge confronted in modern alications of machine learning to sequence data. Yet desite the rominence of this issue, the basic roblem of measuring long-range deendence, either in a given data source or as reresented in a trained dee model, remains largely limited to heuristic tools. We contribute a statistical framework for investigating long-range deendence in current alications of dee sequence modeling, drawing on the well-develoed theory of long memory stochastic rocesses. This framework yields testable imlications concerning the relationshi between long memory in real-world data and its learned reresentation in a dee learning architecture, which are exlored through a semiarametric framework adated to the high-dimensional setting. 
"
120,2019,Diagnosing Bottlenecks in Deep Q-learning Algorithms,Oral,"Q-learning methods reresent a commonly used class of algorithms in reinforcement learning: they are generally efficient and simle, and can be combined readily with function aroximators for dee reinforcement learning. However, the behavior of Q-learning methods with function aroximation is oorly understood, both theoretically and emirically. In this work, we aim to exerimentally investigate otential issues in Q-learning, by means of a ""unit testing"" framework where we can utilize oracles to disentangle sources of error. 
Secifically, we investigate questions related to convergence, function aroximation, samling error and nonstationarity, and where available, verify if trends found in oracle settings hold true with modern dee RL methods.
We find that large neural network architectures have many benefits with regards to learning stability; offer several ractical comensations for overfitting; and develo a novel samling method based on exlicitly comensating for function aroximation error that yields significant imrovement on high-dimensional continuous control domains. 
"
121,2019,Optimal Transport for structured data with application on graphs,Oral,"This work considers the roblem of comuting distances between structured
objects such as undirected grahs, seen as robability distributions in a
secific metric sace. We consider a new transortation distance (
i.e. that minimizes a total cost of transorting robability masses) that unveils
the geometric nature of the structured objects sace. Unlike Wasserstein or
Gromov-Wasserstein metrics that focus solely and resectively on features (by
considering a metric in the feature sace) or structure (by seeing structure as
a metric sace), our new distance exloits jointly both information, and is
consequently called Fused Gromov-Wasserstein  (FGW). After discussing its
roerties and comutational asects, we show results on a grah classification
task, where our method outerforms both grah kernels and
dee grah convolutional networks.  Exloiting further on the metric roerties
of FGW, interesting geometric objects such as Fr{\'e}chet means or barycenters
of grahs are illustrated and discussed in a clustering context.
"
122,2019,On the Universality of Invariant Networks,Oral,"Constraining linear layers in neural networks to resect symmetry transformations from a grou $G$ is a common design rincile for invariant networks that has found many alications in machine learning. 		
In this aer, we consider a fundamental question that has received very little attention to date: Can these networks aroximate any (continuous) invariant function? 		
We tackle the rather general case where $G\leq S_n$ (an arbitrary subgrou of the symmetric grou) that acts on $\R^n$ by ermuting coordinates. This setting includes several recent oular invariant networks. We resent two main results: First, $G$-invariant networks are universal if high-order tensors are allowed. Second, there are grous $G$ for which higher-order tensors are unavoidable for obtaining universality. 		
$G$-invariant networks consisting of only first-order tensors are of secial interest due to their ractical value. We conclude the aer by roving a necessary condition for the universality of $G$-invariant networks that incororate only first-order tensors. Lastly, we roose a conjecture stating that this condition is also sufficient. "
123,2019,Random Matrix Improved Covariance Estimation for a Large Class of Metrics,Oral,"Relying on recent advances in statistical estimation of covariance distances based on random matrix theory, this article rooses an imroved covariance and recision matrix estimation for a wide family of metrics. 
The method is shown to largely outerform the samle covariance matrix estimate and to comete with state-of-the-art methods, while at the same time being comutationally simler. Alications to linear and quadratic discriminant analyses also demonstrate significant gains, therefore suggesting ractical interest to statistical machine learning.
"
124,2019,Revisiting precision recall definition for generative modeling,Oral,"In this article we revisit the definition of Precision-Recall (PR) curves for generative models roosed by (Sajjadi et al., 2018). Rather than roviding a scalar for generative quality, PR curves distinguish mode-collase (oor recall) and bad quality (oor recision). We first generalize their formulation to arbitrary measures hence removing any restriction to finite suort. We also exose a bridge between PR curves and tye I and tye II error (a.k.a. false detection and rejection) rates of likelihood ratio classifiers on the task of discriminating between samles of the two distributions. Building uon this new ersective, we roose a novel algorithm to aroximate recision-recall curves, that shares some interesting methodological roerties with the hyothesis testing technique from (Loez-Paz  &am;  Oquab,  2017). We demonstrate the interest of the roosed formulation over the original aroach on controlled multi-modal datasets.
"
125,2019,Neural Logic Reinforcement Learning,Oral,"Dee reinforcement learning (DRL) has achieved significant breakthroughs in various tasks. However, most DRL algorithms suffer a roblem of generalising the learned olicy which makes the learning erformance largely affected even by minor modifications of the training environment. Excet that, the use of dee neural networks makes the learned olicies hard to be interretable. To tackle these two challenges, we roose a novel algorithm named Neural Logic Reinforcement Learning (NLRL) to reresent the olicies in the reinforcement learning by first order logic. NLRL is based on olicy gradient methods and differentiable inductive logic rogramming that have demonstrated significant advantages in terms of interretability and generalisability in suervised tasks. Extensive exeriments conducted on cliff-walking and blocks maniulation tasks demonstrate that NLRL can induce interretable olicies achieving near-otimal erformance, while demonstrating good generalisability to environments of different initial states and roblem sizes.
"
126,2019,Characterization of Convex Objective Functions and Optimal Expected Convergence Rates for SGD,Oral,"We study Stochastic Gradient Descent (SGD) with diminishing ste sizes for convex objective functions. We introduce a definitional framework and theory that defines and characterizes a core roerty, called curvature, of convex objective functions. In terms of curvature we can derive a new inequality that can be used to comute an otimal sequence of diminishing ste sizes by solving a differential equation. Our exact solutions confirm known results in literature and allows us to fully characterize a new regularizer with its corresonding exected convergence rates.
"
127,2019,Deep Factors for Forecasting,Oral,"Producing robabilistic forecasts for large collections of similar andor deendent time series is a ractically highly relevant, yet challenging task. Classical time series models fail to cature comlex atterns in the data and multivariate techniques struggle to scale to large roblem sizes, but their reliance on strong structural assumtions makes them data-efficient and allows them to rovide estimates of uncertainty. The converse is true for models based on dee neural networks, which can learn comlex atterns and deendencies given enough data. In this aer, we roose a hybrid model that incororates the benefits of both aroaches. Our new method is data-driven and scalable via a latent, global, dee comonent.  It also handles uncertainty through a local classical model. We rovide both theoretical and emirical evidence for the soundness of our aroach through a necessary and sufficient decomosition of exchangeable time series into a global and a local art and extensive exeriments.  Our exeriments demonstrate the advantages of our model both in term of data efficiency and comutational comlexity.
"
128,2019,Dual Entangled Polynomial Code: Three-Dimensional Coding for Distributed Matrix Multiplication,Oral,"Matrix multilication is a fundamental building block in various machine learning algorithms. When the matrix comes from a large dataset, the multilication will be slit into multile tasks which calculate the multilication of submatrices on different nodes. As some nodes may be stragglers, coding schemes have been roosed to tolerate stragglers in such distributed matrix multilication. However, existing coding schemes tyically slit the matrices in only one or two dimensions, limiting their caabilities to handle large-scale matrix multilication. Three-dimensional coding, however, does not have any code construction that achieves the otimal number of tasks required for decoding. The best result is twice the otimal number, achieved by entangled olynomial (EP) codes. In this aer, we roose dual entangled olynomial (DEP) codes that significantly imrove this bound from $2$x to $1.5$x. With exeriments in a real cloud environment, we show that DEP codes can also save the decoding overhead and memory consumtion of tasks."
129,2019,Wasserstein of Wasserstein Loss for Learning Generative Models,Oral,"The Wasserstein distance serves as a loss function for unsuervised learning which deends on the choice of a ground metric on samle sace. We roose to use the Wasserstein distance itself as the ground metric on the samle sace of images. This ground metric is known as an effective distance for image retrieval, that correlates with human ercetion. We derive the Wasserstein ground metric on ixel sace and define a Riemannian Wasserstein gradient enalty to be used in the Wasserstein Generative Adversarial Network (WGAN) framework. The new gradient enalty is comuted efficiently via convolutions on the $L^2$ gradients with negligible additional comutational cost. The new formulation is more robust to the natural variability of the data and rovides for a more continuous discriminator in samle sace.
"
130,2019,Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,Oral,"Recent works have cast some light on the mystery of why dee nets fit any data and generalize desite being very overarametrized. This aer analyzes training and generalization for a simle 2-layer ReLU net with random initialization, and rovides the following imrovements over recent works:

(i) Using a tighter characterization of training seed than recent aers, an exlanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR'17].

(ii) Generalization bound indeendent of network size, using a data-deendent comlexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by exeriments. Moreover, recent aers require samle comlexity to increase (slowly) with the size, while our samle comlexity is comletely indeendent of the network size.

(iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent.

The key idea is to track dynamics of training and generalization via roerties of a related kernel.
"
131,2019,Learning to Collaborate in Markov Decision Processes,Oral,"We consider a two-agent MDP framework where agents reeatedly solve a task in a collaborative setting. We study the roblem of designing a learning algorithm for the first agent (A1) that facilitates a successful collaboration even in cases when the second agent (A2) is adating its olicy in an unknown way. The key challenge in our setting is that the resence of the second agent leads to non-stationarity and non-obliviousness of rewards and transitions for the first agent.

We design novel online learning algorithms for agent A1 whose regret decays as $O(T^{1-\frac{3}{7} \cdot \alha})$ with $T$ learning eisodes rovided that the magnitude of agent A2's olicy changes between any two consecutive eisodes are uer bounded by $O(T^{-\alha})$. Here, the arameter $\alha$ is assumed to be strictly greater than $0$, and we show that this assumtion is necessary rovided that the {\em learning arity with noise} roblem is comutationally hard. We show that sub-linear regret of agent A1 further imlies near-otimality of the agents' joint return for MDPs that manifest the roerties of a {\em smooth} game. "
132,2019,Collaborative Evolutionary Reinforcement Learning,Oral,"Dee reinforcement learning algorithms have been successfully alied to a range of challenging control tasks. However, these methods tyically struggle with achieving effective exloration and are extremely sensitive to the choice of hyerarameters. One reason is that most aroaches use a noisy version of their oerating olicy to exlore - thereby limiting the range of exloration. In this aer, we introduce Collaborative Evolutionary Reinforcement Learning (CERL), a scalable framework that comrises a ortfolio of olicies that simultaneously exlore and exloit diverse regions of the solution sace. A collection of learners - tyically roven algorithms like TD3 - otimize over varying time-horizons leading to this diverse ortfolio. All learners contribute to and use a shared relay buffer to achieve greater samle efficiency. Comutational resources are dynamically distributed to favor the best learners as a form of online algorithm selection. Neuroevolution binds this entire rocess to generate a single emergent learner that exceeds the caabilities of any individual learner. Exeriments in a range of continuous control benchmarks demonstrate that the emergent learner significantly outerforms its comosite learners while remaining overall more samle-efficient - notably solving the Mujoco Humanoid benchmark where all of its comosite learners (TD3) fail entirely in isolation.
"
133,2019,A Conditional-Gradient-Based Augmented Lagrangian Framework,Oral,"This aer considers a generic convex minimization temlate with affine constraints over a comact domain, which covers key semidefinite rogramming alications. The existing conditional gradient methods either do not aly to our temlate or are too slow in ractice. To this end, we roose a new conditional gradient method, based on a unified treatment of smoothing and augmented Lagrangian frameworks. The roosed method maintains favorable roerties of the classical conditional gradient method, such as chea linear minimization oracle calls and sarse reresentation of the decision variable.  We rove O(1\sqrt{k}) convergence rate of our method in the objective residual and the feasibility ga. This rate is essentially the same as the state of the art CG-tye methods for our roblem temlate, but the roosed method is arguably suerior in ractice comared to existing methods in various alications.
"
134,2019,A Persistent Weisfeiler--Lehman Procedure for Graph Classification,Oral,"Insired by the Weisfeiler--Lehman grah kernel, we augment its
  iterative feature ma construction aroach by a set of multi-scale
  toological features. More recisely, we leverage roagated node
  label information to transform an unweighted grah into a metric one.
  We then use ersistent homology, a technique from toological data
  analysis, to assess the toological roerties, i.e. connected
  comonents and cycles, of the metric grah. Through this rocess, each
  grah can be reresented similarly to the original Weisfeiler--Lehman
  sub-tree feature ma.

We demonstrate the utility and imroved accuracy of our method on
  numerous grah data sets while also discussing theoretical asects of
  our aroach.
"
135,2019,Learning Optimal Linear Regularizers,Oral,"We resent algorithms for efficiently learning regularizers that imrove generalization.  Our aroach is based on the insight that regularizers can be viewed as uer bounds on the generalization ga, and that reducing the slack in the bound can imrove erformance on test data.  For a broad class of regularizers, the hyerarameters that give the best uer bound can be comuted using linear rogramming.  Under certain Bayesian assumtions, solving the LP lets us ""jum"" to
the otimal hyerarameters given very limited data.  This suggests a natural algorithm for tuning regularization hyerarameters, which we show to be effective on both real and synthetic data.
"
136,2019,Predictor-Corrector Policy Optimization,Oral,"We resent a redictor-corrector framework, called PicCoLO, that can transform a first-order model-free reinforcement or imitation learning algorithm into a new hybrid method that leverages redictive models to accelerate olicy learning. The new ``PicCoLOed'' algorithm otimizes a olicy by recursively reeating two stes: In the Prediction Ste, the learner uses a model to redict the unseen future gradient and then alies the redicted estimate to udate the olicy; in the Correction Ste, the learner runs the udated olicy in the environment, receives the true gradient, and then corrects the olicy using the gradient error. Unlike revious algorithms, PicCoLO corrects for the mistakes of using imerfect redicted gradients and hence does not suffer from model bias. The develoment of PicCoLO is made ossible by a novel reduction from redictable online learning to adversarial online learning,  which rovides a systematic way to modify existing first-order algorithms to achieve the otimal regret with resect to redictable information. We show, in both theory and simulation, that the convergence rate of several first-order model-free algorithms can be imroved by PicCoLO. 
"
137,2019,Rehashing Kernel Evaluation in High Dimensions,Oral,"Kernel methods are effective but do not scale well to large scale data: a larger training set imroves accuracy but incurs a quadratic increase in overall evaluation time. This is esecially true in high dimensions where the geometric data structures  used to accelerate kernel evaluation suffer from the curse of dimensionality. Recent theoretical advances have  roosed fast kernel evaluation algorithms  leveraging hashing techniques  with worst-case asymtotic imrovements. However, these advances are largely confined to the theoretical realm due to concerns such as  suer-linear rerocessing time and diminishing gains in non-worst case datasets. In this aer, we close the ga between theory and ractice by addressing these challenges via rovable and ractical rocedures for adative samle size selection, rerocessing time reduction, and new refined data-deendent variance bounds that quantify the erformance of random samling and hashing-based kernel evaluation methods on a given dataset. Our exeriments show that these new tools offer u to 10x imrovement in evaluation time on a range of synthetic and real world datasets.
"
138,2019,Flat Metric Minimization with Applications in Generative Modeling,Oral,"We take the novel ersective to view data not merely as a robability distribution but as a current. Primarily studied in the field of geometric measure theory, k-currents are continuous linear functionals acting on comactly suorted smooth differential forms and can be understood as a generalized notion of oriented k-dimensional manifold. By moving from distributions (which are 0-currents) to k-currents, we can exlicitly orient the data by attaching a k-dimensional tangent lane to each samle oint. Based on the flat metric which is a fundamental distance between currents, we derive FlatGAN, a formulation in the sirit of generative adversarial networks but generalized to k-currents. In our theoretical contribution we rove that the flat metric between a arametrized current and a reference current is continuous in the arameters. In exeriments, we show that the roosed shift to k0 leads to interretable and disentangled latent reresentations which behave equivariantly to the secified oriented tangent lanes.
"
139,2019,Gauge Equivariant Convolutional Networks and the Icosahedral CNN,Oral,"The idea of equivariance to symmetry transformations rovides one of the first theoretically grounded rinciles for neural network architecture design. Equivariant networks have shown excellent erformance and data efficiency on vision and medical imaging roblems that exhibit symmetries. In this aer we show how the theory can be extended from global symmetries to local gauge transformations, which makes it ossible in rincile to develo equivariant networks on general manifolds.

We imlement gauge equivariant CNNs for signals defined on the icosahedron, which rovides a reasonable aroximation of sherical signals. By choosing to work with this very regular manifold, we are able to imlement the gauge equivariant convolution using a single conv2d call, making it a highly scalable and ractical alternative to Sherical CNNs.

We evaluate the effectiveness of Icosahedral CNNs on a number of different roblems, and show that they yield excellent accuracy and comutational efficiency.
"
140,2019,EMI: Exploration with Mutual Information,Oral,"Reinforcement learning algorithms struggle when the reward signal is very sarse. In these cases, naive random exloration methods essentially rely on a random walk to stumble onto a rewarding state. Recent works utilize intrinsic motivation to guide the exloration via generative models, redictive forward models, or discriminative modeling of novelty. We roose EMI, which is an exloration method that constructs embedding reresentation of states and actions that does not rely on generative decoding of the full observation but extracts redictive signals that can be used to guide exloration based on forward rediction in the reresentation sace. Our exeriments show that the roosed method significantly outerforms a number of existing exloration methods on challenging locomotion task with continuous control and on image-based exloration tasks with discrete actions on Atari.
"
141,2019,Weakly-Supervised Temporal Localization via Occurrence Count Learning,Oral,"We roose a novel model for temoral detection and localization which allows the training of dee neural networks using only counts of event occurrences as training labels. This owerful weakly-suervised framework alleviates the burden of the imrecise and time consuming rocess of annotating event locations in temoral data. Unlike existing methods, in which localization is exlicitly achieved by design, our model learns localization imlicitly as a byroduct of learning to count instances. This unique feature is a direct consequence of the model's theoretical roerties. We validate the effectiveness of our aroach in a number of exeriments (drum hit and iano onset detection in audio, digit detection in images) and demonstrate erformance comarable to that of fully-suervised state-of-the-art methods, desite much weaker training requirements.
"
142,2019,On Symmetric Losses for Learning from Corrupted Labels,Oral,"This aer aims to rovide a better understanding of a symmetric loss. 
First, we show that using a symmetric loss is advantageous in the balanced error rate (BER) minimization and area under the receiver oerating characteristic curve (AUC) maximization from corruted labels. 
Second, we rove general theoretical roerties of symmetric losses, including a classification-calibration condition, excess risk bound, conditional risk minimizer, and AUC-consistency condition. 
Third, since all nonnegative symmetric losses are non-convex, we roose a convex barrier hinge loss that benefits significantly from the symmetric condition, although it is not symmetric everywhere. 
Finally, we conduct exeriments on BER and AUC otimization from corruted labels to validate the relevance of the symmetric condition. 
"
143,2019,Neural Joint Source-Channel Coding,Oral,"For reliable transmission across a noisy communication channel, classical results from information theory show that it is asymtotically otimal to searate out the source and channel coding rocesses. However, this decomosition can fall short in the finite bit-length regime, as it requires non-trivial tuning of hand-crafted codes and assumes infinite comutational ower for decoding. In this work, we roose to jointly learn the encoding and decoding rocesses using a new discrete variational autoencoder model. By adding noise into the latent codes to simulate the channel during training, we learn to both comress and error-correct given a fixed bit-length and comutational budget. We obtain codes that are not only cometitive against several searation schemes, but also learn useful robust reresentations of the data for downstream tasks such as classification. Finally, inference amortization yields an extremely fast neural decoder, almost an order of magnitude faster comared to standard decoding methods based on iterative belief roagation. 
"
144,2019,SGD: General Analysis and Improved Rates,Oral,"We roose a  general yet simle theorem describing the convergence of SGD under the arbitrary samling aradigm.  Our theorem describes the convergence of an infinite array of variants of SGD, each of which is associated with a secific robability law governing the data selection rule used to form  minibatches. This is the first time such an analysis is erformed, and most of our variants of SGD were never exlicitly considered in the literature before.  Our analysis relies on the recently introduced notion of exected smoothness and does not rely on a uniform bound on the variance of the stochastic gradients. By secializing our theorem to different mini-batching strategies, such as samling with relacement and indeendent samling, we derive exact exressions for the stesize as a function of the mini-batch size. With this we can also determine the mini-batch size that otimizes the total comlexity, and show exlicitly that as the variance of the stochastic gradient evaluated at the minimum grows, so does the otimal mini-batch size. For zero variance, the otimal mini-batch size is one. Moreover, we rove insightful  stesize-switching rules  which describe when one should switch from a constant to a decreasing stesize regime. 
"
145,2019,Imitation Learning from Imperfect Demonstration,Oral,"Imitation learning (IL) aims to learn an otimal olicy from demonstrations. However, such demonstrations are often imerfect since collecting otimal ones is costly. To effectively learn from imerfect demonstrations, we roose a novel aroach that utilizes confidence scores, which describe the quality of demonstrations. More secifically, we roose two confidence-based IL methods, namely two-ste imortance weighting IL (2IWIL) and generative adversarial IL with imerfect demonstration and confidence (IC-GAIL). We show that confidence scores given only to a small ortion of sub-otimal demonstrations significantly imrove the erformance of IL both theoretically and emirically.
"
146,2019,Large-Scale Sparse Kernel Canonical Correlation Analysis,Oral,"This aer resents gradKCCA, a large-scale sarse non-linear canonical correlation method. Like Kernel Canonical Correlation Analysis (KCCA), our method finds non-linear correlations through kernel functions, but unlike KCCA, our method does not incororate a kernel matrix, a known bottleneck for scaling u kernel methods. gradKCCA corresonds to solving KCCA with the additional constraint that the canonical rojection directions in the kernel-induced feature sace have re-images in the original data sace. Firstly, this modification allows us to very efficiently maximize kernel canonical correlation through an alternating rojected gradient algorithm working in the original data sace. Secondly, we can control the sarsity of the rojection directions by constraining the $\ell_1$ norm of the re-images of the rojection directions, facilitating the interretation of the discovered atterns, which is not available through KCCA. Our emirical exeriments demonstrate that gradKCCA outerforms state-of-the-art CCA methods in terms of seed and robustness to noise both in simulated and real-world datasets."
147,2019,Curvature-Exploiting Acceleration of Elastic Net Computations,Oral,"This aer introduces an efficient second-order method for solving the elastic net roblem. Its key innovation is a comutationally efficient technique for injecting curvature information in the otimization rocess which admits a strong theoretical erformance guarantee. In articular, we show imroved run time over oular first-order methods and quantify the seed-u in terms of statistical measures of the data matrix. The imroved time comlexity is the result of an extensive exloitation of the roblem structure and a careful combination of second-order information, variance reduction techniques, and momentum acceleration. Beside theoretical seed-u, exerimental results demonstrate great ractical erformance benefits of curvature information, esecially for ill-conditioned data sets.
"
148,2019,Switching Linear Dynamics for Variational Bayes Filtering,Oral,"System identification of comlex and nonlinear systems is a central roblem for model redictive control and model-based reinforcement learning. 
Desite their comlexity, such systems can often be aroximated well by a set of linear dynamical systems if broken into aroriate subsequences. 
This mechanism not only hels us find good aroximations of dynamics, but also gives us deeer insight into the underlying system. 
Leveraging Bayesian inference and Variational Autoencoders, we show how to learn a richer and more meaningful state sace, e.g. encoding joint constraints and collisions with walls in a maze, from artial and high-dimensional observations. 
This reresentation translates into a gain of accuracy of the learned dynamics which we showcase on various simulated tasks.
"
149,2019,Feature-Critic Networks for Heterogeneous Domain Generalization,Oral,"Domain shift is the well-known issue that model erformance degrades when deloyed to a new target domain with different statistics to training. Domain adatation techniques alleviate this, but need some instances from the target domain to drive adatation. Domain generalization is the recently toical roblem of learning a model that generalizes to unseen domains out of the box, without accessing any target data. Various domain generalization aroaches aim to train a domain-invariant feature extractor, tyically by adding some manually designed losses. In this work, we roose a “learning to learn” aroach, where the auxiliary loss that hels generalization is itself learned. This aroach is concetually simle and flexible, and leads to considerable imrovement in robustness to domain shift. Beyond conventional domain generalization, we consider a more challenging setting of “heterogeneous” domain generalization, where the unseen domains do not share label sace with the seen ones, and the goal is to train a feature which is useful off-the-shelf for novel data and novel categories. Exerimental evaluation demonstrates that our method outerforms state-of-the-art solutions in both settings.
"
150,2019,Entropic GANs meet VAEs: A Statistical Approach to Compute Sample Likelihoods in GANs,Oral,"Building on the success of dee learning, two modern aroaches to learn a robability model from the data are Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs). VAEs consider an exlicit robability model for the data and comute a generative distribution by maximizing a variational lower-bound on the log-likelihood function. GANs, however, comute a generative model by minimizing a distance between observed and generated robability distributions without considering an exlicit model for the observed data. The lack of having exlicit robability models in GANs rohibits comutation of samle likelihoods in their frameworks and limits their use in statistical inference roblems. In this work, we resolve this issue by constructing an exlicit robability model that can be used to comute samle likelihood statistics in GANs. In articular, we rove that under this robability model, a family of Wasserstein GANs with an entroy regularization can be viewed as a generative model that maximizes a variational lower-bound on average samle log likelihoods, an aroach that VAEs are based on. This result makes a rinciled connection between two modern generative models, namely GANs and VAEs. In addition to the aforementioned theoretical results, we comute likelihood statistics for GANs trained on Gaussian, MNIST, SVHN, CIFAR-10 and LSUN datasets. Our numerical results match consistently with the roosed theory.
"
151,2019,Doubly-Competitive Distribution Estimation,Oral,"Distribution estimation is a statistical-learning cornerstone. Its classical min-max formulation minimizes the estimation error for the worst distribution, hence under-erforms for ractical distributions that, like ower-law, are often rather simle. Modern research has therefore focused on two frameworks: structural estimation that imroves learning accuracy by assuming a simle structure of the underlying distribution; and cometitive, or instance-otimal, estimation that achieves the erformance of a genie aided estimator u to a small excess error that vanishes as the samle size grows, regardless of the distribution. This aer combines and strengthens the two frameworks. It designs a single estimator whose excess error vanishes both at a universal rate as the samle size grows, as well as when the (unknown) distribution gets simler. We show that the resulting algorithm significantly imroves the erformance guarantees for numerous cometitive- and structural-estimation results. The algorithm runs in near-linear time and is robust to model missecification and domain-symbol ermutations.
"
152,2019,Learning a Prior over Intent via Meta-Inverse Reinforcement Learning,Oral,"A significant challenge for the ractical alication of reinforcement learning to real world roblems is the need to secify an oracle reward function that correctly defines a task. Inverse reinforcement learning (IRL) seeks to avoid this challenge by instead inferring a reward function from exert demonstrations. While aealing, it can be imractically exensive to collect datasets of demonstrations that cover the variation common in the real world (e.g. oening any tye of door). Thus in ractice, IRL must commonly be erformed with only a limited set of demonstrations where it can be exceedingly difficult to unambiguously recover a reward function. In this work, we exloit the insight that demonstrations from other tasks can be used to constrain the set of ossible reward functions by learning a ''rior'' that is secifically otimized for the ability to infer exressive reward functions from limited numbers of demonstrations. We demonstrate that our method can efficiently recover rewards from images for novel tasks and rovide intuition as to how our aroach is analogous to learning a rior.
"
153,2019,AUCµ: A Performance Metric for Multi-Class Machine Learning Models,Oral,"The area under the receiver oerating characteristic curve (AUC) is arguably the most common metric in machine learning for assessing the quality of a two-class classification model. As the number and comlexity of machine learning alications grows, so too does the need for measures that can gracefully extend to classification models trained for more than two classes. Prior work in this area has roven comutationally intractable andor inconsistent with known roerties
of AUC, and thus there is still a need for an imroved multi-class efficacy metric. We rovide in this work a multi-class extension of AUC that we call AUCµ that is derived from first rinciles of the binary class AUC. AUCµ has similar comutational comlexity to AUC and maintains the roerties of AUC critical to its interretation and use.
"
154,2019,Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,Oral,"We consider decentralized stochastic otimization with the objective function (e.g. data samles for machine learning task) being distributed over n machines that can only communicate to their neighbors on a fixed communication grah. To reduce the communication bottleneck, the nodes comress (e.g. quantize or sarsify) their model udates. We cover both unbiased and biased comression oerators with quality denoted by \omega = 1 (\omega=1 meaning no comression).
We (i) roose a novel gossi-based stochastic gradient descent algorithm, CHOCO-SGD, that converges at rate O(1(nT) + 1(T \delta^2 \omega)^2) for strongly convex objectives, where T denotes the number of iterations and $\delta$ the eigenga of the connectivity matrix. Desite comression quality and network connectivity affecting the higher order terms, the first term in the rate, O(1(nT)), is the same as for the centralized baseline with exact communication. We (ii) resent a novel gossi algorithm, CHOCO-GOSSIP, for the average consensus roblem that converges in time O(1(\delta^2\omega) \log (1\esilon)) for accuracy \esilon  0. This is (u to our knowledge) the first gossi algorithm that suorts arbitrary comressed messages for \omega  0 and still exhibits linear convergence. We (iii) show in exeriments that both of our algorithms do outerform the resective state-of-the-art baselines and CHOCO-SGD can reduce communication by at least two orders of magnitudes."
155,2019,Curiosity-Bottleneck: Exploration By Distilling Task-Specific Novelty ,Oral,"Exloration based on state novelty has brought great success in challenging reinforcement learning roblems with sarse rewards. However, existing novelty-based strategies become inefficient in real-world roblems where observation contains not only task-deendent state novelty of our interest but also task-irrelevant information that should be ignored. We introduce an information-theoretic exloration strategy named Curiosity-Bottleneck that distills task-relevant information from observation. Based on the Information Bottleneck rincile, our exloration bonus is quantified as the comressiveness of observation with resect to the learned reresentation of a comressive value network. With extensive exeriments on static image classification, grid-world and three hard-exloration Atari games, we show that Curiosity-Bottleneck learns effective exloration strategy by robustly measuring the state novelty in distractive environment where state-of-the-art exloration methods often degenerate.
"
156,2019,Learning to Convolve: A Generalized Weight-Tying Approach,Oral,"Recent work (Cohen &am; Welling, 2016) has shown that generalizations of convolutions, based on grou theory, rovide owerful inductive biases for learning. In these generalizations, filters are not only translated but can also be rotated, flied, etc. However, coming u with exact models of how to rotate a 3x3 filter on a square ixel-grid is difficult.

In this aer, we learn how to transform filters for use in the grou convolution, focussing on roto-translation. For this, we learn a filter basis and all rotated versions of that filter basis. Filters are then encoded by a set of rotation invariant coefficients. To rotate a filter, we switch the basis. We demonstrate we can roduce feature mas with low sensitivity to inut rotations, while achieving high erformance on MNIST and CIFAR-10.
"
157,2019,Homomorphic Sensing,Oral,"A recent line of research termed ""unlabeled sensing"" and ""shuffled linear regression"" has been exloring under great generality the recovery of signals from subsamled and ermuted measurements; a challenging roblem in diverse fields of data science and machine learning. In this aer we introduce an abstraction of this roblem which we call ""homomorhic sensing"". Given a linear subsace and a finite set of linear transformations we develo an algebraic theory which establishes conditions guaranteeing that oints in the subsace are uniquely determined from their homomorhic image under some transformation in the set. As a secial case, we recover known conditions for unlabeled sensing, as well as new results and extensions. On the algorithmic level we exhibit two dynamic rogramming based algorithms, which to the best of our knowledge are the first working solutions for the unlabeled sensing roblem for small dimensions. One of them, additionally based on branch-and-bound, when alied to image registration under affine transformations, erforms on ar with or outerforms state-of-the-art methods on benchmark datasets.
"
158,2019,A Kernel Theory of Modern Data Augmentation,Oral,"Data augmentation, a technique in which a training set is exanded with class-reserving transformations, is ubiquitous in modern machine learning ielines. In this aer, we seek to establish a theoretical framework for understanding data augmentation. We aroach this from two directions: First, we rovide a general model of augmentation as a Markov rocess, and show that kernels aear naturally with resect to this model, even when we do not emloy kernel classification. Next, we analyze more directly the effect of augmentation on kernel classifiers, showing that data augmentation can be aroximated by first-order feature averaging and second-order variance regularization comonents. These frameworks both serve to illustrate the ways in which data augmentation affects the downstream learning model, and the resulting analyses rovide novel connections between rior work in invariant kernels, tangent roagation, and robust otimization. Finally, we rovide several roof-of-concet alications showing that our theory can be useful for accelerating machine learning workflows, such as reducing the amount of comutation needed to train using augmented data, and redicting the utility of a transformation rior to training.
"
159,2019,DeepMDP: Learning Continuous Latent Space Models for Representation Learning,Oral,"Many reinforcement learning tasks rovide the agent with high-dimensional observations that can be simlified into low-dimensional continuous states.
To formalize this rocess, we introduce the concet of a \textit{DeeMDP}, a Markov Decision Process (MDP) arameterized by neural networks that is able to recover these reresentations.
We mathematically develo several desirable notions of similarity between the original MDP and the DeeMDP based on two main objectives: (1) modeling the dynamics of an MDP, and (2) learning a useful abstract reresentation of the states of an MDP.
While the motivation for each of these notions is distinct, we find that they are intimately related.
Secifically, we derive tractable training objectives of the DeeMDP comonents which simultaneously and rovably encourage \textit{all} notions of similarity.
We validate our theoretical findings by showing that we are able to learn DeeMDPs and recover the latent structure underlying high-dimensional observations on a synthetic environment.
Finally, we show that learning a DeeMDP as an auxiliary task in the Atari domain leads to large erformance imrovements.
"
160,2019,Imputing Missing Events in Continuous-Time Event Streams,Oral,"Events that we observe in the world may be caused by other, unobserved events. We consider sequences of discrete events in continuous time. Given a robability model of comlete sequences, we roose article smoothing---a form of sequential imortance samling---to imute the missing events in an incomlete sequence. We develo a trainable family of roosal distributions based on a tye of continuous-time bidirectional LSTM. Thus, unlike in article filtering, our roosed events are conditioned on the future and not just on the ast. Our method can samle an ensemble of ossible comlete sequences (articles), from which we form a single consensus rediction that has low Bayes risk under our chosen loss metric. We exeriment in multile synthetic and real domains, using different missingness mechanisms, and modeling the comlete sequences in each domain with a neural Hawkes rocess (Mei &am; Eisner 2017). On held-out incomlete sequences, our method is effective at inferring the ground truth unobserved events. In articular, article smoothing consistently imroves uon article filtering, showing the benefit of training a bidirectional roosal distribution. We further use multinomial resamling to mitigate the article skewness roblem, which further imroves results.
"
161,2019,Non-Parametric Priors For Generative Adversarial Networks,Oral,"The advent of generative adversarial networks (GAN) has enabled new caabilities in synthesis, interolation, and data augmentation heretofore considered very challenging. However, one of the common assumtions in most GAN architectures is the assumtion of simle arametric latent-sace distributions. While easy to imlement, a simle latent-sace distribution can be roblematic for uses such as interolation, as the samles drawn often lead to distributional mismatches when interolated in the  latent-sace. We resent a rather simle formalization of this roblem; using basic results from robability theory and off-the-shelf-otimization tools, we develo ways to arrive at aroriate non-arametric riors. The obtained rior exhibits unusual qualitative roerties in terms of its shae, and quantitative benefits in terms of lower divergence with its mid-oint distribution. We demonstrate that our designed rior  hels to imrove the quality of image generation along any Euclidean straight line during interolation, both qualitatively and quantitatively, without any additional training or architectural modifications. The roosed formulation is quite flexible, aving the way to imose newer constraints on the latent-sace statistics.
"
162,2019,Regularization in directable environments with application to Tetris,Oral,"We examine regularized linear models on small data sets where the directions of features are known. We find that traditional regularizers, such as ridge regression and the Lasso, induce unnecessarily high bias in order to reduce variance. We roose an alternative regularizer that enalizes the differences between the weights assigned to the features. This model often finds a better bias-variance tradeoff than its cometitors in suervised learning roblems. We also give an examle of its use within reinforcement learning, when learning to lay the game of Tetris.
"
163,2019,On Dropout and Nuclear Norm Regularization,Oral,"We give a formal and comlete characterization of the exlicit regularizer induced by droout in dee linear networks with the squared loss. We show that (a) the exlicit regularizer is comosed of an $\ell_2$-ath regularizer and other terms that are also re-scaling invariant, (b) the convex enveloe of the induced regularizer is the squared nuclear norm of the network ma, and (c) for a sufficiently large droout rate, we characterize the global otima of the droout objective. We validate our theoretical findings with emirical results."
164,2019,Understanding and Controlling Memory in Recurrent Neural Networks,Oral,"To be effective in sequential data rocessing, Recurrent Neural Networks (RNNs) are required to kee track of ast events by creating memories. While  the  relation  between  memories  and  the network’s hidden state dynamics was established over the last decade,  revious works in this direction were of a redominantly descritive nature focusing mainly on locating the dynamical objects of interest.  In articular, it remained unclear how dynamical observables affect the erformance, how they form and whether they can be maniulated. Here, we utilize different training rotocols, datasets and architectures to obtain a  range  of  networks  solving  a  delayed  classification task with similar erformance, alongside substantial differences in their ability to extraolate for longer delays. We analyze the dynamics  of  the  network’s  hidden  state,  and  uncover the reasons for this difference. Each memory is found to be associated with a nearly steady state of  the  dynamics  which  we  refer  to  as  a  ’slow oint’. Slow oint seeds redict extraolation erformance across all datasets, rotocols and architectures tested. Furthermore, by tracking the formation of the slow oints we are able to understand the origin of differences between training rotocols. Finally, we roose a novel regularization technique that is based on the relation be-tween hidden state seeds and memory longevity. Our technique maniulates these seeds, thereby leading to a dramatic imrovement in memory robustness over time, and could ave the way for a new class of regularization methods.
"
165,2019,Lipschitz Generative Adversarial Nets,Oral,"In this aer we study the convergence of generative adversarial networks (GANs) from the ersective of the informativeness of the gradient of the otimal discriminative function. We show that GANs without restriction on the discriminative function sace commonly suffer from the roblem that the gradient roduced by the discriminator is uninformative to guide the generator. By contrast, Wasserstein GAN (WGAN), where the discriminative function is restricted to 1-Lischitz, does not suffer from such a gradient uninformativeness roblem. We further show in the aer that the model with a comact dual form of Wasserstein distance, where the Lischitz condition is relaxed, also suffers from this issue. This imlies the imortance of Lischitz condition and motivates us to study the general formulation of GANs with Lischitz constraint, which leads to a new family of GANs that we call Lischitz GANs (LGANs). We show that LGANs guarantee the existence and uniqueness of the otimal discriminative function as well as the existence of a unique Nash equilibrium. We rove that LGANs are generally caable of eliminating the gradient uninformativeness roblem. According to our emirical analysis, LGANs are more stable and generate consistently higher quality samles comared with WGAN.
"
166,2019,Importance Sampling Policy Evaluation with an Estimated Behavior Policy,Oral,"We consider the roblem of off-olicy evaluation in Markov decision rocesses. Off-olicy evaluation is the task of evaluating the exected return of one olicy with data generated by a different, behavior olicy. Imortance samling is a technique for off-olicy evaluation that re-weights off-olicy returns to account for differences in the likelihood of the returns between the two olicies. In this aer, we study imortance samling with an estimated behavior olicy where the behavior olicy estimate comes from the same set of data used to comute the imortance samling estimate. We find that this estimator often lowers the mean squared error of off-olicy evaluation comared to imortance samling with the true behavior olicy or using a behavior olicy that is estimated from a searate data set. Intuitively, estimating the behavior olicy in this way corrects for error due to samling in the action-sace. Our emirical results also extend to other oular variants of imortance samling and show that estimating a non-Markovian behavior olicy can further lower large-samle mean squared error even when the true behavior olicy is Markovian.
"
167,2019,Phaseless PCA: Low-Rank Matrix Recovery from Column-wise Phaseless Measurements,Oral,"This work rooses the first set of simle, ractically useful, and rovable algorithms for two inter-related roblems. (i) The first is low-rank matrix recovery from magnitude-only (haseless) linear rojections of each of its columns. This finds imortant alications in haseless dynamic imaging, e.g., Fourier Ptychograhic imaging of live biological secimens. Our guarantee shows that, in the regime of small ranks, the samle comlexity required is only a little larger than the order-otimal one, and much smaller than what standard (unstructured) hase retrieval methods need. %Moreover our algorithm is fast and memory-efficient if only the minimum required number of measurements is used (ii) The second roblem we study is a dynamic extension of the above: it allows the low-dimensional subsace from which each imagesignal (each column of the low-rank matrix) is generated to change with time. We introduce a simle algorithm that is rovably correct as long as the subsace changes are iecewise constant.
"
168,2019,Improved Dynamic Graph Learning through Fault-Tolerant Sparsification,Oral,"Grah sarsification has been used to imrove the comutational cost of learning over grahs, \e.g., Lalacian-regularized estimation and grah semi-suervised learning (SSL). However, when grahs vary over time, reeated sarsification requires olynomial order comutational cost er udate. We roose a new tye of grah sarsification namely fault-tolerant (FT) sarsification to significantly reduce the cost to only a constant. Then the comutational cost of subsequent grah learning tasks can be significantly imroved with limited loss in their accuracy. In articular, we give theoretical analyze to uer bound the loss in the accuracy of the subsequent Lalacian-regularized estimation and grah SSL, due to the FT sarsification. In addition, FT sectral sarsification can be generalized to FT cut sarsification, for cut-based grah learning. Extensive exeriments have confirmed the comutational efficiencies and accuracies of the roosed methods for learning on dynamic grahs.
"
169,2019,Dynamic Weights in Multi-Objective Deep Reinforcement Learning,Oral,"Many real-world decision roblems are characterized by multile conflicting objectives which must be balanced based on their relative imortance. In the dynamic weights setting the relative imortance changes over time and secialized algorithms that deal with such change, such as a tabular Reinforcement Learning (RL) algorithm by Natarajan and Tadealli (2005), are required. However, this earlier work is not feasible for RL settings that necessitate the use of function aroximators. We generalize across weight changes and high-dimensional inuts by roosing a multi-objective Q-network whose oututs are conditioned on the relative imortance of objectives and we introduce Diverse Exerience Relay (DER) to counter the inherent non-stationarity of the Dynamic Weights setting. We erform an extensive exerimental evaluation and comare our methods to adated algorithms from Dee Multi-TaskMulti-Objective Reinforcement Learning and show that our roosed network in combination with DER dominates these adated algorithms across weight change scenarios and roblem domains.
"
170,2019,Safe Grid Search with Optimal Complexity,Oral,"Poular machine learning estimators involve regularization arameters that can be challenging to tune, and standard strategies rely on grid search for this task.
In this aer, we revisit the techniques of aroximating the regularization  ath u to redefined tolerance $\esilon$ in a unified framework and show that its comlexity is $O(1\sqrt[d]{\esilon})$ for uniformly convex loss of order $d0$ and $O(1\sqrt{\esilon})$ for Generalized Self-Concordant functions.
This framework encomasses least-squares but also logistic regression, a case that as far as we know was not handled as recisely in revious works.
We leverage our technique to rovide refined bounds on the validation error as well as a ractical algorithm for hyerarameter tuning.
The later has global convergence guarantee when targeting a rescribed accuracy on the validation set.
Last but not least, our aroach hels relieving the ractitioner from the (often neglected) task of selecting a stoing criterion when otimizing over the training set: our method automatically calibrates this criterion based on the targeted accuracy on the validation set."
171,2019,kernelPSI: a Post-Selection Inference Framework for Nonlinear Variable Selection,Oral,"Model selection is an essential task for many alications in scientific discovery. The most common aroaches rely on univariate linear measures of association between each feature and the outcome. Such classical selection rocedures fail to take into account nonlinear effects and interactions between features. Kernel-based selection rocedures have been roosed as a solution. However, current strategies for kernel selection fail to measure the significance of a joint model constructed through the combination of the basis kernels. In the resent work, we exloit recent advances in ost-selection inference to roose a valid statistical test for the association of a joint model of the selected kernels with the outcome. The kernels are selected via a ste-wise rocedure which we model as a succession of quadratic constraints in the outcome variable. 
"
172,2019,HexaGAN: Generative Adversarial Nets for Real World Classification,Oral,"Most dee learning classification studies assume clean data. However, when dealing with the real world data, we encounter three roblems such as 1) missing data, 2) class imbalance, and 3) missing label roblems. These roblems undermine the erformance of a classifier. Various rerocessing techniques have been roosed to mitigate one of these roblems, but an algorithm that assumes and resolves all three roblems together has not been roosed yet. In this aer, we roose HexaGAN, a generative adversarial network framework that shows romising classification erformance for all three roblems. We interret the three roblems from a single ersective to solve them jointly. To enable this, the framework consists of six comonents, which interact with each other. We also devise novel loss functions corresonding to the architecture. The designed loss functions allow us to achieve state-of-the-art imutation erformance, with u to a 14% imrovement, and to generate high-quality class-conditional data. We evaluate the classification erformance (F1-score) of the roosed method with 20% missingness and confirm u to a 5% imrovement in comarison with the erformance of combinations of state-of-the-art methods.
"
173,2019,Rate Distortion For Model Compression:From Theory To Practice,Oral,"The enormous size of modern dee neural networks makes it challenging to deloy those models in memory and communication limited scenarios. Thus, comressing a trained model without a significant loss in erformance has become an increasingly imortant task. Tremendous advances has been made recently, where the main technical building blocks are arameter runing, arameter sharing  (quantization), and low-rank factorization. In this aer, we roose rinciled aroaches to imrove uon the common heuristics used in those building blocks, namely runing and quantization.  

We first study the fundamental limit for model comression via rate distortion theory. We bring the rate distortion function from data comression to model comression to quantify this fundamental limit. We rove a lower bound for the rate distortion function and rove its achievability for linear models.  Although this achievable comression scheme is intractable in ractice, this analysis motivates a novel model comression framework. This framework rovides a new objective function in model comression, which can be alied together with other classes of model comressors such as runing or quantization. Theoretically, we rove that the roosed scheme is otimal for comressing one-hidden-layer ReLU neural networks. Emirically, we show that the roosed scheme imroves uon the baseline in the comression-accuracy tradeoff. 
"
174,2019,SAGA with Arbitrary Sampling,Oral,"We study the roblem of minimizing the average of a very large number of smooth functions, which is of key imortance in  training suervised  learning models.  One of the most celebrated methods in this context is the SAGA algorithm of Defazio et al. (2014). Desite years of research on the toic, a general-urose version of SAGA---one that would include arbitrary imortance samling and minibatching schemes---does not exist.  We remedy this situation and roose a general and flexible variant of SAGA following the arbitrary samling aradigm. We erform an iteration comlexity analysis of the method, largely ossible due to the construction of  new stochastic Lyaunov functions. We establish linear convergence rates in the smooth and  strongly convex regime, and  under certain error bound conditions also in a  regime without strong convexity.  Our rates match those of the rimal-dual method Quartz (Qu et al., 2015) for which an arbitrary samling analysis is available, which makes a significant ste towards closing the ga in our understanding of comlexity of rimal and dual methods for finite sum roblems. Finally, we show through exeriments that secific variants of our general SAGA method can erform better in ractice than other cometing methods.
"
175,2019,Scalable Learning in Reproducing Kernel Krein Spaces,Oral,"We rovide the first mathematically comlete derivation of the Nyström method for low-rank aroximation of indefinite kernels and roose an efficient method for finding an aroximate eigendecomosition of such kernel matrices. Building on this result, we devise highly scalable methods for learning in reroducing kernel Krein saces. The devised aroaches rovide a rinciled and theoretically well-founded means to tackle large scale learning roblems with indefinite kernels. The main motivation for our work comes from roblems with structured reresentations (e.g., grahs, strings, time-series), where it is relatively easy to devise a airwise (dis)similarity function based on intuition andor knowledge of domain exerts. Such functions are tyically not ositive definite and it is often well beyond the exertise of ractitioners to verify this condition. The effectiveness of the devised aroaches is evaluated emirically using indefinite kernels defined on structured and vectorial data reresentations.
"
176,2019,Gradient Descent Finds Global Minima of Deep Neural Networks,Oral,"Gradient descent finds a global minimum in training dee neural networks desite the objective function being non-convex. The current aer roves gradient descent achieves zero training loss in olynomial time for a dee over-arameterized neural network with residual connections (ResNet). Our analysis relies on the articular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training rocess and this stability imlies the global otimality of the gradient descent algorithm. We further extend our analysis to dee residual convolutional neural networks and obtain a similar convergence result.
"
177,2019,Heterogeneous Model Reuse via Optimizing Multiparty Multiclass Margin,Oral,"Nowadays, many roblems require learning a model from data owned by different articiants who are restricted to share their examles due to rivacy concerns, which is referred to as multiarty learning in the literature. In conventional multiarty learning, a global model is usually trained from scratch via a communication rotocol, ignoring the fact that each arty may already have a local model trained on her own dataset. In this aer, we define a multiarty multiclass margin to measure the global behavior of a set of heterogeneous local models, and roose a general learning method called HMR (Heterogeneous Model Reuse) to otimize the margin. Our method reuses local models to aroximate a global model, even when data are non-i.i.d distributed among arties, by exchanging few examles under redefined budget. Exeriments on synthetic and real-world data covering different multiarty scenarios show the effectiveness of our roosal.
"
178,2019,Learning from a Learner,Oral,"In this aer, we roose a novel setting for Inverse Reinforcement Learning (IRL), namely ""Learning from a Learner"" (LfL). As oosed to standard IRL, it does not consist in learning a reward by observing an otimal agent but from observations of another learning (and thus sub-otimal) agent. To do so, we leverage the fact that the observed agent's olicy is assumed to imrove over time. The ultimate goal of this aroach is to recover the actual environment's reward and to allow the observer to outerform the learner. To recover that reward in ractice, we roose methods based on the entroy-regularized olicy iteration framework. We discuss different aroaches to learn solely from trajectories in the state-action sace. We demonstrate the genericity of our method by observing agents imlementing various reinforcement learning algorithms. Finally, we show that, on both discrete and continuous stateaction tasks, the observer's erformance (that otimizes the recovered reward) can surass those of the observed agent. 
"
179,2019,Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces,Oral,"In order to integrate uncertainty estimates into dee time-series modelling, Kalman Filters (KFs) (Kalman, 1960) have been integrated with dee learning models. br 
Yet, such aroaches tyically rely on aroximate inference techniques such as variational inference which makes learning more comlex and often less scalable due to aroximation errors. We roose a new dee aroach to Kalman filtering which can be learned directly in an end-to-end manner using backroagation without additional aroximations. Our aroach uses a high-dimensional factorized latent state reresentation for which the Kalman udates simlify to scalar oerations and thus avoids hard to backroagate, comutationally heavy and otentially unstable matrix inversions. Moreover, we use locally linear dynamic models to efficiently roagate the latent state to the next time ste. The resulting network architecture, which we call Recurrent Kalman Network (RKN), can be used for any time-series data, similar to a LSTM  (Hochreiter &am; Schmidhuber, 1997) but uses an exlicit reresentation of uncertainty. As shown by our exeriments, the RKN obtains much more accurate uncertainty estimates than an LSTM or Gated Recurrent Units (GRUs) (Cho et al., 2014) while also showing a slightly imroved rediction erformance and outerforms various recent generative models on an image imutation task.
"
180,2019,Fingerprint Policy Optimisation for Robust Reinforcement Learning,Oral,"Policy gradient methods ignore the otential value of adjusting environment variables: unobservable state features that are randomly determined by the environment in a hysical setting, but are controllable in a simulator. This can lead to slow learning, or convergence to subotimal olicies, if the environment variable has a large imact on the transition dynamics. In this aer, we resent fingerrint olicy otimisation (FPO), which finds a olicy that is otimal in exectation across the distribution of environment variables. The central idea is to use Bayesian otimisation (BO) to actively select the distribution of the environment variable that maximises the imrovement generated by each iteration of the olicy gradient method. To make this BO ractical, we contribute two easy-to-comute low-dimensional fingerrints of the current olicy. Our exeriments show that FPO can efficiently learn olicies that are robust to significant rare events, which are unlikely to be observable under random samling, but are key to learning good olicies.
"
181,2019,Natural Analysts in Adaptive Data Analysis,Oral,"Adative data analysis is frequently criticized for its essimistic generalization guarantees. The source of these essimistic bounds is a model that ermits arbitrary, ossibly adversarial analysts that otimally use information to bias results. While being a central issue in the field, still lacking are notions of natural analysts that allow for more otimistic bounds faithful to the reality that tyical analysts aren't adversarial. In this work, we roose notions of natural analysts that smoothly interolate between the otimal non-adative bounds and the best-known adative generalization bounds. To accomlish this, we model the analyst's knowledge as evolving according to the rules of an unknown dynamical system that takes in revealed information and oututs new statistical queries to the data. This allows us to restrict the analyst through different natural control-theoretic notions. One such notion corresonds to a recency bias, formalizing an inability to arbitrarily use distant information. Another comlementary notion formalizes an anchoring bias, a tendency to weight initial information more strongly. Both notions come with quantitative arameters that smoothly interolate between the non-adative case and the fully adative case, allowing for a rich sectrum of intermediate analysts that are neither non-adative nor adversarial. Natural not only from a cognitive ersective, we show that our notions also cature standard otimization methods, like gradient descent in various settings. This gives a new interretation to the fact that gradient descent tends to overfit much less than its adative nature might suggest.
"
182,2019,Formal Privacy for Functional Data with Gaussian Perturbations,Oral,"Motivated by the raid rise in statistical tools in Functional Data Analysis, we consider the Gaussian mechanism for achieving differential rivacy with arameter estimates taking values in a, otentially infinite-dimensional, searable Banach sace. Using classic results from robability theory, we show how densities over function saces can be utilized to achieve the desired differential rivacy bounds. This extends rior results of Hall et al. (2013) to a much broader class of statistical estimates and summaries, including “ath level” summaries, nonlinear functionals, and full function releases. By focusing on Banach saces, we rovide a deeer icture of the challenges for rivacy with comlex data, esecially the role regularization lays in balancing utility and rivacy. Using an alication to enalized smoothing, we exlicitly highlight this balance in the context of mean function estimation. Simulations and an alication to diffusion tensor imaging are briefly resented, with extensive additions included in a sulement.
"
183,2019,Composable Core-sets for Determinant Maximization: A Simple Near-Optimal Algorithm,Oral,"``Comosable core-sets'' are an efficient framework for solving otimization roblems in massive data models. In this work, we consider efficient construction of comosable core-sets for the determinant maximization roblem.
This can also be cast as the MAP inference task for ``determinantal oint rocesses"", that have recently gained a lot of interest for modeling diversity and fairness. The roblem was recently studied in \cite{indyk2018comosable}, where they designed comosable core-sets with the otimal aroximation bound of $O(k)^k$. On the other hand, the more ractical ``Greedy"" algorithm has been reviously used in similar contexts. In this work, first we rovide a theoretical aroximation guarantee of $C^{k^2}$ for the Greedy algorithm in the context of comosable core-sets. Further, we roose to use a ``Local Search"" based algorithm that while being still ractical, achieves a nearly otimal aroximation bound of $O(k)^{2k}$. Finally, we imlement all three algorithms and show the effectiveness of our roosed algorithm on standard data sets."
184,2019,Subspace Robust Wasserstein Distances,Oral,"Making sense of Wasserstein distances between discrete measures in high-dimensional settings remains a challenge. Recent work has advocated a two-ste aroach to imrove robustness and facilitate the comutation of otimal transort, using for instance rojections on random real lines, or a reliminary quantization of the measures to reduce the size of their suort. We roose in this work a ""max-min"" robust variant of the Wasserstein distance by considering the maximal ossible distance that can be realized between two measures, assuming they can be rojected orthogonally on a lower k-dimensional subsace. Alternatively, we show that the corresonding ""min-max"" OT roblem has a tight convex relaxation which can be cast as that of finding an otimal transort lan with a low transortation cost, where the cost is alternatively defined as the sum of the k largest eigenvalues of the second order moment matrix of the dislacements (or matchings) corresonding to that lan (the usual OT definition only considers the trace of that matrix). We show that both quantities inherit several favorable roerties from the OT geometry. We roose two algorithms to comute the latter formulation using entroic regularization, and illustrate the interest of this aroach emirically.
"
185,2019,An Investigation into Neural Net Optimization via Hessian Eigenvalue Density,Oral,"To understand the dynamics of training in dee neural networks, we study the evolution of the Hessian eigenvalue density throughout the otimization rocess. In non-batch normalized networks, we observe the raid aearance of large isolated eigenvalues in the sectrum, along with a surrising concentration of the gradient in the corresonding eigensaces. In a batch normalized network, these two effects are almost absent. We give a theoretical rationale to artially exlain these henomena. As art of this work, we adat advanced tools from numerical linear algebra that allow scalable and accurate estimation of the entire Hessian sectrum of ImageNet-scale neural networks; this technique may be of indeendent interest in other alications. 
"
186,2019,Graph Matching Networks for Learning the Similarity of Graph Structured Objects,Oral,"This aer addresses the challenging roblem of retrieval and matching of grah structured objects, and makes two key contributions. First, we demonstrate how Grah Neural Networks (GNN), which have emerged as an effective model for various suervised rediction roblems defined on structured data, can be trained to roduce embedding of grahs in vector saces that enables efficient similarity reasoning. Second, we roose a novel Grah Matching Network model that, given a air of grahs as inut, comutes a similarity score between them by jointly reasoning on the air through a new cross-grah attention-based matching mechanism. We demonstrate the effectiveness of our models on different domains including the challenging roblem of control-flow grah based function similarity search that lays an imortant role in the detection of vulnerabilities in software systems. The exerimental analysis demonstrates that our models are not only able to exloit structure in the context of similarity learning but they can also outerform domain secific baseline systems that have been carefully hand-engineered for these roblems.
"
187,2019,Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff,Oral,"Lossy comression algorithms are tyically designed and analyzed through the lens of Shannon's rate-distortion theory, where the goal is to achieve the lowest ossible distortion (e.g., low MSE or high SSIM) at any given bit rate. However, in recent years, it has become increasingly acceted that ""low distortion"" is not a synonym for ""high ercetual quality"", and in fact otimization of one often comes at the exense of the other. In light of this understanding, it is natural to seek for a generalization of rate-distortion theory which takes ercetual quality into account. In this aer, we adot the mathematical definition of ercetual quality recently roosed by Blau &am; Michaeli (2018), and use it to study the three-way tradeoff between rate, distortion, and ercetion. We show that restricting the ercetual quality to be high, generally leads to an elevation of the rate-distortion curve, thus necessitating a sacrifice in either rate or distortion. We rove several fundamental roerties of this trile-tradeoff, calculate it in closed form for a Bernoulli source, and illustrate it visually on a toy MNIST examle.
"
188,2019,Separable value functions across time-scales,Oral,"In many finite horizon eisodic reinforcement learning (RL) settings, it is desirable to otimize for the undiscounted return - in settings like Atari, for instance, the goal is to collect the most oints while staying alive in the long run. Yet, it may be difficult (or even intractable) mathematically to learn with this target. As such, temoral discounting is often alied to otimize over a shorter effective lanning horizon. This comes at the cost of otentially biasing the otimization target away from the undiscounted goal. In settings where this bias is unaccetable - where the system must otimize for longer horizons at higher discounts - the target of the value function aroximator may increase in variance leading to difficulties in learning. We resent an extension of temoral difference (TD) learning, which we call TD($\Delta$), that breaks down a value function into a series of comonents based on the differences between value functions with smaller discount factors. The searation of a longer horizon value function into these comonents has useful roerties in scalability and erformance. We discuss these roerties and show theoretic and emirical imrovements over standard TD learning in certain settings."
189,2019,Dirichlet Simplex Nest and Geometric Inference,Oral,"We roose Dirichlet Simlex Nest, a class of robabilistic models suitable for a variety of data tyes, and develo fast and rovably accurate inference algorithms by accounting for the model's convex geometry and low dimensional simlicial structure. By exloiting the connection to Voronoi tessellation and roerties of Dirichlet distribution, the roosed inference algorithm is shown to achieve consistency and strong error bound guarantees on a range of model settings and data distributions. The effectiveness of our model and the learning algorithm is demonstrated by simulations and by analyses of text and financial data.
"
190,2019,Collaborative Channel Pruning for Deep Networks,Oral,"Dee networks have achieved imressive erformance in various domains, but their alications are largely limited by the rohibitive comutational overhead. In this aer, we roose a novel algorithm, namely collaborative channel runing (CCP), to reduce the comutational overhead with negligible erformance degradation. The joint imact of runedreserved channels on the loss function is quantitatively   analyzed, and such inter-channel deendency is exloited to determine which channels to be runed. The channel selection roblem is then reformulated as a constrained 0-1 quadratic otimization roblem, and the Hessian matrix, which is essential in constructing the above otimization, can be efficiently aroximated. Emirical evaluation on two benchmark data sets indicates that our roosed CCP algorithm achieves higher classification accuracy with similar comutational comlexity than other state-of-the-art channel runing algorithms.
"
191,2019,Learning Action Representations for Reinforcement Learning,Oral,"Most model-free reinforcement learning methods leverage state reresentations (embeddings) for generalization, but either ignore structure in the sace of actions or assume the structure is rovided a riori.  We show how a olicy can be decomosed into a comonent that acts in a low-dimensional sace of action reresentations and a comonent that transforms these reresentations into actual actions. These reresentations imrove generalization over large, finite action sets by allowing the agent to infer the outcomes of actions similar to actions already taken.  We rovide an algorithm to both learn and use action reresentations and rovide conditions for its convergence. The efficacy of the roosed method is demonstrated on large-scale real-world roblems.
"
192,2019,Graphical-model based estimation and inference for differential privacy,Oral,"Many rivacy mechanisms reveal high-level information about a data distribution through noisy measurements. It is common to use this information to estimate the answers to new queries. In this work, we rovide an aroach to solve this estimation roblem efficiently using grahical models, which is articularly effective when the distribution is high-dimensional but the measurements are over low-dimensional marginals. We show that our aroach is far more efficient than existing
estimation techniques from the rivacy literature and that it can imrove the accuracy and scalability of many state-of-the-art mechanisms.
"
193,2019,Bayesian leave-one-out cross-validation for large data,Oral,"Model inference, such as model comarison, model checking, and model selection, is an imortant art of model develoment. Leave-one-out cross-validation (LOO) is a general aroach for assessing the generalizability of a model, but unfortunately, LOO does not scale well to large datasets. We roose a combination of using aroximate inference techniques and robability-roortional-to-size-samling (PPS) for fast LOO model evaluation for large datasets. We rovide both theoretical and emirical results showing good roerties for large data.
"
194,2019,Differentiable Linearized ADMM,Oral,"Recently, a great many learning-based otimization methods that combine data-driven architectures with the classical otimization algorithms have been roosed and exlored, showing suerior emirical erformance in solving various ill-osed inverse roblems. However, there is still a scarcity of rigorous analysis about the convergence behaviors of learning-based otimization. In articular, most existing theories are secific to unconstrained roblems but cannot aly to the more general cases where some variables of interest are subject to certain constraints. In this aer, we roose Differentiable Linearized ADMM (D-LADMM) for solving the roblems with linear constraints. Secifically, D-LADMM is a K-layer LADMM insired dee neural network, which is obtained by firstly introducing some learnable weights in the classical Linearized ADMM algorithm and then generalizing the roximal oerator to some learnable activation function. Notably, we mathematically rove that there exist a set of learnable arameters for D-LADMM to generate globally converged solutions, and we show that those desired arameters can be attained by training D-LADMM in a roer way. To the best of our knowledge, we are the first one to rovide the convergence analysis for the learning-based otimization method on constrained roblems. Exeriments on simulative and real alications verify the sueriorities of D-LADMM over LADMM.
"
195,2019,CapsAndRuns: An Improved Method for Approximately Optimal Algorithm Configuration,Oral,"We consider the roblem of configuring general-urose solvers to run efficiently on roblem instances drawn from an unknown distribution, a roblem of major interest in solver autoconfiguration. Following revious work, we focus on designing algorithms that find a configuration with near-otimal exected caed runtime while doing the least amount of work, with the ca chosen in a configuration-secific way so that most instances are solved. In this aer we resent a new algorithm, CasAndRuns, which finds a near-otimal configuration while using time that scales (in a roblem deendent way) with the otimal exected caed runtime, significantly strengthening revious results which could only guarantee a bound that scaled with the otentially much larger otimal exected uncaed runtime. The new algorithm is simler and more intuitive than the revious methods: first it estimates the otimal runtime ca for each configuration, then it uses a Bernstein race to find a near otimal configuration given the cas. Exeriments verify that our method can significantly outerform its cometitors.
"
196,2019,Sublinear Time Nearest Neighbor Search over Generalized Weighted Space,Oral,"Nearest Neighbor Search (NNS) over generalized weighted sace is a fundamental roblem which has many alications in various fields. However, to the best of our knowledge, there is no sublinear time solution to this roblem. Based on the idea of Asymmetric Locality Sensitive Hashing (ALSH), we introduce a novel sherical asymmetric transformation and roose the first two novel weight-oblivious hashing schemes SL-ALSH and S2-ALSH accordingly. We further show that both schemes enjoy a quality guarantee and can answer the NNS queries in sublinear time. Evaluations over three real datasets demonstrate the suerior erformance of the two roosed schemes.
"
197,2019,BayesNAS: A Bayesian Approach for Neural Architecture Search,Oral,"One-Shot Neural Architecture Search (NAS) is a romising method to significantly reduce search time without any searate training. It can be treated as a Network Comression roblem on the architecture arameters from an over-arameterized network. However, there are two issues associated with most one-shot NAS methods.  First, deendencies between a node and its redecessors and successors are often disregarded which result in imroer treatment over \emh{zero} oerations.  Second, architecture arameters runing based on their magnitude is questionable. In this aer, we emloy the classic Bayesian learning aroach to alleviate these two issues by modeling architecture arameters using \emh{hierarchical automatic relevance determination} (HARD) riors. Unlike other NAS methods, we train the over-arameterized network for only \emh{one} eoch then udate the architecture. Imressively, this enabled us to find the architecture in both roxy and roxyless tasks on CIFAR-10 within only $0.2$ GPU days using a single GPU. As a byroduct, our aroach can be transferred directly to comress convolutional neural networks by enforcing structural sarsity which achieves extremely sarse networks without accuracy deterioration."
198,2019,Decomposing feature-level variation with Covariate Gaussian Process Latent Variable Models,Oral,"The interretation of comlex high-dimensional data tyically requires the use of dimensionality reduction techniques to extract exlanatory low-dimensional reresentations. However, these reresentations may not be sufficient or aroriate to aid interretation articularly where dimensionality reduction is achieved through highly non-linear transformations. For examle, in transcritomics, the exression of many thousands of genes can be simultaneously measured and low-dimensional reresentations develoed for visualisation and understanding grouings of coordinated gene behaviour. Nonetheless, the underlying biology is ultimately hysically driven by variation at the level of individual genes and we would like to decomose that exression variability into a number of meaningful sub-comonents using a nonlinear alternative to traditional mixed model regression analysis.

Gaussian Process Latent Variable Models (GPLVMs) offer a rinciled way of erforming robabilistic non-linear dimensionality reduction and can be extended to incororate additional covariate information that is available in real-life alications. For examle, in transcritomics, covariate information might include categorical labels (e.g. denoting known disease sub-oulations), continuous-valued measurements (e.g. biomarkers), or censored information (e.g. atient survival times). However, the objective of such extensions in revious works has often been to boost redictive or classification ower of the GPLVM. For examle, the suervised GPLVM, uses class information to effectively build a distinct GPLVM for each class of data. Our motivation is discovery-led and we wish to understand the nature of the feature-level variability, searating the covariate effects from the contribution of latent variables, e.g. to identify sets of features which are fully exlained by covariates. We rincially do this in a high-dimensional observations setting where the number of features is vastly greater than the number of known covariates.

In this aer, we roose the Covariate Gaussian Process Latent Variable Model (c-GPLVM) to achieve this through a structured sarsity-inducing kernel decomosition for the GPLVM which allows us to exlicitly disentangle variation in the observed data vectors induced by variation in the covariate inuts or latent variables and interaction effects where the covariate inuts act in concert with the latent variables. The novelty of our aroach is that the structured kernel ermits both the develoment of a nonlinear maing into a latent sace where confounding factors are already adjusted for and feature-level variation that can be deconstructed. 

We demonstrate the utility of this model on a number of simulated examles and alications in disease rogression modelling from high-dimensional gene exression data in the resence of additional henotyes. In each setting we show that the c-GPLVM is able to effectively extract low-dimensional structures from high-dimensional data sets whilst allowing a breakdown of feature-level variability that is not resent in other commonly used dimensionality reduction aroaches.
"
199,2019,Active Manifolds: A non-linear analogue to Active Subspaces,Oral,"We resent an aroach to analyze $C^1(\mathbb{R}^m)$ functions that addresses limitations resent in the Active Subsaces (AS) method of Constantine et al. (2014; 2015). Under aroriate hyotheses, our Active Manifolds (AM) method identifies a 1-D curve in the domain (the active manifold) on which nearly all values of the unknown function are attained, which can be exloited for aroximation or analysis, esecially when $m$ is large (high-dimensional inut sace). We rovide theorems justifying our AM technique and an algorithm ermitting functional aroximation and sensitivity analysis. Using accessible, low-dimensional functions as initial examles, we show AM reduces aroximation error by an order of magnitude comared to AS, at the exense of more comutation. Following this, we revisit the sensitivity analysis by Glaws et al. (2017), who aly AS to analyze a magnetohydrodynamic ower generator model, and comare the erformance of AM on the same data. Our analysis rovides detailed information not catured by AS, exhibiting the influence of each arameter individually along an active manifold.  Overall, AM reresents a novel technique for analyzing functional models with benefits including:  reducing $m$-dimensional analysis to a 1-D analogue, ermitting more accurate regression than AS (at more comutational exense), enabling more informative sensitivity analysis, and granting accessible visualizations (2-D lots) of arameter sensitivity along the AM."
200,2019,Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks,Oral,"Many machine learning tasks such as multile instance learning, 3D shae recognition, and few-shot image classification are defined on sets of instances. Since solutions to such roblems do not deend on the order of elements of the set, models used to address them should be ermutation invariant. We resent an attention-based neural network module, the Set Transformer, secifically designed to model interactions among elements in the inut set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce comutational comlexity, we introduce an attention scheme insired by inducing oint methods from sarse Gaussian rocess literature. It reduces the comutation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art erformance comared to recent methods for set-structured data.
"
201,2019,Adaptive Stochastic Natural Gradient Method for One-Shot Neural Architecture Search,Oral,"High sensitivity of neural architecture search (NAS) methods against their inut such as ste-size (i.e., learning rate) and search sace revents ractitioners from alying them out-of-the-box to their own roblems, albeit its urose is to automate a art of tuning rocess. Aiming at a fast, robust, and widely-alicable NAS, we develo a generic otimization framework for NAS. We turn a couled otimization of connection weights and neural architecture into a differentiable otimization by means of stochastic relaxation. It accets arbitrary search sace (widely-alicable) and enables to emloy a gradient-based simultaneous otimization of weights and architecture (fast). We roose a stochastic natural gradient method with an adative ste-size mechanism built uon our theoretical investigation (robust). Desite its simlicity and no roblem-deendent arameter tuning, our method exhibited near state-of-the-art erformances with low comutational budgets both on image classification and inainting tasks.
"
202,2019,Leveraging Low-Rank Relations Between Surrogate Tasks in Structured Prediction,Oral,"We study the interlay between surrogate methods for structured rediction and techniques from multitask learning designed to leverage relationshis between surrogate oututs. br 
We roose an efficient algorithm based on trace norm regularization which, differently from revious methods, does not require exlicit knowledge of the codingdecoding functions of the surrogate framework. 
As a result, our algorithm can be alied to the broad class of roblems in which the surrogate sace is large or even infinite dimensional. We study excess risk bounds for trace norm regularized structured rediction roving the consistency and learning rates for our estimator. We also identify relevant regimes in which our aroach can enjoy better generalization erformance than revious methods. 
Numerical exeriments on ranking roblems indicate that enforcing low-rank relations among surrogate oututs may indeed rovide a significant advantage in ractice.
"
203,2019,"Same, Same But Different: Recovering Neural Network Quantization Error Through Weight Factorization",Oral,"Quantization of neural networks has become common ractice, driven by the need for efficient imlementations of dee neural networks on embedded devices. In this aer, we exloit an oft-overlooked degree of freedom in most networks - for a given layer, individual outut channels can be scaled by any factor rovided that the corresonding weights of the next layer are inversely scaled. Therefore, a given network has many factorizations which change the weights of the network without changing its function. We resent a concetually simle and easy to imlement method that uses this roerty and show that roer factorizations significantly decrease the degradation caused by quantization. We show imrovement on a wide variety of networks and achieve state-of-the-art degradation results for MobileNets. While our focus is on quantization, this tye of factorization is alicable to other domains such as network-runing, neural nets regularization and network interretability.
"
204,2019,Compressing Gradient Optimizers via Count-Sketches,Oral,"Many oular first-order otimization methods (e.g., Momentum, AdaGrad, Adam) accelerate the convergence rate of dee learning models. However, these algorithms require auxiliary arameters, which cost additional memory roortional to the number of arameters in the model. The roblem is becoming more severe as dee learning models continue to grow larger in order to learn from comlex, large-scale datasets. Our roosed solution is to maintain a linear sketch to comress the auxiliary variables. We demonstrate that our technique has the same erformance as the full-sized baseline, while using significantly less sace for the auxiliary variables. Theoretically, we rove that count-sketch otimization maintains the SGD convergence rate, while gracefully reducing memory usage for large-models. On the large-scale 1-Billion Word dataset, we save 25% of the memory used during training (8.6 GB instead of 11.7 GB) with minimal accuracy and erformance loss. For an Amazon extreme classification task with over 49.5 million classes, we also reduce the training time by 38%, by increasing the mini-batch size 3.5x using our count-sketch otimizer.
"
205,2019,Bayesian Counterfactual Risk Minimization,Oral,"We resent a Bayesian view of counterfactual risk minimization (CRM), also known as offline olicy otimization from logged bandit feedback. Using PAC-Bayesian analysis, we derive a new generalization bound for the truncated IPS estimator. We aly the bound to a class of Bayesian olicies, which motivates a novel, otentially data-deendent, regularization technique for CRM. Exerimental results indicate that this technique outerforms standard $L_2$ regularization, and that it is cometitive with variance regularization while being both simler to imlement and more comutationally efficient."
206,2019,White-box vs Black-box: Bayes Optimal Strategies for Membership Inference,Oral,"Membershi inference determines, given a samle and trained arameters of a machine learning model, whether the samle was art of the training set.
In this aer, we derive the otimal strategy for membershi inference with a few assumtions on the distribution of the arameters. 
We show that otimal attacks only deend on the loss function, and thus black-box attacks are as good as white-box attacks. 
As the otimal strategy is not tractable, we rovide aroximations of it leading to several inference methods, and show that existing membershi inference methods are other aroximations as well.
Our membershi attacks outerform the state of the art in various settings, ranging from a simle logistic regression to more comlex architectures and datasets, such as ResNet-101 and Imagenet.
"
207,2019,Rao-Blackwellized Stochastic Gradients for Discrete Distributions,Oral,"We wish to comute the gradient of an exectation
over a finite or countably infinite samle
sace having K ≤ ∞ categories. When K is indeed
infinite, or finite but very large, the relevant
summation is intractable. Accordingly, various
stochastic gradient estimators have been roosed.
In this aer, we describe a technique that can be
alied to reduce the variance of any such estimator,
without changing its bias—in articular,
unbiasedness is retained. We show that our technique
is an instance of Rao-Blackwellization, and 
we demonstrate the imrovement it yields on a 
semi-suervised classification roblem and a ixel attention task.
"
208,2019,Scalable Fair Clustering,Oral,"We study the fair variant of the classic k-median roblem introduced by (Chierichetti et al., NeurIPS 2017). In the standard k-median roblem, given an inut ointset P, the goal is to find k centers C and assign each inut oint to one of the centers in C such that the average distance of oints to their cluster center is minimized. In the fair variant of k-median, the oints are colored, and the goal is
to minimize the same average distance objective while ensuring that all clusters have an “aroximately equal” number of oints of each color.

(Chierichetti et al., NeurIPS 2017) roosed a two-hase algorithm for fair k-median. In the first ste, the ointset is artitioned into subsets called fairlets that satisfy the fairness requirement and aroximately reserve the k-median objective. In the second ste, fairlets are merged into k clusters by one of the existing k-median algorithms. The running time of this algorithm is dominated by the first ste, which takes suer-quadratic time. 

In this aer, we resent a ractical aroximate fairlet decomosition algorithm that runs in nearly linear time. We comlement our theoretical bounds with emirical evaluation.
"
209,2019,Neurally-Guided Structure Inference,Oral,"Most structure inference methods either rely on exhaustive search or are urely data-driven. Exhaustive search robustly infers the structure of arbitrarily comlex data, but it is slow. Data-driven methods allow efficient inference, but do not generalize when test data has more comlex structures than training data. In this aer, we roose a hybrid inference algorithm, Neurally-Guided Structure Inference (NG-SI), keeing the advantages of both search-based and data-driven methods. The key idea of NG-SI is to use a neural network to guide the hierarchical, layer-wise search over the comositional sace of structures. We evaluate our algorithm on two reresentative structure inference tasks: robabilistic matrix factorization and symbolic rogram arsing. It outerforms data-driven and search-based alternatives on both tasks.
"
210,2019,A Quantitative Analysis of the Effect of Batch Normalization on Gradient Descent,Oral,"Desite its emirical success and recent theoretical rogress, there generally lacks a quantitative analysis of the effect of batch normalization (BN) on the convergence and stability of gradient descent. In this aer, we rovide such an analysis on the simle roblem of ordinary least squares (OLS). Since recise dynamical roerties of gradient descent (GD) is comletely known for the OLS roblem, it allows us to isolate and comare the additional effects of BN. More recisely, we show that unlike GD, gradient descent with BN (BNGD) converges for arbitrary learning rates for the weights, and the convergence remains linear under mild conditions. Moreover, we quantify two different sources of acceleration of BNGD over GD -- one due to over-arameterization which imroves the effective condition number and another due having a large range of learning rates giving rise to fast descent. These henomena set BNGD aart from GD and could account for much of its robustness roerties. These findings are confirmed quantitatively by numerical exeriments, which further show that many of the uncovered roerties of BNGD in OLS are also observed qualitatively in more comlex suervised learning roblems.
"
211,2019,Shallow-Deep Networks: Understanding and Mitigating Network Overthinking,Oral,"We characterize a revalent weakness of dee neural networks (DNNs), 'overthinking', which occurs when a DNN can reach correct redictions before its final layer. Overthinking is comutationally wasteful, and it can also be destructive when, by the final layer, a correct rediction changes into a misclassification. Understanding overthinking requires studying how each rediction evolves during a DNN's forward ass, which conventionally is oaque. For rediction transarency, we roose the Shallow-Dee Network (SDN), a generic modification to off-the-shelf DNNs that introduces internal classifiers. We aly SDN to four modern architectures, trained on three image classification tasks, to characterize the overthinking roblem. We show that SDNs can mitigate the wasteful effect of overthinking with confidence-based early exits, which reduce the average inference cost by more than 50% and reserve the accuracy. We also find that the destructive effect occurs for 50% of misclassifications on natural inuts and that it can be induced, adversarially, with a recent backdooring attack. To mitigate this effect, we roose a new confusion metric to quantify the internal disagreements that will likely to lead to misclassifications.
"
212,2019,An Optimal Private Stochastic-MAB Algorithm based on Optimal Private Stopping Rule,Oral,"We resent a rovably otimal differentially rivate algorithm for the stochastic multi-arm bandit roblem, as oosed to the rivate analogue of the UCB-algorithm (Mishra and Thakurta, 2015; Tossou and Dimitrakakis, 2016) which doesn't meet the recently discovered lower-bound of $\Omega \left(\frac{K\log(n)}{\esilon} \right)$ (Shariff and Sheffet, 2018). Our construction is based on a different algorithm, Successive Elimination (Even-Dar et al., 2002), that reeatedly ulls all remaining arms until an arm is found to be subotimal and is then eliminated. In order to devise a rivate analogue of Successive Elimination we visit the roblem of rivate \emh{stoing rule}, that takes as inut a stream of i.i.d samles from an unknown distribution and returns a \emh{multilicative} $(1 \m \alha)$-aroximation of the distribution's mean, and rove the otimality of our rivate stoing rule. We then leverage on the rivate stoing rule algorithm and resent the rivate Successive Elimination algorithm which meets both the non-rivate lower bound (Lai and Robbins, 1985) and the above-mentioned rivate lower bound. We also comare emirically the erformance of our algorithm with the rivate UCB algorithm."
213,2019,GDPP: Learning Diverse Generations using Determinantal Point Processes,Oral,"Generative models have roven to be an outstanding tool for reresenting high-dimensional robability distributions and generating realistic looking images. An essential characteristic of generative models is their ability to roduce multi-modal oututs. However, while training, they are often suscetible to mode collase, that is models are limited in maing the inut noise to only a few modes of the true data distribution. In this aer, we draw insiration from Determinantal Point Process (DPP) to roose an unsuervised enalty loss that alleviates mode collase while roducing higher quality samles. DPP is an elegant robabilistic measure used to model negative correlations within a subset and hence quantify its diversity. We use DPP kernel to model the diversity in real data as well as in synthetic data. Then, we devise an objective term that encourages the generator to synthesize data with a similar diversity to real data. In contrast to revious state-of-the-art generative models that tend to use additional trainable arameters or comlex training aradigms, our method does not change the original training scheme. Embedded in an adversarial training and variational autoencoder, our Generative DPP aroach shows a consistent resistance to mode-collase on a wide-variety of synthetic data and natural image datasets including MNIST, CIFAR10, and CelebA, while outerforming state-of-the-art methods for data-efficiency, convergence-time, and generation quality whereas being 5.8x faster than its closest cometitor. Our code, attached to the submission, will be made ublicly available.
"
214,2019,Optimal Minimal Margin Maximization with Boosting,Oral,"Boosting algorithms iteratively roduce linear combinations of more and more base hyotheses and it has been observed exerimentally that the generalization error kees imroving even after achieving zero training error. One oular exlanation attributes this to imrovements in margins. A common goal in a long line of research, is to obtain large margins using as few base hyotheses as ossible, culminating with the AdaBoostV algorithm by Rätsch and Warmuth [JMLR’05]. The AdaBoostV algorithm was later conjectured to yield an otimal trade-off between number of hyotheses trained and the minimal margin over all training oints (Nie, Warmuth, Vishwanathan and Zhang [JMLR’13]). Our main contribution is a new algorithm refuting this conjecture. Furthermore, we rove a lower bound which imlies that our new algorithm is otimal.
"
215,2019,Per-Decision Option Discounting,Oral,"In order to solve comlex roblems, an agent must be able to reason over a sufficiently long horizon. Temoral abstraction, commonly modeled through otions, offers the ability to reason at many time scales, but the horizon length is still determined by the single discount factor of the underlying Markov Decision Process. We roose a modification to the otions framework that allows the agent’s horizon to grow naturally as its actions become more comlex and extended in time. We show that the roosed otion-ste discount controls a bias-variance trade-off, with larger discounts (counter-intuitively) leading to less estimation variance.
"
216,2019,Training Well-Generalizing Classifiers for Fairness Metrics and Other Data-Dependent Constraints,Oral,"Classifiers can be trained with data-deendent constraints to satisfy fairness goals, reduce churn, achieve a targeted false ositive rate, or other olicy goals. We study the generalization erformance for such constrained otimization roblems, in terms of how well the constraints are satisfied at evaluation time, given that they are satisfied at training time. To imrove generalization, we frame the roblem as a two-layer game where one layer otimizes the model arameters on a training dataset, and the other layer enforces the constraints on an indeendent validation dataset. We build on recent work in two-layer constrained otimization to show that if one uses this two-dataset aroach, then constraint generalization can be significantly imroved. As we illustrate exerimentally, this aroach works not only in theory, but also in ractice.
"
217,2019,The Effect of Network Width on Stochastic Gradient Descent and Generalization: an Empirical Study,Oral,"We investigate how the behavior of stochastic gradient descent is influenced by model size. By studying families of models obtained by increasing the number of channels in a base network, we examine how the otimal hyerarameters---the batch size and learning rate at which the test error is minimized---correlate with the network width. We find that the otimal ""normalized noise scale,"" which we define to be a function of the batch size, learning rate and the initialization conditions, is roortional to the number of channels (in the absence of batch normalization). This conclusion holds for MLPs, ConvNets and ResNets. A surrising consequence is that if we wish to maintain otimal erformance as the network width increases, we must use increasingly small batch sizes. Based on our exeriments, we also conjecture that there may be a critical width, beyond which the otimal erformance of networks trained with constant SGD ceases to imrove unless additional regularization is introduced.
"
218,2019,Generalized Linear Rule Models,Oral,"This aer considers generalized linear models using rule-based features, also referred to as rule ensembles, for regression and robabilistic classification. Rules facilitate model interretation while also caturing nonlinear deendences and interactions. Our roblem formulation accordingly trades off rule set comlexity and rediction accuracy. Column generation is used to otimize over an exonentially large sace of rules without re-generating a large subset of candidates or greedily boosting rules one by one. The column generation subroblem is solved using either integer rogramming or a heuristic otimizing the same objective. In exeriments involving logistic and linear regression, the roosed methods obtain better accuracy-comlexity trade-offs than existing rule ensemble algorithms. At one end of the trade-off, the methods are cometitive with less interretable benchmark models.
"
219,2019,Sublinear Space Private Algorithms Under the Sliding Window Model,Oral,"The Differential rivacy overview of Ale states, ``Ale retains the collected data for a maximum of three months."" Analysis of recent data is formalized by the {\em sliding window model}. This begs the question: what is the rice of rivacy in the {sliding window model}. In this aer, we study heavy hitters in the sliding window model with window size $w$. Previous works of~\citet{chan2012differentially} estimates heavy hitters using $O(w)$ sace and incur an error of order $\theta w$ for a constant $\theta 0$. In this aer, we give an efficient differentially rivate algorithm to estimate heavy hitters in the sliding window model with $\widetilde O(w^{34})$ additive error and using $\widetilde O(\sqrt{w})$ sace.
"
220,2019,Optimality Implies Kernel Sum Classifiers are Statistically Efficient,Oral,"We roose a novel combination of otimization tools with learning theory bounds in order to analyze the samle comlexity of otimal classifiers. This contrasts the tyical learning theoretic results which hold for all (otentially subotimal) classifiers.
Our work also justifies assumtions made in rior work on multile kernel learning. As a byroduct of this analysis, we rovide a new form of Rademacher hyothesis sets for considering otimal classifiers.
"
221,2019,Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds,Oral,"Strong worst-case erformance bounds for eisodic reinforcement learning exist 
but fortunately in ractice RL algorithms erform much better than 
such bounds would redict. Algorithms and theory that rovide strong 
roblem-deendent bounds could hel illuminate the key features of what 
makes a RL roblem hard and reduce the barrier to using RL algorithms 
in ractice. As a ste towards this
we derive an algorithm and analysis for finite horizon discrete MDPs br 
with state-of-the-art worst-case regret bounds and substantially tighter bounds if the RL 
environment has secial features but without ariori 
knowledge of the environment from the algorithm. As a result of our analysis, 
we also hel address an oen learning theory question~\cite{jiang2018oen} 
about eisodic MDPs with a constant uer-bound on the sum of rewards, 
roviding a regret bound function of the number of eisodes with no 
deendence on the horizon.
"
222,2019,Bayesian Joint Spike-and-Slab Graphical Lasso,Oral,"In this article, we roose a new class of riors for Bayesian inference with multile Gaussian grahical models. We introduce Bayesian treatments of two oular rocedures, the grou grahical lasso and the fused grahical lasso, and extend them to a continuous sike-and-slab framework to allow self-adative shrinkage and model selection simultaneously.  We develo an EM algorithm that erforms fast and dynamic exlorations of osterior modes. Our aroach selects sarse models efficiently and automatically with substantially smaller bias than would be induced by alternative regularization rocedures. The erformance of the roosed methods are demonstrated through simulation and two real data examles.
"
223,2019,Graph U-Nets,Oral,"We consider the roblem of reresentation learning for grah data. Convolutional neural networks can naturally oerate on images, but have significant challenges in dealing with grah data. Given images are secial cases of grahs with nodes lie on 2D lattices, grah embedding tasks have a natural corresondence with image ixel-wise rediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully alied on many image ixel-wise rediction tasks, similar methods are lacking for grah data. This is due to the fact that ooling and u-samling oerations are not natural on grah data. To address these challenges, we roose novel grah ooling (gPool) and unooling (gUnool) oerations in this work. The gPool layer adatively selects some nodes to form a smaller grah based on their scalar rojection values on a trainable rojection vector. We further roose the gUnool layer as the inverse oeration of the gPool layer. The gUnool layer restores the grah into its original structure using the osition information of nodes selected in the corresonding gPool layer. Based on our roosed gPool and gUnool layers, we develo an encoder-decoder model on grah, known as the grah U-Nets. Our exerimental results on node classification and grah classification tasks demonstrate that our methods achieve consistently better erformance than revious models.
"
224,2019,Co-Representation Network for Generalized Zero-Shot Learning,Oral,"Generalized zero-shot learning is a significant toic but faced with bias roblem, which leads to unseen classes being easily misclassified into seen classes. Hence we roose a embedding model called co-reresentation network to learn a more uniform visual embedding sace that effectively alleviates the bias roblem and hels with classification. We mathematically analyze our model and find it learns a rojection with high local linearity, which is roved to cause less bias roblem. The network consists of a cooeration module for reresentation and a relation module for classification, it is simle in structure and can be easily trained in an end-to-end manner. Exeriments show that our method outerforms existing generalized zero-shot learning methods on several  benchmark datasets.
"
225,2019,Conditional Gradient Methods via Stochastic Path-Integrated Differential Estimator,Oral,"We roose a class of novel variance-reduced stochastic conditional gradient methods. By adoting the recent stochastic ath-integrated differential estimator technique (SPIDER) of Fang et. al. (2018) for the classical Frank-Wolfe (FW) method, we introduce SPIDER-FW for finite-sum minimization as well as the more general exectation minimization roblems. SPIDER-FW enjoys suerior comlexity guarantees in the non-convex setting, while matching the best known FW variants in the convex case. We also extend our framework a la conditional gradient sliding (CGS) of Lan &am; Zhou. (2016), and roose SPIDER-CGS to further reduce the stochastic first-order oracle comlexity. Our numerical evidence suorts our theoretical findings, and demonstrates the sueriority of SPIDER-FW and SPIDER-CGS.
"
226,2019,"Fast Incremental von Neumann Graph Entropy Computation: Theory, Algorithm, and Applications",Oral,"The von Neumann grah entroy (VNGE) facilitates the measure of information divergence and distance between grahs in a grah sequence and has successfully been alied to various learning tasks driven by network-based data. Albeit its effectiveness, it is comutationally demanding by requiring the full eigensectrum of the grah Lalacian matrix.  In this aer, we roose a fast incremental von Neumann grah entroy (FINGER) framework, which aroaches VNGE with a erformance guarantee. FINGER reduces the cubic comlexity of VNGE to linear comlexity in the number of nodes and edges, and thus enables online comutation based on incremental grah changes. We also show asymtotic equivalency of FINGER to the exact VNGE, and derive its aroximation error bounds. Based on FINGER, we roose efficient algorithms for comuting Jensen-Shannon distance between grahs. Our exerimental results on different random grah models demonstrate the comutational efficiency and the asymtotic equivalency of FINGER. In addition, we also aly FINGER to two real-world alications and one synthesized anomaly detection dataset, and corroborate its suerior erformance over seven baseline grah similarity methods.
"
227,2019,The Implicit Fairness Criterion of Unconstrained Learning,Oral,"We clarify what fairness guarantees we can and cannot exect to follow from unconstrained machine learning. Secifically, we show that in many settings, unconstrained learning on its own imlies grou calibration, that is, the outcome variable is conditionally indeendent of grou membershi given the score. 
A lower bound confirms the otimality of our uer bound. Moreover, we rove that as the excess risk of the learned score decreases, the more strongly it violates searation and indeendence, two other standard fairness criteria. Our results challenge the view that grou calibration necessitates an active intervention, suggesting that often we ought to think of it as a byroduct of unconstrained machine learning. 
"
228,2019,AdaGrad stepsizes: sharp convergence over nonconvex landscapes,Oral,"Adative gradient methods such as AdaGrad and its variants udate the stesize in stochastic gradient descent on the fly according to the gradients received along the way; such methods have gained widesread use in large-scale otimization for their ability to converge robustly, without the need to fine tune
arameters such as the stesize schedule. Yet, the theoretical guarantees to date for AdaGrad are for online and convex otimization.   We bridge this ga by roviding strong theoretical guarantees for the convergence of AdaGrad over smooth, nonconvex landscaes.  We show that AdaGrad converges to a stationary oint at the otimal $O(1\sqrt{N})$ rate (u to a $\log(N)$ factor), and  at the otimal $O(1N)$ rate in the non-stochastic setting  . In articular, both our theoretical and numerical  results imly that AdaGrad is robust to the \emh{unknown Lischitz constant and level of stochastic noise on the gradient, in a near-otimal sense.  } "
229,2019,A Theory of Regularized Markov Decision Processes,Oral,"Many recent successful (dee) reinforcement learning algorithms make use of regularization, generally based on entroy or on Kullback-Leibler divergence. We roose a general theory of regularized Markov Decision Processes that generalizes these aroaches in two directions: we consider a larger class of regularizers, and we consider the general modified olicy iteration aroach, encomassing both olicy iteration and value iteration. The core building blocks of this theory are a notion of regularized Bellman oerator and the Legendre-Fenchel transform, a classical tool of convex otimizatoin. This aroach allows for error roagation analyses of general algorithmic schemes of which (ossibly variants of) classical algorithms such as Trust Region Policy Otimization, Soft Q-learning, Stochastic Actor Critic or Dynamic Policy Programming are secial cases. This also draws connections to roximal convex otimization, esecially to Mirror Descent.
"
230,2019,SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver,Oral,"Integrating logical reasoning within dee learning architectures has been a major goal of modern AI systems. In this aer, we roose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loo of larger dee learning systems. Our (aroximate) solver is based uon a fast coordinate descent aroach to solving the semidefinite rogram (SDP) associated with the MAXSAT roblem. We show how to analytically differentiate through the solution to this SDP and efficiently solve the associated backward ass. We demonstrate that by integrating this solver into end-to-end learning systems, we can learn the logical structure of challenging roblems in a minimally suervised fashion. In articular, we show that we can learn the arity function using single-bit suervision (a traditionally hard task for dee networks) and learn how to lay 9x9 Sudoku solely from examles. We also solve a ``visual Sudoku'' roblem that mas images of Sudoku uzzles to their associated logical solutions by combining our MAXSAT solver with a traditional convolutional architecture. Our aroach thus shows romise in integrating logical structures within dee learning.
"
231,2019, Fault Tolerance in Iterative-Convergent Machine Learning,Oral,"Machine learning (ML) training algorithms often ossess an inherent self-correcting behavior due to their iterative- convergent nature. Recent systems exloit this roerty to achieve adatability and efficiency in unreliable comuting environments by relaxing the consistency of execution and allowing calculation errors to be self-corrected during training. However, the behavior of such systems are only well understood for secific tyes of calculation errors, such as those caused by staleness, reduced recision, or asynchronicity, and for secific algorithms, such as stochastic gradient descent. In this aer, we develo a general framework to quantify the effects of calculation errors on iterative-convergent algorithms. We then use this framework to derive a worst-case uer bound on the cost of arbitrary erturbations to model arameters during training and to design new strategies for checkoint-based fault tolerance. Our system, SCAR, can reduce the cost of artial failures by 78%–95% when comared with traditional checkoint-based fault tolerance across a variety of ML models and training algorithms, roviding near-otimal erformance in recovering from failures.
"
232,2019,Rotation Invariant Householder Parameterization for Bayesian PCA,Oral,"We consider robabilistic PCA and related factor models from a Bayesian ersective. These models are in general not identifiable as the likelihood has a rotational symmetry. This gives rise to comlicated osterior distributions with continuous subsaces of equal density and thus hinders efficiency of inference as well as interretation of obtained arameters. In articular, osterior averages over factor loadings become meaningless and only model redictions are unambiguous. Here, we roose a arameterization based on Householder transformations, which remove the rotational symmetry of the osterior. Furthermore, by relying on results from random matrix theory, we establish the arameter distribution which leaves the model unchanged comared to the original rotationally symmetric formulation. In articular, we avoid the need to comute the Jacobian determinant of the arameter transformation. This allows us to efficiently imlement robabilistic PCA in a rotation invariant fashion in any state of the art toolbox. Here, we imlemented our model in the robabilistic rogramming language Stan and illustrate it on several examles.
"
233,2019,GEOMetrics: Exploiting Geometric Structure for Graph-Encoded Objects,Oral,"Mesh models are a romising aroach for encoding the structure of 3D objects. Current mesh reconstruction systems redict uniformly distributed vertex locations of a redetermined grah through a series of grah convolutions, leading to comromises with resect to erformance or resolution. In this aer, we argue that the grah reresentation of geometric objects allows for additional structure, which should be leveraged for enhanced reconstruction. Thus, we roose a system which roerly benefits from the advantages of the geometric structure of grah-encoded objects by introducing (1) a grah convolutional udate reserving vertex information; (2) an adative slitting heuristic allowing detail to emerge; and (3) a training objective oerating both on the local surfaces defined by vertices as well as the global structure defined by the mesh. Our roosed method is evaluated on the task of 3D object reconstruction from images with the ShaeNet dataset, where we demonstrate state of the art erformance, both visually and numerically, while having far smaller sace requirements by generating adative meshes.
"
234,2019,Locally Private Bayesian Inference for Count Models,Oral,"We resent a general method for rivacy-reserving Bayesian inference in Poisson factorization, a broad class of models that includes some of the most widely used models in the social sciences. Our method satisfies limited recision local rivacy, a generalization of local differential rivacy, which we introduce to formulate rivacy guarantees aroriate for sarse count data. We develo an MCMC algorithm that aroximates the locally rivate osterior over model arameters given data that has been locally rivatized by the geometric mechanism (Ghosh et al., 2012). Our solution is based on two insights: 1) a novel reinterretation of the geometric mechanism in terms of the Skellam distribution (Skellam, 1946) and 2) a general theorem that relates the Skellam to the Bessel distribution (Yuan &am; Kalbfleisch, 2000). We demonstrate our method in two case studies on real-world email data in which we show that our method consistently outerforms the commonly-used \naive aroach, obtaining higher quality toics in text and more accurate link rediction in networks. On some tasks, our rivacy-reserving method even outerforms non-rivate inference which conditions on the true data.
"
235,2019,Weak Detection of Signal in the Spiked Wigner Model,Oral,"We consider the roblem of detecting the resence of the signal in a rank-one signal-lus-noise data matrix. In case the signal-to-noise ratio is under the threshold below which a reliable detection is imossible, we roose a hyothesis test based on the linear sectral statistics of the data matrix. The error of the roosed test is otimal as it matches the error of the likelihood ratio test that minimizes the sum of the Tye-I and Tye-II errors. The test is data-driven and does not deend on the distribution of the signal or the noise. If the density of the noise is known, it can be further imroved by an entrywise transformation to lower the error of the test.
"
236,2019,Static Automatic Batching In TensorFlow,Oral,"Dynamic neural networks are becoming increasingly common, and yet it is hard to imlement them efficiently. One-the-fly oeration batching for such models is sub-otimal and suffers from run time overheads, while writing manually batched versions can be hard and error-rone. To address this we extend TensorFlow with for, a arallel-for loo otimized using static loo vectorization. With for, users can exress comutation using nested loos and conditional constructs, but get erformance resembling that of a manually batched version. Benchmarks demonstrate seedus of one to two orders of magnitude on range of tasks, from jacobian comutation, to TreeLSTMs. 
"
237,2019,Discovering Options for Exploration by Minimizing Cover Time,Oral,"One of the main challenges in reinforcement learning is on solving tasks with sarse reward. We first show that the difficulty of discovering the rewarding state is bounded by the exected cover time of the underlying random walk induced by a olicy. We roose a method to discover otions automatically which reduce the cover time so as to seed u the exloration in sarse reward domains. We show emirically that the roosed algorithm successfully reduces the cover time, and imroves the erformance of the reinforcement learning agents.
"
238,2019,Area Attention,Oral,"Existing attention mechanisms are trained to attend to individual items in a collection (the memory) with a redefined, fixed granularity, e.g., a word token or an image grid. We roose area attention: a way to attend to areas in the memory, where each area contains a grou of items that are structurally adjacent, e.g., satially for a 2D memory such as images, or temorally for a 1D memory such as natural language sentences. Imortantly, the shae and the size of an area are dynamically determined via learning, which enables a model to attend to information with varying granularity. Area attention can easily work with existing model architectures such as multi-head attention for simultaneously attending to multile areas in the memory. We evaluate area attention on two tasks: neural machine translation (both character and token-level) and image cationing, and imrove uon strong (state-of-the-art) baselines in all the cases. These imrovements are obtainable with a basic form of area attention that is arameter free.
"
239,2019,Variational Inference for sparse network reconstruction from count data,Oral,"Networks rovide a natural yet statistically grounded way to deict and understand how a set of entities interact. However, in many situations interactions are not directly observed and the network needs to be reconstructed based on observations collected for each entity. Our work focuses on the situation where these observations consist of counts. A tyical examle is the reconstruction of an ecological network based on abundance data. In this setting, the abundance of a set of secies is collected in a series of samles andor environments and we aim at inferring direct interactions between the secies. The abundances at hand can be, for examle, direct counts of individuals (ecology of macro-organisms) or read counts resulting from metagenomic sequencing (microbial ecology). 

Whatever the aroach chosen to infer such a network, it has to account for the eculiaraties of the data at hand. The first, obvious one, is that the data are counts, i.e. non continuous. Also, the observed counts often vary over many orders of magnitude and are more disersed than exected under a simle model, such as the Poisson distribution. The observed counts may also result from different samling efforts in each samle andor for each entity, which hamers direct comarison. Furthermore, because the network is suosed to reveal only direct interactions, it is highly desirable to account for covariates describing the environment to avoid surious edges.

Many methods of network reconstruction from count data have been roosed. In the context of microbial ecology, most methods (SarCC, REBACCA, SPIEC-EASI, gCODA, BanOCC) rely on a two-ste strategy: transform the counts to seudo Gaussian observations using simle transforms before moving back to the setting of Gaussian Grahical Models, for which state of the art methods exist to infer the network, but only in a Gaussian world. In this work, we consider instead a full-fledged robabilistic model with a latent layer where the counts follow Poisson distributions, conditional to latent (hidden) Gaussian correlated variables. In this model, known as Poisson log-normal (PLN), the deendency structure is comletely catured by the latent layer and we model counts, rather than transformations thereof. To our knowledge, the PLN framework is quite new and has only been used by two other recent methods (Mint and lnDAG) to reconstruct networks from count data. In this work, we use the same mathematical framework but adot a different otimization strategy which alleviates the whole otimization rocess. We also fully exloit the connection between the PLN framework and generalized linear models to account for the eculiarities of microbiological data sets.

The network inference ste is done as usual by adding sarsity inducing constraints on the inverse covariance matrix of the latent Gaussian vector to select only the most imortant interactions between secies. Unlike the usual Gaussian setting, the enalized likelihood is generally not tractable in this framework. We resort instead to a variational aroximation for arameter inference and solve the corresonding otimization roblem by alternating a gradient descent on the variational arameters and a grahical-Lasso ste on the covariance matrix. We also select the sarsity arameter using the resamling-based StARS rocedure.

We show that the sarse PLN aroach has better erformance than existing methods on simulated datasets and that it extracts relevant signal from microbial ecology datasets. We also show that the inference scales to datasets made u of hundred of secies and samles, in line with other methods in the field.

In short, our contributions to the field are the following: we extend the use of PLN distributions in network inference by (i) accounting for covariates and offset and thus removing some surious edges induced by confounding factors, (ii) accounting for different samling effort to  integrate data sets from different sources and thus infer interactions between different tyes of organisms (e.g. bacteria - fungi), (iii) develoing an inference rocedure based on the iterative otimization of a well defined objective function. Our objective function is a rovable lower bound of the observed likelihood and our rocedure accounts for the uncertainty associated with the estimation of the latent variable, unlike the algorithm resented in Mint and lnDAG.
"
240,2019,A Framework for Bayesian Optimization in Embedded Subspaces,Oral,"We resent a theoretically founded aroach for high-dimensional Bayesian otimization based on low-dimensional subsace embeddings.  We
rove that the error in the Gaussian rocess model is  bounded  tightly  when  going  from  the  original high-dimensional search domain to the low-dimensional embedding. This imlies that the otimization rocess in the low-dimensional embedding roceeds essentially as if it were run directly on the unknown active subsace. The argument alies to a large class of algorithms and GP mod-
els, including non-stationary kernels. Moreover, we rovide an efficient imlementation based on hashing and demonstrate emirically that this sub-
sace embedding achieves considerably better results than the reviously roosed methods for high-dimensional BO based on Gaussian matrix
rojections and structure-learning.
"
241,2019,Beyond Backprop: Online Alternating Minimization with Auxiliary Variables,Oral,"Desite significant recent advances in dee neural networks,  training them remains a challenge  due to highly non-convex nature of the objective function.  State-of-art methods rely on error backroagation, which suffers from   several well-known issues, such as vanishing and exloding gradients, inability to handle non-differentiable nonlinearities and to arallelize weight-udate across layers, and biological imlausibility. These limitations continue to motivate exloration of alternative training algorithms,  including several recently roosed auxiliary-variable methods  which break the comlex nested objective function into local subroblems,   avoiding gradient chains and thus the  vanishing gradient  issue, allowing weight udate arallelization, among other advantages.  However, those techniques are mainly offline (batch), which limits their alicability to   extremely large datasets or  unlimited data streams in online, continual or reinforcement learning.  The main contribution of our work is a novel online (stochasticmini-batch) alternating minimization (AM) algorithm for training dee neural networks, together  with  the first theoretical convergence guarantees for AM in stochastic settings, and  extensive emirical evaluation on various architectures  and datasets, demonstrating advantages of the roosed aroach  as comared to both offline auxiliary variable methods and to the backroagation-based stochastic gradient descent.
"
242,2019,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,Oral,"Convolutional Neural Networks (ConvNets) are commonly develoed at a fixed comutational cost, and then scaled u for better accuracy if more resources are given. Conventional ractice is to arbitrarily make ConvNets deeer or wider, or use larger image resolution, but is there a more rinciled method to scale u a ConvNet? In this aer, we systematically study this roblem and identify that carefully balancing network deth, width, and resolution can lead to better accuracy and efficiency. Based on this observation, we roose a new scaling method that uniformly scales all dimensions of network dethwidthresolution using a simle yet highly effective comound coefficient. Results show our method imroves the erformance on scaling u rior MobileNets. To further demonstrate the effectiveness of our scaling method, we also develo a new mobile-size EMNAS-B0 baseline, and scale it u to achieve state-of-the-art 84.4% to-1  97.1% to-5 accuracy on ImageNet, but being 8.4x smaller and 6x faster on inference than the best existing ConvNet (Huang et al., 2018). Our scaled EMNAS models also achieve new state-of-the-art accuracy on five commonly used transfer learning datasets, such as CIFAR-100 (91.7%) and Flowers (98.8%), with an order of magnitude fewer arameters.
"
243,2019,Low Latency Privacy Preserving Inference,Oral,"When alying machine learning to sensitive data, one has to balance between accuracy, information leakage, and comutational-comlexity. Recent studies combined Homomorhic Encrytion with neural networks to make inferences while rotecting against information leakage. However, these methods are limited by the width and deth of neural networks that can be used (and hence the accuracy) and exhibit high latency even for relatively simle networks. In this study we rovide two solutions that address these limitations. In the first solution, we resent more than 10x imrovement in latency and enable inference on wider networks comared to rior attemts with the same level of security. The imroved erformance is achieved by novel methods to reresent the data during the comutation. In the second solution, we aly the method of transfer learning to rovide rivate inference services using dee networks with latency lower than 0.2 seconds. We demonstrate the efficacy of our methods on several comuter vision tasks.
"
244,2019,Policy Certificates: Towards Accountable Reinforcement Learning,Oral,"The erformance of a reinforcement learning algorithm can vary drastically during learning because of exloration. Existing algorithms rovide little information about the quality of their current olicy before executing it, and thus have limited use in high-stakes alications, such as healthcare. We address this lack of accountability by roosing that algorithms outut olicy certificates. These certificates bound the sub-otimality and return of the olicy in the next eisode, allowing humans to intervene when the certified quality is not satisfactory. We further introduce two new algorithms with certificates and resent a new  framework for theoretical analysis that guarantees the quality of their olicies and certificates. For tabular MDPs, we show that comuting certificates can even imrove the samle-efficiency of otimism-based exloration. As a result, one of our algorithms achieves regret and PAC bounds that are tighter than state of the art and minimax u to lower-order terms.
"
245,2019,Convolutional Poisson Gamma Belief Network,Oral,"To analyze a text corus, one often resorts to a lossy reresentation that either comletely ignores word order or embeds the words as low-dimensional dense feature vectors. In this aer, we roose convolutional Poisson factor analysis (CPFA) that directly oerates on a lossless reresentation that rocesses the words in each document as a sequence of high-dimensional one-hot vectors. To boost its erformance, we further roose the convolutional Poisson gamma belief network (CPGBN) that coules CPFA with the gamma belief network via a novel robabilistic ooling layer. CPFA forms words into hrases and catures very secific hrase-level toics, and CPGBN further builds a hierarchy of increasingly more general hrase-level toics. We develo both an uward-downward Gibbs samler, which makes the comutation feasible by exloiting the extreme sarsity of the one-hot vectors, and a Weibull distribution based convolutional variational auto-encoder that makes CPGBN become even more scalable in both training and testing. Exerimental results demonstrate that CPGBN can extract high-quality text latent reresentations that cature the word order information, and hence can be leveraged as a building block to enrich a wide variety of existing discrete latent variable models that ignore word order.
"
246,2019,The Evolved Transformer,Oral,"Recent works have highlighted the strengths of the Transformer architecture for dealing with sequence tasks. At the same time, neural architecture search has advanced to the oint where it can outerform human-designed models.  The goal of this work is to use neural architecture search to design a better Transformer architecture.  We first construct a large search sace insired by the recent advances in feed-forward sequential models  and  then  run  evolutionary  architecture search,  seeding  our  initial  oulation  with  the Transformer. To effectively run this search on the comutationally exensive WMT 2014 English-German translation task, we develo the rogressive dynamic hurdles (PDH) method, which allows us to dynamically allocate more resources to more romising candidate models. The architecture found in our exeriments - the Evolved Transformer (ET) - demonstrates consistent imrovement  over  the  Transformer  on  four  well-established language tasks: WMT 2014 English-German, WMT 2014 English-French, WMT 2014 English-Czech and LM1B. At big model size, the Evolved Transformer is twice as efficient as the Transformer in terms of FLOPS without loss in quality.   At  a  much  smaller  –  mobile-friendly – model size of ~7M arameters,  the Evolved Transformer outerforms the Transformer by 0.8 BLEU on WMT’14 English-German.
"
247,2019,Simplifying Graph Convolutional Networks,Oral,"Grah Convolutional Networks (GCNs) and their variants have exerienced significant attention and have become the de facto methods for learning grah reresentations. 
GCNs derive insiration rimarily from recent dee learning aroaches, and as a result, may inherit unnecessary comlexity and redundant comutation. 
In this aer, we reduce this excess comlexity through successively removing nonlinearities and collasing weight matrices between consecutive layers. 
We theoretically analyze the resulting linear model and show that it corresonds to a fixed low-ass filter followed by a linear classifier. 
Notably, our exerimental evaluation demonstrates that these simlifications do not negatively imact accuracy in many down-stream alications.
Moreover, the resulting model scales to larger datasets, is naturally interretable, and yields u to two orders of magnitude seedu over FastGCN. 
"
248,2019,SWALP : Stochastic Weight Averaging in Low Precision Training,Oral,"Low recision oerations can rovide scalability, memory savings, ortability, and energy efficiency. This aer rooses SWALP, an aroach to low recision training that averages low-recision SGD iterates with a modified learning rate schedule. 
SWALP is easy to imlement and can match the erformance of full-recision SGD even with all numbers quantized down to 8 bits, including the gradient accumulators. Additionally, we show that SWALP converges arbitrarily close to the otimal solution for quadratic objectives, and to a noise ball asymtotically smaller than low recision SGD in strongly convex settings. 
"
249,2019,Rademacher Complexity for Adversarially Robust Generalization,Oral,"Many machine learning models are vulnerable to adversarial attacks; for examle, adding adversarial erturbations that are imercetible to humans can often make machine learning models roduce wrong redictions with high confidence. Moreover, although we may obtain robust models on the training dataset via adversarial training, in some roblems the learned models cannot generalize well to the test data. In this aer, we focus on $\ell_\infty$ attacks, and study the adversarially robust generalization roblem through the lens of Rademacher comlexity. For binary linear classifiers, we rove tight bounds for the adversarial Rademacher comlexity, and show that the adversarial Rademacher comlexity is never smaller than its natural counterart, and it has an unavoidable dimension deendence, unless the weight vector has bounded $\ell_1$ norm. The results also extend to multi-class linear classifiers. For (nonlinear) neural networks, we show that the dimension deendence in the adversarial Rademacher comlexity also exists. We further consider a surrogate adversarial loss for one-hidden layer ReLU network and rove margin bounds for this setting. Our results indicate that having $\ell_1$ norm constraints on the weight matrices might be a otential way to imrove generalization in the adversarial setting. We demonstrate exerimental results that validate our theoretical findings. "
250,2019,Geometry Aware Convolutional Filters for Omnidirectional Images Representation,Oral,"Due to their wide field of view, omnidirectional cameras are frequently used by autonomous vehicles, drones and robots for navigation and other comuter vision tasks. The images catured by such cameras, are often analyzed and classified with techniques designed for lanar images that unfortunately fail to roerly handle the native geometry of such images and therefore results in subotimal erformance. In this aer we aim at imroving oular dee convolutional neural networks so that they can roerly take into account the secific roerties of omnidirectional data. In articular we roose an algorithm that adats convolutional layers, which often serve as a core building block of a CNN, to the roerties of omnidirectional images. Thus, our filters have a shae and size that adat to the location on the omnidirectional image. We show that our method is not limited to sherical surfaces and is able to incororate the knowledge about any kind of rojective geometry inside the dee learning network. As deicted by our exeriments, our method outerforms the existing dee neural network techniques for omnidirectional image classification and comression tasks.
"
251,2019,Improving Neural Network Quantization without Retraining using Outlier Channel Splitting,Oral,"Quantization can imrove the execution latency and energy efficiency of neural networks on both commodity GPUs and secialized accelerators. The majority of existing literature focuses on training quantized DNNs, while this work examines the less-studied toic of quantizing a floating-oint model without (re)training. DNN weights and activations follow a bell-shaed distribution ost-training, while ractical hardware uses a linear quantization grid. This leads to challenges in dealing with outliers in the distribution. Prior work has addressed this by cliing the outliers or using secialized hardware. In this work, we roose outlier channel slitting (OCS), which dulicates channels containing outliers, then halves the channel values. The network remains functionally identical, but affected outliers are moved toward the center of the distribution. OCS requires no additional training and works on commodity hardware. Exerimental evaluation on ImageNet classification and language modeling shows that OCS can outerform state-of-the-art cliing techniques with only minor overhead.
"
252,2019,Communication Complexity in Locally Private Distribution Estimation and Heavy Hitters,Oral,"We consider the roblems of distribution estimation and frequencyheavy hitter estimation under local differential rivacy (LDP), and communication constraints. While each constraint has been studied searately, otimal schemes for one are sub-otimal for the other. We rovide a one-bit \es-LDP scheme that requires no shared randomness and has the otimal erformance. We also show that a recently roosed scheme (Acharya et al., 2018b) for \es-LDP distribution estimation is also otimal for frequency estimation. Finally, we show that if we consider LDP schemes for heavy hitter estimation that do not use shared randomness then their communication budget must be w(1) bits.
"
253,2019,Automatic Posterior Transformation for Likelihood-Free Inference,Oral,"How can one erform Bayesian inference on stochastic simulators with intractable likelihoods? A recent aroach is to learn the osterior from adatively roosed simulations using neural-network based conditional density estimators. However, existing methods are limited to a narrow range of roosal distributions or require imortance-weighting that can limit erformance in ractice. Here we resent automatic osterior transformation (APT), a new aroach for simulation-based inference via neural osterior estimation. APT is able to modify the osterior estimate using arbitrary, dynamically udated roosals, and is comatible with owerful flow-based density estimators. We show that APT is more flexible, scalable and efficient than revious simulation-based inference techniques and can directly learn informative features from high-dimensional and time series data.
"
254,2019,A Personalized Affective Memory Model for Improving Emotion Recognition,Oral,"Recent models of emotion recognition strongly rely on suervised dee learning solutions for the distinction of general emotion exressions. However, they are not reliable when recognizing online and ersonalized facial exressions, e.g., for erson-secific affective understanding. In this aer, we resent a neural model based on a conditional adversarial autoencoder to learn how to reresent and edit general emotion exressions. We then roose Grow-When-Required networks as ersonalized affective memories to learn individualized asects of emotion exressions. Our model achieves state-of-the-art erformance on emotion recognition when evaluated on \textit{in-the-wild} datasets. Furthermore, our exeriments include ablation studies and neural visualizations in order to exlain the behavior of our model.
"
255,2019,Robust Influence Maximization for Hyperparametric Models,Oral,"In this aer we study the roblem of robust influence maximization in the indeendent cascade model under a hyerarametric assumtion. In social networks users influence and are influenced by individuals with similar characteristics and as such they are associated with some features. A recent surging research direction in influence maximization focuses on the case where the edge robabilities on the grah are not arbitrary but are generated as a function of the features of the users and a global hyerarameter. We roose a model where the objective is to maximize the worst-case number of influenced users for any ossible value of that hyerarameter. We rovide theoretical results showing that roer robust solution in our model is NP-hard and an algorithm that achieves imroer robust otimization. We make-use of samling based techniques and of the renowned multilicative weight udates algorithm. Additionally we validate our method emirically and rove that it outerforms the state-of-the-art robust influence maximization techniques.
"
256,2019,Action Robust Reinforcement Learning and Applications in Continuous Control,Oral,"A olicy is said to be robust if it maximizes the reward while considering a bad, or even adversarial, model. In this work we formalize two new criteria of robustness to action uncertainty. 
Secifically, we consider two scenarios in which the agent attemts to erform an action $\action$, and (i) with robability $\alha$, an alternative adversarial action $\bar \action$ is taken, or (ii) an adversary adds a erturbation to the selected action in the case of continuous action sace. We show that our criteria are related to common forms of uncertainty in robotics domains, such as the occurrence of abrut forces, and suggest algorithms in the tabular case. Building on the suggested algorithms, we generalize our aroach to dee reinforcement learning (DRL) and rovide extensive exeriments in the various MuJoCo domains. 
Our exeriments show that not only does our aroach roduce robust olicies, but it also imroves the erformance in the absence of erturbations. 
This generalization indicates that action-robustness can be thought of as imlicit regularization in RL roblems."
257,2019,Poission Subsampled R\'enyi Differential Privacy,Oral,"We consider the roblem of rivacy-amlification by under the Renyi Differential Privacy framework. This is the main technique underlying the moments accountants (Abadi et al., 2016) for differentially rivate dee learning. 
Unlike revious attemts on this roblem which deals with Samling with Relacement, we consider the Poisson subsamling scheme which selects each data oint indeendently with a coin toss. This allows us to significantly simlify and tighten the bounds for the RDP of subsamled mechanisms and derive numerically stable aroximation schemes.  In articular, for subsamled Gaussian mechanism and subsamled Lalace mechanism, we rove an analytical formula of their RDP that exactly matches the lower bound. The result is the first of its kind and we numerically demonstrate an order of magnitude imrovement in the rivacy-utility tradeoff. 
"
258,2019,Provably efficient RL with Rich Observations via Latent State Decoding,Oral,"We study the exloration roblem in eisodic MDPs with rich observations generated from a small number of latent states. Under certain identifiability assumtions, we demonstrate how to estimate a maing from the observations to latent states inductively through a sequence of regression and clustering stes---where reviously decoded latent states rovide labels for later regression roblems---and use it to construct good exloration olicies. We rovide finite-samle guarantees on the quality of the learned state decoding function and exloration olicies, and comlement our theory with an emirical evaluation on a class of hard exloration roblems. Our method exonentially imroves over $Q$-learning with na\""ive exloration, even when $Q$-learning has cheating access to latent states."
259,2019,Memory-Optimal Direct Convolutions for Maximizing Classification Accuracy in Embedded Applications,Oral,"In the age of Internet of Things (IoT), embedded devices ranging from ARM Cortex M0s with 100s of KB of RAM to Arduinos with 2KB RAM are exected to erform increasingly intelligent classification tasks, such as voice and gesture recognition, activity tracking, and biometric security. While convolutional neural networks (CNNs), together with sectrogram rerocessing, are a natural solution to many of these classification tasks, storage of the network's activations often exceeds the hard memory constraints of embedded latforms. This aer resents memory-otimal direct convolutions as a way to ush classification accuracy as high as ossible given strict hardware memory constraints at the exense of extra comute, exloring the oosite end of the comute-memory trade-off curve from standard aroaches that minimize latency at the exense of extra memory. We evaluate classification accuracy across a variety of small image and time series datasets emloying memory-otimal CNNs and memory-efficient sectrogram rerocessing. We also validate the memory-otimal CNN technique with an Arduino imlementation of the 10-class MNIST classification task, fitting the network secification, weights, and activations entirely within 2KB SRAM and achieving a state-of-the-art classification accuracy for small-scale embedded systems of 99.15%.
"
260,2019,Efficient optimization of loops and limits with randomized telescoping sums,Oral,"We consider otimization roblems in which the objective requires an inner loo with many stes or is the limit of a sequence of increasingly costly aroximations.
Meta-learning, training recurrent neural networks, and otimization of the solutions to differential equations are all examles of otimization roblems with this character.
In such roblems, it can be exensive to comute the objective function value and its gradient, but truncating the loo or using less accurate aroximations can induce biases that damage the overall solution.
We roose \emh{randomized telescoe} (RT) gradient estimators, which reresent the objective as the sum of a telescoing series and samle linear combinations of terms to rovide chea unbiased gradient estimates.
We identify conditions under which RT estimators achieve otimization convergence rates indeendent of the length of the loo or the required accuracy of the aroximation.
We also derive a method for tuning RT estimators online to maximize a lower bound on the exected decrease in loss er unit of comutation.
We evaluate our adative RT estimators on a range of alications including meta-otimization of learning rates, variational inference of ODE arameters, and training an LSTM to model long sequences.
"
261,2019,Jumpout : Improved Dropout for Deep Neural Networks with ReLUs,Oral,"Droout is a simle and effective way to imrove the generalization erformance of 
dee neural networks (DNNs) and revent overfitting.
This aer discusses three novel observations about droout
when alied to DNNs with rectified linear unit (ReLU): 
1) droout encourages each local linear model of a DNN to be trained on data
oints from nearby regions; 2) alying the same droout rate to different layers 
can result in significantly different (effective) deactivation rates; 
and 3) when batch normalization is also used, the rescaling factor of droout 
causes a normalization inconsistency between training and testing.
The above leads to three simle but nontrivial droout modifications resulting
in our roosed method ``jumout.''
Jumout samles the droout rate
from a monotone decreasing distribution (e.g., the right half of a Gaussian), 
so each local linear model is trained, with high robability, 
to work better for data oints from nearby than from more distant regions.
Jumout moreover adatively normalizes the droout rate at each
layer and every training batch, so the effective deactivation rate
alied to the activated neurons are ket the same.
Furthermore, it rescales the oututs for a better trade-off that kees both
the variance and mean of neurons more consistent between
training and test hases, thereby mitigating the incomatibility between
droout and batch normalization. 
Jumout shows significantly imroved erformance on 
CIFAR10, CIFAR100, Fashion-MNIST, STL10, SVHN,  ImageNet-1k, etc.,
while introducing negligible additional memory and comutation costs.
"
262,2019,DL2: Training and Querying Neural Networks with Logic,Oral,"We resent DL2, a system for training and querying neural networks with logical constraints. Using DL2, one can declaratively secify domain knowledge to be enforced during training or ose queries on the model with the goal of finding inuts that satisfy a set of constraints. DL2 works by translating logical constraints into a differentiable loss with desirable mathematical roerties, then minimized with standard gradient-based methods.  We evaluate DL2 by training networks with interesting constraints in unsuervised, semi-suervised and suervised settings. Our exerimental evaluation demonstrates that DL2 is both, more exressive than rior aroaches combining logic and neural networks, and its resulting loss is better suited for otimization. Further, we show that for a number of queries, DL2 can find the desired inuts within seconds (even for large models such as ResNet-50 on ImageNet).
"
263,2019,Stochastic Deep Networks,Oral,"Machine learning is increasingly targeting areas where inut data cannot be accurately described by a single vector, but can be modeled instead using the more flexible concet of random vectors, namely robability measures or more simly oint clouds of varying cardinality. Using dee architectures on measures oses, however, many challenging issues. Indeed, dee architectures are originally designed to handle fixed-length vectors, or, using recursive mechanisms, ordered sequences thereof. In shar contrast, measures describe a varying number of weighted observations with no articular order. We roose in this work a dee framework designed to handle crucial asects of measures, namely ermutation invariances, variations in weights and cardinality. Architectures derived from this ieline can (i) ma measures to measures - using the concet of ush-forward oerators; (ii) bridge the ga between measures and Euclidean saces - through integration stes. This allows to design discriminative networks (to classify or reduce the dimensionality of inut measures), generative architectures (to synthesize measures) and recurrent ielines (to redict measure dynamics). We rovide a theoretical analysis of these building blocks, review our architectures' aroximation abilities and robustness w.r.t. erturbation, and try them on various discriminative and generative tasks.
"
264,2019,Self-similar Epochs: Value in arrangement,Oral,"Otimization of a machine learning model is tyically carried out by  erforming stochastic gradient udates on eochs that consist of   randomly ordered training examles. This ractice means that each fraction of an eoch comrises an indeendent   random samle of the training data that may not reserve informative   structure resent in the full data.  We hyothesize that the   training can be more effective, allowing each eoch to   rovide some of the benefits of multile ones, with more rinciled,   ``self-similar''  arrangements.

Our case study is matrix factorization, commonly used to learn metric embeddings of entities such as videos or words from examle associations. We construct arrangements that reserve the weighted Jaccard similarities of  rows and columns and exerimentally observe that our arrangements yield training acceleration of 3\%-30\% on synthetic and recommendation datasets.  Princiled arrangements of training examles emerge as a novel and otentially owerful erformance knob for SGD that merits further exloration.
"
265,2019,Information-Theoretic Considerations in Batch Reinforcement Learning,Oral,"Value-function aroximation methods that oerate in batch mode have foundational imortance to reinforcement learning (RL). Finite samle guarantees for these methods often crucially rely on two tyes of assumtions: (1) mild distribution shift, and (2) reresentation conditions that are stronger than realizability. However, the necessity (“why do we need them?”) and the naturalness (“when do they hold?”) of such assumtions have largely eluded the literature. In this aer, we revisit these assumtions and rovide theoretical results towards answering the above questions, and make stes towards a deeer understanding of value-function aroximation.
"
266,2019,Active Learning for Decision-Making from Imbalanced Observational Data,Oral,"Machine learning can hel ersonalized decision suort by learning models to redict individual treatment effects (ITE). This work studies the reliability of rediction-based decision-making in a task of deciding which action a to take for a target unit after observing its covariates x and redicted outcomes (y \mid x, a). An examle case is ersonalized medicine and the decision of which treatment to give to a atient. A common roblem when learning these models from observational data is imbalance, that is, difference in treatedcontrol covariate distributions, which is known to increase the uer bound of the exected ITE estimation error. We roose to assess the decision-making reliability by estimating the ITE model's Tye S error rate, which is the robability of the model inferring the sign of the treatment effect wrong. Furthermore, we use the estimated reliability as a criterion for active learning, in order to collect new (ossibly exensive) observations, instead of making a forced choice based on unreliable redictions. We demonstrate the effectiveness of this decision-making aware active learning in two decision-making tasks: in simulated data with binary outcomes and in a medical dataset with synthetic and continuous treatment outcomes.
"
267,2019,Temporal Gaussian Mixture Layer for Videos,Oral,"We introduce a new convolutional layer named the Temoral Gaussian Mixture (TGM) layer and resent how it can be used to efficiently cature longer-term temoral information in continuous activity videos. The TGM layer is a temoral convolutional layer governed by a much smaller set of arameters (e.g., locationvariance of Gaussians) that are fully differentiable. We resent our fully convolutional video models with multile TGM layers for activity detection. The extensive exeriments on multile datasets, including Charades and MultiTHUMOS, confirm the effectiveness of TGM layers, significantly outerforming the state-of-the-arts.
"
268,2019,"HyperGAN: A Generative Model for Diverse, Performant Neural Networks",Oral,"We introduce HyerGAN, a generative model that learns to generate all the arameters of a dee neural network. HyerGAN first transforms low dimensional noise into a latent sace, which can be samled from to obtain diverse, erformant sets of arameters for a target architecture. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of generated samles with a classification loss. This is equivalent to minimizing the KL-divergence between the distribution of generated arameters, and the unknown true arameter distribution. We aly HyerGAN to classification, showing that HyerGAN can learn to generate arameters which solve the MNIST and CIFAR-10 datasets with cometitive erformance to fully suervised learning, while also generating a rich distribution of effective arameters. We also show that HyerGAN can also rovide better uncertainty estimates than standard ensembles. This is evidenced by the ability of HyerGAN-generated ensembles to detect out of distribution data as well as adversarial examles.
"
269,2019,Benefits and Pitfalls of the Exponential Mechanism with Applications to Hilbert Spaces and Functional PCA,Oral,"The exonential mechanism is a fundamental tool of Differential Privacy (DP) due to its strong rivacy guarantees and flexibility. We study its extension to settings with summaries based on infinite dimensional oututs such as with functional data analysis, shae analysis, and nonarametric statistics. We show that one can design the mechanism with resect to a secific base measure over the outut sace, such as a Guassian rocess. We rovide a ositive result that establishes a Central Limit Theorem for the exonential mechanism quite broadly. We also rovide an aarent negative result, showing that the  magnitude of the noise introduced for rivacy is asymtotically non-negligible relative to the statistical estimation error. We develo an $\e$-DP mechanism for functional rincial comonent analysis, alicable in searable Hilbert saces. We demonstrate its erformance via simulations and alications to two datasets."
270,2019,The Value Function Polytope in Reinforcement Learning,Oral,"We establish geometric and toological roerties of the sace of value functions in finite state-action Markov decision rocesses. 
Our main contribution is the characterization of the nature of its shae: a general olytoe \cite{aigner2010roofs}. To demonstrate this result, we exhibit several roerties of the structural relationshi between olicies and value functions including the line theorem, which shows that the value functions of olicies constrained on all but one state describe a line segment. Finally, we use this novel ersective and introduce visualizations to enhance the understanding of the dynamics of reinforcement learning algorithms.
"
271,2019,Distribution calibration for regression,Oral,"We are concerned with obtaining well-calibrated outut distributions from regression models. Such distributions allow us to quantify the uncertainty that the model has regarding the redicted target value. 
We introduce the novel concet of distribution calibration, and demonstrate its advantages over the existing definition of quantile calibration.
We further roose a ost-hoc aroach to imroving the redictions from reviously trained regression models, using multi-outut Gaussian Processes with a novel Beta link function.
The roosed method is exerimentally verified on a set of common regression models and shows imrovements for both distribution-level and quantile-level calibration.
"
272,2019,Distributed Learning with Sublinear Communication,Oral,"In distributed statistical learning, $N$ samles are slit across $m$ machines and a learner wishes to use minimal communication to learn as well as if the examles were on a single machine. 
This model has received substantial interest in machine learning due to its scalability and otential for arallel seedu. However, in the high-dimensional regime, where the number examles is smaller than the number of features (``dimension''), the seedu afforded by distributed learning may be overshadowed by the cost of communicating a single examle. This aer investigates
 the following question: When is it ossible to learn a $d$-dimensional model in the distributed setting with total communication sublinear in $d$?

Starting with a negative result, we show that for learning the usual variants
of (sarse or norm-bounded) linear models, no algorithm can obtain otimal error
until communication is linear in dimension. Our main result is to show
that by slightly relaxing the standard statistical assumtions for
this setting we can obtain distributed algorithms that enjoy otimal
error and communication logarithmic in dimension. Our uer
bounds are based on family of algorithms that combine mirror descent
with randomized sarsificationquantization of iterates, and extend to
the general stochastic convex otimization model."
273,2019,Sum-of-Squares Polynomial Flow,Oral,"Triangular ma is a recent construct in robability theory that allows one to transform any source robability density function to any target density function. 
Based on triangular mas, we roose a general framework for high-dimensional density estimation, by secifying one-dimensional transformations (equivalently conditional densities) and aroriate conditioner networks. This framework (a) reveals the commonalities and differences of existing autoregressive and flow based methods, (b) allows a unified understanding of the limitations and reresentation ower of these recent aroaches and, (c) motivates us to uncover a new Sum-of-Squares (SOS) flow that is interretable, universal, and easy to train. We erform several synthetic exeriments on various density geometries to demonstrate the benefits (and short-comings) of such transformations. SOS flows achieve cometitive results in simulations and several real-world datasets. 
"
274,2019,Theoretically Principled Trade-off between Robustness and Accuracy,Oral,"We identify a trade-off between robustness and accuracy that serves as a guiding rincile in the design of defenses against adversarial examles. Although the roblem has been widely studied emirically, much remains unknown concerning the theory underlying this  trade-off. In this work, we quantify the trade-off in terms of the ga between the risk for adversarial examles and the risk for non-adversarial examles. The challenge is to rovide tight bounds on this quantity in terms of a surrogate loss. We give an otimal uer bound on this quantity in terms of classification-calibrated loss, which matches the lower bound in the worst case. Insired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our roosed algorithm erforms well exerimentally in real-world datasets. The methodology is the foundation of our entry to the adversarial cometition of a 2018 conference in which we won the 1st lace out of ~2,000 submissions, surassing the runner-u aroach by 11.41% in terms of mean L_2 erturbation distance.
"
275,2019,Optimal Algorithms for Lipschitz Bandits with Heavy-tailed Rewards,Oral,"We study Lischitz bandits, where a learner reeatedly lays one arm from an infinite arm set and then receives a stochastic reward whose exectation is a Lischitz function of the chosen arm. Most of existing work assume the reward distributions are bounded or at least sub-Gaussian, and thus do not aly to heavy-tailed rewards arising in many real-world scenarios such as web advertising and financial markets. To address this limitation, in this aer we relax the assumtion on rewards to allow arbitrary distributions that have finite $(1+\esilon)$-th moments for some $\esilon \in (0, 1]$, and roose algorithms that enjoy a sublinear regret of $\tilde{O}(T^{(d \esilon + 1)(d \esilon + \esilon + 1)})$ where $T$ is the time horizon and $d$ is the zooming dimension. The key idea is to exloit the Lischitz roerty of the exected reward function by adatively discretizing the arm set, and emloy uer confidence bound olicies with robust mean estimators designed for heavy-tailed distributions. Furthermore, we resent a lower bound for Lischitz bandits with heavy-tailed rewards, and show that our algorithms are otimal in terms of $T$. Finally, we conduct numerical exeriments to demonstrate the effectiveness of our algorithms."
276,2019,Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,Oral,"We roose a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal influence over other agents' actions. Causal influence is assessed using counterfactual reasoning. At each timeste, an agent simulates alternate actions that it could have taken, and comutes their effect on the behavior of other agents. Actions that lead to bigger changes in other agents' behavior are considered influential and are rewarded. We show that this is equivalent to rewarding agents for having high mutual information between their actions. 
Emirical results demonstrate that influence leads to enhanced coordination and communication in challenging social dilemma environments, dramatically increasing the learning curves of the dee RL agents, and leading to more meaningful learned communication rotocols. The influence rewards for all agents can be comuted in a decentralized way by enabling agents to learn a model of other agents using dee neural networks. In contrast, key revious works on emergent communication in the MARL setting were unable to learn diverse olicies in a decentralized manner and had to resort to centralized training. 
Consequently, the influence reward oens u a window of new oortunities for research in this area.
"
277,2019,Complexity of Linear Regions in Deep Networks,Oral,"It is well-known that the exressivity of a neural network deends on its architecture, with deeer networks exressing more comlex functions. In the case of networks that comute iecewise linear functions, such as those with ReLU activation, the number of distinct linear regions is a natural measure of exressivity. It is ossible to construct networks for which the number of linear regions grows exonentially with deth, or with merely a single region; it is not clear where within this range most networks fall in ractice, either before or after training. In this aer, we rovide a mathematical framework to count the number of linear regions of a iecewise linear network and measure the volume of the boundaries between these regions. In articular, we rove that for networks at initialization, the average number of regions along any one-dimensional subsace grows linearly in the total number of neurons, far below the exonential uer bound. We also find that the average distance to the nearest region boundary at initialization scales like the inverse of the number of neurons. Our theory suggests that, even after training, the number of linear regions is far below exonential, an intuition that matches our emirical observations. We conclude that the ractical exressivity of neural networks is likely far below that of the theoretical maximum, and that this ga can be quantified.
"
278,2019,Exploiting Worker Correlation for Label Aggregation in Crowdsourcing,Oral,"Crowdsourcing has emerged as a core comonent of data science ielines. From collected noisy worker labels, aggregation models that incororate worker reliability arameters aim to infer a latent true annotation. In this aer, we argue that existing crowdsourcing aroaches do not sufficiently model worker correlations observed in ractical settings; we roose in resonse an enhanced Bayesian classifier combination (EBCC) model, with inference based on a mean-field variational aroach. An introduced mixture of intra-class reliabilities---connected to tensor decomosition and item clustering---induces inter-worker correlation. EBCC does not suffer the limitations of existing correlation models: intractable marginalisation of missing labels and oor scaling to large worker cohorts. Extensive emirical comarison on 17 real-world datasets sees EBCC achieving the highest mean accuracy across 10 benchmark crowdsourcing methods.
"
279,2019,On the Convergence and Robustness of Adversarial Training,Oral,"Imroving the robustness of dee neural networks (DNNs) to adversarial examles is an imortant yet challenging roblem for secure dee learning. Across existing defense techniques, adversarial training with Projected Gradient Decent (PGD) is amongst the most effective. Adversarial training solves a min-max otimization roblem, with the \textit{inner maximization} generating adversarial examles by maximizing the classification loss, and the \textit{outer minimization} finding model arameters by minimizing the loss on adversarial examles generated from the inner maximization. A criterion that measures how well the inner maximization is solved is therefore crucial for adversarial training. In this aer, we roose such a criterion, namely First-Order Stationary Condition for constrained otimization (FOSC), to quantitatively evaluate the convergence quality of adversarial examles found in the inner maximization. With FOSC, we find that to ensure better robustness, it is essential to use adversarial examles with better convergence quality at the \textit{later stages} of training. Yet at the early stages, high convergence quality adversarial examles are not necessary and may even lead to oor robustness. Based on these observations, we roose a \textit{dynamic} training strategy to gradually increase the convergence quality of the generated adversarial examles, which significantly imroves the robustness of adversarial training. Our theoretical and emirical results show the effectiveness of the roosed method.
"
280,2019,The Odds are Odd: A Statistical Test for Detecting Adversarial Examples,Oral,"We investigate conditions under which test statistics exist that can reliably detect examles, which have been adversarially maniulated in a white-box attack. These statistics can be easily comuted and calibrated by randomly corruting inuts. 
They exloit certain anomalies that adversarial attacks introduce, in articular if they follow the aradigm of choosing erturbations otimally under -norm constraints. Access to the log-odds is the only requirement to defend models.  We justify our aroach emirically, but also rovide conditions under which detectability via the suggested test statistics is guaranteed to be effective. In our exeriments, we show that it is even ossible to correct test time redictions for adversarial attacks with high accuracy.
"
281,2019,Learning with Bad Training Data via Iterative Trimmed Loss Minimization,Oral,"In this aer, we study a simle and generic framework to tackle the roblem of learning model arameters when a fraction of the training samles are corruted. Our aroach is motivated by a simle observation: in a variety of such settings, the evolution of training accuracy (as a function of training eochs) is different for clean samles and bad samles. We roose to iteratively minimize the trimmed loss, by alternating between (a) selecting  samles with lowest current loss, and (b)  retraining a model on only these samles. Analytically, we characterize the statistical erformance and convergence rate of the algorithm for simle and natural linear and non-linear models. Exerimentally, we demonstrate its effectiveness in three settings: (a) dee image classifiers with errors only in labels, (b) generative adversarial networks with bad training images, and (c) dee image classifiers with adversarial (image, label) airs (i.e., backdoor attacks). For the well-studied setting of random label noise, our algorithm achieves  state-of-the-art erformance without having access to any a-riori guaranteed clean samles. 
"
282,2019,Graph Convolutional Gaussian Processes,Oral,"We roose a novel Bayesian nonarametric method to learn translation-invariant relationshis on non-Euclidean domains. The resulting grah convolutional Gaussian rocesses can be alied to roblems in machine learning for which the inut observations are functions with domains on general grahs. The structure of these models allows for high dimensional inuts while retaining exressibility, as is the case with convolutional neural networks. We resent alications of grah convolutional Gaussian rocesses to images and triangular meshes, demonstrating their versatility and effectiveness, comaring favorably to existing methods, desite being relatively simle models.
"
283,2019,On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization,Oral,"Recent develoments on large-scale distributed machine learning alications, e.g., dee neural networks, benefit enormously from the advances in distributed non-convex otimization techniques, e.g., distributed Stochastic Gradient Descent (SGD).  A series of recent works study the linear seedu roerty of distributed SGD variants with reduced communication. The linear seedu roerty enable us to scale out the comuting caability by adding more comuting nodes into our system. The reduced communication comlexity is desirable since communication overhead is often the erformance bottleneck in distributed systems. Recently, momentum methods are more and more widely adoted in training machine learning models and can often converge faster and generalize better.  For examle, many ractitioners use distributed SGDs with momentum to train dee neural networks with big data. However,  it remains unclear whether any distributed momentum SGD ossesses the same linear seedu roerty as distributed SGDs and has reduced communication comlexity.  This aer fills the ga between ractice and theory by considering a distributed communication efficient momentum SGD method and roving its linear seedu roerty.
"
284,2019,Maximum Entropy-Regularized Multi-Goal Reinforcement Learning,Oral,"In Multi-Goal Reinforcement Learning, an agent learns to achieve multile goals with a goal-conditioned olicy. During learning, the agent first collects the trajectories into a relay buffer and later these trajectories are selected randomly for relay. However, the achieved goals in the relay buffer are often biased towards the behavior olicies. From a Bayesian ersective, when there is no rior knowledge of the target goal distribution, the agent should learn uniformly from diverse achieved goals. Therefore, we first roose a novel multi-goal RL objective based on weighted entroy. This objective encourages the agent to maximize the exected return, as well as to achieve more diverse goals.  Secondly, we develoed a maximum entroy-based rioritization framework to otimize the roosed objective. For evaluation of this framework, we combine it with Dee Deterministic Policy Gradient, both with or without Hindsight Exerience Relay. On a set of multi-goal robotic tasks in OenAI Gym, we comare our method with other baselines and show romising imrovements in both erformance and samle-efficiency. 
"
285,2019,FloWaveNet : A Generative Flow for Raw Audio,Oral,"Most modern text-to-seech architectures use a WaveNet vocoder for synthesizing high-fidelity waveform audio, but there have been limitations, such as high inference time, in ractical alications due to its ancestral samling scheme. The recently suggested Parallel WaveNet and ClariNet has achieved real-time audio synthesis caability by incororating inverse autoregressive flow (IAF) for arallel samling. However, these aroaches require a two-stage training ieline with a well-trained teacher network and can only roduce natural sound by using robability distillation along with heavily-engineered auxiliary loss terms. We roose FloWaveNet, a flow-based generative model for raw audio synthesis. FloWaveNet requires only a single-stage training rocedure and a single maximum likelihood loss, without any additional auxiliary terms, and it is inherently arallel due to the characteristics of generative flow. The model can efficiently samle raw audio in real-time, with clarity comarable to revious two-stage arallel models. The code and samles for all models, including our FloWaveNet, are available on GitHub.
"
286,2019,On Connected Sublevel Sets in Deep Learning,Oral,"This aer shows that every sublevel set of the loss function of a class of dee over-arameterized neural nets with iecewise linear activation functions is connected and unbounded. This imlies that the loss has no bad local valleys and all of its global minima are connected within a unique and otentially very large global valley.
"
287,2019,Efficient Amortised Bayesian Inference for Hierarchical and Nonlinear Dynamical Systems,Oral,"We introduce a flexible, scalable Bayesian inference framework for nonlinear dynamical systems characterised by distinct and hierarchical variability at the individual, grou, and oulation levels. Our model class is a generalisation of nonlinear mixed-effects (NLME) dynamical systems, the statistical workhorse for many exerimental sciences.
We cast arameter inference as stochastic otimisation of an end-to-end differentiable, block-conditional variational autoencoder. We secify the dynamics of the data-generating rocess as an ordinary differential equation (ODE) such that both the ODE and its solver are fully differentiable.
This model class is highly flexible: the ODE right-hand sides can be a mixture of user-rescribed or codewhite-box"" sub-comonents and neural network orcodeblack-box"" sub-comonents.  Using stochastic otimisation, our amortised inference algorithm could seamlessly scale u to massive data collection ielines (common in labs with robotic automation).  Finally, our framework suorts interretability with resect to the underlying dynamics, as well as redictive generalization to unseen combinations of grou comonents (also called ``zero-shot"" learning). br 
We emirically validate our method by redicting the dynamic behaviour of bacteria that were genetically engineered to function as biosensors.
"
288,2019,Target Tracking for Contextual Bandits: Application to Demand Side Management,Oral,"We roose a contextual-bandit aroach for demand side management by offering rice incentives.
More recisely, a target mean consumtion is set at each round and the mean consumtion is modeled as a comlex function of the distribution of rices sent and of some contextual variables such as the temerature, weather, and so on.
The erformance of our strategies is measured in quadratic losses through a regret criterion.
We offer $\sqrt{T}$ uer bounds on this regret (u to oly-logarithmic terms), for strategies insired by standard strategies for contextual bandits (like LinUCB, see Li et al., 2010).
Simulations on a real data set gathered by UK Power Networks, in which rice incentives were offered, show that our strategies are effective and may indeed manage demand resonse by suitably icking the rice levels."
289,2019,ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation,Oral,"Dee neural networks are vulnerable to adversarial attacks. The literature is rich with algorithms that can easily craft successful adversarial examles. In contrast, the erformance of defense techniques still lags behind. This aer rooses ME-Net, a defense method that leverages matrix estimation (ME). In ME-Net, images are rerocessed using two stes: first ixels are randomly droed from the image; then, the image is reconstructed using ME. We show that this rocess destroys the adversarial structure of the noise, while re-enforcing the global structure in the original image. Since humans tyically rely on such global structures in classifying images, the rocess makes the network mode comatible with human ercetion. We conduct comrehensive exeriments on revailing benchmarks such as MNIST, CIFAR-10, SVHN, and Tiny-ImageNet. Comaring ME-Net with state-of-the-art defense mechanisms shows that ME-Net consistently outerforms rior techniques, imroving robustness against both black-box and white-box attacks.
"
290,2019,Adversarial Examples Are a Natural Consequence of Test Error in Noise,Oral,"recodeOver the last few years, the henomenon of \emh{adversarial examles} --- maliciously constructed inuts that fool trained machine learning models --- has catured the attention of the research community, esecially when restricted to small modifications of a correctly handled inut. Less surrisingly, image classifiers also lack human-level erformance on randomly corruted images, such as images with additive Gaussian noise. In this aer we rovide both emirical and theoretical evidence that these are two manifestations of the same underlying henomenon, and therefore the adversarial robustness and corrution robustness research rograms are closely related. This suggests that imroving adversarial robustness should go hand in hand with imroving erformance in the resence of more general and realistic image corrutions. This yields a comutationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.
codere
"
291,2019,A Multitask Multiple Kernel Learning Algorithm for Survival Analysis with Application to Cancer Biology,Oral,"Predictive erformance of machine learning algorithms on related roblems can be imroved using multitask learning aroaches. Rather than erforming survival analysis on each data set to redict survival times of cancer atients, we develoed a novel multitask aroach based on multile kernel learning (MKL). Our multitask MKL algorithm both works on multile cancer data sets and integrates cancer-related athwaysgene sets into survival analysis. We tested our algorithm, which is named as Path2MSurv, on the Cancer Genome Atlas data sets analyzing gene exression rofiles of 7,655 atients from 20 cancer tyes together with cancer-secific athwaygene set collections. Path2MSurv obtained better or comarable redictive erformance when comared against random survival forest, survival suort vector machine, and single-task variant of our algorithm. Path2MSurv has the ability to identify key athwaysgene sets in redicting survival times of atients from different cancer tyes.
"
292,2019,Asynchronous Batch Bayesian Optimisation with Improved Local Penalisation,Oral,"Batch Bayesian otimisation (BO) has been successfully alied to hyerarameter tuning using arallel comuting, but it is wasteful of resources: workers that comlete jobs ahead of others are left idle. We address this roblem by develoing an aroach, Penalising Locally for Asynchronous Bayesian Otimisation on K Workers (PLAyBOOK), for asynchronous arallel BO. We demonstrate emirically the efficacy of PLAyBOOK and its variants on synthetic tasks and a real-world roblem. We undertake a comarison between synchronous and asynchronous BO, and show that asynchronous BO often outerforms synchronous batch BO in both wall-clock time and samle efficiency.
"
293,2019,Correlated bandits or: How to minimize mean-squared error online,Oral,"While the objective in traditional multi-armed bandit roblems is to  find the arm with the highest mean, in many settings, finding an arm that best catures information about other arms is of interest.  This objective, however, requires learning the underlying correlation structure and not just the means.  Sensors lacement for industrial surveillance and cellular network monitoring are a few alications, where the underlying correlation structure lays an imortant role. Motivated by such alications, we formulate the correlated bandit roblem, where the objective is to find the arm with the lowest mean-squared error (MSE) in estimating all the arms. To this end, we derive first an MSE estimator based on samle variancescovariances and show that our estimator exonentially concentrates around the true MSE. Under a best-arm identification framework, we roose a successive rejects tye algorithm and rovide bounds on the robability of error in identifying the best arm. Using minimax theory, we also derive fundamental erformance limits for the correlated bandit roblem. 
"
294,2019,Stochastic Gradient Push for Distributed Deep Learning,Oral,"Distributed data-arallel algorithms aim to accelerate the training of dee neural networks by arallelizing the comutation of large mini-batch gradient udates across multile nodes. Aroaches that synchronize nodes using exact distributed averaging (e.g., via All-Reduce) are sensitive to stragglers and communication delays. The PushSum gossi algorithm is robust to these issues, but only erforms aroximate distributed averaging. This aer studies Stochastic Gradient Push (SGP), which combines PushSum with stochastic gradient udates. We rove that SGP converges to a stationary oint of smooth, non-convex objectives at the same sub-linear rate as SGD, that all nodes achieve consensus, and that SGP achieves a linear seedu with resect to the number of comute nodes. Furthermore, we emirically validate the erformance of SGP on image classification and machine translation workloads. Our code, attached to the submission, will be made ublicly available.
"
295,2019,Are Generative Classifiers More Robust to Adversarial Attacks?,Oral,"There is a rising interest in studying the robustness of dee neural network classifiers against adversaries, with both advanced attack and defence techniques being actively develoed. However, most recent work focuses on discriminative classifiers, which only model the conditional distribution of the labels given the inuts. In this aer, we roose and investigate the dee Bayes classifier, which imroves classical naive Bayes with conditional dee generative models. We further develo detection methods for adversarial examles, which reject inuts with low likelihood under the generative model. Exerimental results suggest that dee Bayes classifiers are more robust than dee discriminative classifiers, and that the roosed detection methods are effective against many recently roosed attacks.
"
296,2019,On discriminative learning of prediction uncertainty,Oral,"In classification with reject otion the classifier is allowed in uncertain cases to abstain from rediction. The classical cost based model of an otimal classifier with reject otion requires the cost of rejection to be defined exlicitly. An alternative bounded-imrovement model, avoiding the notion of the reject cost, seeks for a classifier with a guaranteed selective risk and maximal cover. We rove that both models share the same class of otimal strategies, and we rovide an exlicit relation between the reject cost and the target risk being the arameters of the two models. An otimal rejection strategy for both models is based on thresholding the conditional risk defined by osterior robabilities which are usually unavailable. We roose a discriminative algorithm learning an uncertainty function which reserves ordering of the inut sace induced by the conditional risk, and hence can be used to construct otimal rejection strategies. We show exerimentally that the roosed algorithm learns better rejection strategies than common lug-in rules and rules based on the distance to the decision boundary.
"
297,2019,Imitating Latent Policies from Observation,Oral,"In this aer, we describe a novel aroach to imitation learning that infers latent olicies directly from state observations. We introduce a method that characterizes the causal effects of latent actions on observations while simultaneously redicting their likelihood. We then outline an action alignment rocedure that leverages a small amount of environment interactions to determine a maing between the latent and real-world actions. We show that this corrected labeling can be used for imitating the observed behavior, even though no exert actions are given. We evaluate our aroach within classic control environments and a latform game and demonstrate that it erforms better than standard aroaches. Code and videos for this work are available in the sulementary.
"
298,2019,Greedy Layerwise Learning Can Scale To ImageNet,Oral,"Shallow suervised 1-hidden layer neural networks have a number of favorable roerties that make them easier to interret, analyze, and otimize than their dee counterarts, but lack their reresentational ower. Here we use 1-hidden layer learning roblems to sequentially build dee networks layer by layer, which can inherit roerties from shallow networks. Contrary to revious aroaches using shallow networks, we focus on roblems where dee learning is reorted as critical for success. We thus study CNNs on image classification tasks using the large-scale ImageNet dataset and the CIFAR-10 dataset. Using a simle set of ideas for architecture and training we find that solving sequential 1-hidden-layer auxiliary roblems lead to a CNN that exceeds AlexNet erformance on ImageNet. Extending this training methodology to construct individual layers by solving 2-and-3-hidden layer auxiliary roblems, we obtain an 11-layer network that exceeds several members of the VGG model family on ImageNet, and can train a VGG-11 model to the same accuracy as end-to-end learning. To our knowledge, this is the first cometitive alternative to end-to-end training of CNNs that can scale to ImageNet. We illustrate several interesting roerties of these models theoretically and conduct a range of exeriments to study the roerties this training induces on the intermediate layers.
"
299,2019,Collective Model Fusion for Multiple Black-Box Experts,Oral,"Model fusion is a fundamental roblem in collective machine learning (ML) where indeendent exerts with heterogeneous learning architectures are required to combine exertise to imrove redictive erformance. This is articularly challenging in information-sensitive domains (e.g., medical records in health-care analytics) where exerts do not have access to each other's internal architecture and local data. To address this challenge, this aer resents the first collective model fusion framework for multile exerts with heterogeneous black-box architectures. The roosed method will enable this by addressing the following key issues of how black-box exerts interact to understand the redictive behaviors of one another; how these understandings can be reresented and shared efficiently among themselves; and how the shared understandings can be combined to generate high-quality consensus rediction. The erformance of the resulting framework is analyzed theoretically and demonstrated emirically on several datasets.
"
300,2019,GOODE: A Gaussian Off-The-Shelf Ordinary Differential Equation Solver,Oral,"There are two tyes of ordinary differential equations
(ODEs): initial value roblems (IVPs) and
boundary value roblems (BVPs). While many
robabilistic numerical methods for the solution
of IVPs have been resented to-date, there exists
no efficient robabilistic general-urose solver
for nonlinear BVPs. Our method based on iterated
Gaussian rocess (GP) regression returns a
GP osterior over the solution of nonlinear ODEs,
which rovides a meaningful error estimation via
its redictive osterior standard deviation. Our
solver is fast (tyically of quadratic convergence
rate) and the theory of convergence can be transferred
from rior non-robabilistic work. Our
method erforms on ar with standard codes for
on an established benchmark of test roblems.
"
301,2019,"A Gradual, Semi-Discrete Approach to Generative Network Training via Explicit Wasserstein Minimization",Oral,"This aer rovides a simle rocedure to fit generative networks to target distributions, with the goal of a small Wasserstein distance (or other otimal transort costs).  The aroach is based on two rinciles: (a) if the source randomness of the network is a continuous distribution (the ``semi-discrete'' setting), then the Wasserstein distance is realized by a deterministic otimal transort maing; (b) given an otimal transort maing between a generator network and a target distribution, the Wasserstein distance may be reduced via a regression between the generated data and the maed target oints.  The rocedure here therefore alternates these two stes, forming an otimal transort and regressing against it, gradually adjusting the generator network towards the target distribution.  Mathematically, this aroach is shown to both minimize the Wasserstein distance to both the emirical target distribution, and its underlying oulation counterart.  Emirically, good erformance is demonstrated on the training and test sets of MNIST, and Thin-8 datasets. The aer closes with a discussion of the unsuitability of the Wasserstein distance for certain tasks, as has been identified in rior work (Arora et al., 2017; Huang et al., 2017).
"
302,2019,Fast and Flexible Inference of Joint Distributions from their Marginals,Oral,"Across the social sciences and elsewhere, ractitioners frequently have to reason about relationshis between random variables, desite lacking joint observations of the variables. This is sometimes called an ``ecological'' inference; given samles from the marginal distributions of the variables, one attemts to infer their joint distribution. The roblem is inherently ill-osed, yet only a few models have been roosed for bringing rior information into the roblem, often relying on restrictive or unrealistic assumtions and lacking a unified aroach. In this aer, we treat the inference roblem generally and roose a unified class of models that encomasses some of those reviously roosed while including many new ones. Previous work has relied on either relaxation or aroximate inference via MCMC, with the latter known to mix rohibitively slowly for this tye of roblem. Here we instead give a single exact inference algorithm that works for the entire model class via an efficient fixed oint iteration called Dykstra's method. We investigate emirically both the comutational cost of our algorithm and the accuracy of the new models on real datasets, showing favorable erformance in both cases and illustrating the imact of increased flexibility in modeling enabled by this work.
"
303,2019,Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels,Oral,"Noisy labels are ubiquitous in real-world datasets, which oses a challenge for robustly training dee neural networks (DNNs) as DNNs usually have the high caacity to memorize the noisy labels. In this aer, we find that the test accuracy can be quantitatively characterized in terms of the noise ratio in datasets. In articular, the test accuracy is a quadratic function of the noise ratio in the case of symmetric noise, which exlains the exerimental findings reviously ublished. Based on our analysis, we aly cross-validation to randomly slit noisy datasets, which identifies most samles that have correct labels. Then we adot the Co-teaching strategy which takes full advantage of the identified samles to train DNNs robustly against noisy labels. Comared with extensive state-of-the-art methods, our strategy consistently imroves the generalization erformance of DNNs under both synthetic and real-world training noise.
"
304,2019,SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning,Oral,"Model-based reinforcement learning (RL) has roven to be a data efficient aroach for learning control tasks but is difficult to utilize in domains with comlex observations such as images. In this aer, we resent a method for learning reresentations that are suitable for iterative model-based olicy imrovement, in that these reresentations are otimized for inferring simle dynamics and cost models given the data from the current olicy. This enables a model-based RL method based on the linear-quadratic regulator (LQR) to be used for systems with image observations. We evaluate our aroach on a suite of robotics tasks, including maniulation tasks on a real Sawyer robot arm directly from images, and we find that our method results in better final erformance than other model-based RL methods while being significantly more efficient than model-free RL.
"
305,2019,Certified Adversarial Robustness via Randomized Smoothing,Oral,"Recent work has used randomization to create classifiers that are rovably robust to adversarial erturbations with small L2 norm.  However, existing guarantees for such classifiers are unnecessarily loose. In this work we rovide the first tight analysis for these ""randomized smoothing"" classifiers.  We then use the method to train an ImageNet classifier with e.g. a rovable to-1 accuracy of 59% under adversarial erturbations with L2 norm less than 57255.  No other rovable adversarial defense has been shown to be feasible on ImageNet. On the smaller-scale datasets where alternative aroaches are viable, randomized smoothing outerforms all alternatives by a large margin.  While our secific method can only certify robustness in L2 norm, the emirical success of the aroach suggests that rovable methods based on randomization are a romising direction for future research into adversarially robust classification.
"
306,2019,Stay With Me: Lifetime Maximization Through Heteroscedastic Linear Bandits With Reneging,Oral,"Sequential decision making for lifetime maximization is a critical roblem in many real-world alications, such as medical treatment and ortfolio selection.
In these alications, a ``reneging'' henomenon, where articiants may disengage from future interactions after observing an unsatisfiable outcome, is rather revalent.
To address the above issue, this aer rooses a model of heteroscedastic linear bandits with reneging. The model allows each articiant to have a distinct ``satisfaction level,"" with any interaction outcome falling short of that level resulting in that articiant reneging. Moreover, it allows the variance of the outcome to be context-deendent. Based on this model, we develo a UCB-tye olicy, called HR-UCB, and rove that with high robability it achieves $\mathcal{O}\Big(\sqrt{{T}}\big(\log({T})\big)^{32}\Big)$ regret. Finally, we validate the erformance of HR-UCB via simulations."
307,2019,Cognitive model priors for predicting human decisions,Oral,"Human decision-making underlies all economic behavior. For the ast four decades, human decision-making under uncertainty has continued to be exlained by theoretical models based on rosect theory, a framework that was awarded the Nobel Prize in Economic Sciences. However, theoretical models of this kind have develoed slowly, and robust, high-recision redictive models of human decisions remain a challenge. While machine learning is a natural candidate for solving these roblems, it is currently unclear to what extent it can imrove redictions obtained by current theories. We argue that this is mainly due to data scarcity, since noisy human behavior requires massive samle sizes to be accurately catured by off-the-shelf machine learning methods. To solve this roblem, what is needed are machine learning models with aroriate inductive biases for caturing human behavior, and larger datasets. We offer two contributions towards this end: first, we construct “cognitive model riors” by retraining neural networks with synthetic data generated by cognitive models (i.e., theoretical models develoed by cognitive sychologists). We find that fine-tuning these networks on small datasets of real human decisions results in unrecedented state-of-the-art imrovements on two benchmark datasets. Second, we resent the first large-scale dataset for human decision-making, containing over 240,000 human judgments across over 13,000 decision roblems. This dataset reveals the circumstances where cognitive model riors are useful, and rovides a new standard for benchmarking rediction of human decisions under uncertainty.
"
308,2019,Overcoming Mean-Field Approximations in Recurrent Gaussian Process Models,Oral,"We identify a new variational inference scheme for dynamical systems whose transition function is modelled by a Gaussian rocess. Inference in this setting has, so far, either emloyed comutationally intensive MCMC methods, or relied on factorisations of the variational osterior. As we demonstrate in our exeriments, the factorisation between latent system states and transition function can lead to a miscalibrated osterior and to learning unnecessarily large noise terms. We eliminate this factorisation by exlicitly modelling the deendence between the states and the low-rank reresentation of our Gaussian rocess osterior. Samles of the latent states can then be tractably generated by conditioning on this reresentation. The method we obtain gives better redictive erformance and more calibrated estimates of the transition function, yet maintains the same time and sace comlexities as mean-field methods.
"
309,2019,Disentangling Disentanglement in Variational Autoencoders,Oral,"We develo a generalisation of disentanglement in variational auto-encoders (VAEs)—decomosition of the latent reresentation—characterising it as the fulfilment of two factors: a) the latent encodings of the data having an aroriate level of overla, and b) the aggregate encoding of the data conforming to a desired structure, reresented through the rior. Decomosition ermits disentanglement, i.e. exlicit indeendence between latents, as a secial case, but also allows for a much richer class of roerties to be imosed on the learnt reresentation, such as sarsity, clustering, indeendent subsaces, or even intricate hierarchical deendency relationshis. We show that the beta-VAE varies from the standard VAE redominantly in its control of latent overla and that for the standard choice of an isotroic Gaussian rior, its objective is invariant to rotations of the latent reresentation. Viewed from the decomosition ersective, breaking this invariance with simle maniulations of the rior can yield better disentanglement with little or no detriment to reconstructions. We further demonstrate how other choices of rior can assist in roducing different decomositions and introduce an alternative training objective that allows the control of both decomosition factors in a rinciled manner.
"
310,2019,Dimension-Wise Importance Sampling Weight Clipping for Sample-Efficient Reinforcement Learning,Oral,"In imortance samling (IS)-based reinforcement learning algorithms such as Proximal Policy Otimization (PPO), IS weights are tyically clied to avoid large variance in learning. However, olicy udate from clied statistics induces large bias in tasks with high action dimensions, and bias from cliing makes it difficult to reuse old samles with large IS weights. In this aer, we consider PPO, a reresentative on-olicy algorithm, and roose its imrovement by dimension-wise IS weight cliing which searately clis the IS weight of each action dimension to avoid large bias and adatively controls the IS weight to bound olicy udate from the current olicy. This new technique enables efficient learning for high action-dimensional tasks and reusing of old samles like in off-olicy learning to increase the samle efficiency. Numerical results show that the roosed new algorithm outerforms PPO and other RL algorithms in various Oen AI Gym tasks.
"
311,2019,On the Impact of the Activation function on Deep Neural Networks Training,Oral,"The weight initialization and the activation function of dee neural networks have a crucial imact on the erformance of the training rocedure. An inaroriate selection can lead to the loss of information of the inut during forward roagation and the exonential vanishingexloding of gradients during back-roagation. Understanding the theoretical roerties of untrained random networks is key to identifying which dee networks may be trained successfully as recently demonstrated by Samuel et al. (2017) who showed that for dee feedforward neural networks only a secific choice of hyerarameters known as the `Edge of Chaos' can lead to good erformance. While the work by Samuel et al. (2017) discuss trainability issues, we focus here on training acceleration and overall erformance. We give a comrehensive theoretical analysis of the Edge of Chaos and show that we can indeed tune the initialization arameters and the activation function in order to accelerate the training and imrove the erformance.
"
312,2019,"Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition",Oral,"Adversarial examles are inuts to machine learning models designed by an adversary to cause an incorrect outut. So far, adversarial examles have been studied most extensively in the image domain. In this domain, adversarial examles can be constructed by imercetibly modifying images to cause misclassification, and are ractical in the hysical world. In contrast, current targeted adversarial examles on seech recognition systems have neither of these roerties: humans can easily identify the adversarial erturbations, and they are not effective when layed over-the-air. This aer makes rogress on both of these fronts. First, we develo effectively imercetible audio adversarial examles (verified through a human study) by leveraging the sychoacoustic rincile of auditory masking, while retaining 100% targeted success rate on arbitrary full-sentence targets. Then, we make rogress towards hysical-world audio adversarial examles by constructing erturbations which remain effective even after alying highly-realistic simulated environmental distortions.
"
313,2019,Does Data Augmentation Lead to Positive Margin?,Oral,"Data augmentation (DA) is commonly used during model training, as it significantly imroves test error and model robustness. DA artificially exands the training set by alying random noise, rotations, cros, or even adversarial erturbations to the inut data. Although DA is widely used, its caacity to rovably imrove robustness is not fully understood. In this work, we analyze the robustness that DA begets by quantifying the margin that DA enforces on emirical risk minimizers. We first focus on linear searators, and then a class of nonlinear models whose labeling is constant within small convex hulls of data oints. We resent lower bounds on the number of augmented data oints required for non-zero margin, and show that commonly used DA techniques may only introduce significant margin after adding exonentially many oints to the data set.
"
314,2019,"Garbage In, Reward Out: Bootstrapping Exploration in Multi-Armed Bandits",Oral,"We roose a bandit algorithm that exlores by randomizing its history of observations. The algorithm estimates the value of the arm from a non-arametric bootstra samle of its history, which is augmented with seudo observations. Our novel design of seudo observations guarantees that the bootstra estimates are otimistic with a high robability. We call our algorithm Giro, which stands for garbage in, reward out. We analyze Giro in a Bernoulli bandit and rove a $O(K \Delta^{-1} \log n)$ bound on its $n$-round regret, where $K$ is the number of arms and $\Delta$ is the difference in the exected rewards of the otimal and the best subotimal arms. The key advantage of our exloration design is that it can be easily alied to structured roblems. To show this, we roose contextual Giro with an arbitrary non-linear reward generalization model. We evaluate Giro and its contextual variant on multile synthetic and real-world roblems, and observe that Giro erforms well."
315,2019,Trading Redundancy for Communication: Speeding up Distributed SGD for Non-convex Optimization,Oral,"The communication overhead is one of the key challenges that hinders the scalability of distributed otimization algorithms to train large neural networks. In recent years, there has been a great deal of research to alleviate communication cost by comressing the gradient vector or using local udates and eriodic model averaging. In this aer, we aim at develoing communication-efficient distributed stochastic algorithms for non-convex otimization by effective data relication strategies. In articular, we, both theoretically and ractically, show that by roerly infusing redundancy to the training data with model averaging, it is ossible to significantly reduce the number of communications rounds. To be more recise, for a redetermined level of redundancy, the roosed algorithm samles min-batches from redundant chunks of data from multile workers in udating local solutions. As a byroduct, we also show that the roosed algorithm is robust to failures. Our emirical studies on CIFAR10 and
CIFAR100 datasets in a distributed environment comlement our theoretical results.
"
316,2019,EDDI: Efficient Dynamic Discovery of High-Value Information with Partial VAE,Oral,"Making decisions requires information relevant to the task at hand.
Many real-life decision-making situations allow further relevant information to be acquired at a secific cost. For examle, in assessing the health status of a atient we may decide to take additional measurements such as diagnostic tests or imaging scans before making a final assessment.  Acquiring more relevant information enables better decision making, but may be costly. How can we trade off the desire to make good decisions by acquiring further information with the cost of erforming that acquisition?

To this end, we roose a rinciled framework, named EDDI (Efficient Dynamic Discovery of high-value Information), based on the theory of Bayesian exerimental design. In EDDI we roose a novel \emh{artial variational autoencoder} (Partial VAE), to efficiently handle missing data with different missing atterns. EDDI combines this Partial VAE with an acquisition function that maximizes exected information gain on a set of target variables. EDDI is efficient and demonstrates that dynamic discovery of high-value information is ossible; we show cost reduction at the same decision quality and imroved decision quality at the same cost in benchmarks and two health-care alications. We are confident that there is great otential for realizing these gains in real-world decision suort systems.
"
317,2019,Structured agents for physical construction,Oral,"Physical construction---the ability to comose objects, subject to hysical dynamics, in order to serve some function---is fundamental to human intelligence. Here we introduce a suite of challenging hysical construction tasks insired by how children lay with blocks, such as matching a target configuration, stacking and attaching blocks to connect objects together, and creating shelter-like structures over target objects. We then examine how a range of modern dee reinforcement learning agents fare on these challenges, and introduce several new aroaches which rovide suerior erformance. Our results show that agents which use structured reresentations (e.g., objects and scene grahs) and structured olicies (e.g., object-centric actions) outerform those which use less structured reresentations, and generalize better beyond their training. Agents which use model-based lanning via Monte-Carlo Tree Search also outerform strictly model-free agents in our most challenging construction roblems. We conclude that aroaches which combine structured reresentations and reasoning with owerful learning are a key ath toward agents that can erform comlex construction behaviors.
"
318,2019,AReS and MaRS - Adversarial and MMD-Minimizing Regression for SDEs,Oral,"Stochastic differential equations are an imortant modeling class in many discilines. Consequently, there exist many methods relying on various discretization and numerical integration schemes. In this aer, we roose a novel, robabilistic model for estimating the drift and diffusion given noisy observations of the underlying stochastic system. Using state-of-the-art adversarial and moment matching inference techniques, we avoid the discretization schemes of classical aroaches. This leads to significant imrovements in arameter accuracy and robustness given random initial guesses. On four commonly used benchmark systems, we demonstrate the erformance of our algorithms comared to state-of-the-art solutions based on extended Kalman filtering and Gaussian rocesses.
"
319,2019,Robust Learning from Untrusted Sources,Oral,"Modern machine learning methods often require more data for training than a single exert can rovide. Therefore, it has become a standard rocedure to collect data from external sources, e.g. via crowdsourcing. Unfortunately, the quality of these sources is not always guaranteed. As additional comlications, the data might be stored in a distributed way, or might even have to remain rivate. In this work, we address the question of how to learn robustly in such scenarios. Studying the roblem through the lens of statistical learning theory, we derive a rocedure that allows for learning from all available sources, yet automatically suresses irrelevant or corruted data. We show by extensive exeriments that our method rovides significant imrovements over alternative aroaches from robust statistics and distributed otimization. 
"
320,2019,Estimating Information Flow in Deep Neural Networks,Oral,"We study the estimation of the mutual information I(X;Temℓ) between the inut X to a dee neural network (DNN) and the outut vector Temℓ of its ℓ-th hidden layer (an “internal reresentation”). Focusing on feedforward networks with fixed weights and noisy internal reresentations, we develo a rigorous framework for accurate estimation of I(X;Temℓ). By relating I(X;Temℓ) to information transmission over additive white Gaussian noise channels, we reveal that comression, i.e. reduction in I(X;Temℓ) over the course of training, is driven by rogressive geometric clustering of the reresentations of samles from the same class. Exerimental results verify this connection. Finally, we shift focus to urely deterministic DNNs, where I(X;Temℓ) is rovably vacuous, and show that nevertheless, these models also cluster inuts belonging to the same class. The binning-based aroximation of I(X;T_ℓ) emloyed in ast works to measure comression is identified as a measure of clustering, thus clarifying that these exeriments were in fact tracking the same clustering henomenon. Leveraging the clustering ersective, we rovide new evidence that comression and generalization may not be causally related and discuss otential future research ideas.
"
321,2019,Beating Stochastic and Adversarial Semi-bandits Optimally and Simultaneously,Oral,"We develo the first general semi-bandit algorithm that simultaneously achieves $\mathcal{O}(\log T)$ regret for stochastic environments
and $\mathcal{O}(\sqrt{T})$ regret for adversarial environments
without knowledge of the regime or the number of rounds $T$.
The leading roblem-deendent constants of our bounds are not only otimal in some worst-case sense studied reviously,
but also otimal for two concrete instances of semi-bandit roblems.
Our algorithm and analysis extend the recent work of (Zimmert & Seldin, 2019) for the secial case of multi-armed bandit,
but imortantly requires a novel hybrid regularizer designed secifically for semi-bandit.
Exerimental results on synthetic data show that our algorithm indeed erforms well uniformly over different environments.
We finally rovide a reliminary extension of our results to the full bandit feedback."
322,2019,"Trimming the $\ell_1$ Regularizer: Statistical Analysis, Optimization, and Applications to Deep Learning",Oral,"We study high-dimensional estimators with the trimmed $\ell_1$ enalty, which leaves the h largest arameter entries enalty-free. While otimization techniques for this nonconvex enalty have been studied, the statistical roerties have not yet been analyzed. We resent the first statistical analyses for M-estimation, and characterize suort recovery, $\ell_\infty$ and $\ell_2$ error of the trimmed $\ell_1$ estimates as a function of the trimming arameter h. Our results show different regimes based on how h comares to the true suort size. Our second contribution is a new algorithm for the trimmed regularization roblem, which has the same theoretical convergence rate as difference of convex (DC) algorithms, but in ractice is faster and finds lower objective values. Emirical evaluation of $\ell_1$ trimming for sarse linear regression and grahical model estimation indicate that trimmed $\ell_1$ can outerform vanilla $\ell_1$ and non-convex alternatives. Our last contribution is to show that the trimmed enalty is beneficial beyond M-estimation, and yields romising results for two dee learning tasks: inut structures recovery and network sarsification."
323,2019,Conditioning by adaptive sampling for robust design,Oral,"We consider design roblems wherein the goal is to maximize or secify the value of one or more roerties of interest. For examle, in rotein design, one may wish to find the rotein sequence which maximizes its fluorescence. We assume access to one or more black box stochastic ""oracle"" redictive functions, each of which mas from an inut (e.g., rotein sequences or images) design sace to a distribution over a roerty of interest (e.g., rotein fluorescence or image content). Given such stochastic oracles, our roblem is to find an inut that best achieves our goal. At first glance, this roblem can be framed as one of otimizing the oracle with resect to the inut. However, in most real world settings, the oracle will not exactly cature the ground truth, and critically, may catastrohically fail to do so in extraolation sace. Thus, we frame the goal as one modelling the density of some original set of training data (e.g., a set of real rotein sequences), and then conditioning this distribution on the desired roerties, which yields an annealed adative samling method which is also well-suited to rare conditioning events. We demonstrate exerimentally that our aroach outerforms other recently resented methods for tackling similar roblems.
"
324,2019,Parsimonious Black-Box Adversarial Attacks via Efficient Combinatorial Optimization,Oral,"Solving for adversarial examles with rojected gradient descent has been demonstrated to be highly effective in fooling the neural network based classifiers. However, in the black-box setting, the attacker is limited only to the query access to the network and solving for a successful adversarial examle becomes much more difficult. To this end, recent methods aim at estimating the true gradient signal based on the inut queries but at the cost of excessive queries.

We roose an efficient discrete surrogate to the otimization roblem which does not require estimating the gradient and consequently becomes free of the first order udate hyerarameters to tune. Our exeriments on Cifar-10 and ImageNet show the state of the art black-box attack erformance with significant reduction in the required queries comared to a number of recently roosed methods.
"
325,2019,End-to-End Probabilistic Inference for Nonstationary Audio Analysis,Oral,"A tyical audio signal rocessing ieline includes multile disjoint analysis stages, including calculation of a time-frequency reresentation followed by sectrogram-based feature analysis. We show how time-frequency analysis and nonnegative matrix factorisation can be jointly formulated as a sectral mixture Gaussian rocess model with nonstationary riors over the amlitude variance arameters. Further, we formulate this nonlinear model's state sace reresentation, making it amenable to infinite-horizon Gaussian rocess regression with aroximate inference via exectation roagation, which scales linearly in the number of time stes and quadratically in the state dimensionality. By doing so, we are able to rocess audio signals with hundreds of thousands of data oints. We demonstrate, on various tasks with emirical data, how this inference scheme outerforms more standard techniques that rely on extended Kalman filtering.
"
326,2019,Learning Novel Policies For Tasks,Oral,"Finding multile distinct solutions for a articular task is a challenging roblem for reinforcement learning algorithms. In this work, we resent a reinforcement learning algorithm that can find a variety of olicies (novel olicies) for a task that is given by a task reward function. Our method does this by creating a second reward function that recognizes reviously seen state sequences and rewards those by novelty. Novelty is measured using autoencoders that have been trained on state sequences from reviously discovered olicies. We resent a two-objective udate technique for olicy gradient algorithms that each udate of the olicy is a comromise between imroving the task reward and imroving the novelty reward. Using this method, we end u with a collection of olicies that solves a given task as well as carrying out action sequences that are distinct from one another. We demonstrate this method on maze navigation tasks, a reaching task for a simulated robot arm, and a locomotion task for a hoer. We also demonstrate the effectiveness of our aroach on decetive tasks in which olicy gradient methods often get stuck.
"
327,2019,Bilinear Bandits with Low-rank Structure ,Oral,"  We introduce the bilinear bandit roblem with low-rank structure in which an action takes the form of a air of arms from two different entity tyes, and the reward is a bilinear function of the known feature vectors of the arms.  The roblem is motivated by numerous alications in which the learner must recommend two different entity tyes as a single action, such as a male  female air in an online dating service.  The unknown in the roblem is a $d_1$ by $d_2$ matrix $\mathbf{\Theta}^*$ that defines the reward, and has low rank $r \ll \min\{d_1,d_2\}$. Determination of $\mathbf{\Theta}^*$ with this low-rank structure oses a significant challenge in finding the right exloration-exloitation tradeoff.  In this work, we roose a new two-stage algorithm called ``Exlore-Subsace-Then-Refine'' (ESTR). The first stage is an exlicit subsace exloration, while the second stage is a linear bandit algorithm called ``almost-low-dimensional OFUL'' (LowOFUL) that exloits and further refines the estimated subsace via a regularization technique.  We show that the regret of ESTR is $\widetilde{\mathcal{O}}((d_1+d_2)^{32} \sqrt{r T})$ where $\widetilde{\mathcal{O}}$ hides logarithmic factors and $T$ is the time horizon.  This imroves uon the regret of $\widetilde{\mathcal{O}}(d_1d_2\sqrt{T})$ attained for a na\""ive linear bandit reduction.  We conjecture that the regret bound of ESTR is unimrovable u to olylogarithmic factors.  A reliminary exeriment shows that ESTR outerforms a na\""ive linear bandit reduction.  
"
328,2019,Wasserstein Adversarial Examples via Projected Sinkhorn Iterations,Oral,"A raidly growing area of work has studied the existence of adversarial examles, dataoints which have been erturbed to fool a classifier, but the vast majority of these works have focused rimarily on threat models defined by L- norm-bounded erturbations. In this aer, we roose a new threat model for adversarial attacks based on the Wasserstein distance.  In the image classification setting, such distances measure the cost of moving ixel mass, which naturally cover ""standard"" image maniulations such as scaling, rotation, translation, and distortion (and can otentially be alied to other settings as well).  To generate Wasserstein adversarial examles, we develo a rocedure for rojecting onto the Wasserstein ball, based uon a modified version of the Sinkhorn iteration.  The resulting algorithm can successfully attack image classification models, bringing traditional CIFAR10 models down to 3% accuracy within a Wasserstein ball with radius 0.1 (i.e., moving 10% of the image mass 1 ixel), and we demonstrate that PGD-based adversarial training can imrove this adversarial accuracy to 76%. In total, this work oens u a new direction of study in adversarial robustness, more formally considering convex metrics that accurately cature the invariances that we tyically believe should exist in classifiers.
"
329,2019,Direct Uncertainty Prediction for Medical Second Opinions,Oral,"The issue of disagreements amongst human exerts is a ubiquitous one in both machine learning and medicine. In medicine, this often corresonds to doctor disagreements on a atient diagnosis. In this work, we show that machine learning models can be successfully trained to give uncertainty scores to data instances that result in high exert disagreements. In articular, they can identify atient cases that would benefit most from a medical second oinion. Our central methodological finding is that Direct Uncertainty Prediction (DUP), training a model to redict an uncertainty score directly from the raw atient features, works better than Uncertainty Via Classification, the two ste rocess of training a classifier and ostrocessing the outut distribution to give an uncertainty score. We show this both with a theoretical result, and on extensive evaluations on a large scale medical imaging alication. 
"
330,2019,Compressed Factorization: Fast and Accurate Low-Rank Factorization of Compressively-Sensed Data,Oral,"What learning algorithms can be run directly on comressively-sensed data? In this work, we consider the question of accurately and efficiently comuting low-rank matrix or tensor factorizations given data comressed via random rojections. We examine the aroach of first erforming factorization in the comressed domain, and then reconstructing the original high-dimensional factors from the recovered (comressed) factors. In both the matrix and tensor settings, we establish conditions under which this natural aroach will rovably recover the original factors. While it is well-known that random rojections reserve a number of geometric roerties of a dataset, our work can be viewed as showing that they can also reserve certain solutions of non-convex, NP-Hard roblems like non-negative matrix factorization. We suort these theoretical results with exeriments on synthetic data and demonstrate the ractical alicability of comressed factorization on real-world gene exression and EEG time series datasets.
"
331,2019,A Wrapped Normal Distribution on Hyperbolic Space for Gradient-Based Learning,Oral,"Hyerbolic sace is a geometry that is known to be well-suited for reresentation learning of data with an underlying hierarchical structure.
In this aer, we resent a novel hyerbolic distribution called \textit{seudo-hyerbolic Gaussian}, a Gaussian-like distribution on hyerbolic sace whose density can be evaluated analytically and differentiated with resect to the arameters.
Our distribution enables the gradient-based learning of the robabilistic models on hyerbolic sace that could never have been considered before.
Also, we can samle from this hyerbolic robability distribution without resorting to auxiliary means like rejection samling.
As alications of our distribution, we develo a hyerbolic-analog of variational autoencoder and a method of robabilistic word embedding on hyerbolic sace.
We demonstrate the efficacy of our distribution on various datasets including MNIST, Atari 2600 Breakout, and WordNet.
"
332,2019,SELFIE: Refurbishing Unclean Samples for Robust Deep Learning,Oral,"Owing to the extremely high exressive ower of dee neural networks, their side effect is to totally memorize training data even when the labels are extremely noisy. To overcome overfitting on the noisy labels, we roose a novel robust training method called SELFIE. Our key idea is to selectively refurbish and exloit unclean samles that can be corrected with high recision, thereby gradually increasing the number of available training samles. Taking advantage of this design, SELFIE effectively revents the risk of noise accumulation from the false correction and fully exloits the training data. To validate the sueriority of SELFIE, we conducted extensive exerimentation using three data sets simulated with varying noise rates. The result showed that SELFIE remarkably imroved absolute test error by u to 10.5 ercentage oints comared with two state-of-the-art robust training methods.
"
333,2019,The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects,Oral,"Understanding the behavior of stochastic gradient descent (SGD) in the context of dee neural networks has raised lots of concerns recently. 
Along this line, we  study a general form of gradient based otimization dynamics with unbiased noise, which unifies SGD and standard Langevin dynamics.
Through investigating this general otimization dynamics, we analyze the behavior of SGD on escaing from minima and its regularization effects. A novel indicator is derived to characterize the efficiency of escaing from minima through measuring the alignment of noise covariance and the curvature of loss function.
Based on this indicator, two conditions are established to show which tye of noise structure is suerior to isotroic noise in term of escaing efficiency.
We further show that the anisotroic noise in SGD satisfies the two conditions, and thus hels to  escae from shar and oor minima effectively, towards more stable and flat minima that tyically generalize well.
We systematically design various exeriments to verify the benefits of the anisotroic noise, comared with full gradient descent lus isotroic diffusion (i.e. Langevin dynamics).

The code for reroducibility is rovided in the Sulementary Materials. 
"
334,2019,Deep Gaussian Processes with Importance-Weighted Variational Inference,Oral,"Dee Gaussian rocesses (DGPs) can model comlex marginal densities as well as comlex maings. Non-Gaussian marginals are essential for modelling real-world data, and can be generated from the DGP by incororating uncorrelated variables to the model. Previous work in the DGP model has introduced noise additively, and used variational inference with a combination of sarse Gaussian rocesses and mean-field Gaussians for the aroximate osterior. Additive noise attenuates the signal, and the Gaussian form of variational distribution may lead to an inaccurate osterior. We instead incororate noisy variables as latent covariates, and roose a novel imortance-weighted objective, which leverages analytic results and rovides a mechanism to trade off comutation for imroved accuracy. Our results demonstrate that the imortance-weighted objective works well in ractice and consistently outerforms classical variational inference, esecially for deeer models.
"
335,2019,Zeno: Distributed Stochastic Gradient Descent with Suspicion-based Fault-tolerance,Oral,"We resent Zeno, a technique to make distributed machine learning, articularly Stochastic Gradient Descent (SGD), tolerant to an arbitrary number of faulty workers. This generalizes revious results that assumed a majority of non-faulty nodes; we need assume only one non-faulty worker. Our key idea is to susect workers that are otentially defective. Since this is likely to lead to false ositives, we use a ranking-based reference mechanism. We rove the convergence of SGD for non-convex roblems under these scenarios. Exerimental results show that Zeno outerforms existing aroaches.
"
336,2019,Emerging Convolutions for Generative Normalizing Flows,Oral,"Generative flows are attractive because they admit exact likelihood otimization and efficient image synthesis. Recently, Kingma &am; Dhariwal (2018) demonstrated with Glow that generative flows are caable of generating high quality images. We generalize the 1 × 1 convolutions roosed in Glow to invertible d × d convolutions, which are more flexible since they oerate on both channel and satial axes. We roose two methods to roduce invertible convolutions, that have recetive fields identical to standard convolutions: Emerging convolutions are obtained by chaining secific autoregressive convolutions, and eriodic convolutions are decouled in the frequency domain. Our exeriments show that the flexibility of d × d convolutions significantly imroves the erformance of generative flow models on galaxy images, CIFAR10 and ImageNet.
"
337,2019,Dynamic Measurement Scheduling for Event Forecasting using Deep RL,Oral,"Current clinical ractice for monitoring atients' health follows either regular or heuristic-based lab test (e.g. blood test) scheduling. Such ractice not only gives rise to redundant measurements accruing cost, but may even lead to unnecessary atient discomfort. From the comutational ersective, heuristic-based test scheduling might lead to reduced accuracy of clinical forecasting models. A data-driven measurement scheduling is likely to lead to both more accurate redictions and less measurement costs. We address the scheduling roblem using dee reinforcement learning (RL) and roose a general and scalable framework to achieve high redictive gain and low measurement cost, by scheduling fewer, but strategically timed tests. Using simulations we show that our olicy outerforms heuristic-based measurement scheduling having higher redictive gain and lower cost. We then learn a scheduling olicy for mortality forecasting in the real-world clinical dataset (MIMIC3). Our olicy decreases the total number of measurements by $31\%$ without reducing the redictive erformance, or imroves $3$ times more redictive gain with the same number of measurements."
338,2019,Transferable Clean-Label Poisoning Attacks on Deep Neural Nets,Oral,"In this aer, we exlore the clean-label oisoning attacks on neural networks with no access to neither the networks' outut nor its arameters. We deal with the case of transfer learning, where the network is initialized from a re-trained model on a certain dataset and only its last layer is re-trained on the targeted dataset. The task is to make the re-trained model classify the target image into a target class. To achieve this goal, we generate multile oison images from the target class by adding small erturbations on the clean images. These oison images form a convex hull of the target image in the feature sace, with guarantees the target image to be mis-classified when the requirement is erfectly satisfied. 
"
339,2019,Characterizing Well-Behaved vs. Pathological Deep Neural Networks,Oral,"We introduce a novel aroach, requiring only mild assumtions, for the characterization of dee neural networks at initialization. Our aroach alies both to fully-connected and convolutional networks and easily incororates batch normalization and ski-connections. Our key insight is to consider the evolution with deth of statistical moments of signal and noise, thereby characterizing the resence of athologies in the hyothesis sace encoded by the choice of hyerarameters. We establish: (i) for feedforward networks with and without batch normalization, deth multilicativity inevitably leads to ill-behaved moments and athologies; (ii) for residual networks with batch normalization, on the other hand, identity ski-connections induce ower-law rather than exonential behaviour, leading to well-behaved moments and no athology.
"
340,2019,Taming MAML: Efficient unbiased meta-reinforcement learning,Oral,"While meta reinforcement learning (Meta-RL) methods have achieved remarkable success, obtaining correct and low variance estimates for olicy gradients remains a significant challenge. In articular, estimating a large Hessian, oor samle efficiency and unstable training continue to make Meta-RL difficult. We roose a surrogate objective function named, Tamed MAML (TMAML), that adds control variates into gradient estimation via automatic differentiation. TMAML imroves the quality of gradient estimation by reducing variance without introducing bias. We further roose a version of our method that extends the meta-learning framework to learning the control variates themselves, enabling efficient learning from a distribution of MDPs. We emirically comare our aroach with MAML and other variance-bias trade-off methods including DICE, LVC, and action-deendent control variates. Our aroach is easy to imlement and outerforms existing methods in terms of the variance and accuracy of gradient estimation, ultimately yielding higher erformance across a variety of challenging Meta-RL environments.
"
341,2019,Noisy Dual Principal Component Pursuit,Oral,"Dual Princial Comonent Pursuit (DPCP) is a recently roosed non-convex otimization based method for learning subsaces of high relative dimension from noiseless datasets contaminated by as many outliers as the square of the number of inliers. Exerimentally, DPCP has roved to be robust to noise and outerform the oular RANSAC on 3D vision tasks such as road lane detection and relative oses estimation from three views. This aer extends the global otimality and convergence theory of DPCP to the case of data corruted by noise, and further demonstrates its robustness using synthetic and real data. 
"
342,2019,Online Learning to Rank with Features,Oral,"We introduce a new model for online ranking in which the click robability factors into an examination and attractiveness function and the attractiveness function is a linear function of a feature vector and an unknown arameter. Only relatively mild assumtions are made on the examination function. A novel algorithm for this setu is analysed, showing that the deendence on the number of items is relaced by a deendence on the dimension, allowing the new algorithm to handle a large number of items. When reduced to the orthogonal case, the regret of the algorithm imroves on the state-of-the-art.
"
343,2019,On the Design of Estimators for Bandit Off-Policy Evaluation,Oral,"Off-olicy evaluation is the roblem of estimating the value of a target olicy using data collected under a different olicy. Given a base estimator for bandit off-olicy evaluation and a arametrized class of control variates, we address the roblem of comuting a control variate in that class that reduces the risk of the base estimator. We derive the oulation risk as a function of the class arameters and we establish conditions that guarantee risk imrovement. We resent our main results in the context of multi-armed bandits, and we roose a simle design for contextual bandits that gives rise to an estimator that is shown to erform well in multi-class cost-sensitive classification datasets.
"
344,2019,Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization,Oral,"Dee neural networks are tyically highly over-arameterized with runing techniques able to remove a significant fraction of network arameters with little loss in accuracy. Recently, techniques based on dynamic re-allocation of non-zero arameters have emerged for training sarse networks directly without having to train a large dense model beforehand. We resent a arameter re-allocation scheme that addresses the limitations of revious methods such as their high comutational cost and the fixed number of arameters they allocate to each layer. We investigate the erformance of these dynamic re-allocation methods in dee convolutional networks and show that our method outerforms revious static and dynamic arameterization methods, yielding the best accuracy for a given number of training arameters, and erforming on ar with networks obtained by iteratively runing a trained dense model. We further investigated the mechanisms underlying the suerior erformance of the resulting  sarse networks. We found that neither the structure, nor the initialization of the sarse networks discovered by our arameter reallocation scheme are sufficient to exlain their suerior generalization erformance. Rather, it is the continuous exloration of different sarse network structures during training that is critical to effective learning. We show that it is more fruitful to exlore these structural degrees of freedom than to add extra arameters to the network. Code used to run all exeriments is available under the anonymous reository: htts:gitlab.comanonymous.icml.2019dynamic-arameterization-icml19.
"
345,2019,Automated Model Selection with Bayesian Quadrature,Oral,"We resent a novel techniques for tailoring Bayesian quadrature (BQ) to model selection. The state-of-the-art for comaring the evidence of multile models relies on Monte Carlo methods, which converge slowly and are unreliable for comutationally exensive models. Previous research has shown that BQ offers samle efficiency suerior to Monte Carlo in comuting the evidence of an individual model. However, alying BQ directly to model comarison may waste comutation roducing an overly-accurate estimate for the evidence of a clearly oor model.
We roose an automated and efficient algorithm for comuting the most-relevant quantity for model selection: the osterior robability of a model. Our technique maximize the mutual information between this quantity and observations of the models' likelihoods, yielding efficient acquisition of samles across disarate model saces when likelihood observations are limited. Our method roduces more-accurate model osterior estimates using fewer model likelihood evaluations than standard Bayesian quadrature and Monte Carlo estimators, as we demonstrate on synthetic and real-world examles.
"
346,2019,Learning a Compressed Sensing Measurement Matrix via Gradient Unrolling,Oral,"Linear encoding of sarse vectors is widely oular, but is commonly data-indeendent -- missing any ossible extra (but a-riori unknown) structure beyond sarsity. In this aer we resent a new method to learn linear encoders that adat to data, while still erforming well with the widely used $\ell_1$ decoder. The convex $\ell_1$ decoder revents gradient roagation as needed in standard gradient-based training. Our method is based on the insight that unrolling the convex decoder into $T$ rojected subgradient stes can address this issue. Our method can be seen as a data-driven way to learn a comressed sensing measurement matrix. We comare the emirical erformance of 10 algorithms over 6 sarse datasets (3 synthetic and 3 real). Our exeriments show that there is indeed additional structure beyond sarsity in the real datasets. Our method is able to discover it and exloit it to create excellent reconstructions with fewer measurements (by a factor of 1.1-3x) comared to the revious state-of-the-art methods. We illustrate an alication of our method in learning label embeddings for extreme multi-label classification. Our exeriments show that our method is able to match or outerform the recision scores of SLEEC, which is one of the state-of-the-art embedding-based aroaches for extreme multi-label learning."
347,2019,Understanding Geometry of Encoder-Decoder CNNs,Oral,"Encoder-decoder networks using convolutional neural network (CNN) architecture have been extensively used in dee learning literatures thanks to its excellent erformance for various inverse roblems in comuter vision, medical imaging, etc.
However,  it is still difficult to obtain coherent geometric view why such an architecture gives the desired erformance. Insired by recent theoretical understanding on  generalizability, exressivity and otimization landscae of neural networks, as well as the theory of   convolutional framelets, here we rovide a unified theoretical framework  that leads to a better understanding of geometry of encoder-decoder CNNs. Our unified mathematical framework shows that encoder-decoder CNN architecture is closely related to nonlinear basis reresentation
using  combinatorial convolution frames, whose exressibility increases exonentially with the network deth. We also  demonstrate  the imortance of skied connection  in terms of exressibility,  and  otimization landscae. 
"
348,2019,A Large-Scale Study on Regularization and Normalization in GANs,Oral,"Generative adversarial networks (GANs) are a class of dee generative models which aim to learn a target distribution in an unsuervised fashion. While they were successfully alied to many roblems, training a GAN is a notoriously challenging task and requires a significant amount of hyerarameter tuning, neural architecture engineering, and a non-trivial amount of ``tricks"". The success in many ractical alications couled with the lack of a measure to quantify the failure modes of GANs resulted in a lethora of roosed losses, regularization and normalization schemes, as well as neural architectures. In this work we take a sober view of the current state of GANs from a ractical ersective. We discuss common itfalls and reroducibility issues, oen-source our code on Github, and rovide re-trained models on TensorFlow Hub.
"
349,2019,Concentration Inequalities for Conditional Value at Risk,Oral,"In this aer we derive new concentration inequalities for the conditional value at risk (CVaR) of a random variable, and comare them to the revious state of the art (Brown, 2007).  We show analytically that our lower bound is strictly tighter than Brown's, and emirically that this difference is significant.  While our uer bound may be looser than Brown's in some cases, we show emirically that in most cases our bound is significantly tighter.  After discussing when each uer bound is suerior, we conclude with emirical results which suggest that both of our bounds will often be significantly tighter than Brown's.
"
350,2019,Self-Supervised Exploration via Disagreement,Oral,"Exloration has been a long standing roblem in both model-based and model-free learning methods for sensorimotor control. There have been major advances in recent years demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of the current formulations get stuck when there are stochastic dynamics. In this aer, we roose a formulation for exloration insired from the work in active learning literature. Secifically, we train an ensemble of dynamics models and incentivize the agent to maximize the disagreement or variance of those ensembles. We show that this formulation works as well as other formulations in non-stochastic scenarios, and is able to exlore better in scenarios with stochastic-dynamics. Further, we show that this objective can be leveraged to erform  differentiable olicy otimization. This leads to a samle efficient exloration olicy. We show exeriments on a large number of standard environments to demonstrate the efficacy of this aroach. Furthermore, we imlement our exloration algorithm on a real robot which learns to interact with objects comletely from scratch. Project videos are in sulementary.
"
351,2019,NATTACK: Learning the Distributions of Adversarial Examples for an Improved  Black-Box  Attack on Deep Neural Networks,Oral,"Powerful adversarial attack methods are vital for understanding how to construct robust dee neural networks (DNNs) and for thoroughly testing defense techniques. In this aer, we roose a black-box adversarial attack algorithm that can defeat both vanilla DNNs and those generated by various defense techniques develoed recently. Instead of searching for an ""otimal"" adversarial examle for a benign inut to a targeted DNN, our algorithm finds a robability density distribution over a small region centered around the inut, such that a samle drawn from this  distribution is likely an adversarial examle, without the need of accessing the DNN's internal layers or weights. Our aroach is universal as it can successfully attack different neural networks by a single algorithm. It is also strong; according to the testing against 2 vanilla DNNs and 13 defended ones, it outerforms state-of-the-art black-box or white-box attack methods for most test cases. Additionally, our results reveal that adversarial training remains one of the best defense techniques, and the adversarial examles are not as transferable across defended DNNs as them across vanilla DNNs.
"
352,2019,Data Poisoning Attacks in Multi-Party Learning,Oral,"In this work, we demonstrate universal multi-arty oisoning attacks that adat and aly to any multi-arty learning rocess with arbitrary interaction attern between the arties. More generally, we introduce and study $(k,)$-oisoning attacks in which an adversary controls $k\in[m]$ of the arties, and for each corruted arty $P_i$, the adversary submits some oisoned data $T'_i$ on behalf of $P_i$ that is still ""$(1-)$-close"" to the correct data $T_i$ (e.g., $1-$ fraction of $T'_i$ is still honestly generated). We rove that for any ""bad"" roerty $B$ of the final trained hyothesis $h$ (e.g., $h$ failing on a articular test examle or having ""large"" risk) that has an arbitrarily small constant robability of haening without the attack, there always is a $(k,)$-oisoning attack that increases the robability of $B$ from $\mu$ to by $\mu^{1- \cdot km} = \mu + \Omega( \cdot km)$. Our attack only uses clean labels, and it is online, as it only knows the the data shared so far."
353,2019,Simple Black-box Adversarial Attacks,Oral,"We roose an intriguingly simle method for the construction of adversarial images in the black-box setting. In constrast to the white-box scenario, constructing black-box adversarial images has the additional constraint on query budget, and efficient attacks remain an oen roblem to date. With only the mild assumtion of requiring continuous-valued confidence scores, our highly query-efficient algorithm utilizes the following simle iterative rincile: we randomly samle a vector from a redefined orthonormal basis and either add or subtract it to the target image. Desite its simlicity, the roosed method can be used for both untargeted and targeted attacks -- resulting in reviously unrecedented query efficiency in both settings. We demonstrate the efficacy and efficiency of our algorithm on several real world settings including the Google Cloud Vision API. We argue that our roosed algorithm should serve as a strong baseline for future black-box attacks, in articular because it is  extremely fast and its imlementation requires less than 20 lines of PyTorch code. 
"
354,2019,Traditional and Heavy Tailed Self Regularization in Neural Network Models,Oral,"Random Matrix Theory (RMT) is alied to analyze the weight matrices of Dee Neural Networks (DNNs), including both roduction quality, re-trained models such as AlexNet and Incetion, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Emirical and theoretical results clearly indicate that the emirical sectral density (ESD) of DNN layer matrices dislays signatures of traditionally-regularized statistical models, even in the absence of exogenously secifying traditional forms of regularization, such as Droout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develo a theory to identify \emh{5+1 Phases of Training}, corresonding to increasing amounts of \emh{Imlicit Self-Regularization}.  For smaller andor older DNNs, this Imlicit Self-Regularization is like traditional Tikhonov regularization, in that there is a ``size scale'' searating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of \emh{Heavy-Tailed Self-Regularization}, similar to the self-organization seen in the statistical hysics of disordered systems.  This imlicit Self-Regularization can deend strongly on the many knobs of the training rocess.  By exloiting the generalization ga henomena, we demonstrate that we can cause a small model to exhibit all 5+1 hases of training simly by changing the batch size.
"
355,2019,DeepNose: Using artificial neural networks to represent the space of odorants,Oral,"The olfactory system emloys an ensemble of odorant recetors (ORs) to sense odorants and to derive olfactory ercets. We trained artificial neural networks to reresent the chemical sace of odorants and used that reresentation to redict human olfactory ercets. We hyothesized that ORs may be considered 3D convolutional filters that extract molecular features and can be trained using machine learning methods. First, we trained a convolutional autoencoder, called DeeNose, to deduce a low-dimensional reresentation of odorant molecules which were reresented by their 3D satial structure. Next, we tested the ability of DeeNose features in redicting hysical roerties and odorant ercets based on 3D molecular structure alone. We found that desite the lack of human exertise, DeeNose features led to redictions of both hysical roerties and ercetions of comarable accuracy to molecular descritors often used in comutational chemistry, such as Dragon descritors. We roose that DeeNose network can extract de novo chemical features redictive of various bioactivities and can hel understand the factors influencing the comosition of ORs ensemble. 
"
356,2019,Dynamic Learning with Frequent New Product Launches: A Sequential Multinomial Logit Bandit Problem,Oral,"Motivated by the henomenon that comanies introduce new roducts to kee abreast with customers' raidly changing tastes, we consider a novel online learning setting where a rofit-maximizing seller needs to learn customers' references through offering recommendations, which may contain existing roducts and new roducts that are launched in the middle of a selling eriod. We roose a sequential multinomial logit (SMNL) model to characterize customers' behavior when roduct recommendations are resented in tiers. For  the offline version with known customers' references, we roose a olynomial-time algorithm and characterize the roerties of the otimal tiered roduct recommendation. For the online roblem, we roose a learning algorithm and quantify its regret bound. Moreover, we extend the setting to incororate a constraint which ensures every new roduct is learned to a given accuracy. Our results demonstrate the tier structure can be used to mitigate the risks associated with learning new roducts.
"
357,2019,Screening rules for Lasso with  non-convex Sparse Regularizers,Oral,"Leveraging on the convexity of the Lasso roblem, screening rules
hel in accelerating solvers by discarding irrelevant variables, during the otimization rocess. However, because they rovide better theoretical guarantees in identifying relevant   variables, several non-convex regularizers for the Lasso have been roosed in the literature.  This work is the first that introduces
a screening rule strategy into a non-convex Lasso solver. The aroach we roose is based on a iterative majorization-minimization (MM) strategy that includes a screening rule in the  inner solver and a condition for roagating screened variables between iterations of MM. In addition to imrove efficiency of solvers, we also rovide guarantees that the inner solver is able to identify the zeros comonents of its critical oint in finite time.  Our exerimental analysis illustrates     the significant comutational gain  brought by the new screening rule comared to classical coordinate-descent  or roximal gradient descent methods.
"
358,2019,Variational Annealing of GANs: A Langevin Perspective,Oral,"The generative adversarial network (GAN) has received considerable attention recently as a model for data synthesis, without an exlicit secification of a likelihood function. There has been commensurate interest in leveraging likelihood estimates to imrove GAN training. To enrich the understanding of this fast-growing yet almost exclusively heuristic-driven subject, we elucidate the theoretical roots of some of the emirical attemts to stabilize and imrove GAN training with the introduction of likelihoods. We highlight new insights from variational theory of diffusion rocesses to derive a likelihood-based regularizing scheme for GAN training, and resent a novel aroach to train GANs with an unnormalized distribution instead of emirical samles. To substantiate our claims, we rovide exerimental evidence on how our theoretically-insired new algorithms imrove uon current ractice.
"
359,2019,Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,Oral,"Dee reinforcement learning algorithms require large amounts of exerience to learn an individual task. While in rincile meta-reinforcement learning (meta-RL) algorithms enable agents to learn new skills from small amounts of exerience, several major challenges reclude their racticality. Current methods rely heavily on on-olicy exerience, limiting their samle efficiency, and lack mechanisms to reason about task uncertainty when identifying and learning new tasks, limiting their effectiveness in sarse reward roblems. In this aer, we aim to address these challenges by develoing an off-olicy meta-RL algorithm based on online latent task inference. Our method can be interreted as an imlementation of online robabilistic filtering of latent task variables to infer how to solve a new task from small amounts of exerience. This robabilistic interretation also enables osterior samling for structured exloration. Our method outerforms rior algorithms in asymtotic erformance and samle efficiency on several meta-RL benchmarks.
"
360,2019,Causal Identification under Markov Equivalence: Completeness Results,Oral,"Causal effect identification is the task of determining whether a causal distribution is comutable from the combination of an observational distribution and substantive knowledge about the domain under investigation. One of the most studied versions of this roblem assumes that knowledge is articulated in the form of a fully known causal diagram, which is arguably a strong assumtion in many settings. In this aer, we relax this requirement and consider that the knowledge is articulated in the form of an equivalence class of causal diagrams, in articular, a artial ancestral grah (PAG). This is attractive because a PAG can be learned directly from data, and the data scientist does not need to commit to a articular, unique diagram. There are different sufficient conditions for identification in PAGs, but none is comlete. We derive a comlete algorithm for identification given a PAG. This imlies that whenever the causal effect is identifiable, the algorithm returns a valid identification exression; alternatively, it will throw a failure condition, which means that the effect is rovably not identifiable (unless stronger assumtions are made). We further rovide a grahical characterization of non-identifiability of causal effects in PAGs.
"
361,2019,The Natural Language of Actions,Oral,"We introduce Act2Vec, a general framework for learning context-based action reresentation for Reinforcement Learning. Reresenting actions in a vector sace hel reinforcement learning algorithms achieve better erformance by grouing similar actions and utilizing relations between different actions. We show how rior knowledge of an environment can be extracted from demonstrations and injected into action vector reresentations that encode natural comatible behavior. We then use these for augmenting state reresentations as well as imroving function aroximation of Q-values. We visualize and test action embeddings in three domains including a drawing task, a high dimensional navigation task, and the large action sace domain of StarCraft II.
"
362,2019,Monge blunts Bayes: Hardness Results for Adversarial Training,Oral,"The last few years have seen a staggering number of 
emirical studies of the
robustness of neural networks in a model of adversarial
erturbations of their inuts. Most
rely on an adversary which carries out local
modifications within rescribed balls. None however has so far
questioned the broader icture: how to frame a \textit{resource-bounded} adversary so
that it can be \textit{severely detrimental} to learning,
a non-trivial roblem which entails at a minimum the choice of loss and classifiers.

We suggest a formal answer for losses that satisfy the minimal statistical
requirement of being \textit{roer}. We in down a simle sufficient roerty for any given class of adversaries to be detrimental to learning,
involving a central measure of ``harmfulness''
which generalizes
the well-known class of integral robability metrics.
A key feature of our result is that it holds for \textit{all} roer losses,
and for a oular subset of these, the otimisation of this central measure aears to be
\textit{indeendent of the loss}. When classifiers
are Lischitz -- a now oular aroach in adversarial training --, this
otimisation resorts to \textit{otimal transort} to make a
low-budget comression of class marginals. Toy exeriments reveal a
finding recently searately observed: training against a sufficiently
budgeted adversary of this kind \textit{imroves} generalization.
"
363,2019,Invertible Residual Networks,Oral,"We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Tyically, enforcing invertibility requires artitioning dimensions or restricting network architectures. In contrast, our aroach only requires adding a simle normalization ste during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To comute likelihoods, we introduce a tractable aroximation to the Jacobian log-determinant of a residual block. Our emirical evaluation shows that invertible ResNets erform cometitively with both state-of-the-art image classifiers and flow-based generative models, something that has not been reviously achieved with a single architecture.
"
364,2019,Context-Aware Zero-Shot Learning for Object Recognition,Oral,"Zero-Shot  Learning  (ZSL)  aims  at  classifying unlabeled objects by leveraging auxiliary knowledge, such as semantic reresentations. A limitation of revious aroaches is that only intrinsic roerties of objects, e.g. their visual aearance, are taken into account while their context, e.g. the surrounding objects in the image, is ignored. Following the intuitive rincile that objects tend to be found in certain contexts but not others, we roose a new and challenging aroach, context-aware ZSL, that leverages semantic reresentations in a new way to model the conditional likelihood of an object to aear in a given context. Finally, through extensive exeriments conducted on Visual Genome, we show that contextual information can substantially imrove the standard ZSL aroach and is robust to unbalanced classes.
"
365,2019,Beyond the Chinese Restaurant and Pitman-Yor processes: Statistical Models with double power-law behavior,Oral,"Bayesian nonarametric aroaches, in articular the Pitman-Yor rocess and the associated two-arameter Chinese Restaurant rocess, have been successfully used in alications where the data exhibit a ower-law behavior. Examles include natural language rocessing, natural images or networks. There is also growing emirical evidence that some datasets exhibit a two-regime ower-law behavior: one regime for small frequencies, and a second regime, with a different exonent, for high frequencies. In this aer, we introduce a class of comletely random measures which are doubly regularly-varying. Contrary to the Pitman-Yor rocess, we show that when comletely random measures in this class are normalized to obtain random robability measures and associated random artitions, such artitions exhibit a double ower-law behavior. We discuss in articular three models within this class: the beta rime rocess (Broderick et al. (2015, 2018), a novel rocess call generalized BFRY rocess, and a mixture construction. We derive efficient Markov chain Monte Carlo algorithms to estimate the arameters of these models. Finally, we show that the roosed models rovide a better fit than the Pitman-Yor rocess on various datasets.
"
366,2019,Domain Agnostic Learning with Disentangled Representations,Oral,"Unsuervised model transfer has the otential to greatly imrove the generalizability of dee models to novel domains. Yet the current literature assumes that the searation of target data into distinct domains is known a riori. In this aer, we roose the task of Domain-Agnostic Learning (DAL): How to transfer knowledge from a labeled source domain to unlabeled data from arbitrary target domains? To tackle this roblem, we devise a novel Dee Adversarial Disentangled Autoencoder (DADA) caable of disentangling domain-secific features from class identity. We demonstrate exerimentally that when the target domain labels are unknown, DADA leads to state-of-the-art erformance on several image classification datasets.
"
367,2019,Almost surely constrained convex optimization,Oral,"We roose a stochastic gradient framework for solving stochastic comosite convex otimization roblems with (ossibly) infinite number of linear inclusion constraints that need to be satisfied almost surely. We use smoothing and homotoy techniques to handle constraints without the need for matrix-valued rojections. We show for our stochastic gradient algorithm $\mathcal{O}(\log(k)\sqrt{k})$ convergence rate for general convex objectives and $\mathcal{O}(\log(k)k)$ convergence rate for restricted strongly convex objectives. 
These rates are known to be otimal u to logarithmic factors, even without constraints. 
We demonstrate the erformance of our algorithm with numerical exeriments on basis ursuit, a hard margin suort vector machines and a ortfolio otimization and show that our algorithm achieves state-of-the-art ractical erformance."
368,2019,Distributed Weighted Matching via Randomized Composable Coresets,Oral,"Maximum weight matching is one of the most fundamental combinatorial otimization roblems with a wide range of alications in data mining and bioinformatics. Develoing distributed weighted matching algorithms has been challenging due to the sequential nature of efficient algorithms for this roblem. In this aer, we develo a simle distributed algorithm for the roblem on general grahs with aroximation guarantee of 2 + es that (nearly) matches that of the sequential greedy algorithm. A key advantage of this algorithm is that it can be easily imlemented in only two rounds of comutation in modern arallel comutation frameworks such as MaReduce. We also demonstrate the efficiency of our algorithm in ractice on various grahs (some with half a trillion edges) by achieving objective values always close to what is achievable in the centralized setting.
"
369,2019,DP-GP-LVM: A Bayesian Non-Parametric Model for Learning Multivariate Dependency Structures,Oral,"We resent a non-arametric Bayesian latent variable model caable of learning deendency structures across dimensions in a multivariate setting. Our aroach is based on flexible Gaussian rocess riors for the generative maings and interchangeable Dirichlet rocess riors to learn the structure. The introduction of the Dirichlet rocess as a secific structural rior allows our model to circumvent issues associated with revious Gaussian rocess latent variable models. Inference is erformed by deriving an efficient variational bound on the marginal log-likelihood of the model. We demonstrate the efficacy of our aroach via analysis of discovered structure and suerior quantitative erformance on missing data imutation.
"
370,2019,Control Regularization for Reduced Variance Reinforcement Learning,Oral,"Dealing with high variance is a significant challenge in model-free reinforcement learning (RL). Existing methods are unreliable, exhibiting high variance in erformance from run to run using different initializationsseeds. Focusing on roblems arising in continuous control, we roose a functional regularization aroach to augmenting model-free RL. In articular, we regularize the behavior of the dee olicy to be similar to a control rior, i.e., we regularize in function sace.  We show that functional regularization yields a bias-variance trade-off, and roose an adative tuning strategy to otimize this trade-off. When the rior olicy has control-theoretic stability guarantees, we further show that this regularization aroximately reserves those stability guarantees throughout learning. We validate our aroach emirically on a wide range of settings, and demonstrate significantly reduced variance, guaranteed dynamic stability, and more efficient learning than dee RL alone.
"
371,2019,Multivariate Submodular Optimization,Oral,"Submodular functions have found a wealth of new alications in data science and machine learning models in recent years. This has been couled with many algorithmic advances in the area of submodular otimization: (SO) $\min\max~f(S): S \in \mathcal{F}$, where $\mathcal{F}$ is a given family of feasible sets over a ground set $V$ and $f:2^V \rightarrow \mathbb{R}$ is submodular. In this work we focus on a more general class of \emh{multivariate submodular otimization} (MVSO) roblems:  $\min\max~f (S_1,S_2,\ldots,S_k):  S_1 \ulus S_2 \ulus \cdots \ulus S_k \in \mathcal{F}$. Here we use $\ulus$ to denote disjoint union and hence this model is attractive where resources are being allocated across  $k$ agents, who share a ``joint'' multivariate nonnegative objective $f(S_1,S_2,\ldots,S_k)$ that catures some tye of submodularity (i.e. diminishing returns) roerty. We rovide some exlicit examles and otential alications for this new framework. For maximization, we show that ractical algorithms such as accelerated greedy variants and distributed algorithms achieve good aroximation guarantees for very general families (such as matroids and $$-systems). For arbitrary families, we show that monotone (res. nonmonotone) MVSO admits an $\alha (1-1e)$ (res. $\alha \cdot 0.385$) aroximation whenever monotone (res. nonmonotone) SO admits an $\alha$-aroximation over the multilinear formulation. This substantially exands the family of tractable models. On the minimization side we give essentially otimal aroximations in terms of the curvature of $f$."
372,2019,NAS-Bench-101: Towards Reproducible Neural Architecture Search,Oral,"Recent advances in neural architecture search (NAS) demand tremendous comutational resources. This makes it difficult to reroduce exeriments and imoses a barrier to entry to researchers without access to large scale comutation. We aim to ameliorate these roblems by introducing NAS-Bench-101, the first ublic architecture dataset for NAS research. To build it, we carefully constructed a comact---yet exressive---search sace, exloiting grah isomorhisms to identify 423K unique architectures. Utilizing machine-years of comutation, we trained them all with ublic code, and comiled the results into a large table. This  allows researchers to evaluate the quality of a roosed model in milliseconds using various recomuted metrics. NAS-Bench-101 resents a unique oortunity to study the entire NAS loss landscae from a data-driven ersective, which we illustrate with our analysis. We also demonstrate the dataset's alication to benchmarking by comaring a range of oular architecture otimization algorithms on it.
"
373,2019,Better generalization with less data using robust gradient descent,Oral,"For learning tasks where the data (or losses) may be heavy-tailed, algorithms based on emirical risk minimization may require a substantial number of observations in order to erform well off-samle. In ursuit of stronger erformance under weaker assumtions, we roose a technique which uses a chea and robust iterative estimate of the risk gradient, which can be easily fed into any steeest descent rocedure. Finite-samle risk bounds are rovided under weak moment assumtions on the loss gradient. The algorithm is simle to imlement, and emirical tests using simulations and real-world data illustrate that more efficient and reliable learning is ossible without rior knowledge of the loss tails.
"
374,2019,Composing Value Functions in Reinforcement Learning,Oral,"An imortant roerty for lifelong-learning agents is the ability to combine existing skills to solve new unseen tasks.  In general, however, it is unclear how to comose existing skills in a rinciled manner. We show that otimal value function comosition can be achieved in entroy-regularised reinforcement learning (RL), and then extend this result to the standard RL setting.  Comosition is demonstrated in a high-dimensional video game environment, where an agent with an existing library of skills is immediately able to solve new tasks without the need for further learning.
"
375,2019,Band-limited Training and Inference for Convolutional Neural Networks,Oral,"The convolutional layers are core building blocks of neural network architecture. In general, a convolutional filter alies to the entire frequency sectrum of an inut signal. We exlore artificially constraining the frequency sectra of these filters, called band-limiting, during Convolutional Neural Networks (CNN) training. The band-limiting alies to both the feedforward and backroagation stes. Through an extensive evaluation over time-series and image datasets, we observe that CNNs are resilient to this comression scheme and results suggest that CNNs learn to leverage lower-frequency comonents. An extensive exerimental evaluation across 1D and 2D CNN training tasks illustrates: (1) band-limited training can effectively control the resource usage (GPU and memory); (2) models trained with band-limited layers retain high rediction accuracy; and (3) requires no modification to existing training algorithms or neural network architecture to use unlike other comression schemes. 
"
376,2019,Counterfactual Off-Policy Evaluation with Gumbel-Max Structural Causal Models,Oral,"We introduce an off-olicy evaluation rocedure for highlighting eisodes where alying a reinforcement learned (RL) olicy is likely to have roduced a substantially different outcome than the observed olicy.  In articular, we introduce a class of structural causal models (SCMs) for generating counterfactual trajectories in finite artially observable Markov Decision Processes (POMDPs).  We see this as a useful rocedure for off-olicy ``debugging'' in high-risk settings (e.g., healthcare); by decomosing the exected reward under the RL olicy into secific eisodes, we can identify grous where it is more likely to dramatically under- or over-erform the observed olicy.  This in turn can be used to facilitate review of secific eisodes by domain exerts, as well as to guide data collection (e.g., to characterize atient sub-tyes).  We demonstrate the utility of this rocedure in the setting of the management of sesis.
"
377,2019,Generalized Majorization-Minimization,Oral,"Non-convex otimization is ubiquitous in machine learning. Majorization-Minimization (MM) is a owerful iterative rocedure for otimizing non-convex functions that works by otimizing a sequence of bounds on the function. In MM, the bound at each iteration is required to touch the objective function at the otimizer of the revious bound. We show that this touching constraint is unnecessary and overly restrictive. We generalize MM by relaxing this constraint, and roose a new otimization framework, named Generalized Majorization-Minimization (G-MM), that is more flexible. For instance, G-MM can incororate alication-secific biases into the otimization rocedure without changing the objective function. We derive G-MM algorithms for several latent variable models and show emirically that they consistently outerform their MM counterarts in otimizing non-convex objectives. In articular, G-MM algorithms aear to be less sensitive to initialization.
"
378,2019,Causal Discovery and Forecasting in Nonstationary Environments with State-Space Models,Oral,"In many scientific fields, such as economics and neuroscience, we are often faced with nonstationary time series, and concerned with both finding causal relations and forecasting the values of variables of interest, both of which are articularly challenging in such nonstationary environments. In this aer, we study causal discovery and forecasting for nonstationary time series. By exloiting a articular tye of state-sace model to reresent the rocesses, we show that nonstationarity hels to identify causal structure, and that forecasting naturally benefits from learned causal knowledge. Secifically, we allow changes in both causal strengths and noise variances in the nonlinear state-sace models, which, interestingly, renders both the causal structure and model arameters identifiable. Given the causal model, we treat forecasting as a roblem in Bayesian inference in the causal model, which exloits the time-varying roerty of the data and adats to new observations in a rinciled manner.  Exerimental results on synthetic and real-world data sets demonstrate the efficacy of the roosed methods.
"
379,2019,On the Generalization Gap in Reparameterizable Reinforcement Learning,Oral,"Understanding generalization in reinforcement learning (RL) is a significant challenge, as many common assumtions of traditional suervised learning theory do not aly. We argue that the ga between training and testing erformance of RL agents is caused by two tyes of errors: intrinsic error due to the randomness of the environment and an agent's olicy, and external error by the change of environment distribution. We focus on the secial class of rearameterizable RL roblems, where the trajectory distribution can be decomosed using the rearametrization trick. For this roblem class, estimating the exected reward is efficient and does not require costly trajectory re-samling. This enables us to study rearametrizable RL using suervised learning and transfer learning theory. Our bound suggests the generalization caability of rearameterizable RL is related to multile factors including ``smoothness"" of the environment transition, reward and agent olicy function class. We also emirically verify the relationshi between the generalization ga and these factors through simulations.  
"
380,2019,Random Function Priors for Correlation Modeling,Oral,"Many hidden structures underlying high dimensional data can be comactly exressed by a discrete random measure $\xi_n=\sum_{k\in[K]} Z_{nk}\delta_{\theta_k}$, where $(\theta_k)_{k\in[K]}\subset\Theta$ is a collection of hidden atoms shared across observations (indexed by $n$). Previous Bayesian nonarametric methods focus on embedding $\xi_n$ onto alternative saces to resolve comlex atom correlations. However, these methods can be rigid and hard to learn in ractice. In this aer, we temorarily ignore the atom sace $\Theta$ and embed oulation random measures $(\xi_n)_{n\in\bbN}$ altogether as $\xi'$ onto an infinite stri $[0,1]\times\bbR_+$, where the order of atoms is \textit{removed} by assuming searate exchangeability. Through a ``de Finetti tye"" result,  we can reresent $\xi'$ as a couling of a 2d Poisson rocess and exchangeable random functions $(f_n)_{n\in\bbN}$, where each $f_n$ is an object-secific atom samling function. In this way, we transform the roblem from learning comlex correlations with discrete random measures into learning comlex functions that can be learned with dee neural networks. In ractice, we introduce an efficient amortized variational inference algorithm to learn $f_n$ without ain; i.e., no local gradient stes are required during stochastic inference."
381,2019,On the Computation and Communication Complexity of Parallel SGD with Dynamic Batch Sizes for Stochastic Non-Convex Optimization,Oral,"For SGD based distributed stochastic otimization,  comutation comlexity, measured by the convergence rate in terms of the number of stochastic gradient access, and communication comlexity, measured by the number of inter-node communication rounds, are the most imortant two erformance metrics.  The classical data-arallel imlementation of SGD over $N$ workers can achieve a linear seedu of its convergence rate but incurs an inter-node communication round at each batch. We study the benefit of using dynamically increasing batch sizes in arallel SGD for stochastic non-convex otimization by charactering the attained convergence rate and the required number of communication rounds.  We show that for stochastic non-convex otimization under the P-L condition, the classical data arallel SGD with exonentially increasing  batch sizes can achieve the fastest known $O(1(NT))$ convergence with linear seedu using only $\log(T)$ communication rounds. For general stochastic non-convex otimization, we roose a Catalyst-like algorithm that  achieves the fastest known $O(1\sqrt{NT})$ convergence with linear seedu using only $O(\sqrt{NT}\log(\frac{T}{N}))$ communication rounds.  "
382,2019,Approximated Oracle Filter Pruning for Destructive CNN Width Optimization,Oral,"It is never easy to design and run Convolutional Neural Networks (CNNs) due to: 1) no one knows the otimal number of filters at each layer, given a network architecture; and 2) the comutational intensity of CNNs imedes the deloyment on comutationally limited devices. The need for an automatic method to otimize the number of filters, i.e., the width of convolutional layers, brings us to Oracle Pruning, which is the most accurate filter runing method but suffers from intolerant time comlexity. To address this roblem, we roose Aroximated Oracle Filter Pruning (AOFP), a training-time filter runing framework, which is ractical on very dee CNNs. By AOFP, we can rune an existing dee CNN with accetable time cost, negligible accuracy dro and no heuristic knowledge, or re-design a model which exerts higher accuracy and faster inference.
"
383,2019,Fast Context Adaptation via Meta-Learning,Oral,"We roose CAVIA, a meta-learning method for fast adatation that is scalable, flexible, and easy to imlement. CAVIA artitions the model arameters into two arts: context arameters that serve as additional inut to the model and are adated on individual tasks, and shared arameters that are meta-trained and shared across tasks. At test time, the context arameters are udated with one or several gradient stes on a task-secific loss that is backroagated through the shared art of the network. Comared to aroaches that adjust all arameters on a new task (e.g., MAML), CAVIA can be scaled u to larger networks without overfitting on a single task, is easier to imlement, and is more robust to the inner-loo learning rate. We show emirically that CAVIA outerforms MAML on regression, classification, and reinforcement learning roblems.
"
384,2019,Near optimal finite time identification of arbitrary linear dynamical systems,Oral,"We derive finite time error bounds for estimating general linear time-invariant (LTI) systems from a single observed trajectory using the method of least squares. We rovide the first analysis of the general case when eigenvalues of the LTI system are arbitrarily distributed in three regimes:  stable, marginally stable, and exlosive. Our analysis yields shar uer bounds for each of these cases searately. We observe that although the underlying rocess behaves quite differently in each of these three regimes, the systematic analysis of a self--normalized martingale difference term hels bound identification error u to logarithmic factors of the lower bound. On the other hand, we demonstrate that the least squares solution may be statistically inconsistent under certain conditions even when the signal-to-noise ratio is high. 
"
385,2019,Learning Classifiers for Target Domain with Limited or No Labels,Oral,"In comuter vision alications, such as domain adatation  (DA),  few  shot  learning  (FSL)  and zero-shot learning (ZSL), we encounter new objects and environments, for which insufficient examles exist to allow for training “models from scratch,” and methods that adat existing models, trained on the resented training environment(PTE), to the new scenario are required. We roose  a  novel  visual  attribute  encoding  method that encodes each image as a low-dimensional
robability vector comosed of rototyical art-tye robabilities, where the rototyical arts are learnt so as to be reresentative to all images in PTE. We show that the resulting encoding is universal in that it serves as an inut for adating or learning classifiers for different roblem contexts; with limited annotated labels in FSL; with no data and only semantic attributes in ZSL; and with unlabeled data for domain adatation. We conduct extensive exeriments on benchmark datasets and demonstrate that our method outerforms state-of-art DA, FSL or ZSL methods.
"
386,2019,Beyond Adaptive Submodularity: Approximation Guarantees of Greedy Policy with Adaptive Submodularity Ratio,Oral,"We roose a new concet named adative submodularity ratio to study the greedy olicy for sequential decision making. While the greedy olicy is known to erform well for a wide variety of adative stochastic otimization roblems in ractice, its theoretical roerties have been analyzed only for a limited class of roblems. We narrow the ga between theory and ractice by using adative submodularity ratio, which enables us to rove aroximation guarantees of the greedy olicy for a substantially wider class of roblems. Examles of newly analyzed roblems include imortant alications such as adative influence maximization and adative feature selection. Our adative submodularity ratio also rovides bounds of adativity gas. Exeriments confirm that the greedy olicy erforms well with the alications being considered comared to standard heuristics.
"
387,2019,LegoNet: Efficient Convolutional Neural Networks with Lego Filters,Oral,"This aer aims to build efficient convolutional neural networks using a set of Lego filters. Many successful building blocks, e.g., incetion and residual modules, have been designed to refresh state-of-the-art records of CNNs on visual recognition tasks. Beyond these high-level modules, we suggest that an ordinary filter in the neural network can be ugraded to a sohisticated module as well. Filter modules are established by assembling a shared set of Lego filters that are often of much lower dimensions. Weights in Lego filters and binary masks to stack Lego filters for these filter modules can be simultaneously otimized in an end-to-end manner as usual. Insired by network engineering, we develo a slit-transform-merge strategy for an efficient convolution by exloiting intermediate Lego feature mas. The comression and acceleration achieved by Lego Networks using the roosed Lego filters have been theoretically discussed. Exerimental results on benchmark datasets and dee models demonstrate the advantages of the roosed Lego filters and their otential real-world alications on mobile devices.
"
388,2019,Variational Russian Roulette for Deep Bayesian Nonparametrics,Oral,"Bayesian nonarametric models rovide a rinciled way to automatically adat the comlexity of a model to the amount of the data available, but comutation in such models is difficult. Amortized variational aroximations are aealing because of their comutational efficiency, but current methods rely on a fixed finite truncation of the infinite model. This truncation level can be difficult to set, and also interacts oorly with amortized methods due to the over-runing roblem. Instead, we roose a new variational aroximation, based on a method from statistical hysics called Russian roulette samling. This allows the variational distribution to adat its comlexity during inference, without relying on a fixed truncation level, and while still obtaining an unbiased estimate of the gradient of the original variational objective. We demonstrate this method on infinite sized variational auto-encoders using a Beta-Bernoulli (Indian buffet rocess) rior.
"
389,2019,Simple Stochastic Gradient Methods for Non-Smooth Non-Convex Regularized Optimization,Oral,"Our work focuses on stochastic gradient methods for otimizing a smooth non-convex loss function with a non-smooth non-convex regularizer. Research on this class of roblem is quite limited, and until very recently no non-asymtotic convergence results have been reorted. We resent two simle stochastic gradient algorithms, for finite-sum and general stochastic otimization roblems, which have suerior convergence comlexities comared to the current state of the art. We also demonstrate suerior erformance of our algorithms in ractice for emirical risk minimization on well known datasets.  
"
390,2019,Classifying Treatment Responders Under Causal Effect Monotonicity,Oral,"In the context of individual-level causal inference, we study the roblem of redicting whether someone will resond or not to a treatment based on their features and ast examles of features, treatment indicator (e.g., drugno drug), and a binary outcome (e.g., recovery from disease). As a classification task, the roblem is made difficult by not knowing the examle outcomes under the oosite treatment indicators. We assume the effect is monotonic, as in advertising's effect on a urchase or bail-setting's effect on reaearance in court: either it would have haened regardless of treatment, not haened regardless, or haened only deending on exosure to treatment. Predicting whether the latter is latently the case is our focus. While revious work focuses on conditional average treatment effect estimation, formulating the roblem as a classification task allows us to develo new tools more suited to this roblem. By leveraging monotonicity, we develo new discriminative and generative algorithms for the resonder-classification roblem. We exlore and discuss connections to corruted data and olicy learning. We rovide an emirical study with both synthetic and real datasets to comare these secialized algorithms to standard benchmarks.
"
391,2019,Trajectory-Based Off-Policy Deep Reinforcement Learning,Oral,"Policy gradient methods are owerful reinforcement learning algorithms and have been demonstrated to solve many comlex tasks. However, these methods are also data-inefficient, afflicted with high variance gradient estimates, and get frequently stuck in local otima. This work addresses these weaknesses by combining recent imrovements in the reuse of off-olicy data and exloration in arameter sace with deterministic behavioral olicies. The resulting objective is amenable to standard neural network otimization strategies, like stochastic gradient descent or stochastic gradient Hamiltonian Monte Carlo. Incororation of revious rollouts via imortance samling greatly imroves data efficiency, whilst stochastic otimization schemes facilitate the escae from local otima. We evaluate the roosed aroach on a series of continuous control benchmark tasks. The results show that the roosed algorithm is able to successfully and reliably learn solutions using fewer system interactions than standard olicy gradient methods. 
"
392,2019,Provable Guarantees for Gradient-Based Meta-Learning,Oral,"We study the roblem of meta-learning through the lens of online convex otimization, develoing a meta-algorithm bridging the ga between oular gradient-based meta-learning and classical regularization-based multi-task transfer methods. Our method is the first to simultaneously satisfy good samle efficiency guarantees in the convex setting, with generalization bounds that imrove with task-similarity, while also being comutationally scalable to modern dee learning architectures and the many-task setting. Desite its simlicity, the algorithm matches, u to a constant factor, a lower bound on the erformance of any such arameter-transfer method under natural task similarity assumtions. We use exeriments in both convex and dee learning settings to verify and demonstrate the alicability of our theory.
"
393,2019,Approximating Orthogonal Matrices with Effective Givens Factorization,Oral,"We develo effective aroximation methods for unitary matrices.
In our formulation, a unitary matrix is reresented as a roduct of rotations in two-dimensional subsaces, so-called Givens rotations.
Instead of the quadratic dimension deendence when alying a dense matrix, alying such an aroximation scales with the number factors, each of which can be imlemented efficiently.
Consequently, in settings where an aroximation is once comuted and then alied many times, such an effective reresentation becomes advantageous.
Although efficient Givens factorizations are not ossible for generic unitary oerators, 
we show that minimizing a sarsity-inducing objective with a coordinate descent algorithm on the unitary grou yields good factorizations for structured matrices. 
Canonical alications of such a setu are orthogonal basis transforms.
We demonstrate that our methods imrove the aroximate reresentation of the grah Fourier transform, the matrix obtained when diagonalizing a grah Lalacian.
"
394,2019,Lossless or Quantized Boosting with Integer Arithmetic,Oral,"In suervised learning, efficiency often starts with the choice of a good loss: suort vector machines oularised Hinge loss, Adaboost oularised
the exonential loss, etc. Recent trends in machine learning have
highlighted the necessity for training routines to meet
tight requirements on communication, bandwidth, energy, oerations,
encoding, among others. Fitting the often decades-old state of the art
training routines into these new constraints does not go without ain and uncertainty or
reduction in the original guarantees. 

Our
aer starts with the design of a new strictly roer canonical, twice differentiable loss called the
Q-loss. Imortantly, its mirror udate over
(arbitrary) rational inuts uses only integer arithmetics --
more recisely, the sole use of $+, -, , \times, |.|$. We build a
learning algorithm which is able, under mild assumtions, to achieve a
lossless boosting-comliant training. We
give conditions for a quantization of its main memory footrint,
weights, to be done while keeing the whole algorithm boosting-comliant. Exeriments
dislay that the algorithm can achieve a fast convergence
during the early boosting rounds comared to AdaBoost, even with a weight storage
that can be 30+ times smaller. Lastly, we show that the Bayes risk of the
Q-loss can be used as node slitting criterion for decision trees and
guarantees otimal boosting convergence.
"
395,2019,Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules,Oral,"A key challenge of leveraging data augmentation for neural network training is choosing an effective augmentation olicy from a large search sace of candidate oerations. Proerly chosen augmentation olicies can lead to significant generalization imrovements; however, state-of-the-art aroaches such as AutoAugment are comutationally infeasible to run for an ordinary user. In this aer, we introduce a new data augmentation algorithm, Poulation Based Augmentation (PBA), which generates augmentation olicy schedules orders of magnitude faster than revious aroaches. We show that PBA can match the erformance of AutoAugment with orders of magnitude less overall comute. On CIFAR-10 we achieve a mean test error of 1.46%, which is slightly better than current state-of-the-art. The code for PBA is fully oen source and will be made available.
"
396,2019,Orthogonal Random Forest for Causal Inference,Oral,"We roose orthogonal random forest, an algorithm that incororates double machine learning---a method of using Neyman-orthogonal moments to reduce sensitivity with resect to nuisance arameters to estimate the target arameter---with generalized random forests---a flexible non-arametric method for statistical estimation of conditional moment models using random forests. We rovide a consistency rate and establish asymtotic normality for our estimator. We show that under mild assumtion on the consistency rate of the nuisance estimator, we can achieve the same error rate as an oracle with a riori knowledge of these nuisance arameters. We show that when the nuisance functions have a locally sarse arametrization, then a local $\ell_1$-enalized regression achieves the required rate. We aly our method to estimate heterogeneous treatment effects from observational data with discrete treatments or continuous treatments, and we show that, unlike rior work, our method rovably allows to control for a high-dimensional set of variables under standard sarsity conditions. We also rovide a comrehensive emirical evaluation of our algorithm on both synthetic data and real data, and show that it consistently outerforms baseline aroaches."
397,2019,New results on information theoretic clustering,Oral,"An imurity measure  I is a function that assigns a  vector v to a non-negative value so that the more homogeneous v, with resect to the values of its comonents, the larger its imurity.  We study the roblem of otimizing the clustering of a set of vectors when the quality of the clustering is measured by the Entroy or the Gini imurity. Notwithstanding the wide use in relevant alications of this tye of clustering, what is known in terms of otimal (aroximation) algorithms andor related comlexity  limitations (inaroximability) is still limited.

Our results imrove the state of the art both in terms of best known aroximation guarantees and inaroximability bounds: (i) we give the first olynomial time algorithm for Entroy imurity based clustering with aroximation guarantee indeendent of the number of vectors and  (ii) we show that the roblem of clustering based on entroy imurity does not admit a PTAS. This also imlies on an inaroximability result in information theoretic clustering for robability distributions closing a  roblem left oen in [Chaudhury and McGregor, COLT08] and [Ackermann et al., ECCC11].

We also reort exeriments on a novel clustering method based on the theoretical tools leading to the above results.  Tested for word clustering, our imlementation roduces clusterings with imurity close to those given by state of the art algorithms with the advantage of running u to 3 orders of magnitude faster. 
"
398,2019,Surrogate Losses for Online Learning of Stepsizes in Stochastic Non-Convex Optimization,Oral,"Stochastic Gradient Descent (SGD) has layed a central role in machine learning. However, it requires a carefully hand-icked stesize for fast convergence, which is notoriously tedious and time-consuming to tune. Over the last several years, a lethora of adative gradient-based algorithms have emerged to ameliorate this roblem. In this aer, we roose new surrogate losses to cast the roblem of learning the otimal stesizes for the stochastic otimization of a non-convex smooth objective function onto an online convex otimization roblem. This allows the use of no-regret online algorithms to comute otimal stesizes on the fly. In turn, this results in a SGD algorithm with self-tuned stesizes that guarantees convergence rates that are automatically adative to the level of noise.
"
399,2019,Anomaly Detection With Multiple-Hypotheses Predictions,Oral,"In one-class-learning tasks, only the normal case (foreground) can be modeled with data, whereas the variation of all ossible anomalies is too erratic to be described by samles. Thus, due to the lack of reresentative data, the wide-sread discriminative aroaches cannot cover such learning tasks, and rather generative models,which attemt to learn the inut density of the foreground, are used. However, generative models suffer from a large inut dimensionality (as in images) and are tyically inefficient learners.We roose to learn the data distribution of the foreground more efficiently with a multi-hyotheses autoencoder. Moreover, the model is criticized by a discriminator, which revents artificial data modes not suorted by data, and which enforces diversity across hyotheses. Our multile-hyotheses-based anomaly detection framework allows the reliable identification of out-of-distribution samles. For anomaly detection on CIFAR-10, it yields u to 3.9% oints imrovement over reviously reorted results. On a real anomaly detection task, the aroach reduces the error of the baseline models from 6.8% to 1.5%.
"
400,2019,Learning Models from Data with Measurement Error: Tackling Underreporting,Oral,"Measurement error in observational datasets can lead to systematic bias in inferences based on these datasets. As studies based on observational data are increasingly used to inform decisions with real-world imact, it is critical that we develo a robust set of techniques for analyzing and adjusting for these biases. In this aer we resent a method for estimating the distribution of an outcome given a binary exosure that is subject to underreorting. Our method is based on a missing data view of the measurement error roblem, where the true exosure is treated as a latent variable that is marginalized out of a joint model. We rove three different conditions under which the outcome distribution can still be identified from data containing only an error-rone observations of the exosure. We demonstrate this method on synthetic data and analyze its sensitivity to near violations of the identifiability conditions. Finally, we use this method to estimate the effects of maternal smoking and oioid use during regnancy on childhood obesity, two imort roblems from ublic health. Using the roosed method, we estimate these effects using only subject-reorted drug use data and substantially refine the range of estimates generated by a sensitivity analysis-based aroach. Further, the estimates roduced by our method are consistent with existing literature on both the effects of maternal smoking and the rate at which subjects underreort smoking.
"
401,2019,A Deep Reinforcement Learning Perspective on Internet Congestion Control,Oral,"We resent and investigate a novel and timely alication domain for dee reinforcement learn-ing (RL): Internet congestion control. Congestion control is the core networking task of modulating traffic sources’ data-transmission rates so as to efficiently utilize network caacity.  Congestion control is fundamental to comuter networking research and ractice, and has recently been the subject of extensive attention in light of the advent of Internet services such as live video, augmented and virtual reality, Internet-of-Things, and more. 
We show that casting congestion control as an RL task enables the training of dee network olicies that cature intricate atterns in data traffic and network conditions, and leveraging this to outerform state-of-the-art congestion control schemes.Alongside these romising ositive results, we also highlight significant challenges facing real-world adotion of RL-based congestion control solutions, such as fairness, safety, and generalization, which are not trivial to address within conventional RL formalism. To facilitate further research of these challenges and reroducibility of our results, we resent a test suite for RL-guided congestion control based on the OenAI Gym interface.
"
402,2019,Sorting Out Lipschitz Function Approximation,Oral,"Training neural networks subject to a Lischitz constraint is useful for generalization bounds, rovable adversarial robustness, interretable gradients, and Wasserstein distance estimation. By the comosition roerty of Lischitz functions, it suffices to ensure that each individual affine transformation or nonlinear activation function is 1-Lischitz. The challenge is to do this while maintaining the exressive ower. We identify a necessary roerty for such an architecture: each of the layers must reserve the gradient norm during backroagation. Based on this, we roose to combine a gradient norm reserving activation function, GrouSort, with norm-constrained weight matrices. We show that norm-constrained GrouSort architectures are universal Lischitz function aroximators. Emirically, we show that norm-constrained GrouSort networks achieve tighter estimates of Wasserstein distance than their ReLU counterarts and can achieve rovable adversarial robustness guarantees with little cost to accuracy.
"
403,2019,Towards Understanding Knowledge Distillation,Oral,"Knowledge distillation, i.e., one classifier being trained on the oututs of another classifier, is an emirically very successful technique for knowledge transfer between classifiers. It has even been observed that classifiers learn much faster and more reliably if trained with the oututs of another classifier as soft labels, instead of from ground truth data. So far, however, there is no satisfactory theoretical exlanation of this henomenon.
In this work, we rovide the first insights into the working mechanisms of distillation by studying the secial case of linear and dee linear classifiers. Secifically, we rove a generalization bound that establishes fast convergence of the exected risk of a distillation-trained linear classifier. From the bound and its roof we extract three key factors that determine the success of distillation: 
* data geometry -- geometric roerties of the data distribution, in articular class searation, has a direct influence on the convergence seed of the risk;
* otimization bias -- gradient descent otimization finds a very favorable minimum of the distillation objective; and
* strong monotonicity -- the exected risk of the student classifier always decreases when the size of the training set grows.
"
404,2019,Incorporating Grouping Information into Bayesian Decision Tree Ensembles,Oral,"We consider the roblem of nonarametric regression in the high-dimensional setting in which $P \gg N$. We study the use of overlaing grou structures to imrove rediction and variable selection. These structures arise commonly when analyzing DNA microarray data, where genes can naturally be groued according to genetic athways. We incororate overlaing grou structure into a Bayesian additive regression trees model using a rior constructed so that, if a variable from some grou is used to construct a slit, this increases the robability that subsequent slits will use redictors from the same grou. We refer to our model as an overlaing grou Bayesian additive regression trees (OG-BART) model, and our rior on the slits an overlaing grou Dirichlet (OG-Dirichlet) rior. Like the sarse grou lasso, our rior encourages sarsity both within and between grous. We study the correlation structure of the rior, illustrate the roosed methodology on simulated data, and aly the methodology to gene exression data to learn which genetic athways are redictive of breast cancer tumor metastasis. "
405,2019,Transferable Adversarial Training: A General Approach to Adapting Deep Classifiers,Oral,"Domain adatation enables knowledge transfer from a labeled source domain to an unlabeled target domain. A mainstream aroach is adversarial feature adatation, which learns domain-invariant reresentations through aligning the feature distributions of both domains. However, a theoretical rerequisite of domain adatation is the adatability measured by the exected risk of an ideal joint hyothesis over the source and target domains. In this resect, adversarial feature adatation may otentially deteriorate the adatability, since it distorts the original feature distributions when suressing domain-secific variations. To this end, we roose transferable adversarial training (TAT) to enable the adatation of dee classifiers. The aroach generates transferable examles to fill in the ga between the source and target domains, and adversarially trains the dee classifiers to make consistent redictions over transferable examles.
Without learning domain-invariant reresentations at the exense of distorting the feature distributions, the adatability in the theoretical learning bound is algorithmically guaranteed. A series of exeriments validate that our aroach advances the state-of-the-arts on a variety of domain adatation tasks in vision and NLP, including object recognition, learning from synthetic to real, and sentiment classification.
"
406,2019,MONK --  Outlier-Robust Mean Embedding Estimation by Median-of-Means,Oral,"Mean embeddings rovide an extremely flexible and owerful tool in machine learning and statistics to reresent robability distributions and define a semi-metric (MMD, maximum mean discreancy; also called N-distance or energy distance), with numerous successful alications. The reresentation is constructed as the exectation of the feature ma defined by a kernel. As a mean, its classical emirical estimator, however, can be arbitrary severely affected even by a single outlier in case of unbounded features. To the best of our knowledge, unfortunately even the consistency of the existing few techniques trying to alleviate this serious sensitivity bottleneck is unknown. In this aer, we show how the recently emerged rincile of median-of-means can be used to design estimators for kernel mean embedding and MMD with excessive resistance roerties to outliers, and otimal sub-Gaussian deviation bounds under mild assumtions.
"
407,2019,Efficient Dictionary Learning with Gradient Descent,Oral,"Randomly initialized first-order otimization algorithms are the method of choice for solving many high-dimensional nonconvex roblems in machine learning, yet general theoretical guarantees cannot rule out convergence to critical oints of oor objective value. For some highly structured nonconvex roblems however, the success of gradient descent can be understood by studying the geometry of the objective. We study one such roblem -- comlete orthogonal dictionary learning, and rovide converge guarantees for randomly initialized gradient descent to the neighborhood of a global otimum. The resulting rates scale as low order olynomials in the dimension even though the objective ossesses an exonential number of saddle oints. This efficient convergence can be viewed as a consequence of negative curvature normal to the stable manifolds associated with saddle oints, and we rovide evidence that this feature is shared by other nonconvex roblems of imortance as well. 
"
408,2019,Model-Based Active Exploration,Oral,"Efficient exloration is an unsolved roblem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This aer introduces an efficient active exloration algorithm, Model-Based Active eXloration (MAX), which uses an ensemble of forward models to lan to observe novel events, where novelty is assessed by measuring the otential disagreement between ensemble members using a rinciled criterion derived from the Bayesian ersective. We show emirically that in semi-random discrete environments where directed exloration is critical to make rogress, MAX is at least an order of magnitude more efficient than strong baselines. MAX also scales to high-dimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.
"
409,2019,Variational Implicit Processes,Oral,"We introduce the imlicit rocess (IP), a stochastic rocess that laces imlicitly defined multivariate distributions over any finite collections of random variables. IPs are therefore highly flexible imlicit riors over \emh{functions}, with examles include data simulators, Bayesian neural networks and non-linear transformations of stochastic rocesses. A novel and efficient function sace aroximate Bayesian inference algorithm for IPs, namely the variational imlicit rocesses (VIPs), is derived using generalised wake-slee udates. This method returns simle udate equations and allows scalable hyer-arameter learning with stochastic otimization. Exeriments demonstrate that VIPs return better uncertainty estimates and suerior erformance over existing inference methods for challenging models such as Bayesian LSTMs, Bayesian neural networks, and Gaussian rocesses.
"
410,2019,Improved Parallel Algorithms for Density-Based Network Clustering,Oral,"Clustering large-scale networks is  a central toic in unsuervised learning with many alications in machine learning and data mining. A classic aroach to cluster a network is to identify regions of high edge density. In literature this aroach is catured by two fundamental roblems: the densest subgrah and the $k$-core decomosition roblems. We design massively arallel comutation (MPC) algorithms for these roblems that are considerably faster than rior work. In the case of $k$-core decomosition, our work imroves exonentially on the algorithm rovided by Esfandiari et al.~(ICML'18). Comared to the rior work on densest subgrah resented by Bahmani et al.~(VLDB'12, '14), our result requires quadratically fewer MPC rounds. We comlement our analysis with an exerimental scalability analysis of our techniques.
"
411,2019,Adjustment Criteria for Generalizing Experimental Findings,Oral,"One of the central roblems across the data-driven sciences is of that generalizing exerimental findings  across  changing  conditions,  for  instance, whether a causal distribution obtained from a controlled exeriment is valid in settings beyond the study oulation. While a roer design and careful execution of the exeriment can suort, under mild conditions, the validity of inferences about the oulation in which the exeriment was conducted,  two challenges make the extraolation ste difficult – transortability and samling selection bias.  The former oses the question of whether the domain (i.e., settings, oulation, environment) where the exeriment is realized differs from the target domain in their distributions and causal mechanisms; the latter refers to distortions in the samle’s roortions due to referential selection of units into the study.  In this aer, we investigate the assumtions and machinery necessary for using covariate adjustment to correct for the biases generated by both of these roblems, to generalize biased exerimental data to infer causal effect in the target domain. We rovide comlete grahical conditions to determine if a set of covariates is admissible for adjustment.
Building on the grahical characterization,  we develo an efficient algorithm that enumerates all ossible admissible sets with oly-time delay guarantee; this can be useful for when some variables are referred over the others due to different costs or amenability to measurement.
"
412,2019,Kernel Mean Matching for Content Addressability of GANs,Oral,"We roose a novel rocedure which adds ""content-addressability"" to any given unconditional imlicit model e.g., a generative adversarial network (GAN). The rocedure allows users to control the generative rocess by secifying a set (arbitrary size) of desired examles based on which similar samles are generated from the model. The roosed aroach, based on kernel mean matching, is alicable to any generative models which transform latent vectors to samles, and does not require retraining of the model. Exeriments on various high-dimensional image generation roblems (CelebA-HQ, LSUN bedroom, bridge, tower) show that our aroach is able to generate images which are consistent with the inut set, while retaining the image quality of the original model. To our knowledge, this is the first work that attemts to construct, at test time, a content-addressable generative model from a trained marginal model. 
"
413,2019,"Graph Element Networks: adaptive, structured computation and memory",Oral,"We exlore the use of grah-structured neural-networks (GNNs) to model satial rocesses in which there is no {\em a riori} grahical structure.  Similar to {\em finite element analysis}, we assign nodes of a GNN to satial locations and use a comutational rocess defined on the grah to model the relationshi between an initial function defined over a sace and a resulting function.   The encoding of inuts to node states, the decoding of node states to oututs, as well as the maings defining the GNN are learned from a training set consisting of data from multile function airs. The locations of the nodes in sace as well as their connectivity can be adjusted during the training rocess.  This grah-based reresentational strategy allows the learned inut-outut relationshi to generalize over the size and even toology of the underlying sace.  We demonstrate this method on a traditional PDE roblem, a hysical rediction roblem from robotics, and a roblem of learning to redict scene images from novel viewoints.
"
414,2019,Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations,Oral,"A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outerform the demonstrator. This is a consequence of the general reliance of IRL algorithms uon some form of mimicry, such as feature-count matching, rather than inferring the underlying intentions of the demonstrator that may have been oorly executed in ractice.  In this aer, we introduce a novel reward learning from observation algorithm, Trajectory-ranked Reward EXtraolation (T-REX), that extraolates beyond a set of (aroximately) ranked demonstrations in order to infer high-quality reward functions from a set of otentially oor demonstrations.
When combined with dee reinforcement learning, we show that this aroach can achieve erformance that is more than an order of magnitude better than the best-erforming demonstration, as well as a state-of-the-art behavioral cloning from observation method, on multile Atari and MuJoCo benchmark tasks. Finally, we demonstrate that T-REX is robust to modest amounts of ranking noise, oening u future ossibilities for automating the ranking rocess, for examle, by watching a learner noisily imrove at a task over time.  
"
415,2019,Plug-and-Play Methods Provably Converge with Properly Trained Denoisers,Oral,"Plug-and-lay (PnP) is a non-convex framework that integrates modern denoising riors, such as BM3D or dee learning-based denoisers, into ADMM or other roximal algorithms. An advantage of PnP is that one  can use re-trained denoisers when there is not sufficient data for end-to-end training. Although PnP has been recently studied extensively and has exhibited great emirical results, theoretical analysis addressing even the most basic question of convergence has been insufficient. In this aer, we theoretically establish convergence of PnP-FBS and PnP-ADMM, without using diminishing stesizes, under a certain Lischitz condition on the denoisers. We then roose a technique, which we call real sectral normalization, to train dee learning-based denoisers that satisfy the roosed Lischitz condition. Finally, we resent exerimental results that validate the theory.
"
416,2019,The advantages of multiple classes for reducing overfitting from test set reuse,Oral,"Excessive reuse of holdout data can lead to overfitting. Yet, there is no concrete evidence of significant overfitting due to holdout reuse in oular multiclass benchmarks.

Known results show that, in the worst-case, revealing the accuracy of $k$ adatively chosen classifiers on a data set of size $n$ allows to create a classifier with bias of $\Theta(\sqrt{kn})$ for any binary rediction roblem. We show a new uer bound of $\tilde O(\max\{\sqrt{k\log(n)(mn)},kn\})$ on the bias that any attack with $k \geq \tilde\Omega(m)$ queries can achieve in a rediction roblem with $m$ classes. Moreover, we show a natural attack that, under lausible technical condition, achieves the nearly matching bias of $\Omega(\sqrt{k(mn)})$. Comlementing our theoretical work, we give new ractical attacks to stress test multiclass benchmarks by aiming to create as large a bias as ossible with a given number of queries. Through extensive exeriments, we show that the additional uncertainty of rediction with a large number of classes indeed mitigates the effect of our best attacks.

Our work extends imortant develoments in understanding of overfitting in adative data analysis to multiclass rediction roblems. In addition it bears out the surrising fact that multiclass rediction roblems are significantly more robust to overfitting from reusing the test set. This hels to exlain why oular multiclass rediction benchmarks, such as ImageNet, may enjoy a longer lifesan than what intuition from the binary case would have suggested. "
417,2019,Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation,Oral,"Adversarial domain adatation has made remarkable advances in learning transferable reresentations for knowledge transfer across domains. While adversarial learning strengthens the feature transferability which the community focuses on, its imact on the feature discriminability has not been fully exlored. In this aer, a series of exeriments based on sectral analysis of the feature reresentations have been conducted, revealing an unexected deterioration of the discriminability while learning transferable features adversarially. Our key finding is that the eigenvectors with the largest singular values will dominate the feature transferability. As a consequence, the transferability is enhanced at the exense of over enalization of other eigenvectors that embody rich structures crucial for discriminability. Towards this roblem, we resent Batch Sectral Penalization (BSP), a general aroach to enalizing the largest singular values so that other eigenvectors can be relatively strengthened to boost the feature discriminability. Exeriments show that the aroach significantly imroves uon reresentative adversarial domain adatation methods to achieve state-of-art results.
"
418,2019,Neural Inverse Knitting: From Images to Manufacturing Instructions,Oral,"Motivated by the recent otential of mass customization brought by whole-garment knitting machines, we introduce the new roblem of automatic machine instruction generation using a single image of the desired hysical roduct, which we aly to
machine knitting.
We roose to tackle this roblem by directly learning to synthesize regular machine instructions from real images.
We create a cured dataset of real samles with their instruction counterart and roose to use synthetic images to augment it in a novel way.
We theoretically motivate our data mixing framework and show emirical results suggesting that making real images look more synthetic is beneficial in our roblem setu.
We will make our dataset and code ublicly available for reroducibility and to motivate further research related to manufacturing and rogram synthesis.
"
419,2019,Submodular Observation Selection and Information Gathering for Quadratic Models,Oral,"We study the roblem of selecting a subset of observations that enable accurate rediction of unknown arameters, i.e., the task of choosing the most informative observations from a otentially significantly larger set. This roblem arises in a variety of settings in machine learning and signal rocessing including feature selection, hase retrieval, and target localization. For quadratic measurement models, the moment matrix is generally unknown and the ractice of otimizing the so-called alhabetical selection criteria no longer guarantees selection of a near-otimal subset. Majority of rior work resorts to aroximation techniques such as linearization of the measurement model to otimize the utility of an aroximate moment matrix. In contrast, by exloiting a connection to the classical Van Trees' inequality, we derive new alhabetical otimality criteria without distorting the relational structure of the measurement model. We further show that under certain conditions on arameters of the roblem these otimality criteria are monotone and (weak) submodular set functions. These results enable us to develo an efficient greedy observation selection algorithm uniquely tailored for quadratic models, and rovide theoretical bounds on its achievable utility. Extensive numerical exeriments demonstrate efficacy of the roosed framework.
"
420,2019,Training CNNs with Selective Allocation of Channels,Oral,"Recent rogress in dee convolutional neural networks (CNNs) have enabled a simle aradigm of architecture design: larger models tyically achieve better accuracy. Due to this, in modern CNN architectures, it becomes more imortant to design models that generalize well under certain resource constraints, e.g. the number of arameters. In this aer, we roose a simle way to imrove the caacity of any CNN model having large-scale features, without adding more arameters. In articular, we modify a standard convolutional layer to have a new functionality of channel-selectivity, so that the layer is trained to select imortant channels to re-distribute their arameters. Our exerimental results under various CNN architectures and datasets demonstrate that the roosed new convolutional layer allows new otima that generalize better via efficient resource utilization, comared to the baseline.
"
421,2019,Discovering Latent Covariance Structures for Multiple Time Series,Oral,"Analyzing multivariate time series data is imortant to redict future events and changes of comlex systems in finance, manufacturing, and administrative decisions. The exressiveness ower of Gaussian Process (GP) regression methods has been significantly imroved by comositional covariance structures. In this aer, we resent a new GP model which naturally handles multile time series by lacing an Indian Buffet Process (IBP) rior on the resence of shared kernels. Our selective covariance structure decomosition allows exloiting shared arameters over a set of multile, selected time series. We also investigate the well-definedness of the models when infinite latent comonents are introduced. We resent a ragmatic search algorithm which exlores a larger structure sace efficiently. Exeriments conducted on five real-world data sets demonstrate that our new model outerforms existing methods in term of structure discoveries and redictive erformances.
"
422,2019,Conditional Independence in Testing Bayesian Networks,Oral,"Testing Bayesian Networks (TBNs) were introduced recently to reresent a set of distributions, one of which is selected based on the given evidence and used for reasoning. TBNs are more exressive than classical Bayesian Networks (BNs): Marginal queries corresond to multi-linear functions in BNs and to iecewise multi-linear functions in TBNs. Moreover, marginal TBN queries are universal aroximators, like neural networks. In this aer, we study conditional indeendence in TBNs, showing that it can be inferred from d-searation as in BNs. We also study the role of TBN exressiveness and indeendence in dealing with the roblem of learning using incomlete models (i.e., ones that are missing nodes or edges from the data-generating model). Finally, we illustrate our results on a number of concrete examles, including a case study on (high order) Hidden Markov Models.
"
423,2019,Learning-to-Learn Stochastic Gradient Descent with Biased Regularization,Oral,"We study the roblem of learning-to-learn: inferring a learning algorithm that works well on tasks samled from an unknown distribution. As class of algorithms we consider Stochastic Gradient Descent on the true risk regularized by the square euclidean distance to a bias vector. We resent an average excess risk bound for such a learning algorithm. This result quantifies the otential benefit of using a bias vector with resect to the unbiased case. We then address the roblem of estimating the bias from a sequence of tasks. We roose a meta-algorithm which incrementally udates the bias, as new tasks are observed. The low sace and time comlexity of this aroach makes it aealing in ractice. We rovide guarantees on the learning ability of the meta-algorithm.  A key feature of our results is that, when the number of tasks grows and their variance is relatively small, our learning-to-learn aroach has a significant advantage over learning each task in isolation by Stochastic Gradient Descent without a bias term. We reort on numerical exeriments which demonstrate the effectiveness of our aroach. 
"
424,2019,Sensitivity Analysis of Linear Structural Causal Models,Oral,"Causal inference requires assumtions about the data generating rocess, many of which are unverifiable from the data. Given that some causal assumtions might be uncertain or disuted, formal methods are needed to quantify how sensitive research conclusions are to violations of those assumtions. Although an extensive literature exists on the toic, most results are limited to secific model structures, while a general-urose algorithmic framework for sensitivity analysis is still lacking. In this aer, we develo a formal, systematic aroach to sensitivity analysis for arbitrary linear Structural Causal Models (SCMs). We start by formalizing sensitivity analysis as a constrained identification roblem. We then develo an efficient, grah-based identification algorithm that exloits non-zero constraints on both directed and bidirected edges. This allows researchers to systematically derive sensitivity curves for a target causal quantity with an arbitrary set of ath coefficients and error covariances as sensitivity arameters. These results can be used to dislay the degree to which violations of causal assumtions affect the target quantity of interest, and to judge, on scientific grounds, whether roblematic degrees of violations are lausible.
"
425,2019,Riemannian adaptive stochastic gradient algorithms on matrix manifolds,Oral,"Adative stochastic gradient algorithms in the Euclidean sace have attracted much attention lately. Such exlorations on Riemannian manifolds, on the other hand, are relatively new, limited, and challenging. This is because of the intrinsic non-linear structure of the underlying manifold and the absence of a canonical coordinate system. In machine learning alications, however, most manifolds of interest are reresented as matrices with notions of row and column subsaces. In addition, the imlicit manifold-related constraints may also lie on such subsaces. For examle, the Grassmann manifold is the set of column subsaces. To this end, such a rich structure should not be lost by transforming matrices to just a stack of vectors while develoing otimization algorithms on manifolds.

We roose novel stochastic gradient algorithms for roblems on Riemannian manifolds by adating the row and column subsaces of gradients. Our algorithms are rovably convergent and they achieve the convergence rate of order O(log(T)sqrt(T)), where T is the number of iterations. Our exeriments illustrate that the roosed algorithms outerform existing Riemannian adative stochastic algorithms. 
"
426,2019,Submodular Cost Submodular Cover with an Approximate Oracle,Oral,"In this work, we study the Submodular Cost Submodular Cover roblem, which is to minimize the submodular cost required to ensure that the submodular benefit function exceeds a given threshold. Existing aroximation ratios for the greedy algorithm assume a value oracle to the benefit function. However, access to a value oracle is not a realistic assumtion for many alications of this roblem, where the benefit function is difficult to comute. We resent two incomarable aroximation ratios for this roblem with an aroximate value oracle and demonstrate that the ratios take on emirically relevant values through a case study with the Influence Threshold roblem in online social networks.
"
427,2019,Making Convolutional Networks Shift-Invariant Again,Oral,"Modern convolutional networks are not shift-invariant, desite their convolutional nature: small shifts in the inut can cause drastic changes in the outut. Commonly used downsamling methods, such as max-ooling, ignore the classical samling theorem. The well-known fix is alying a low-ass filter before downsamling. However, revious work has assumed that including such anti-aliasing filter necessarily \textit{excludes} max-ooling. We show that when integrated correctly, these oerations are in fact \textit{comatible}. The technique is general and can be incororated across other layer tyes, such as average-ooling and strided-convolution, and alications, such as image classification and translation. In addition, engineering the inductive bias of shift-equivariance largely removes the need for shift-based data augmentation at training time. Our results demonstrate that this classical signal rocessing technique has been overlooked in modern networks.
"
428,2019,Equivariant Transformer Networks,Oral,"How can rior knowledge on the transformation invariances of a domain be incororated into the architecture of a neural network? We roose Equivariant Transformers (ETs), a family of differentiable image-to-image maings that imrove the robustness of models towards re-defined continuous transformation grous. Through the use of secially-derived canonical coordinate systems, ETs incororate functions that are equivariant by construction with resect to these transformations. We show emirically that ETs can be flexibly comosed to imrove model robustness towards more comlicated transformation grous in several arameters. On a real-world image classification task, ETs imrove the samle efficiency of ResNet classifiers, achieving relative imrovements
in error rate of u to 15% in the limited data regime while increasing model arameter count by less than 1%.
"
429,2019,Scalable Training of Inference Networks for Gaussian-Process Models,Oral,"Inference in Gaussian rocess (GP) models is comutationally challenging for large data, and often difficult to aroximate with a small number of inducing oints. We exlore an alternative aroximation that emloys stochastic inference networks (e.g., Bayesian neural networks) for a flexible inference. Unfortunately, for such networks, minibatch training is difficult to be able to learn meaningful correlations over function oututs for a large dataset. We roose an algorithm that enables such training by tracking a stochastic, functional mirror-descent algorithm. At each iteration, this only requires considering a finite number of inut locations, resulting in a scalable and easy-to-imlement algorithm. Emirical results show comarable and, sometimes, suerior erformance to existing sarse variational GP methods.
"
430,2019,On the statistical rate of nonlinear recovery in generative models with heavy-tailed data,Oral,"We consider estimating a high-dimensional vector from non-linear measurements where the unknown vector is reresented by a generative model $G:\mathbb{R}^k\rightarrow\mathbb{R}^d$ with $k\ll d$. Such a model oses structural riors on the unknown vector without having a dedicated basis, and in articular allows new and efficient aroaches solving recovery roblems with number of measurements far less than the ambient dimension of the vector. While rogresses have been made recently regarding theoretical understandings on the linear Gaussian measurements, much less is known when the model is ossibly missecified and the measurements are non-Gaussian. 
In this aer, we make a ste towards such a direction by considering the scenario where the measurements are non-Gaussian, subject to ossibly unknown nonlinear transformations and the resonses are heavy-tailed. We then roose new estimators via score functions based on the first and second order Stein's identity, and rove the samle size bound of 
$m=\mathcal{O}(k\varesilon^{-2}\log(L\varesilon))$ achieving an $\varesilon$ error in the form of exonential concentration inequalities. Furthermore, for the secial case of multi-layer ReLU generative model, we imrove the samle bound by a logarithm factor to $m=\mathcal{O}(k\varesilon^{-2}\log(d))$, matching the state-of-art statistical rate in comressed sensing for estimating $k$-sarse vectors. 
On the technical side, we develo new chaining methods bounding heavy-tailed rocesses, which could be of indeendent interest.
"
431,2019,Distributional Multivariate Policy Evaluation and Exploration with the Bellman GAN,Oral,"The recently roosed distributional aroach to reinforcement learning (DiRL) is centered on learning the distribution of the reward-to-go, often referred to as the value distribution. In this work, we show that the distributional Bellman equation, which drives DiRL methods, is equivalent to a generative adversarial network (GAN) model. In this formulation, DiRL can be seen as learning a dee generative model of the value distribution, driven by the discreancy between the distribution of the current value, and the distribution of the sum of current reward and next value. We use this insight to roose a GAN-based aroach to DiRL, which leverages the strengths of GANs in learning distributions of high dimensional data. In articular, we show that our GAN aroach can be used for DiRL with multivariate rewards, an imortant setting which cannot be tackled with rior methods. The multivariate setting also allows us to unify learning the distribution of values and state transitions, and we exloit this idea to devise a novel exloration method that is driven by the discreancy in estimating both values and states.
"
432,2019,BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning,Oral,"Multi-task learning allows the sharing of useful information between multile related tasks. In natural language rocessing several recent aroaches have successfully leveraged unsuervised re-training on large amounts of data to erform well on various tasks, such as those in the GLUE benchmark. These results are based on fine-tuning on each task searately. We exlore the multi-task learning setting for the recent BERT model on the GLUE benchmark, and how to best add task-secific arameters to a re-trained BERT network, with a high degree of arameter sharing between tasks. We introduce new adatation modules, PALs or ‘rojected attention layers’, which use a low-dimensional multi-head attention mechanism, based on the idea that it is imortant to include layers with inductive biases useful for the inut domain. By using PALs in arallel with BERT layers, we match the erformance of fine-tuned BERT on the GLUE benchmark with ≈7 times fewer arameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset.
"
433,2019,More Efficient Off-Policy Evaluation through Regularized Targeted Learning,Oral,"We study the roblem of off-olicy evaluation (OPE) in Reinforcement Learning (RL), where the aim is to estimate the erformance of a new olicy given historical data that may have been generated by a different olicy, or olicies. In articular, we introduce a novel doubly-robust estimator for the OPE roblem in RL, based on the Targeted Maximum Likelihood Estimation rincile from the statistical causal inference literature. We also introduce several variance reduction techniques that lead to imressive erformance gains in off-olicy evaluation. We show emirically that our estimator uniformly wins over existing off-olicy evaluation methods across multile RL environments and various levels of model missecification. Finally, we further the existing theoretical analysis of estimators for the RL off-olicy estimation roblem by showing their $O_P(1\sqrt{n})$ rate of convergence and characterizing their asymtotic distribution."
434,2019,Generative Modeling of Infinite Occluded Objects for Compositional Scene Representation,Oral,"We resent a dee generative model which exlicitly models object occlusions for comositional scene reresentation. Latent reresentations of objects are disentangled into location, size, shae, and aearance, and the visual scene can be generated comositionally by integrating these reresentations and an infinite-dimensional binary vector indicating resences of objects in the scene. By training the model to learn satial deendences of ixels in the unsuervised setting, the number of objects, ixel-level segregation of objects, and resences of objects in overlaing regions can be estimated through inference of latent variables. Extensive exeriments conducted on a series of secially designed datasets demonstrate that the roosed method outerforms two state-of-the-art methods when object occlusions exist.
"
435,2019,Overcoming Multi-model Forgetting,Oral,"We identify a henomenon, which we refer to as multi-model forgetting, that occurs when sequentially training multile dee networks with artially-shared arameters; the erformance of reviously-trained models degrades as one otimizes a subsequent one, due to the overwriting of shared arameters. To overcome this, we introduce a statistically-justified weight lasticity loss that regularizes the learning of a model's shared arameters according to their imortance for the revious models, and demonstrate its effectiveness when training two models sequentially and for neural architecture search. Adding weight lasticity in neural architecture search reserves the best models to the end of the search and yields imroved results in both natural language rocessing and comuter vision tasks.
"
436,2019,Bayesian Optimization Meets Bayesian Optimal Stopping,Oral,"Bayesian otimization (BO) is a oular aradigm for otimizing the hyerarameters of machine learning (ML) models due to its samle efficiency. Many ML models require running an iterative training rocedure (e.g., stochastic gradient descent). This motivates the question whether information available during the training rocess (e.g., validation accuracy after each eoch) can be exloited for imroving the eoch efficiency of BO algorithms by early-stoing model training under hyerarameter settings that will end u under-erforming and hence eliminating unnecessary training eochs. This aer rooses to unify BO (secifically, Gaussian rocess-uer confidence bound (GP-UCB)) with Bayesian otimal stoing (BO-BOS) to boost the eoch efficiency of BO. To achieve this, while GP-UCB is samle-efficient in the number of function evaluations, BOS comlements it with eoch efficiency for each function evaluation by roviding a rinciled otimal stoing mechanism for early stoing. BO-BOS reserves the (asymtotic) no-regret erformance of GP-UCB using our secified choice of BOS arameters that is amenable to an elegant interretation in terms of the exloration-exloitation trade-off. We emirically evaluate the erformance of BO-BOS and demonstrate its generality in hyerarameter otimization of ML models and two other interesting alications.
"
437,2019,Stochastic Optimization for DC Functions and Non-smooth Non-convex Regularizers with Non-asymptotic Convergence,Oral,"Difference of convex (DC) functions cover a broad family of non-convex and ossibly non-smooth and non-differentiable functions, and have wide alications in machine learning and statistics. Although deterministic algorithms for DC functions have been extensively studied, stochastic otimization that is more suitable for learning with big data remains under-exlored. In this aer, we roose new stochastic otimization algorithms and study their first-order convergence theories for solving a broad family of DC functions. We imrove the existing algorithms and theories of stochastic otimization for DC functions from both ractical and theoretical ersectives. Moreover, we extend the roosed stochastic algorithms for DC functions to solve roblems with a general non-convex non-differentiable regularizer, which does not necessarily have a DC decomosition but enjoys an efficient roximal maing.  To the best of our knowledge, this is the first work that gives the first non-asymtotic convergence for solving non-convex otimization whose objective has a general non-convex non-differentiable regularizer.
"
438,2019,"Submodular Streaming in All Its Glory: Tight Approximation, Minimum Memory and Low Adaptive Complexity",Oral,"Streaming algorithms are generally judged by the quality of their solution, memory footrint, and comutational comlexity. In this aer, we study the roblem of maximizing a  monotone submodular function in the streaming setting with a cardinality constraint $k$. We first roose SIEVE-STREAMING++, which requires just one ass over the data, kees only $O(k)$ elements and achieves the tight $\frac{1}{2}$-aroximation guarantee. The best reviously known streaming algorithms either achieve a subotimal $\frac{1}{4}$-aroximation with $\Theta(k)$ memory or the otimal $\frac{1}{2}$-aroximation with $O(k\log k)$ memory. 

Next, we show that by buffering a small fraction of the stream and alying a careful filtering rocedure, one can heavily reduce the number of adative comutational rounds, thus substantially lowering the comutational comlexity of SIEVE-STREAMING++. We then generalize our results to the more challenging multi-source streaming setting. We show how one can achieve the tight $\frac{1}{2}$-aroximation guarantee with $O(k)$ shared memory, while minimizing not only the rounds of comutations but also the total number of communicated bits. Finally, we demonstrate the efficiency of our algorithms on real-world data summarization tasks for multi-source streams of tweets and of YouTube videos."
439,2019,A Baseline for Any Order Gradient Estimation in Stochastic Computation Graphs,Oral,"By enabling correct differentiation in Stochastic Comutation Grahs (SCGs), the infinitely differentiable Monte-Carlo estimator (DiCE) can generate correct estimates for the higher order gradients that arise in, e.g., multi-agent reinforcement learning and meta-learning. However, the baseline term in DiCE that serves as a control variate for reducing variance alies only to first order gradient estimation, limiting the utility of higher-order gradient estimates. To imrove the samle efficiency of DiCE, we roose a new baseline term for higher order gradient estimation. This term may be easily included in the objective, and roduces unbiased variance-reduced estimators under (automatic) differentiation, without affecting the estimate of the objective itself or of the first order gradient estimate. It reuses the same baseline function (e.g., the state-value function in reinforcement learning) already used for the first order baseline. We rovide theoretical analysis and numerical evaluations of this new baseline, which demonstrate that it can dramatically reduce the variance of DiCE's second order gradient estimators and also show emirically that it reduces the variance of third and fourth order gradients. This comutational tool can be easily used to estimate higher order gradients with unrecedented efficiency and simlicity wherever automatic differentiation is utilised, and it has the otential to unlock alications of higher order gradients in reinforcement learning and meta-learning.
"
440,2019,"Phase transition in PCA with missing data: Reduced signal-to-noise ratio, not sample size!",Oral,"How does missing data affect our ability to learn signal structures? It has been shown that learning signal structure in terms of rincial comonents is deendent on the ratio of samle size and dimensionality and that a critical number of observations is needed before learning starts (Biehl and Mietzner, 1993). Here we generalize this analysis to include missing data. Probabilistic rincial comonent analysis is regularly used for estimating signal structures in datasets with missing data. Our analytic result suggest that the effect of  missing data  is to effectively reduce signal-to-noise ratio rather than - as generally believed - to reduce samle size. The theory redicts a hase transition in the learning curves and this is indeed found both in simulation data and in real datasets.
"
441,2019,Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation,Oral,"Dee unsuervised domain adatation (Dee UDA) methods successfully leverage readily-accessible labeled source data to boost the erformance on relevant but unlabeled target data. However, algorithm comarison is cumbersome in Dee UDA due to the lack of a satisfying and standardized model selection method, osing an obstacle to further advances in the field.  Existing model selection methods for Dee UDA are either highly biased, constrained, unstable, or controversial (requiring labeled target data). To this end, we roose Dee Embedded Validation (\textbf{DEV}), which embeds adated feature reresentation into the validation rocedure to obtain unbiased target risk estimation with bounded variance.  Variance is further reduced by the technique of control variate. The effectiveness of the roosed method is validated both theoretically and emirically. 
"
442,2019,Bayesian Nonparametric Federated Learning of Neural Networks,Oral,"In federated learning roblems, data is scattered across different servers and exchanging or ooling it is often imractical or rohibited. We develo a Bayesian nonarametric framework for federated learning with neural networks. Each data server is assumed to rovide local neural network weights, which are modeled through our framework. We then develo an inference aroach that allows us to synthesize a more exressive global network without additional suervision, data ooling and with as few as a single communication round. We then demonstrate the efficacy of our aroach on federated learning roblems simulated from two oular image classification datasets.
"
443,2019,Remember and Forget for Experience Replay,Oral,"Exerience relay (ER) is a fundamental  comonent of off-olicy dee reinforcement learning (RL). ER recalls exeriences from ast iterations to comute gradient estimates for the current olicy, increasing data-efficiency. However, the accuracy of such udates may deteriorate when the olicy diverges from ast behaviors and can undermine the erformance of ER. Many algorithms mitigate this issue by tuning hyer-arameters to slow down olicy changes. An alternative is to actively manage the exeriences in the relay memory.
We introduce Remember and Forget Exerience Relay (ReF-ER), a novel method that can enhance RL algorithms with arameterized olicies. ReF-ER (1) skis gradients comuted from exeriences that are too unlikely with the current olicy and (2) regulates olicy changes within a trust region of the relayed behaviors. We coule ReF-ER with Q-learning, deterministic olicy gradient and off-olicy gradient methods. We find that ReF-ER consistently imroves the erformance of continuous-action, off-olicy RL on fully observable benchmarks and artially observable flow control roblems.
"
444,2019,Hiring Under Uncertainty,Oral,"In this aer we introduce the hiring under uncertainty roblem to model the questions faced by hiring committees in large enterrises and universities alike. Given a set of $n$ eligible candidates, the decision maker needs to choose the sequence of candidates to make offers so as to hire the $k$ best candidates. However, candidates may choose to reject an offer (for instance, due to a cometing offer) and the decision maker has a time limit by which all ositions must be filled. Given an estimate of the robabilities of accetance for each candidate, the hiring under uncertainty roblem is to design a strategy of making offers so that total exected value of all candidates hired by the time limit is maximized."
445,2019,Alternating Minimizations Converge to Second-Order Optimal Solutions,Oral,"This work studies the second-order convergence for both standard alternating minimization and roximal alternating minimization. We show that under mild assumtions on the (nonconvex) objective function, both algorithms avoid strict saddles almost surely from random initialization. Together with known first-order convergence results, this imlies both algorithms converge to a second-order stationary oint. This solves an oen roblem for the second-order convergence of alternating minimization algorithms that have been widely used in ractice to solve large-scale nonconvex roblems due to their simle imlementation, fast convergence, and suerb emirical erformance.
"
446,2019,On Medians of (Randomized) Pairwise Means,Oral,"Tournament rocedures, recently introduced in \cite{lugosi2016risk}, offer an aealing alternative, from a theoretical ersective at least, to the rincile of \textit{Emirical Risk Minimization} in machine learning. Statistical learning by  Median-of-Means (MoM) basically consists in segmenting the training data into blocks of equal size and comaring the statistical erformance of every air of candidate decision rules on each data block: that with highest erformance on the majority of the blocks is declared as the winner. In the context of nonarametric regression, functions having won all their duels have been shown to outerform emirical risk minimizers w.r.t. the mean squared error under minimal assumtions, while exhibiting robustness roerties. It is the urose of this aer to extend this aroach, in order to address other learning roblems in articular, for which the erformance criterion takes the form of an exectation over airs of observations rather than over one single observation, as may be the case in airwise ranking, clustering or metric learning. Precisely, it is roved here that the bounds achieved by MoM are essentially conserved when the blocks are built by means of indeendent samling without relacement schemes instead of a simle segmentation. These results are next extended to situations where the risk is related to a airwise loss function and its emirical counterart is of the form of a $U$-statistic. Beyond theoretical results guaranteeing the erformance of the learningestimation methods roosed, some numerical exeriments rovide emirical evidence of their relevance in ractice."
447,2019,IMEXnet - A Forward Stable Deep Neural Network,Oral,"Dee convolutional neural networks have revolutionized
many machine learning and comuter
vision tasks. Desite their enormous success,
remaining key challenges limit their wider use.
Pressing challenges include imroving the network’s
robustness to erturbations of the inut images
and simlifying the design of architectures
that generalize. Another roblem relates to the
limited “field of view” of convolution oerators,
which means that very dee networks are required
to model nonlocal relations in high-resolution image
data.
We introduce the IMEXnet that addresses these
challenges by adating semi-imlicit methods for
artial differential equations. Comared to similar
exlicit networks such as the residual networks
(ResNets) our network is more stable. This stability
has been recently shown to reduce the sensitivity
to small changes in the inut features and imrove
generalization. The imlicit ste connects
all ixels in the images and therefore addresses
the field of view roblem, while being comarable
to standard convolutions in terms of the number
of arameters and comutational comlexity.
We also resent a new dataset for semantic segmentation
and demonstrate the effectiveness of
our architecture using the NYU deth dataset.
"
448,2019,Inferring Heterogeneous Causal Effects in Presence of Spatial Confounding,Oral,"We address the roblem of inferring the causal effect of an exosure on an outcome across sace, using observational data. The data is ossibly subject to unmeasured confounding variables which, in a standard aroach, must be adjusted for by estimating a nuisance function. Here we develo a method that eliminates the nuisance function, while mitigating the resulting errors-in-variables. The result is a robust and accurate inference method for satially varying heterogeneous causal effects. The roerties of the method are demonstrated on synthetic as well as real data from Germany and the US.
"
449,2019,Learning interpretable continuous-time models of latent stochastic dynamical systems,Oral,"We develo an aroach to learn an interretable semi-arametric model of a latent continuous-time stochastic dynamical system, assuming noisy high-dimensional oututs samled at uneven times. The dynamics are described by a nonlinear
stochastic differential equation (SDE) driven by a Wiener rocess, with a drift evolution function drawn from a Gaussian rocess (GP) conditioned on a set of learnt fixed oints and corresonding local Jacobian matrices.  This form yields a
flexible nonarametric model of the dynamics, with a reresentation corresonding directly to the interretable ortraits routinely emloyed in the study of nonlinear dynamical systems.  The learning algorithm combines inference of continuous latent aths underlying observed data with a sarse variational descrition of the dynamical rocess. We demonstrate our aroach on simulated data from different nonlinear dynamical systems.
"
450,2019,How does Disagreement Help Generalization against Label Corruption?,Oral,"Learning with noisy labels is one of the hottest roblems in weakly-suervised learning. Based on memorization effects of dee neural networks, training on small-loss samles becomes very romising for handling noisy labels. This fosters the state-of-the-art aroach ""Co-teaching"" that cross-trains two dee neural networks using small-loss trick. However, with the increase of eochs, two networks will converge to a consensus gradually and Co-teaching reduces to the self-training MentorNet. To tackle this issue, we roose a robust learning aradigm called Co-teaching+, which bridges the ""Udate by Disagreement"" strategy with the original Co-teaching. First, two networks redict all data, and feed forward rediction disagreement data only. Then, among such disagreement data, each network selects its small-loss data, but back roagates the small-loss data by its eer network and udates its own arameters. Emirical results on noisy benchmark datasets demonstrate that Co-teaching+ is much suerior to many state-of-the-art methods in the robustness of trained models.
"
451,2019,Do ImageNet Classifiers Generalize to ImageNet?,Oral,"Generalization is the main goal in machine learning, but few researchers systematically investigate how well models erform on truly unseen data. This raises the danger that the community may be overfitting to excessively re-used test sets. To investigate this question, we conduct a novel reroducibility exeriment on CIFAR-10 and ImageNet by assembling new test sets and then evaluating a wide range of classification models. Desite our careful efforts to match the distribution of the original datasets, the accuracy of many models dros around 10%. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results show that the accuracy dros are likely not caused by adative overfitting, but by the models' inability to generalize reliably to slightly ""harder"" images than those found in the original test set.
"
452,2019,Active Embedding Search via Noisy Paired Comparisons,Oral,"Suose that we wish to estimate a user’s reference vector w from aired comarisons of the form “does user w refer item  or item q?,” where both the user and items are embedded in a low-dimensional Euclidean sace with distances that reflect user and item similarities. Such observations arise in numerous settings, including sychometrics and sychology exeriments, search tasks, advertising, and recommender systems. In such tasks, queries can be extremely costly and subject to varying levels of resonse noise; thus, we aim to actively choose airs that are most informative given the results of revious comarisons. We rovide new theoretical insights into the benefits and challenges of greedy information maximization in this setting, and develo two novel heuristics that maximize lower bounds on information gain and are simler to analyze and comute resectively. We use simulated resonses from a real-world dataset to validate our heuristics through their similar erformance to greedy information maximization, and their suerior reference estimation over state-of-the-art selection methods as well as random queries.
"
453,2019,Tensor Variable Elimination for Plated Factor Graphs,Oral,"A wide class of machine learning algorithms can be reduced to variable elimination on factor grahs. While factor grahs rovide a unifying notation for these algorithms, they do not rovide a comact way to exress reeated structure when comared to late diagrams for directed grahical models. To exloit efficient tensor algebra in grahs with lates of variables, we generalize undirected factor grahs to lated factor grahs and variable elimination to a tensor variable elimination algorithm that oerates directly on lated factor grahs. Moreover, we generalize comlexity bounds based on treewidth and characterize the class of lated factor grahs for which inference is tractable. As an alication, we integrate tensor variable elimination into the Pyro robabilistic rogramming language to enable exact inference in discrete latent variable models with reeated structure. We validate our methods with exeriments on both directed and undirected grahical models, including alications to olyhonic music modeling, animal movement modeling, and latent sentiment analysis.
"
454,2019,Adversarially Learned Representations for Information Obfuscation and Inference,Oral,"Data collection and sharing are ervasive asects of modern society. This rocess can either be voluntary, as in the case of a erson taking a facial image to unlock hisher hone, or incidental, such as traffic cameras collecting videos on edestrians. An undesirable side effect of these rocesses is that shared data can carry information about attributes that users might consider as sensitive, even when such information is of limited use for the task. It is therefore desirable for both data collectors and users to design rocedures that minimize sensitive information leakage. Balancing the cometing objectives of roviding meaningful individualized service levels and inference while obfuscating sensitive information is still an oen roblem. In this work, we take an information theoretic aroach that is imlemented as an unconstrained adversarial game between Dee Neural Networks in a rinciled, data-driven manner. This aroach enables us to learn domain-reserving stochastic transformations that maintain erformance on existing algorithms while minimizing sensitive information leakage.
"
455,2019,A Tree-Based Method for Fast Repeated Sampling of Determinantal Point Processes,Oral,"It is often desirable for recommender systems and other information retrieval alications to rovide diverse results, and determinantal oint rocesses (DPPs) have become a oular way to cature the trade-off between the quality of individual results and the diversity of the overall set.  However, comutational concerns limit the usefulness of DPPs in ractice. Samling from a DPP is inherently exensive: if the underlying collection contains N items, then generating each DPP samle requires O(N) time following a one-time rerocessing hase. Additionally, results often need to be ersonalized to a user, but standard aroaches to ersonalization invalidate the rerocessing, making ersonalized samles esecially exensive. In this work we address both of these shortcomings. First, we roose a new algorithm for generating DPP samles in O(log N) time following a slightly more exensive rerocessing hase. We then extend the algorithm to suort arbitrary query-time feature weights, allowing us to generate samles customized to individual users while still retaining logarithmic runtime. Exeriments show that our aroach runs over 300 times faster than traditional DPP samling on collections of 100,000 items for samles of size 10.
"
456,2019,Position-aware Graph Neural Networks,Oral,"Learning node embedding that catures the osition of the node within a broader grah structure is crucial for many rediction tasks on grahs.
However, while exressive and most oular, existing Grah Neural Network (GNN) aroaches have limited ower for reresenting ositionslocations of nodes in a bigger network structure.

Here we roose {\em Position-aware Grah Neural Networks (P-GNN)}, a new class of GNNs for comuting osition-aware node embeddings. P-GNN first selects a set of anchor nodes, characterizes the distance of a given target node towards the anchor-set, and then learns a non-linear aggregation scheme over the anchor-sets adjacent to the target node. P-GNN has several advantages: it is inductive, scalable, and can incororate node feature information.

We aly P-GNNs to multile rediction tasks including link rediction and community detection. We show that P-GNNs consistently outerform state of the art GNN variants, with an imrovement u to $38\%$ in terms of AUC score."
457,2019,Accelerated Linear Convergence of Stochastic Momentum Methods in Wasserstein Distances,Oral,"Momentum methods such as Polyak's heavy ball (HB) method, Nesterov's accelerated gradient (AG) as well as accelerated rojected gradient (APG) method have been commonly used in machine learning ractice, but their erformance is quite sensitive to noise in the gradients. We study these methods under a first-order stochastic oracle model where noisy estimates of the gradients are available.

For strongly convex roblems, we show that the distribution of the iterates of AG converges with the accelerated $O(\sqrt{\kaa}\log(1\varesilon))$ linear rate to a ball of radius $\varesilon$ centered at a unique invariant distribution in the 1-Wasserstein metric where $\kaa$ is the condition number as long as the noise variance is smaller than an exlicit uer bound we can rovide. Our analysis also certifies linear convergence rates as a function of the stesize, momentum arameter and the noise variance; recovering the accelerated rates in the noiseless case and quantifying the level of noise that can be tolerated to achieve a given erformance. In the secial case of strongly convex quadratic objectives, we can show accelerated linear rates in the $$-Wasserstein  metric for any $\geq 1$ with imroved sensitivity to noise for both AG and HB through a non-asymtotic analysis under some additional assumtions on the noise structure. Our analysis for HB and AG also leads to imroved non-asymtotic convergence bounds in subotimality for both deterministic and stochastic settings which is of indeendent interest. To the best of our knowledge, these are the first linear convergence results for stochastic momentum methods under the stochastic oracle model. We also extend our results to the APG method and weakly convex functions showing accelerated rates when the noise magnitude is sufficiently small."
458,2019,Provably Efficient Imitation Learning from Observation Alone,Oral,"We study Imitation Learning (IL) from Observations alone (ILFO) in large-scale MDPs. While most IL algorithms rely on an exert to directly rovide actions to the learner, in this setting the exert only sulies sequences of observations. We design a new model-free algorithm for ILFO, Forward Adversarial Imitation Learning (FAIL), which learns a sequence of time-deendent olicies by minimizing an Integral Probability Metric between the observation distributions of the exert olicy and the learner. FAIL rovably learns a near-otimal olicy with a number of samles that is olynomial in all relevant arameters but indeendent of the number of unique observations. The resulting theory extends the domain of rovably samle efficient learning algorithms beyond existing results that tyically only consider tabular RL settings or settings that require access to a near-otimal reset distribution.  We also demonstrate the efficacy ofFAIL on multile OenAI Gym control tasks.
"
459,2019,SGD without Replacement: Sharper Rates for General Smooth Convex Functions,Oral,"	We study stochastic gradient descent {\em without relacement} (\sgdwor) for smooth convex functions. \sgdwor is widely observed to converge faster than true \sgd where each samle is drawn indeendently {\em with relacement}~\cite{bottou2009curiously} and hence, is more oular in ractice. But it's convergence roerties are not well understood as samling without relacement leads to couling between iterates and gradients. By using method of exchangeable airs to bound Wasserstein distance, we rovide the first non-asymtotic results for \sgdwor when alied to {\em general smooth, strongly-convex} functions. In articular, we show that \sgdwor converges at a rate of $O(1K^2)$ while \sgd~is known to converge at $O(1K)$ rate, where $K$ denotes the number of asses over data and is required to be {\em large enough}. Existing results for \sgdwor in this setting require additional {\em Hessian Lischitz assumtion}~\cite{gurbuzbalaban2015random,haochen2018random}. 

	For {\em small} $K$, we show \sgdwor can achieve same convergence rate as \sgd for {\em general smooth strongly-convex} functions. Existing results in this setting require $K=1$ and hold only for generalized linear models \cite{shamir2016without}. In addition, by careful analysis of the couling, for both large and small $K$, we obtain better deendence on roblem deendent arameters like condition number. "
460,2019,Fast Direct Search in an Optimally Compressed Continuous Target Space for Efficient Multi-Label Active Learning,Oral,"Active learning for multi-label classification oses fundamental challenges given the comlex label correlations and a otentially large and sarse label sace. We roose a novel CS-BPCA rocess that integrates comressed sensing and Bayesian rincial comonent analysis to erform a two-level label transformation, resulting in an otimally comressed continuous target sace. Besides leveraging correlation and sarsity of a large label sace for effective comression, an otimal comressing rate and the relative imortance of the resultant targets are automatically determined through Bayesian inference. Furthermore, the orthogonality of the transformed sace comletely decoules the correlations among targets, which significantly simlifies multi-label samling in the target sace. We define a novel samling function that leverages  a multi-outut  Gaussian  Process  (MOGP). Gradient-free otimization strategies are develoed to achieve fast online hyer-arameter learning and model retraining for active learning. Exerimental results over multile real-world datasets and comarison with cometitive multi-label active learning models demonstrate the effectiveness of the roosed framework. 
"
461,2019,Detecting Overlapping and Correlated Communities without Pure Nodes: Identifiability and Algorithm,Oral,"Many machine learning roblems come in the form of networks with relational data between entities, and one of the key unsuervised learning tasks is to detect communities from such a network. We adot the mixed-membershi stochastic blockmodel as the underlying robabilistic model, and give conditions under which the membershi of a subset of nodes can be uniquely identified. Our method start by constructing a second-order grah moment, which can be shown to converge to a secific roduct of the true arameters as the size of the network increases. To correctly recover the true membershi arameters, we carefully formulate an otimization roblem using insights from convex geometry. We show that if the true membershis satisfy a so called sufficiently scattered condition, then solving the roosed roblem correctly identifies the ground truth. We also develo an extremely efficient algorithm, which is significantly faster than rior work and with better convergence roerties. Exeriments on synthetic and real data justify the effectiveness of the roosed learning framework for network data.
"
462,2019,Exploring the Landscape of Spatial Robustness,Oral,"The study of adversarial examles has so far largely focused on the l
setting. However, neural networks turn out to be also vulnerable to other, very
natural classes of erturbations such as translations and rotations.
Unfortunately, the standard methods effective in remedying l
vulnerabilities are not as effective in this new regime. 

With the goal of classifier robustness, we thoroughly investigate the vulnerabilities of
neural network--based
classifiers to rotations and translations. We uncover that while data
augmentation generally hels very little, using ideas from robust otimization
and test-time inut aggregation we can significantly imrove robustness.

In our exloration we find that, in contrast to the l case, first-order
methods cannot reliably find fooling inuts. This highlights fundamental
differences in satial robustness as comared to l robustness, and
suggests that we need a more comrehensive understanding of robustness in
general.
"
463,2019,Nonlinear Stein Variational Gradient Descent for Learning Diversified Mixture Models,Oral,"Diversification has been shown to be a owerful mechanism for learning robust models in non-convex settings. A notable examle is learning mixture models, in which enforcing diversity between the different mixture comonents allows us to revent the model collasing henomenon and cature more atterns from the observed data. In this work, we resent a variational aroach for diversity-romoting learning, which leverages the entroy functional as a natural mechanism for enforcing diversity. We develo a simle and efficient functional gradient-based algorithm for otimizing the variational objective function, which rovides a significant generalization of Stein variational gradient descent (SVGD). We test our method on various challenging real world roblems, including dee embedded clustering and dee anomaly detection. Emirical results show that our method rovides an effective mechanism for diversity-romoting learning, achieving substantial imrovement over existing methods. 
"
464,2019,EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis,Oral,"Reducing the test time resource requirements of a neural network while reserving test accuracy is crucial for running inference on low-ower devices. To achieve this goal, we introduce a novel network rearameterization based on the Kronecker-factored eigenbasis (KFE), and then aly Hessian-based structured runing methods in this basis. As oosed to existing Hessian-based runing algorithms which do runing in arameter coordinates, our method works in the KFE where different weights are aroximately indeendent, enabling accurate runing and fast comutation.We demonstrate emirically the effectiveness of the roosed method through extensive exeriments. In articular, we highlight that the imrovements are esecially significant for more challenging datasets and networks. With negligible loss of accuracy, a iterative-runing version gives a 10x reduction in model size and a 8x reduction in FLOPs on wide ResNet32.
"
465,2019,Dead-ends and Secure Exploration in Reinforcement Learning,Oral,"Many interesting alications of reinforcement learning (RL) involve MDPs that include many codedead-end"" states. Uon reaching a dead-end state, the agent continues to interact with the environment in a dead-end trajectory before reaching a terminal state, but cannot collect any ositive reward, regardless of whatever actions are chosen by the agent. The situation is even worse when existence of many dead-end states is couled with distant ositive rewards from any initial state (it is called Bridge Effect). Hence, conventional exloration techniques often incur rohibitively large training stes before convergence. To deal with the bridge effect, we roose a condition for exloration, called security. We next establish formal results that translate the security condition into the learning roblem of an auxiliary value function. This new value function is used to cacodeany"" given exloration olicy and is guaranteed to make it secure. As a secial case, we use this theory and introduce secure random-walk. We next extend our results to the dee RL settings by identifying and addressing two main challenges that arise. Finally, we emirically comare secure random-walk with standard benchmarks in two sets of exeriments including the Atari game of Montezuma's Revenge.
"
466,2019,Adaptive Neural Trees,Oral,"Dee neural networks and decision trees oerate on largely searate aradigms; tyically, the former erforms reresentation learning with re-secified architectures, while the latter is characterised by learning hierarchies over re-secified features with data-driven architectures. We unite the two via adative neural trees (ANTs), a model that incororates reresentation learning into edges, routing functions and leaf nodes of a decision tree, along with a backroagation-based training algorithm that adatively grows the architecture from rimitive modules (e.g., convolutional layers). We demonstrate that, whilst achieving cometitive erformance on classification and regression datasets, ANTs benefit from (i) lightweight inference via conditional comutation, (ii) hierarchical searation of features useful to the redictive task e.g. learning meaningful class associations, such as searating natural vs. man-made objects, and (iii) a mechanism to adat the architecture to the size and comlexity of the training dataset.
"
467,2019,Predicate Exchange: Inference with Declarative Knowledge,Oral,"We address the roblem of conditioning robabilistic models on redicates, as a means to exress declarative knowledge. Models conditioned on redicates rarely have a tractable likelihood; samling from them requires likelihood-free inference. Existing likelihood-free inference methods focus on redicates which exress observations. To address a broader class of redicates, we develo an inference rocedure called redicate exchange, which \emh{softens} redicates.
Soft redicates return values in a continuous Boolean algebra and can serve as a roxy likelihood function in inference. However, softening introduces an aroximation error which deends on a temerature arameter. At zero-temerature redicates are identical, but are often intractable to condition on. At higher temeratures, soft redicates are easier to samle from, but introduce more error. To mitigate this trade-off, we simulate Markov chains at different temeratures and use relica exchange to swa between chains. We imlement redicate exchange through a nonstandard execution of a simulation based model, and rovide a light-weight tool that can be sulanted on to of existing robabilistic rogramming formalisms.  We demonstrate the aroach on sequence models of health and inverse rendering.
"
468,2019,Understanding and Accelerating Particle-Based Variational Inference,Oral,"Particle-based variational inference methods (ParVIs) have gained notable attention in the Bayesian inference area, for their flexible aroximation ability and effective iteration. In this work, we exlore in-deth the ersective of ParVIs as Wasserstein gradient flows and make both theoretical and ragmatic contributions. On the theory side, we unify various finite-article aroximations that existing ParVIs use, and recognize that the aroximation is essentially a comulsory smoothing treatment, in either of two equivalent forms. This novel understanding reveals the assumtion and relations of existing ParVIs, and also insires new ParVIs as we will demonstrate. On the technical side, we roose an acceleration framework and a rinciled bandwidth selection method for general ParVIs. They are based on the develoed theory and our excavation on the geometry of the Wasserstein sace. Exerimental results show the imroved convergence by the acceleration framework and enhanced samle accuracy by the bandwidth selection method.
"
469,2019,Statistics and Samples in Distributional Reinforcement Learning,Oral,"We resent a unifying framework for designing and analysing distributional reinforcement learning (DRL) algorithms in terms of recursively estimating statistics of the return distribution. Our key insight is that DRL algorithms can be decomosed as the combination of some statistical estimator and a method for imuting a return distribution consistent with that set of statistics. With this new understanding, we are able to rovide imroved analyses of existing DRL algorithms as well as construct a new algorithm (EDRL) based uon estimation of the exectiles of the return distribution. We comare EDRL with existing methods on a variety of MDPs to illustrate concrete asects of our analysis.
"
470,2019,Learning Generative Models across Incomparable Spaces,Oral,"Generative Adversarial Networks have shown remarkable success in learning a distribution that faithfully recovers a reference distribution in its entirety. However, in some cases, we may want to only learn some asects (e.g., cluster or manifold structure), while modifying others (e.g., style, orientation or dimension). In this work, we roose an aroach to learn generative models across such incomarable saces, and demonstrate how to steer the learned distribution towards target roerties. A key comonent of our model is the Gromov-Wasserstein distance, a notion of discreancy that comares distributions relationally rather than absolutely. While this framework subsumes current generative models in identically reroducing distributions, its inherent flexibility allows alication to tasks in manifold learning, relational learning and cross-domain learning.
"
471,2019,Connectivity-Optimized Representation Learning via Persistent Homology,Oral,"We study the roblem of learning reresentations with controllable connectivity roerties. This is beneficial in situations when the imosed structure can be leveraged ustream. In articular, we control the connectivity of an autoencoder's latent sace via a novel tye of loss, oerating on information from ersistent homology. Under mild conditions, this loss is differentiable and we resent a theoretical analysis of the roerties induced by the loss. We choose one-class learning as our ustream task and demonstrate that the imosed structure enables informed arameter selection for modeling the in-class distribution via kernel density estimators. Evaluated on comuter vision data, these one-class models exhibit cometitive erformance and, in a low samle size regime, outerform other methods by a large margin. Notably, our results indicate that a single 
autoencoder, trained on auxiliary (unlabeled) data, yields a maing into latent sace that can be reused across datasets for one-class learning.
"
472,2019,Myopic Posterior Sampling for Adaptive Goal Oriented Design of Experiments,Oral,"Bayesian methods for adative decision-making, such as Bayesian otimisation,
active learning, and active search have seen great success in relevant alications.
However, real world data collection tasks are more broad and comlex, as we may need to
achieve a combination of the above goals andor alication secific goals.
In such scenarios, secialised methods have limited alicability.
In this work,
we design a new myoic strategy for a wide class of adative design of
exeriment (DOE) roblems, where we wish to collect data in order to fulfil a given goal.
Our aroach, Myoic Posterior Samling (\ms),
which is insired by the classical osterior samling algorithm
for multi-armed bandits,
enables us to address a broad suite of DOE tasks where a ractitioner may
incororate domain exertise about the system and secify her
desired goal via a reward function.
Emirically, this general-urose strategy is cometitive with more
secialised methods in a wide array of synthetic and real world DOE tasks.
More imortantly, it enables
addressing comlex DOE goals where no existing method seems alicable.
On the theoretical side, we leverage ideas from adative submodularity and
reinforcement learning to derive conditions under which \mss achieves
sublinear regret against natural benchmark olicies.
"
473,2019,On the Complexity of Approximating Wasserstein Barycenters,Oral,"We study the comlexity of aroximating Wassertein barycenter of $m$ discrete measures, or histograms of size $n$ by contrasting two alternative aroaches using entroic regularization. The first aroach is based on the Iterative Bregman Projections (IBP) algorithm for which our novel analysis gives a comlexity bound roortional to $\frac{mn^2}{\varesilon^2}$ to aroximate the original non-regularized barycenter. 
 Using an alternative accelerated-gradient-descent-based aroach, we obtain a comlexity roortional to
$\frac{mn^{2}}{\varesilon} $.
As a byroduct, we show that the regularization arameter in both aroaches has to be roortional to $\e$, which causes instability of both algorithms when the desired accuracy is high. To overcome this issue, we roose a novel roximal-IBP algorithm, which can be seen as a roximal gradient method, which uses IBP on each iteration to make a roximal ste.
We also consider the question of scalability of these algorithms using aroaches from distributed otimization and show that the first algorithm can be imlemented in a centralized distributed setting (masterslave), while the second one is amenable to a more general decentralized distributed setting with an arbitrary network toology."
474,2019,Sever: A Robust Meta-Algorithm for Stochastic Optimization,Oral,"In high dimensions, most machine learning methods are brittle to even a small fraction of structured outliers. To address this, we introduce a new meta-algorithm that can take in a base learner such as least squares or stochastic gradient descent, and harden the learner to be resistant to outliers. Our method, Sever, ossesses strong theoretical guarantees yet is also highly scalable -- beyond running the base learner itself, it only requires comuting the to singular vector of a certain n×d matrix. We aly Sever on a drug design dataset and a sam classification dataset, and find that in both cases it has substantially greater robustness than several baselines. On the sam dataset, with 1% corrutions, we achieved 7.4% test error, comared to 13.4%−20.5% for the baselines, and 3% error on the uncorruted dataset. Similarly, on the drug design dataset, with 10% corrutions, we achieved 1.42 mean-squared error test error, comared to 1.51-2.33 for the baselines, and 1.23 error on the uncorruted dataset.
"
475,2019,Discriminative Regularization for Latent Variable Models with Applications to Electrocardiography,Oral,"Generative models use latent variables to reresent structured variation in high-dimensional data, such as images, audio, and medical waveforms. These latent variables, however, may ignore subtle, yet meaningful features in the data. Some features may redict an outcome of interest (e.g. heart disease) but account for only a small fraction of variation in the data. We roose a generative model training objective that uses a black-box discriminative model as a regularizer to learn reresentations that reserve this redictive variation. With these discriminatively regularized generative models, we visualize and measure variation in the data that influence a black-box redictive model, enabling an exert to better understand each rediction. With this technique, we study models that use electrocardiograms to redict outcomes of clinical interest. We measure our aroach on synthetic and real data with statistical summaries and an exeriment carried out by a hysician.
"
476,2019,Addressing the Loss-Metric Mismatch with Adaptive Loss Alignment,Oral,"In most machine learning training aradigms a fixed, often handcrafted, loss function is assumed to be a good roxy for an underlying evaluation metric. In this work we assess this assumtion by meta-learning an adative loss function to directly otimize the evaluation metric. We roose a samle efficient reinforcement learn- ing aroach for adating the loss dynamically during training. We emirically show how this formulation imroves erformance by simultaneously otimizing the evaluation metric and smoothing the loss land- scae. We verify our method in metric learning and classification scenarios, showing consider- able imrovements over the state-of-the-art on a diverse set of tasks. Imortantly, our method is alicable to a wide range of loss functions and evaluation metrics. Furthermore, the learned olicies are transferable across tasks and data, demonstrating the versatility of the method. 
"
477,2019,Minimal Achievable Sufficient Statistic Learning,Oral,"We introduce Minimal Achievable Sufficient Statistic (MASS) Learning, a training objective for machine learning models whose minimizers are minimal sufficient statistics with resect to the class of functions being otimized over (e.g. dee networks).  In deriving MASS Learning, we also introduce Conserved Differential Information (CDI), an information-theoretic quantity that - unlike standard mutual information - can be usefully alied to deterministically-deendent continuous random variables like the inut and outut of a dee network.  In a series of exeriments, we show that dee networks trained with MASS Learning match state–of–the–art erformance on suervised learning, uncertainty quantification, and adversarial robustness benchmarks.
"
478,2019,Analyzing Federated Learning through an Adversarial Lens,Oral,"Federated learning distributes model training among a multitude of agents, who, guided by rivacy concerns, erform training using their local data but share only model arameter udates, for iterative aggregation at the server to train an overall global model. In this work, we exlore how the federated learning setting gives rise to a new threat, namely model oisoning, which differs from traditional data oisoning. Model oisoning is carried out by an adversary controlling a small number of malicious agents (usually 1) with the aim of causing the global model to misclassify a set of chosen inuts with high conﬁdence. We exlore a number of strategies to carry out this attack on dee neural networks, starting with targeted model oisoning using a simle boosting of the malicious agent’s udate to overcome the effects of other agents. We also roose two critical notions of stealth to detect malicious udates. We byass these by including them in the adversarial objective to carry out stealthy model oisoning. We imrove its stealth with the use of an alternating minimization strategy which alternately otimizes for stealth and the adversarial objective. We also emirically demonstrate that Byzantine-resilient aggregation strategies are not robust to our attacks. Our results indicate that highly constrained adversaries can carry out model oisoning attacks while maintaining stealth, thus highlighting the vulnerability of the federated learning setting and the need to develo effective defense strategies.
"
479,2019,Deep Compressed Sensing,Oral,"Comressed sensing (CS) rovides an elegant framework for recovering sarse signals from comressed measurements. For examle, CS can exloit the structure of natural images and recover an image from only a few random measurements. CS is highly flexible and data efficient, but its alication has been restricted by the strong assumtion of sarsity and costly otimisation rocess. A recent aroach that combines CS with neural network generators has removed the constraint of sarsity, but reconstruction remains slow. Here we roose a novel framework that significantly imroves both the erformance and seed of signal recovery by jointly training a generator and the otimisation rocess for reconstruction via meta-learning. We exlore training the measurements with different objectives, and derive a family of models based on minimising measurement errors. We show that Generative Adversarial Nets (GANs) can be viewed as a secial case in this family of models. Borrowing insights from the CS ersective, we develo a novel way of stabilising GAN training using gradient information from the discriminator.
"
480,2019,Hessian Aided Policy Gradient,Oral,"	Reducing the variance of estimators for olicy gradient has long been the focus of reinforcement learning research.
	While classic algorithms like REINFORCE find an $\esilon$-aroximate first-order stationary oint in $\OM({1}{\esilon^4})$ random trajectory simulations, no rovable  imrovement on the comlexity has been made so far.
	This aer resents a Hessian aided olicy gradient method with the first imroved samle comlexity of $\OM({1}{\esilon^3})$.
	While our method exloits information from the olicy Hessian, it can be imlemented in linear time with resect to the arameter dimension and is hence alicable to sohisticated DNN arameterization.
	Simulations on standard tasks validate the efficiency of our method."
481,2019,Relational Pooling for Graph Representations,Oral,"This work generalizes grah neural networks (GNNs) beyond those based on the Weisfeiler-Lehman (WL) algorithm, grah Lalacians, and grah diffusion kernels.  Our aroach, denoted Relational Pooling (RP), draws from the theory of finite artial exchangeability to rovide a framework with maximal reresentation ower for grahs. RP can work with existing grah reresentation models, and somewhat counterintuitively, can make them even more owerful than the original WL isomorhism test. Additionally, RP is the first theoretically sound framework to use architectures like Recurrent Neural Networks and Convolutional Neural Networks for grah classification. RP also has grah kernels as a secial case. We demonstrate imroved erformance of novel RP-based grah reresentations over current state-of-the-art methods on a number of tasks.
"
482,2019,Hierarchical Decompositional Mixtures of Variational Autoencoders,Oral,"Variational autoencoders (VAEs) have received considerable attention, since they allow us to learn exressive neural density estimators effectively and efficiently. 
However, learning and inference in VAEs is still roblematic due to the sensitive interlay between the generative model and the
inference network. 
Since these roblems become generally more severe in high dimensions, we roose a novel hierarchical mixture model over low-dimensional VAE exerts. 
Our model decomoses the overall learning roblem into many smaller roblems, which are coordinated by the hierarchical mixture, reresented by a sum-roduct network. 
In exeriments we show that our model consistently outerforms classical VAEs on all of our exerimental benchmarks. 
Moreover, we show that our model is highly data efficient and degrades very gracefully in extremely low data regimes.
"
483,2019,Estimate Sequences for Variance-Reduced Stochastic Composite Optimization,Oral,"In this aer, we roose a unified view of gradient-based algorithms for stochastic convex comosite otimization by extending the concet of estimate sequence introduced by Nesterov. This oint of view covers the stochastic gradient descent method, variants of the aroaches SAGA, SVRG, and has several advantages: (i) we rovide a generic roof of convergence for the aforementioned methods; (ii) we show that this SVRG variant is adative to strong convexity; (iii) we naturally obtain new algorithms with the same guarantees; (iv) we derive generic strategies to make these algorithms robust to stochastic noise, which is useful when data is corruted by small random erturbations. Finally, we show that this viewoint is useful to obtain new accelerated algorithms in the sense of Nesterov.
"
484,2019,Efficient learning of smooth probability functions from Bernoulli tests with guarantees,Oral,"We study the fundamental roblem of learning an unknown, smooth robability function via oint-wise Bernoulli tests. We rovide a scalable algorithm for efficiently solving this roblem with rigorous guarantees. In articular, we rove the convergence rate of our osterior udate rule to the true robability function in L2-norm. Moreover, we allow the Bernoulli tests to deend on contextual features, and rovide a modified inference engine with rovable guarantees for this novel setting. Numerical results show that the emirical convergence rates match the theory, and illustrate the sueriority of our aroach in handling contextual features over the state-of-the-art.
"
485,2019,Bayesian Generative Active Deep Learning,Oral,"Dee learning models have demonstrated outstanding erformance in several roblems, but their training rocess tends to require immense amounts of comutational and human resources for training and labeling, constraining the tyes of roblems that can be tackled.
Therefore, the design of effective training methods that require small labeled training sets is an imortant research direction that will allow a more effective use of resources.
Among current aroaches designed to address this issue, two are articularly interesting: data augmentation and active learning.  
Data augmentation achieves this goal by artificially generating new training oints, while active learning relies on the selection of the ``most informative'' subset of unlabeled training samles to be labelled by an oracle.
Although successful in ractice, data augmentation can waste comutational resources because it indiscriminately generates samles that are not guaranteed to be informative, and active learning selects a small subset of informative samles (from a large un-annotated set) that may be insufficient for the training rocess. 
In this aer, we roose a Bayesian generative active dee learning aroach that combines active learning with data augmentation -- we rovide theoretical and emirical evidence (MNIST, CIFAR-$\{10,100\}$, and SVHN) that our aroach has more efficient training and better classification results than data augmentation and active learning."
486,2019,Learning to Route in Similarity Graphs,Oral,"Recently similarity grahs became the leading aradigm for efficient nearest neighbor search, outerforming traditional tree-based and LSH-based methods. Similarity grahs erform the search via greedy routing: a query traverses the grah and in each vertex moves to the adjacent vertex that is the closest to this query. In ractice, similarity grahs are often suscetible to local minima, when queries do not reach its nearest neighbors, getting stuck in subotimal vertices. In this aer we roose to learn the routing function that overcomes local minima via incororating information about the grah global structure. In articular, we augment the vertices of a given grah with additional reresentations that are learned to rovide the otimal routing from the start vertex to the query nearest neighbor. By thorough exeriments, we demonstrate that the roosed learnable routing successfully diminishes the local minima roblem and significantly imroves the overall search erformance.
"
487,2019,Provably Efficient Maximum Entropy Exploration,Oral,"Suose an agent is in a (ossibly unknown) Markov Decision Process in the absence of a reward signal, what might we hoe that an agent can efficiently learn to do? One natural, intrinsically defined, objective roblem is for the agent to learn a olicy which induces a distribution over state sace that is as uniform as ossible, which can be measured in an entroic sense.  We rovide an efficient algorithm to construct such a maximum-entroy exloratory olicy, when given access to a black box lanning oracle (which is robust to function aroximation). Furthermore, when restricted to the tabular setting where we have samle based access to the MDP, our roosed algorithm is rovably efficient method, both in terms of samle size and comutational comlexity. Key to our algorithmic methodology is utilizing the conditional gradient method (a.k.a. the Frank-Wolfe algorithm) which utilizes an aroximate MDP solver.
"
488,2019,The Variational Predictive Natural Gradient,Oral,"Variational inference transforms osterior inference into arametric otimization thereby enabling the use of latent variable models where otherwise imractical. However, variational inference can be finicky when different variational arameters control variables that are strongly correlated under the model. Traditional natural gradients based on the variational aroximation fail to correct for correlations when the aroximation is not the true osterior. To address this, we construct a new natural gradient called the variational redictive natural gradient. It is constructed as an average of the Fisher information of the rearameterized redictive model distribution. Unlike traditional natural gradients for variational inference, this natural gradient accounts for the relationshi between model arameters and variational arameters. We also show the variational redictive natural gradient relates to the negative Hessian of the exected log-likelihood. A simle examle shows the insight. We demonstrate the emirical value of our method on a classification task, a dee generative model of images, and robabilistic matrix factorization for recommendation.
"
489,2019,Finding Mixed Nash Equilibria of Generative Adversarial Networks,Oral,"Generative adversarial networks (GANs) are known to achieve the state-of-the-art erformance on various generative tasks, but these results come at the exense of a notoriously difficult training hase. Current training strategies tyically draw a connection to otimization theory, whose scoe is restricted to local convergence due to the resence of non-convexity. In this work, we tackle the training of GANs by rethinking the roblem formulation from the mixed Nash Equilibria (NE) ersective. Via a classical lifting trick, we show that essentially all existing GAN objectives can be relaxed into their mixed strategy forms, whose global otima can be solved via samling, in contrast to the exclusive use of otimization framework in revious work. We further roose a mean-aroximation samling scheme, which allows to systematically exloit methods for bi-affine games to delineate novel, ractical training algorithms of GANs. Finally, we rovide exerimental evidence that our aroach yields comarable or suerior results to contemorary training algorithms, and outerforms classical methods such as SGD, Adam, and RMSPro. 
"
490,2019,Active Learning for Probabilistic Structured Prediction of Cuts and Matchings,Oral,"Active learning methods, like uncertainty samling, combined with robabilistic rediction techniques  have  achieved  success  in  various  roblems like image classification and text classification.  For  more  comlex  multivariate  rediction tasks, the relationshis between labels lay an imortant role in designing structured classifiers with better erformance. However, comutational time comlexity limits revalent robabilistic methods from effectively suorting active learning. Secifically, while non-robabilistic methods based on structured suort vector ma-chines can be tractably alied to redicting cuts and biartite matchings, conditional random fields are intractable for these structures. We roose an adversarial aroach for active learning with structured rediction domains that is tractable for cuts  and  matching.  We  evaluate  this  aroach algorithmically in two imortant structured rediction roblems: multi-label classification and object tracking in videos. We demonstrate better accuracy and comutational efficiency for our roosed method.
"
491,2019,A Dynamical Systems Perspective on Nesterov Acceleration,Oral,"This article resents a dynamic system model describing Nesterov's accelerated gradient method. In contrast to earlier work, the derivation does not rely on a vanishing ste size argument. It is shown that Nesterov's accelerated gradient method follows from discretizing an ordinary differential equation with a semi-imlicit Euler integration scheme. We analyze both the corresonding differential equation as well as the discretization for obtaining insights into the henomenon of acceleration. The analysis suggests that a curvature-deendent daming term lies at the heart of acceleration. We further establish connections between the discretized and the continuous-time dynamics.
"
492,2019,Disentangled Graph Convolutional Networks,Oral,"recode    The formation of a real-world grah tyically arises from the highly comlex interaction of many latent factors.
    The existing dee learning methods for grah-structured data neglect the entanglement of the latent factors, rendering the learned reresentations non-robust and hardly exlainable.
    However, learning reresentations that disentangle the latent factors oses great challenges and remains largely unexlored in the literature of grah neural networks.
    In this aer, we introduce the disentangled grah convolutional network (DisenGCN) to learn disentangled node reresentations.
    In articular, we roose a novel neighborhood routing mechanism, which is caable of dynamically identifying the latent factor that may have caused the edge between a node and one of its neighbors,  and accordingly assigning the neighbor to a channel that extracts and convolutes features secific to that factor.
    We theoretically rove the convergence roerties of the routing mechanism.
    Emirical results show that our roosed model can achieve significant erformance gains, esecially when the data demonstrate the existence of many entangled factors.
codere
"
493,2019,Differentiable Dynamic Normalization for Learning Deep Representation,Oral,"This work resents Dynamic Normalization (DN), which is able to learn arbitrary normalization oerations for different convolutional layers in a dee ConvNet. Unlike existing normalization aroaches that redefined comutations of the statistics (mean and variance), DN learns to estimate them. DN has several aealing benefits. First, it adats to various networks, tasks, and batch sizes. Second, it can be easily imlemented and trained in a differentiable end-to-end manner with merely small number of arameters. Third, its matrix formulation reresents a wide range of normalization methods, shedding light on analyzing them theoretically. Extensive studies show that DN outerforms its counterarts in CIFAR10 and ImageNet.
"
494,2019,Fairwashing: the risk of rationalization,Oral,"Black-box exlanation is the roblem of exlaining how a machine learning model -- whose internal logic is hidden to the auditor and generally comlex -- roduces its outcomes. Current aroaches for solving this roblem include model exlanation, outcome exlanation as well as model insection.  While these techniques can be beneficial by roviding interretability, they can be used in a negative manner to erform fairwashing, which we define as romoting the ercetion that a machine learning model resects some ethical values while it might not be the case. In articular, we demonstrate that it is ossible to systematically rationalize decisions taken by an unfair black-box model using the model exlanation as well as the outcome exlanation aroaches with a given fairness metric. Our solution, LaundryML, is based on a regularized rule list enumeration algorithm whose objective is to search for fair rule lists aroximating an unfair black-box model. We emirically evaluate our rationalization technique on black-box models trained on real-world datasets and show that one can obtain rule lists with high fidelity to the black-box model while being considerably less unfair at the same time.
"
495,2019,Invariant-Equivariant Representation Learning for Multi-Class Data,Oral,"Reresentations learnt through dee neural networks tend to be highly informative, but oaque in terms of what information they learn to encode. We introduce an aroach to robabilistic modelling that learns to reresent data with two searate dee reresentations: an invariant reresentation that encodes the information of the class from which the data belongs, and an equivariant reresentation that encodes the symmetry transformation defining the articular data oint within the class manifold (equivariant in the sense that the reresentation varies naturally with symmetry transformations). This aroach is based rimarily on the strategic routing of data through the two latent variables, and thus is concetually transarent, easy to imlement, and in-rincile generally alicable to any data comrised of discrete classes of continuous distributions (e.g. objects in images, toics in language, individuals in behavioural data). We demonstrate qualitatively comelling reresentation learning and cometitive quantitative erformance, in both suervised and semi-suervised settings, versus comarable modelling aroaches in the literature with little fine tuning.
"
496,2019,CompILE: Compositional Imitation Learning and Execution,Oral,"We introduce Comositional Imitation Learning and Execution (ComILE): a framework for learning reusable, variable-length segments of hierarchically-structured behavior from demonstration data. ComILE uses a novel unsuervised, fully-differentiable sequence segmentation module to learn latent encodings of sequential data that can be re-comosed and executed to erform new tasks. Once trained, our model generalizes to sequences of longer length and from environment instances not seen during training. We evaluate ComILE in a challenging 2D multi-task environment and a continuous control task, and show that it can find correct task boundaries and event encodings in an unsuervised manner. Latent codes and associated behavior olicies discovered by ComILE can be used by a hierarchical agent, where the high-level olicy selects actions in the latent code sace, and the low-level, task-secific olicies are simly the learned decoders. We found that our ComILE-based agent could learn given only sarse rewards, where agents without task-secific olicies struggle.
"
497,2019,Understanding the Origins of Bias in Word Embeddings,Oral,"Poular word embedding algorithms exhibit stereotyical biases, such as gender bias. The widesread use of these algorithms in machine learning 
systems can amlify stereotyes in imortant contexts. Although some methods have been develoed to mitigate this roblem, how word embedding biases arise during training is oorly understood. In this work we develo a technique to address this question. Given a word embedding, our method reveals how erturbing the training corus would affect the resulting embedding bias. By tracing the origins of word embedding bias back to the original training documents, one can identify subsets of documents whose removal would most reduce bias. We demonstrate our methodology on Wikiedia and New York Times corora, and find it to be very accurate.
"
498,2019,Random Shuffling Beats SGD after Finite Epochs,Oral,"A long-standing roblem in stochastic otimization is roving that RandomShuffle, the without-relacement version of SGD, converges faster than the usual with-relacement SGD. Building uon Gurbuzbalaban et el, we resent the first (to our knowledge) non-asymtotic results for this roblem by roving that after a reasonable number of eochs RandomShuffle converges faster than SGD. Secifically, we rove that for strongly convex, second-order smooth functions, the iterates of RandomShuffle converge to the otimal solution as O(1T^2+n^3T^3), where n is the number of comonents in the objective, and T is number of iterations. This result imlies that after O(\sqrt{n}) eochs, RandomShuffle is strictly better than SGD (which converges as O(1T)). The key ste toward showing this better deendence on T is the introduction of n into the bound; and as our analysis shows, in general a deendence on n is unavoidable without further changes. To understand how RandomShuffle works in ractice, we further exlore two emirically useful settings: data sarsity and over-arameterization. For sarse data, RandomShuffle has the rate O(1T^2), again strictly better than SGD. Under a setting closely related to over-arameterization, RandomShuffle is shown to converge faster than SGD after any arbitrary number of iterations.  Finally, we extend the analysis of RandomShuffle to smooth non-convex and convex functions.
"
499,2019,Active Learning with Disagreement Graphs,Oral,"We resent two novel enhancements of an online imortance-weighted active learning algorithm IWAL, using the roerties of disagreements among hyotheses. The first enhancement, IWALD, runes the hyothesis set with a more aggressive strategy based on the disagreement grah. We show that IWAL-D imroves the generalization erformance and the label comlexity of the original IWAL, and quantify the imrovement in terms of the disagreement grah coefficient. The second enhancement, IZOOM, further imroves IWAL-D by adatively zooming into the current version sace and thus reducing the best-in-class error. We show that IZOOM admits favorable theoretical guarantees with the changing hyothesis set. We reort exerimental results on multile datasets and demonstrate that the roosed algorithms achieve better test erformances than IWAL given the same amount of labeling budget.
"
500,2019,Toward Understanding the Importance of Noise in Training Neural Networks,Oral,"Numerous emirical evidence has corroborated that the noise lays a crucial rule in effective and efficient training of dee neural networks. The theory behind, however, is still largely unknown. This aer studies this fundamental roblem through training a simle two-layer convolutional neural network model. Although training such a network requires to solve a non-convex otimization roblem with a surious local otimum and a global otimum, we rove that a erturbed gradient descent algorithm in conjunction with noise annealing is guaranteed to converge to a global otimum in olynomial time with arbitrary initialization. This imlies that the noise enables the algorithm to efficiently escae from the surious local otimum. Numerical exeriments are rovided to suort our theory.
"
501,2019,Open Vocabulary Learning on Source Code with a Graph-Structured Cache,Oral,"Machine learning models that take comuter rogram source code as inut tyically use Natural Language Processing (NLP) techniques. However, a major challenge is that code is written using an oen, raidly changing vocabulary due to, e.g., the coinage of new variable and method names.  Reasoning over such a vocabulary is not something for which most NLP methods are designed.  We introduce a Grah-Structured Cache to address this roblem; this cache contains a node for each new word the model encounters with edges connecting each word to its occurrences in the code.  We find that combining this grah-structured cache strategy with recent Grah-Neural-Network-based models for suervised learning on code imroves the models' erformance on a code comletion task and a variable naming task --- with over 100% relative imrovement on the latter --- at the cost of a moderate increase in comutation time.
"
502,2019,Combining parametric and nonparametric models for off-policy evaluation,Oral,"We consider a model-based aroach to erform batch off-olicy evaluation in reinforcement learning. Our method takes a mixture-of-exerts aroach to combine arametric and non-arametric models of the environment such that the final value estimate has the least exected error. We do so by 
first estimating the local accuracy of each model and then using a lanner to select which model to use at every time ste as to minimize the return error estimate along entire trajectories. Across a variety of domains, our mixture-based aroach outerforms the individual models alone as well as state-of-the-art imortance samling-based estimators.
"
503,2019,Scalable Nonparametric Sampling from Multimodal Posteriors with the Posterior Bootstrap,Oral,"Increasingly comlex datasets ose a number of challenges for Bayesian inference. Conventional osterior samling based on Markov chain Monte Carlo can be too comutationally intensive, is serial in nature and mixes oorly between osterior modes. Furthermore, all models are missecified, which brings into question the validity of the conventional Bayesian udate. We resent a scalable Bayesian nonarametric learning routine that enables osterior samling through the otimization of suitably randomized objective functions. A Dirichlet rocess rior on the unknown data distribution accounts for model missecification, and admits an embarrassingly arallel osterior bootstra algorithm that generates indeendent and exact samles from the nonarametric osterior distribution. Our method is articularly adet at samling from multimodal osterior distributions via a random restart mechanism, and we demonstrate this on Gaussian mixture model and sarse logistic regression examles.
"
504,2019,Sparse Multi-Channel Variational Autoencoder for the Joint Analysis of Heterogeneous Data,Oral,"Interretable modeling of heterogeneous data channels is essential in medical alications, for examle when jointly analyzing clinical scores and medical images.
Variational Autoencoders (VAE) are owerful generative models that learn reresentations of comlex data.
The flexibility of VAE may come at the exense of lack of interretability in describing the joint relationshi between heterogeneous data.
To tackle this roblem, in this work we extend the variational framework of VAE to bring arsimony and interretability when jointly account for latent relationshis across multile channels.
In the latent sace, this is achieved by constraining the variational distribution of each channel to a common target rior.
Parsimonious latent reresentations are enforced by variational droout.
Exeriments on synthetic data show that our model correctly identifies the rescribed latent dimensions and data relationshis across multile testing scenarios.
When alied to imaging and clinical data, our method allows to identify the joint effect of age and athology in describing clinical condition in a large scale clinical cohort.
"
505,2019,An Instability in Variational Inference for Topic Models,Oral,"Naive mean field variational methods are the state of-the-art aroach to inference in toic modeling. We show that these methods suffer from an instability that can roduce misleading conclusions. Namely, for certain regimes of the model arameters, variational inference oututs a non-trivial decomosition into toics. However -for the same arameter values- the data contain no actual information
about the true toic decomosition, and the outut of the algorithm is uncorrelated with it. In articular, the estimated osterior mean is wrong, and estimated credible regions do not achieve the nominal coverage. We discuss how this instability is remedied by more accurate mean field aroximations.
"
506,2019,Multi-Frequency Vector Diffusion Maps,Oral,"We introduce multi-frequency vector diffusion mas (MFVDM), a new framework for organizing and analyzing high dimensional data sets. The new method is a mathematical and algorithmic generalization of vector diffusion mas (VDM) and other non-linear dimensionality reduction methods. The idea of MFVDM is to incororates multile unitary irreducible reresentations of the alignment grou which introduces robustness to noise. We illustrate the efficacy of MFVDM on synthetic and cryo-EM image datasets, achieving better nearest neighbors search and alignment estimation than other baselines as VDM and diffusion mas (DM), esecially on extremely noisy data.
"
507,2019,Infinite Mixture Prototypes for Few-shot Learning,Oral,"We roose infinite mixture rototyes to adatively reresent both simle and comlex data distributions for few-shot learning.  Our infinite mixture rototyes reresent each class by a set of clusters, unlike existing rototyical methods that reresent each class by a single cluster. By infer-ring the number of clusters, infinite mixture rototyes interolate between nearest neighbor and rototyical reresentations, which imroves ac-curacy and robustness in the few-shot regime. We show the imortance of adative caacity for caturing comlex data distributions such as alha-bets, with 25% absolute accuracy imrovements over rototyical networks, while still maintain-ing or imroving accuracy on the standard Omniglot and mini-ImageNet benchmarks. In clustering labeled and unlabeled data by the same clustering rule, infinite mixture rototyes achieves state-of-the-art semi-suervised accuracy.  As a further caability, we show that infinite mixture rototyes can erform urely unsuervised clustering, unlike existing rototyical methods.
"
508,2019,First-Order Algorithms Converge Faster than $O(1/k)$ on Convex Problems,Oral,"  It has been known for many years that both gradient descent and
  stochastic coordinate descent achieve a global convergence rate of
  $O(1k)$ in the objective value, when alied to a scheme for
  minimizing a Lischitz-continuously differentiable, unconstrained
  convex function.  In this work, we imrove this rate to $o(1k)$.
  We extend the result to roximal gradient and roximal coordinate
  descent on regularized roblems to show similar $o(1k)$ convergence
  rates. The result is tight in the sense that an
  $O(1k^{1+\esilon})$ rate is not generally attainable for any
  $\esilon0$, for any of these methods."
509,2019,Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group,Oral,"We introduce a novel aroach to erform first-order otimization with orthogonal and unitary constraints.
This aroach is based on a arametrization stemming from Lie grou theory through the exonential ma. 
The arametrization transforms the constrained otimization roblem into an unconstrained one over a Euclidean sace, for which common first-order otimization methods can be used.
The theoretical results resented are general enough to cover the secial orthogonal grou, the unitary grou and, in general, any connected comact Lie grou.
We discuss how this and other arametrizations can be comuted efficiently through an imlementation trick, making numerically comlex arametrizations usable at a negligible runtime cost in neural networks.
In articular, we aly our results to RNNs with orthogonal recurrent weights, yielding a new architecture called exRNN.
We demonstrate how our method constitutes a more robust aroach to otimization with orthogonal constraints, showing faster, accurate, and more stable convergence in several tasks designed to test RNNs.
"
510,2019,Sample-Optimal Parametric Q-Learning Using Linearly Additive Features,Oral,"Consider a Markov decision rocess (MDP) that admits a set of state-action features, which can linearly exress the rocess' s robabilistic transition model. We roose a arametric Q-learning algorithm that finds an aroximate-otimal olicy using a samle size roortional to the feature dimension $K$ and invariant with resect to the size of the state sace. To further imrove its samle efficiency, we exloit the monotonicity roerty and intrinsinc noise structure of the Bellman oerator, rovided the existence of anchor state-actions that imly imlicit non-negativity in the feature sace. We augment the algorithm using techniques of variance reduction, monotonicity reservation and confidence bounds. It is roved to find a olicy which is $\esilon$-otimal from any initial state with high robability using $\wt{O}(K\esilon^2(1-\gamma)^3)$ samle transitions for arbitrarily large-scale MDP with a discount factor $\gamma\in(0,1)$. A matching information-theoretical lower bound is roved, confirming the samle otimality of the roosed method with resect to all arameters (u to olylog factors)."
511,2019,Learning Discrete Structures for Graph Neural Networks,Oral,"Grah neural networks (GNNs) are a oular class of machine learning models that have been successfully alied to a range of roblems. Their major advantage lies in their ability to exlicitly incororate a sarse and discrete deendency structure between data oints. Unfortunately, GNNs can only be used when such a grah-structure is available. In ractice, however, real-world grahs are often noisy and incomlete or might not be available at all. 
With this work, we roose to jointly learn the grah structure and the arameters of grah convolutional networks (GCNs) by aroximately solving a bilevel rogram that learns a discrete robability distribution on the edges of the grah. br 
This allows one to aly GCNs not only in scenarios where the given grah is incomlete or corruted but also in those where a grah is not available.
We conduct a series of exeriments that analyze the behavior of the roosed method and demonstrate that it outerforms related methods by a significant margin. 
"
512,2019,Bias Also Matters: Bias Attribution for Deep Neural Network Explanation,Oral,"The gradient of a dee neural network (DNN) w.r.t. the inut rovides information that can be used to exlain the outut rediction in terms of the inut features and has been widely studied to assist in interreting DNNs.  In a linear model (i.e., $g(x)=wx+b$), the gradient corresonds solely to the weights $w$. Such a model can reasonably locally linearly aroximate a smooth nonlinear DNN, and hence the weights of this local model are the gradient. The other art, however, of a local linear model, i.e., the bias $b$, is usually overlooked in attribution methods since it is not art of the gradient. In this aer, we observe that since the bias in a DNN also has a non-negligible contribution to the correctness of redictions, it can also lay a significant role in understanding DNN behaviors. In articular, we study how to attribute a DNN's bias to its inut features. We roose a backroagation-tye algorithm ``bias back-roagation (BB)'' that starts at the outut layer and iteratively attributes the bias of each layer to its inut nodes as well as combining the resulting bias term of the revious layer. This rocess stos at the inut layer, where summing u the attributions over all the inut features exactly recovers $b$. Together with the backroagation of the gradient generating $w$, we can fully recover the locally linear model $g(x)=wx+b$. Hence, the attribution of the DNN oututs to its inuts is decomosed into two arts, the gradient $w$ and the bias attribution, roviding searate and comlementary exlanations. We study several ossible attribution methods alied to the bias of each layer in BB. In exeriments, we show that BB can generate comlementary and highly interretable exlanations of DNNs in addition to gradient-based attributions."
513,2019,Breaking Inter-Layer Co-Adaptation by Classifier Anonymization,Oral,"This study addresses an issue of co-adatation 
between a feature extractor
and a classifier in a neural network.
A na\""ive joint otimization of a feature extractor and a classifier
often brings situations in which
an excessively comlex feature distribution 
adated to a very secific classifier
degrades the test erformance.
We introduce a method called
Feature-extractor Otimization through Classifier Anonymization (FOCA),
which is designed to avoid an exlicit co-adatation 
between a feature extractor and a articular classifier
by using many randomly-generated, weak classifiers during otimization.
We ut forth a mathematical roosition that states the FOCA features form 
a oint-like distribution within the same class in a class-searable fashion
under secial conditions.
Real-data exeriments under more general conditions rovide suortive evidences.
"
514,2019,Transfer of Samples in Policy Search via Multiple Importance Sampling,Oral,"We consider the transfer of exerience samles in reinforcement learning. Most of the revious works in this context focused on value-based settings, where transferring instances conveniently reduces to the transfer of (s,a,s',r) tules. In this aer, we consider the more comlex case of reusing samles in olicy search methods, in which the agent is required to transfer entire trajectories between environments with different transition models. By leveraging ideas from multile imortance samling, we roose robust gradient estimators that effectively achieve this goal, along with several techniques to reduce their variance. In the case where the transition models are known, we theoretically establish the robustness to the negative transfer for our estimators. In the case of unknown models, we roose a method to efficiently estimate them when the target task belongs to a finite set of ossible tasks and when it belongs to some reroducing kernel Hilbert sace. We rovide emirical results to show the effectiveness of our estimators.
"
515,2019,Interpreting Adversarially Trained Convolutional Neural Networks,Oral,"We attemt to interret how adversarially trained convolutional neural networks (AT-CNNs) recognize objects. We  design systematic aroaches to interret AT-CNNs in both qualitative and quantitative ways,  and comare them with normally trained models. Surrisingly, we find that adversarial training alleviates the texture bias of standard CNNs when trained on object recognition tasks, and hels CNNs learn a more shae-biased reresentation. 
We validate our hyothesis from two asects.  First, we comare the salience mas of AT-CNNs and standard CNNs on clean images and image under different transformations. The comarison could visually show that the redictions of the two tyes of CNNs are sensitive to dramatically different tyes of features. Second, to achieve quantitative verification, we construct additional test datasets that destroy either textures or shaes, such as style-transferred version of clean data,  saturated images and atch-shuffled ones, and then evaluate the classification accuracy of AT-CNNs and normal CNNs on these datasets. br 
Our findings shed some light on why AT-CNNs are more robust than those normally trained ones and contribute to a better understanding of adversarial training over CNNs from an interretation ersective. 

The code for reroducibility is rovided in the Sulementary Materials. 
"
516,2019,Bayesian Optimization of Composite Functions,Oral,"We consider otimization of comosite objective functions, i.e., of the form f(x)=g(h(x)), where h is a black-box derivative-free  exensive-to-evaluate function with vector-valued oututs, and g is a chea-to-evaluate function taking vector-valued inuts.  While these roblems can be solved with standard Bayesian otimization, we roose a novel aroach that exloits the comosite structure of the objective function to substantially imrove samling efficiency. Our aroach models h using a multi-outut Gaussian rocess and chooses where to samle using a natural generalization of the exected imrovement acquisition function, called Exected Imrovement for Comosite Functions (EI-CF).  Although EI-CF cannot be comuted in closed form, we rovide a novel stochastic gradient estimator that allows its efficient maximization.  We then show that our aroach is asymtotically consistent, i.e., that it recovers a globally otimal solution as samling effort grows to infinity, generalizing revious convergence results for classical EI. Numerical exeriments show our aroach dramatically outerforms standard Bayesian otimization benchmarks, achieving simle regret that is smaller by several orders of magnitude.
"
517,2019,Improved Convergence for $\ell_1$ and $\ell_\infty$ Regression via Iteratively Reweighted Least Squares,Oral,"The iteratively reweighted least squares method (IRLS) is a oular technique used in ractice for solving regression roblems. Various versions of this method have been roosed, but their theoretical analyses failed to cature the good ractical erformance. 

In this aer we roose a simle and natural version of IRLS for solving $\ell_\infty$ and $\ell_1$ regression, which rovably converges to a $(1+\esilon)$-aroximate solution in $O(m^{13}\log(1\esilon)\esilon + \log(m\esilon)\esilon^2)$ iterations, where $m$ is the number of rows of the inut matrix. Interestingly, this running time is indeendent of the conditioning of the inut, and the dominant term of the running time deends only linearly in $\esilon^{-1}$, desite the fact that the roblem it is solving is non-smooth, and the algorithm is not using any regularization.

This imroves uon the more comlex algorithms of Chin et al. (ITCS '12), and Christiano et al. (STOC '11) by a factor of at least $1\esilon^{53}$, and yields a truly efficient natural algorithm for the slime mold dynamics (Straszak-Vishnoi, SODA '16, ITCS '16, ITCS '17)."
518,2019,MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing,Oral,"In this work, we show that oular methods for semi-suervised learning with Grah Neural Networks (such as the Grah Convolutional Network) do not model and cannot learn a class of general neighborhood mixing relationshis.  To address this weakness, we roose a new model, MixHo, that can cature these difference relationshis by learning mixed feature reresentations of neighbors at various distances.   MixHo requires no additional memory or comutational comlexity,  and outerforms challenging baselines on several grah datasets.  In addition, we roose a sarsity regularization that allows us to visualize how the network rioritizes neighborhood information across different grah datasets. Our analysis of the learned arameters reveals that different datasets utilize neighborhood mixing in different ways
"
519,2019,Deep Generative Learning via Variational Gradient Flow,Oral,"We roose a general framework to learn dee generative models via \textbf{V}ariational \textbf{Gr}adient Fl\textbf{ow} (VGrow) on robability saces. The evolving distribution that asymtotically converges to the target distribution is governed by a vector field, which is the negative gradient of the first variation of the $f$-divergence between them. We rove that the evolving distribution coincides with the ushforward distribution through the infinitesimal time comosition of residual mas that are erturbations of the identity ma along the vector field. The vector field deends on the density ratio of the ushforward distribution and the target distribution, which can be consistently learned from a binary classification roblem. Connections of our roosed VGrow method with other oular methods, such as VAE, GAN and flow-based methods, have been established in this framework, gaining new insights of dee generative learning. We also evaluated several commonly used divergences, including Kullback-Leibler, Jensen-Shannon, Jeffrey divergences as well as our newly discovered ``logD'' divergence which serves as the objective function of the logD-trick GAN. Exerimental results on benchmark datasets demonstrate that VGrow can generate high-fidelity images in a stable and efficient manner, achieving comarable erformance with state-of-the-art GAN."
520,2019,Compositional Fairness Constraints for Graph Embeddings,Oral,"Learning high-quality node embeddings is a key building block for machine learning models that oerate on grah data, such as social networks and recommender systems. However, existing grah embedding techniques are unable to coe with fairness constraints, e.g., ensuring that the learned reresentations do not correlate with certain attributes, such as race or gender. Here, we introduce an adversarial framework to enforce fairness constraints on grah embeddings. Our aroach is {\em comositional}---meaning that it can (otionally) enforce multile different fairness constraints during inference. Exeriments on standard knowledge grah and recommender system benchmarks highlight the utility of our roosed framework.
"
521,2019,Co-manifold learning with missing data,Oral,"Reresentation learning is tyically alied to only one mode of a data matrix, either its rows or columns. Yet in many alications, there is an underlying geometry to both the rows and the columns. We roose utilizing this couled structure to erform co-manifold learning: uncovering the underlying geometry of both the rows and the columns of a given matrix, where we focus on a missing data setting. Our unsuervised aroach consists of three comonents. We first solve a family of otimization roblems to estimate a comlete matrix at multile scales of smoothness. We then use this collection of smooth matrix estimates to comute airwise distances on the rows and columns based on a new multi-scale metric that imlicitly introduces a couling between the rows and the columns. Finally, we construct row and column reresentations from these multi-scale metrics.  We demonstrate that our aroach outerforms cometing methods in both data visualization and clustering.
"
522,2019,Optimal Mini-Batch and Step Sizes for SAGA,Oral,"Recently it has been shown that the ste sizes of a family of variance reduced gradient methods, the JacSketch methods, deend on the exected smoothness constant. In articular, if this exected smoothness constant could be calculated a riori, then one could safely set much larger ste sizes which would result in a much faster convergence rate. We fill in this ga, and rovide simle closed form exressions for the exected smoothness constant and careful numerical exeriments verifying these bounds. Using these bounds, and since the SAGA algorithm is art of this JacSketch family, we suggest a new standard ractice for setting the ste sizes and mini-batch size for SAGA that are cometitive with a numerical grid search. Furthermore, we can now show that the total comlexity of the SAGA algorithm decreases linearly in the mini-batch size u to a re-defined value: the otimal mini-batch size. This is a rare result in the stochastic variance reduced literature, only reviously shown for the Katyusha algorithm. Finally we conjecture that this is the case for many other stochastic variance reduced methods and that our bounds and analysis of the exected smoothness constant is key to extending these results.
"
523,2019,The Kernel Interaction Trick: Fast Bayesian Discovery of Pairwise Interactions in High Dimensions,Oral,"Discovering interaction effects on a resonse of interest is a fundamental roblem faced in biology, medicine, economics, and many other scientific discilines. In theory, Bayesian methods for discovering airwise interactions enjoy many benefits -- including coherent uncertainty quantification, the ability to incororate background knowledge, and desirable shrinkage roerties. In ractice, however, Bayesian methods are often comutationally intractable for even moderate-dimensional roblems. Our key insight is that many hierarchical models of ractical interest admit a Gaussian rocess reresentation such that a osterior over all O(^2) interactions need never be maintained exlicitly, only a vector of O() kernel hyer-arameters. This imlicit reresentation allows us to run MCMC over model hyer-arameters in time and memory linear in  er iteration. On datasets with a variety of covariate and arameter behaviors such as sarsity, we show that: (1) our method imroves running time by orders of magnitude over naive alications of MCMC, (2) that our method offers imroved Tye I and Tye II error relative to state-of-the-art LASSO-based aroaches, and (3) that our method offers imroved comutational scaling in high dimensions relative to existing Bayesian and LASSO-based aroaches.
"
524,2019,A Recurrent Neural Cascade-based Model for Continuous-Time Diffusion,Oral,"Many works have been roosed in the literature to cature the dynamics of diffusion in networks. While some of them define grahical Markovian models to extract temoral relationshis between node infections in networks, others consider diffusion eisodes as sequences of infections via recurrent neural models. In this aer we roose a model at the crossroads of these two extremes, which embeds the history of diffusion in infected nodes as hidden continuous states. Deending on the trajectory followed by the content before reaching a given node, the distribution of influence robabilities may vary. However, content trajectories  are usually hidden in the data, which induces challenging learning roblems. We roose a toological recurrent neural model which exhibits good exerimental erformances for diffusion modeling and rediction.
"
525,2019,Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting,Oral,"Addressing catastrohic forgetting is one of the key challenges in continual learning where machine learning systems are trained with sequential or streaming tasks. 
Desite recent remarkable rogress in state-of-the-art dee learning, dee neural networks (DNNs) are still lagued with the catastrohic forgetting roblem. This aer resents a concetually simle yet general and effective framework for handling catastrohic forgetting in continual learning with DNNs. The roosed method consists of two comonents: a neural structure otimization comonent and a arameter learning andor fine-tuning comonent. The former learns the best neural structure for the current task on to of the current DNN trained with revious tasks. It learns whether to reuse or adat building blocks in the current DNN, or to create new ones if needed under the differentiable neural architecture search framework. The latter estimates arameters for newly introduced structures, and fine-tunes the old ones if referred.
By searating the exlicit neural structure learning and the arameter estimation, not only is the roosed method caable of evolving neural structures in an intuitively meaningful way, but also shows strong caabilities of alleviating catestrohic forgetting in exeriments. Furthermore, the roosed method outerforms all other baselines on the ermuted MNIST dataset, the slit CIFAR100 dataset and the Visual Domain Decathlon dataset in continual learning setting. 
"
526,2019,Exploration Conscious Reinforcement Learning Revisited,Oral,"The Exloration-Exloitation tradeoff is one of the main roblems of Reinforcement Learning. In ractice, this tradeoff is resolved by using some inherent exloration mechanism, such as the $\esilon$-greedy exloration or adding Gaussian action noise, while still trying to learn an otimal olicy. We take a different aroach, defining a surrogate otimality objective: an otimal olicy with resect to the exloration scheme. As we show throughout the aer, although solving this criterion does not necessarily lead to an otimal olicy, the roblem becomes easier to solve. We continue by analyzing this notion of otimality, devise algorithms derived from this aroach, which reveal connections to existing work, and test them emirically on tabular and dee Reinforcement Learning domains."
527,2019,Understanding the Impact of Entropy on Policy Optimization,Oral,"Entroy regularization is commonly used to imrove olicy otimization in reinforcement learning. It is believed to hel with exloration by encouraging the selection of more stochastic olicies. In this work, we analyze this claim using new visualizations of the otimization landscae based on randomly erturbing the loss function. We first show that even with access to the exact gradient, olicy otimization is difficult due to the geometry of the objective function. Then, we qualitatively show that in some environments, a olicy with higher entroy can make the otimization landscae smoother, thereby connecting local otima and enabling the use of larger learning rates. This manuscrit also resents new tools for understanding the otimization landscae, shows that olicy entroy serves as a regularizer, and highlights the challenge of designing general-urose olicy otimization algorithms.
"
528,2019,Counterfactual Visual Explanations,Oral,"A counterfactual query is tyically of the form ``For situation X, why was the outcome Y and not Z?''. A counterfactual exlanation (or resonse to such a query) is of the form ``If X was X*, then the outcome would have been Z rather than Y.''

In this work, we develo a technique to roduce counterfactual visual exlanations. Given a `query' image $I$ for which a vision system redicts class $c$, a counterfactual visual exlanation identifies how $I$ could change such that the system would outut a different secified class $c'$. To do this, we select a `distractor' image $I'$ that the system redicts as class $c'$ and identify satial regions in $I$ and $I'$ such that relacing the identified region in $I$ with the identified region in $I'$ would ush the system towards classifying $I$ as $c'$. 

We aly our aroach to multile image classification datasets generating qualitative results showcasing the interetability and discriminativeness of our counterfactual exlanations. To exlore the effectiveness of our exlanations in teaching humans, we resent machine teaching exeriments for the task of fine-grained bird classification. We find that users trained to distinguish bird secies fare better when given access to counterfactual exlanations in addition to training examles."
529,2019,Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design,Oral,"Flow-based generative models are owerful exact likelihood models with efficient samling and inference. Desite their comutational efficiency, flow-based models generally have much worse density modeling erformance comared to state-of-the-art autoregressive models. In this aer, we investigate and imrove uon three limiting design choices emloyed by flow-based models in rior work: the use of uniform noise for dequantization, the use of inexressive affine flows, and the use of urely convolutional conditioning networks in couling layers. Based on our findings, we roose Flow++, a new flow-based model that is now the state-of-the-art non-autoregressive model for unconditional density estimation on standard image benchmarks. Our work has begun to close the significant erformance ga that has so far existed between autoregressive models and flow-based models.
"
530,2019,Data Poisoning Attacks on Stochastic Bandits,Oral,"Stochastic multi-armed bandits form a class of online learning roblems that have imortant alications in online recommendation systems, adative medical treatment, and many others. Even though otential attacks against these learning algorithms may hijack their behavior, causing catastrohic loss in real-world alications, little is known about adversarial attacks on bandit algorithms. In this aer, we roose a framework of offline attacks on bandit algorithms and study convex otimization based attacks on several oular bandit algorithms. We show that the attacker can force the bandit algorithm to ull a target arm with high robability by a slight maniulation of the rewards in the data. Then we study a form of online attacks on bandit algorithms and roose an adative attack strategy against any bandit algorithm without the knowledge of the bandit algorithm. Our adative attack strategy can hijack the behavior of the bandit algorithm to suffer a linear regret with only a logarithmic cost to the attacker. Our results demonstrate a significant security threat to stochastic bandits. 
"
531,2019,"Probability Functional Descent: A Unifying Perspective on GANs, Variational Inference, and Reinforcement Learning",Oral,"The goal of this aer is to rovide a unifying view of a wide range of roblems of interest in machine learning by framing them as the minimization of functionals defined on the sace of robability measures. In articular, we show that generative adversarial networks, variational inference, and actor-critic methods in reinforcement learning can all be seen through the lens of our framework. We then discuss a generic otimization algorithm for our formulation, called robability functional descent (PFD), and show how this algorithm recovers existing methods develoed indeendently in the settings mentioned earlier.
"
532,2019,Learning Neurosymbolic Generative Models via Program Synthesis,Oral,"Significant strides have been made toward designing better generative models in recent years. Desite this rogress, however, state-of-the-art aroaches are still largely unable to cature comlex global structure in data. For examle, images of buildings tyically contain satial atterns such as windows reeating at regular intervals; state-of-the-art generative methods can't easily reroduce these structures. We roose to address this roblem by incororating rograms reresenting global structure into the generative model---e.g., a 2D for-loo may reresent a configuration of windows. Furthermore, we roose a framework for learning these models by leveraging rogram synthesis to generate training data. On both synthetic and real-world data, we demonstrate that our aroach is substantially better than the state-of-the-art at both generating and comleting images that contain global structure.
"
533,2019,Quantile Stein Variational Gradient Descent for Batch  Bayesian Optimization,Oral,"Batch Bayesian otimization has been shown to be an efficient and successful aroach for black-box function otimization, esecially when the evaluation of cost function is highly exensive but can be efficiently arallelized. In this aer, we introduce a novel variational framework for batch query otimization, based on the argument that the query batch should be selected to have both high diversity and good worst case erformance. This motivates us to introduce a variational objective that combines a quantile-based risk measure (for worst case erformance) and entroy regularization (for enforcing diversity). We derive a gradient-based article-based algorithm for solving our quantile-based variational objective, which generalizes Stein variational gradient descent (SVGD). We evaluate our method on a number of real-world alications and show that it consistently outerforms other recent state-of-the-art batch Bayesian otimization methods. Extensive exerimental results indicate that our method achieves better or comarable erformance, comared to the existing methods.
"
534,2019,Kernel-Based Reinforcement Learning in Robust Markov Decision Processes,Oral,"The robust Markov decision rocesses (MDP) framework aims to address the roblem of arameter uncertainty due to model mismatch, aroximation errors or even adversarial behaviors. It is esecially relevant when deloying the learned olicies in real-world alications. Scaling u the robust MDP framework to large or continuous state sace remains a challenging roblem. The use of function aroximation in this case is usually inevitable and this can only amlify the roblem of model mismatch and arameter uncertainties. It has been reviously shown that, in the case of MDPs with state aggregation, the robust olicies enjoy a tighter erformance bound comared to standard solutions due to its reduced sensitivity to aroximation errors. We extend these results to the much larger class of kernel-based aroximators and show, both analytically and emirically that the robust olicies can significantly outerform the non-robust counterart.
"
535,2019,Stochastic Blockmodels meet Graph Neural Networks,Oral,"Stochastic blockmodels (SBM) and their variants, $e.g.$, mixed-membershi and overlaing stochastic blockmodels, are latent variable models for grahs. Such methods have roven to be successful for multile tasks, including discovering the community structure and link rediction on grah-structured data. Recently, grah neural networks, $e.g.$, grah convolutional networks, have also emerged as a romising aroach to learn owerful reresentations (embeddings) for the nodes in the grah, by exloiting various grah roerties such as locality and invariance. In this work, we unify these two directions by develoing a novel, \emh{sarse} variational autoencoder for grahs, that retains the nice interretability roerties of SBMs, while also enjoying the excellent redictive erformance of grah neural nets. Moreover, our framework is accomanied by a fast \emh{recognition model} that enables fast inference of the node embeddings (which are of indeendent interest for inference in traditional SBMs). Although we develo this framework for a articular tye of SBM, namely the \emh{overlaing} stochastic blockmodel, the roosed framework can be adated readily for other tyes of SBMs as well. Exerimental results on several benchmarks datasets demonstrate that our model outerforms various state-of-the-art methods, for community discovery and link rediction. For reroducibility, the code is shared in sulementary material and it will be made ublic in the final version."
536,2019,Differential Inclusions for Modeling Nonsmooth ADMM Variants: A Continuous Limit Theory,Oral,"Recently, there has been a great deal of research attention on understanding the convergence behavior of first-order methods. One line of this research focuses on analyzing the convergence behavior of first-order methods using tools from continuous dynamical systems such as ordinary differential equation and different inclusions. These research results shed lights on better understanding first-order methods from a non-otimization oint of view. The alternating direction method of multiliers (ADMM) is a widely used first-order method for solving otimization roblems with searable structure in the variable and objective function, and it is imortant to investigate its behavior using these new techniques from dynamical systems. Existing works along this line have been mainly focusing on roblems with smooth objective functions, which excludes many imortant alications that are traditionally solved by ADMM variants. In this aer, we analyze some well-known and widely used ADMM variants for nonsmooth otimization roblems using the tools of differential inclusions. In articular, we analyze the convergence behavior of linearized ADMM and gradient-based ADMM for nonsmooth roblems. We anticiate that these results will rovide new insights on understanding ADMM for solving nonsmooth roblems. 
"
537,2019,On Sparse Linear Regression in the Local Differential Privacy Model,Oral,"In this aer, we study the sarse linear regression roblem under the Local Differential Privacy (LDP) model. We first show that olynomial deendency on the dimensionality $$ of the sace is unavoidable for the estimation error in both non-interactive and sequential interactive local models, if the rivacy of the whole dataset needs to be reserved.  Similar limitations also exist for other tyes of error measurements and in the relaxed local models. This indicates that differential rivacy in high dimensional sace is unlikely achievable for the roblem. With the understanding of this limitation, we then resent two algorithmic results. The first one is 
a sequential interactive LDP algorithm for the low dimensional sarse case, called Locally Differentially Private Iterative Hard Thresholding (LDP-IHT), which achieves a near otimal uer bound. This algorithm is actually rather general and can be used to solve quite a few other roblems, such as (Local) DP-ERM with sarsity constraints and sarse regression with non-linear measurements.  The second one is for the restricted (high dimensional) case where only  the rivacy  of the resonses (labels) needs to be reserved. For this case, 
we show that the otimal rate of the error estimation can be made logarithmically deending on $$ (i.e., $\log $) in the local model, 
where an uer bound is obtained by a label-rivacy version of LDP-IHT. Exeriments on real world and synthetic datasets confirm our theoretical analysis. "
538,2019,Doubly Robust Joint Learning for Recommendation on Data Missing Not at Random,Oral,"In recommender systems, usually the ratings of a user to most items are missing and a critical roblem is that the missing ratings are often missing not at random (MNAR) in reality.
It is widely acknowledged that MNAR ratings make it difficult to accurately redict the ratings and unbiasedly estimate the erformance of rating rediction.
Recent aroaches use imuted errors to recover the rediction errors for missing ratings, or weight observed ratings with the roensities of being observed.
These aroaches can still be severely biased in erformance estimation or suffer from the variance of the roensities. 
To overcome these limitations, we first roose an estimator that integrates the imuted errors and roensities in a doubly robust way to obtain unbiased erformance estimation and alleviate the effect of the roensity variance.
To achieve good erformance guarantees, based on this estimator, we roose joint learning of rating rediction and error imutation, which outerforms the state-of-the-art aroaches on four real-world datasets.
"
539,2019,Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions,Oral,"By building uon the recent theory that established the connection between imlicit generative modeling (IGM) and otimal transort, in this study, we roose a novel arameter-free algorithm for learning the underlying distributions of comlicated datasets and samling from them. The roosed algorithm is based on a functional otimization roblem, which aims at finding a measure that is close to the data distribution as much as ossible and also exressive enough for generative modeling uroses. We formulate the roblem as a gradient flow in the sace of robability measures. The connections between gradient flows and stochastic differential equations let us develo a comutationally efficient algorithm for solving the otimization roblem. We rovide formal theoretical analysis where we rove finite-time error guarantees for the roosed algorithm. To the best of our knowledge, the roosed algorithm is the first nonarametric IGM algorithm with exlicit theoretical guarantees. Our exerimental results suort our theory and show that our algorithm is able to successfully cature the structure of different tyes of data distributions.
"
540,2019,Geometric Losses for Distributional Learning,Oral,"Building uon recent advances in entroy-regularized otimal transort and uon Fenchel duality between measures and continuous functions, we roose in this aer a generalization of the logistic loss, incororating a metric or cost between classes. Unlike revious attemts to use otimal transort distances for learning, our loss results in unconstrained convex objective functions, suorts infinite (or very large) class saces, and naturally defines a geometric generalization of the softmax oerator. The geometric roerties of this loss makes it suitable for redicting sarse and singular distributions, for instance suorted on curves or hyer-surfaces. We study the theoretical roerties of our loss and showcase its effectiveness on two alications: ordinal regression and drawing generation.
"
541,2019,State-Reification Networks: Improving Generalization by Modeling the Distribution of Hidden Representations,Oral,"Machine learning romises methods that generalize well from finite labeled data. However, the brittleness of existing neural net aroaches is revealed by notable failures, such as the existence of adversarial examles that are misclassified desite being nearly identical to a training examle, or the inability of recurrent sequence-rocessing nets to stay on track without teacher forcing. We introduce a method, which we refer to as emstate reificationem, that involves modeling the distribution of hidden states over the training data and then rojecting hidden states observed during testing toward this distribution. Our intuition is that if the network can remain in a familiar manifold of hidden sace, subsequent layers of the net should be well trained to resond aroriately. We show that this state-reification method hels neural nets to generalize better, esecially when labeled data are sarse, and also hels overcome the challenge of achieving robust generalization with adversarial training.
"
542,2019,Matrix-Free Preconditioning in Online Learning,Oral,"We rovide an online convex otimization algorithm with regret that interolates between the regret of an algorithm using an otimal reconditioning matrix and one using a diagonal reconditioning matrix. Our regret bound is never worse than that obtained by diagonal reconditioning, and in certain setting even surasses that of algorithms with full-matrix reconditioning. Imortantly, our algorithm runs in the same time and sace comlexity as online gradient descent. Along the way we incororate new techniques that mildly streamline and imrove logarithmic factors in rior regret analyses. We conclude by benchmarking our algorithm on synthetic data and dee learning tasks.
"
543,2019,Batch Policy Learning under Constraints,Oral,"When learning olicies for real-world domains, two imortant questions arise: (i) how to efficiently use existing off-line, off-olicy, non-otimal behavior data; and (ii) how to mediate among different cometing objectives and constraints. We study the roblem of batch olicy learning under multile constraints and offer a systematic solution. We first roose a flexible meta algorithm that admits any batch reinforcement learning and online learning rocedure as subroutines. We then resent a secific algorithmic instantiation and rovide erformance guarantees for the main objective and all constraints. To certify constraint satisfaction, we roose a new and simle method for off-olicy olicy evaluation (OPE) and derive PAC-style bounds. Our algorithm achieves strong emirical results in different domains, including in a challenging roblem of simulated car driving subject to lane keeing and smooth driving constraints. We also show exerimentally that our OPE method outerforms other oular OPE techniques on a standalone basis, esecially in a high-dimensional setting.
"
544,2019,Neural Network Attributions: A Causal Perspective,Oral,"We roose a new attribution method for neural networks develoed using first rinciles of causality (to the best of our knowledge, the first such). The neural network architecture is viewed as a Structural Causal Model, and a methodology
to comute the causal effect of each feature on the outut is resented. With reasonable assumtions on the causal structure of the inut data, we roose algorithms to efficiently comute the causal effects, as well as scale the aroach to data with large dimensionality. We also show how this method can be used for recurrent neural networks. We reort exerimental results on both simulated and real datasets showcasing the romise and usefulness of the roosed algorithm.
"
545,2019,Defending Against Saddle Point Attack in Byzantine-Robust Distributed Learning,Oral,"We study robust distributed learning that involves minimizing a non-convex loss function with saddle oints. We consider the Byzantine setting where some worker machines have abnormal or even arbitrary and adversarial behavior. In this setting, the Byzantine machines may create fake local minima near a saddle oint that is far away from any true local minimum, even when robust gradient estimators are used. We develo ByzantinePGD, a robust first-order algorithm that can rovably escae saddle oints and fake local minima, and converge to an aroximate true local minimizer with low iteration comlexity. As a by-roduct, we give a simler algorithm and analysis for escaing saddle oints in the usual non-Byzantine setting. We further discuss three robust gradient estimators that can be used in ByzantinePGD, including median, trimmed mean, and iterative filtering. We characterize their erformance in concrete statistical settings, and argue for their near-otimality in low and high dimensional regimes. 
"
546,2019,Variational Laplace Autoencoders,Oral,"Variational autoencoders emloy an amortized inference model to redict the aroximate osterior of latent variables. However, such amortized variational inference (AVI) faces two challenges: 1) limited exressiveness of the fully-factorized Gaussian osterior assumtion and 2) the amortization error of the inference model. We roose an extended model named Variational Lalace Autoencoders that overcome both challenges to imrove the training of the dee generative models. Secifically, we start from a class of rectified linear activation neural networks with Gaussian outut and make a connection to robabilistic PCA. As a result, we derive iterative udate equations that discover the mode of the osterior and define a local full-covariance Gaussian aroximation centered at the mode. From the ersective of Lalace aroximation, a generalization to a differentiable class of outut distributions and activation functions is resented. Emirical results on MNIST, OMNIGLOT, FashionMNIST, SVHN and CIFAR10 show that the roosed aroach significantly outerforms other amortized or iterative methods. 
"
547,2019,"Classification from Positive, Unlabeled and Biased Negative Data",Oral,"In binary classification, there are situations where negative (N) data are too diverse to be fully labelled and that is when ositive-unlabeled (PU) learning comes into lay. However, collecting a non-reresentative N set that contains only a small ortion of all ossible N data can be much easier in many ractical situations. This aer studies a novel classification framework which incororates such biased N (bN) data in PU learning. We rovide a method based on emirical risk minimization to address this PUbN classification roblem. Our aroach can be regarded as a novel examle-reweighting algorithm, with the weight of each examle comuted through a reliminary ste that draws insiration from PU learning. We also derive an estimation error bound for the roosed method. Exerimental results demonstrate the effectiveness of our algorithm in not only PUbN learning scenarios but also ordinary PU leaning scenarios on several benchmark datasets.
"
548,2019,Linear-Complexity Data-Parallel Earth Mover's Distance Approximations,Oral,"The Earth Mover's Distance (EMD) is a state-of-the art metric for comaring discrete robability distributions. The high distinguishability offered by the EMD comes at a high cost in comutational comlexity. Therefore, linear-comlexity aroximation algorithms have been roosed to imrove its scalability. However, these algorithms are either limited to vector saces with only a few dimensions or they become ineffective when the degree of overla between the robability distributions is high. We roose novel aroximation algorithms that overcome both of these limitations, yet still achieve linear time comlexity. All our algorithms are data arallel, and therefore, we can take advantage of massively arallel comuting engines, such as Grahics Processing Units (GPUs). On the oular text-based 20 Newsgrous dataset, the new algorithms are four orders of magnitude faster than a multi-threaded CPU imlementation of Word Mover's Distance and match its search accuracy. On MNIST images, the new algorithms are four orders of magnitude faster than Cuturi's GPU imlementation of the Sinkhorn's algorithm while offering a slightly higher search accuracy.
"
549,2019,Differentially Private Empirical Risk Minimization with Non-convex Loss Functions,Oral,"We study the roblem of Emirical Risk Minimization (ERM) with (smooth) non-convex loss functions under the differential-rivacy (DP) model. Existing aroaches for this roblem mainly adot gradient norms to measure the error, which in general cannot guarantee the quality of the solution. To address this issue, 
we first study the exected excess emirical (or oulation) risk, which was rimarily used as the utility to measure the quality for convex loss functions. Secifically, we show that
the excess emirical (or oulation) risk can be uer bounded by $\tilde{O}(\frac{d\log (1\delta)}{\log n\esilon^2})$ in the $(\esilon, \delta)$-DP settings, where $n$ is the data size and $d$ is the dimensionality of the sace. 
The $\frac{1}{\log n}$ term in the emirical risk bound can be further imroved to $\frac{1}{n^{\Omega(1)}}$ (when $d$ is a constant) by a highly non-trivial analysis on the time-average error. 
To obtain more efficient solutions, we also consider the connection between achieving differential rivacy and finding aroximate local minimum. 
Particularly, we show that when the size $n$ is large enough, there are $(\esilon, \delta)$-DP algorithms which can find an aroximate local minimum of the emirical risk with high robability in both the constrained and non-constrained settings. 
These results indicate that one can escae saddle oints rivately."
550,2019,Stochastic Iterative Hard Thresholding for Graph-structured Sparsity Optimization,Oral,"Stochastic otimization algorithms udate models with chea er-iteration costs sequentially, which makes them amenable for large-scale data analysis. Such algorithms have been widely studied for structured sarse models where the sarsity information is very secific, e.g., convex sarsity-inducing norms or $\ell^0$-norm. However, these norms cannot be directly alied to the roblem of the comlex (non-convex) grah-structured sarsity models, which have imortant alication in disease outbreak and social networks, etc. In this aer, we roose a stochastic gradient-based method for solving grah-structured sarsity constraint roblems, not restricted to the least square loss. We rove that our algorithm enjoys linear convergence u to a constant error of cometitiveness with the counterarts in the batch learning setting. We conduct extensive exeriments to show the efficiency and effectiveness of the roosed algorithms. To the best of our knowledge, it is the first stochastic gradient-based method with theoretical convergence guarantees for grah-structured constrained otimization roblems."
551,2019,Towards a Deep and Unified Understanding of Deep Neural Models in NLP,Oral,"We define a unified information-based measure to rovide quantitative exlanations on how intermediate layers of dee Natural Language Processing (NLP) models leverage the information of inut words.Our method advances existing exlanation methods by addressing their exhibited issues in coherency and generality. The exlanations generated by using our method are consistent and faithful across different timestams, layers, and models. We show how our method can be used to understand four widely used models in NLP and exlain their erformances on three real-world benchmark datasets.
"
552,2019,Online Convex Optimization in Adversarial Markov Decision Processes,Oral,"We consider online learning in eisodic loo-free Markov decision rocesses (MDPs), where the  loss function can change arbitrarily between eisodes, and the transition function is not known to the learner.
We show $\tilde{O}(L|X|\sqrt{|A|T})$ regret bound, where $T$ is the number of eisodes, $X$ is the state sace, $A$ is the action sace, and $L$ is the length of each eisode. 
Our online algorithm is imlemented using entroic regularization methodology, which allows to extend the original adversarial MDP model to handle convex erformance criteria (A erformance criterion aggregates all the losses of a single eisode to a single objective we would like to minimize), as well as imrove revious regret bounds."
553,2019,Quantifying Generalization in Reinforcement Learning,Oral,"In this aer, we investigate the roblem of overfitting in dee reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This ractice offers relatively little insight into an agent's ability to generalize. We address this issue by using rocedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surrisingly large training sets. We then show that deeer convolutional architectures imrove generalization, as do methods traditionally found in suervised learning, including L2 regularization, droout, data augmentation and batch normalization.
"
554,2019,Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for Non-Convex Optimization,Oral,"Recent studies on diffusion-based samling methods have shown that Langevin Monte Carlo (LMC) algorithms can be beneficial for non-convex otimization, and rigorous theoretical guarantees have been roven for both asymtotic and finite-time regimes. Algorithmically, LMC-based algorithms resemble the well-known gradient descent (GD) algorithm, where the GD recursion is erturbed by an additive Gaussian noise whose variance has a articular form. Fractional Langevin Monte Carlo (FLMC) is a recently roosed extension of LMC, where the Gaussian noise is relaced by a heavy-tailed α-stable noise. As oosed to its Gaussian counterart, these heavy-tailed erturbations can incur large jums and it has been emirically demonstrated that the choice of α-stable noise can rovide several advantages in modern machine learning roblems, both in otimization and samling contexts. However, as oosed to LMC, only asymtotic convergence roerties of FLMC have been yet established. In this study, we analyze the non-asymtotic behavior of FLMC for non-convex otimization and rove finite-time bounds for its exected subotimality. Our results show that the weak-error of FLMC increases faster than LMC, which suggests using smaller ste-sizes in FLMC. We finally extend our results to the case where the exact gradients are relaced by stochastic gradients and show that similar results hold in this setting as well.
"
555,2019,Competing Against Nash Equilibria in Adversarially Changing Zero-Sum Games,Oral,"We study the roblem of reeated lay in a zero-sum game in which the ayoff matrix may change, in a ossibly adversarial fashion, on each round; we call these Online Matrix Games. Finding the Nash Equilibrium (NE) of a two layer zero-sum game is core to many roblems in statistics, otimization, and economics, and for a fixed game matrix this can be easily reduced to solving a linear rogram. But when the ayoff matrix evolves over time our goal is to find a sequential algorithm that can comete with, in a certain sense, the NE of the long-term-averaged ayoff matrix. We design an algorithm with small NE regret--that is, we ensure that the long-term ayoff of both layers is close to minimax otimum in hindsight. Our algorithm achieves near-otimal deendence with resect to the number of rounds and deends oly-logarithmically on the number of available actions of the layers. Additionally, we show that the naive reduction, where each layer simly minimizes its own regret, fails to achieve the stated objective regardless of which algorithm is used. Lastly, we consider the so-called bandit setting, where the feedback is significantly limited, and we rovide an algorithm with small NE regret using one-oint estimates of each ayoff matrix.
"
556,2019,Learning Latent Dynamics for Planning from Pixels,Oral,"Planning has been very successful for control tasks with known environment dynamics. To leverage lanning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for lanning has been a long-standing challenge, esecially in image-based domains. We roose the Dee Planning Network (PlaNet), a urely model-based agent that learns the environment dynamics from images and chooses actions through fast online lanning in latent sace. To achieve high erformance, the dynamics model must accurately redict the rewards ahead for multile time stes. We aroach this roblem using a latent dynamics model with both deterministic and stochastic transition comonents and a multi-ste variational inference objective that we call latent overshooting. Using only ixel observations, our agent solves continuous control tasks with contact dynamics, artial observability, and sarse rewards, which exceed the difficulty of tasks that were reviously solved by lanning with learned models. PlaNet uses substantially fewer eisodes and reaches final erformance close to and sometimes higher than strong model-free algorithms.
"
557,2019,Latent Normalizing Flows for Discrete Sequences,Oral,"Normalizing flows have been shown to be a owerful class of generative models for continuous random variables, giving both strong erformance and the otential for non-autoregressive generation. These benefits are also desired when modeling discrete random variables such as text, but directly alying normalizing flows to discrete sequences oses significant additional challenges. We roose a generative model which jointly learns a normalizing flow-based distribution in the latent sace and a stochastic maing to an observed discrete sace. In this setting, we find that it is crucial for the flow-based distribution to be highly multimodal. To cature this roerty, we roose several normalizing flow architectures to maximize model flexibility. Exeriments consider common discrete sequence tasks of character-level language modeling and olyhonic music generation. Our results indicate that an autoregressive flow-based model can match the erformance of a comarable autoregressive baseline, and a non-autoregressive flow-based model can imrove generation seed with a enalty to erformance.
"
558,2019,Unifying Orthogonal Monte Carlo Methods,Oral,"Many machine learning methods making use of Monte Carlo samling in vector saces have been shown to be imroved by conditioning samles to be mutually orthogonal. Exact orthogonal couling of samles is comutationally intensive, hence aroximate methods have been of great interest. In this aer, we resent a unifying ersective of many aroximate methods by considering Givens transformations, roose new aroximate methods based on this framework, and demonstrate the ﬁrst statistical guarantees for families of aroximate methods in kernel aroximation. We rovide extensive emirical evaluations with guidance for ractitioners.
"
559,2019,Bounding User Contributions: A Bias-Variance Trade-off in Differential Privacy,Oral,"Differentially rivate learning algorithms rotect individual articiants in the training dataset by guaranteeing that their resence does not significantly change the resulting model. In order to make this romise, such algorithms need to know the maximum contribution that can be made by a single user: the more data an individual can contribute, the more noise will need to be added to rotect them. While most existing analyses assume that the maximum contribution is known and fixed in advance -- indeed, it is often assumed that each user contributes only a single examle -- we argue that in ractice there is a meaningful choice to be made. On the one hand, if we allow users to contribute large amounts of data, we may end u adding excessive noise to rotect a few outliers, even when the majority contribute only modestly. On the other hand, limiting users to small contributions kees noise levels low at the cost of otentially discarding significant amounts of excess data, thus introducing bias. Here, we characterize this trade-off for an emirical risk minimization setting, showing that in general there is a ""sweet sot"" that deends on measurable roerties of the dataset, but that there is also a concrete cost to rivacy that cannot be avoided simly by collecting more data.
"
560,2019,Complementary-Label Learning for Arbitrary Losses and Models,Oral,"In contrast to the standard classification aradigm where the true (or ossibly noisy) class is given to each training attern, comlementary-label learning only uses training atterns each equied with a comlementary label, which only secifies one of the classes that the attern does not belong to. The goal of this aer is to derive a novel framework of comlementary-label learning with an unbiased estimator of the classification risk, for arbitrary losses and models---all existing methods have failed to achieve this goal. With this framework, modelhyer-arameter selection (through cross-validation) becomes ossible without the need of any ordinarily labeled validation data, while using any linearnon-linear models or convexnon-convex loss functions. We further imrove the risk estimator by a non-negative correction and gradient-descent-ascent trick, and demonstrate its sueriority through exeriments.
"
561,2019,Neuron birth-death dynamics accelerates gradient descent and converges asymptotically,Oral,"Neural networks with a large number of arameters admit a mean-field descrition, which has recently served as a theoretical exlanation for the favorable training roerties of models with a large number of arameters. In this regime, gradient descent obeys a deterministic artial differential equation (PDE) that converges to a globally otimal solution for networks with a single hidden layer under aroriate assumtions. In this work, we roose a non-local mass transort dynamics that leads to a modified PDE with the same minimizer. We imlement this non-local dynamics as a stochastic neuronal birthdeath rocess and we rove that it accelerates the rate of convergence in the mean-field limit. We subsequently realize this PDE with two classes of numerical schemes that converge to the mean-field equation, each of which can easily be imlemented for neural networks with finite numbers of arameters. We illustrate our algorithms with two models to rovide intuition for the mechanism through which convergence is accelerated.
"
562,2019,Model Comparison for Semantic Grouping,Oral,"We introduce a robabilistic framework for quantifying the semantic similarity between two grous of embeddings. We formulate the task of semantic similarity as a model comarison task in which we contrast a generative model which jointly models two sentences versus one that does not. We illustrate how this framework can be used for the Semantic Textual Similarity tasks using clear assumtions about how the embeddings of words are generated. We aly model comarison that utilises information criteria to address some of the shortcomings of Bayesian model comarison, whilst still enalising model comlexity. We achieve cometitive results by alying the roosed framework with an aroriate choice of likelihood on the STS datasets.
"
563,2019,Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley Value Approximation,Oral,"The roblem of exlaining the behavior of Dee Neural Networks has gained a lot of attention over the last years. While several attribution methods have been roosed, most methods are based on heuristics without clear strong theoretical foundations. This raises the question of whether the resulting attributions are reliable. On the other hand, the literature on cooerative game theory suggests Shaley Values as a unique way of assigning relevance scores such that certain desirable roerties are satisfied. Previous works on attribution methods also showed that exlanations based on Shaley Values better agree with the human intuition. Unfortunately, the exact evaluation of Shaley Values is rohibitively exensive, exonential in the number of inut features. In this work, by leveraging recent results on uncertainty roagation, we roose a novel, olynomial-time aroximation of Shaley Values in dee neural networks. We show that our method roduces significantly better aroximations of Shaley Values than existing state-of-the-art attribution methods.
"
564,2019,Projections for Approximate Policy Iteration Algorithms,Oral,"Aroximate olicy iteration is a class of reinforcement learning algorithms where both the value function and olicy are encoded using function aroximators and which has been esecially rominent in continuous action saces. However, by encoding the olicy with a function aroximator, it often becomes necessary to constrain the change in action distribution during olicy udate to ensure increase of the olicy return. Several aroximations exist in the literature to solve this constrained olicy udate roblem. In this aer, we roose to imrove over such solutions by introducing a set of rojections that transform the constrained roblem into an unconstrained one which is then solved by standard gradient descent. Using these rojections, we emirically demonstrate that our aroach can both imrove the olicy udate solution and the control over exloration of existing aroximate olicy iteration algorithms.
"
565,2019,Adaptive Monte Carlo Multiple Testing via Multi-Armed Bandits,Oral,"Monte Carlo (MC) ermutation testing is considered the gold standard for statistical hyothesis testing, esecially when standard arametric assumtions are not clear or likely to fail. However, in modern data science settings where a large number of hyothesis tests need to be erformed simultaneously, it is rarely used due to its rohibitive comutational cost. In genome-wide association studies, for examle, the number of hyothesis tests m is around 10^6 while the number of MC samles n for each test could be greater than 10^8, totaling more than nm=10^14 samles. In this aer, we roose Adative MC Testing (AMT) to estimate MC -values and control false discovery rate in multile testing. The algorithm oututs the same result as the standard full MC aroach with high robability while requiring only \tilde{O}(\sqrt{n}m) samles. This samle comlexity is shown to be otimal. On a Parkinson GWAS dataset, the algorithm reduces the running time from 2 months for full MC to an hour. The AMT algorithm is derived based on the theory of multi-armed bandits.
"
566,2019,RaFM: Rank-Aware Factorization Machines,Oral,"Fatorization machines (FM) are a oular model class to learn airwise interactions by a low-rank aroximation. Different from existing FM-based aroaches which use a fixed rank for all features, this aer rooses a Rank-Aware FM (RaFM) model which adots airwise interactions from FMs with different ranks. On one hand, the roosed model achieves a better erformance on real-world datasets where different features usually have significantly varying frequencies of occurrences. On the other hand, we rove that the RaFM model can be stored, evaluated, and trained as efficiently as one single FM, and under some reasonable conditions it can be even significantly more efficient than FM. RaFM imroves the erformance of FMs in both regression tasks and classification tasks while incurring less comutational burden, therefore also has attractive otential in industrial alications.
"
567,2019,Online Learning with Sleeping Experts and Feedback Graphs,Oral,"We consider the scenario of online learning with sleeing exerts, where not all exerts are available at each round, and analyze the general framework of learning with stochastic feedback grahs, where loss observations associated with each exert are characterized by a grah. A critical assumtion in this framework is that the loss observations and the set of sleeing exerts at each round are indeendent. We first extend the classical sleeing exert algorithm of Kleinberg et al 2008 to the feedback grahs scenario, and rove matching uer and lower bounds for the sleeing regret of the resulting algorithm under the indeendence assumtion. Our main contribution is then to relax this assumtion, resent a finer notion of sleeing regret, and derive a general algorithm with strong theoretical guarantees. We instantiate our framework to the imortant scenario of online learning with abstention, where a learner can elect to abstain from making a rediction at the rice of a certain cost. We emirically validate our algorithm against multile online abstention algorithms on several real-world datasets, showing substantial erformance imrovements.
"
568,2019,Multi-objective training of Generative Adversarial Networks with multiple discriminators,Oral,"Recent literature has demonstrated romising results on the training of Generative Adversarial Networks by emloying a set of discriminators, as oosed to the traditional game involving one generator against a single adversary. Those methods erform single-objective otimization on some simle consolidation of the losses, e.g. an average. In this work, we revisit the multile-discriminator aroach by framing the simultaneous minimization of losses rovided by different models as a multi-objective otimization roblem. Secifically, we evaluate the erformance of multile gradient descent and the hyervolume maximization algorithm on a number of different datasets. Moreover, we argue that the reviously roosed methods and hyervolume maximization can all be seen as variations of multile gradient descent in which the udate direction comutation can be done efficiently. Our results indicate that hyervolume maximization resents a better comromise between samle quality and diversity, and comutational cost than revious methods.
"
569,2019,Width Provably Matters in Optimization for Deep Linear Neural Networks,Oral,"We rove that for an L-layer fully-connected linear neural network, if the width of every hidden layer is \widetilde{Omega}( L r dem{out} kaa^3 ), where r and kaa are the rank and the condition number of the inut data, and dem{out} is the outut dimension, then gradient descent with Gaussian random initialization converges to a global minimum at a linear rate. The number of iterations to find an esilon-subotimal solution is O( kaa log(1esilon) ). Our olynomial uer bound on the total running time for wide dee linear networks and the ex(Omega(L)) lower bound for narrow dee linear neural networks [Shamir, 2018] together demonstrate that wide layers are necessary for otimizing dee models. 
"
570,2019,Learning to Infer Program Sketches,Oral,"Our goal is to build systems which write code automatically from the kinds of secifications humans can most easily rovide, such as examles and natural language instruction. The key idea of this work is that a flexible combination of attern recognition and exlicit reasoning can be used to solve these comlex rogramming roblems. We roose a method for dynamically integrating these tyes of information. Our novel intermediate reresentation and training algorithm allow a rogram synthesis system to learn, without direct suervision, when to rely on attern recognition and when to erform symbolic search. Our model matches the memorization and generalization erformance of neural synthesis and symbolic search, resectively, and achieves state-of-the-art erformance on a dataset of simle English descrition-to-code rogramming roblems.
"
571,2019,Functional Transparency for Structured Data: a Game-Theoretic Approach,Oral,"We rovide a new aroach to training neural models to exhibit transarency in a well-defined, functional manner. Our aroach naturally oerates over structured data and tailors the redictor, functionally, towards a chosen family of (local) witnesses. The estimation roblem is setu as a co-oerative game between an unrestricted \emh{redictor} such as a neural network, and a set of \emh{witnesses} chosen from the desired transarent family. The goal of the witnesses is to highlight, locally, how well the redictor conforms to the chosen family of functions, while the redictor is trained to minimize the highlighted discreancy. We emhasize that the redictor remains globally owerful as it is only encouraged to agree locally with locally adated witnesses. We analyze the effect of the roosed aroach, rovide examle formulations in the context of dee grah and sequence models, and emirically illustrate the idea in chemical roerty rediction, temoral modeling, and molecule reresentation learning.
"
572,2019,Differentially Private Learning of Geometric Concepts,Oral,"We resent differentially rivate efficient algorithms for learning union of olygons in the lane (which are not necessarily convex). Our algorithms achieve $(\alha,\beta)$-PAC learning and $(\esilon,\delta)$-differential rivacy using a samle of size $\tilde{O}\left(\frac{1}{\alha\esilon}k\log d\right)$, where the domain is $[d]\times[d]$ and $k$ is the number of edges in the union of olygons."
573,2019,Hierarchically Structured Meta-learning,Oral,"In order to learn quickly with few samles, meta-learning utilizes rior knowledge learned from revious tasks. However, a critical challenge in meta-learning is task uncertainty and heterogeneity, which can not be handled via globally sharing knowledge among tasks. In this aer, based on gradient-based meta-learning, we roose a hierarchically structured meta-learning (HSML) algorithm that exlicitly tailors the transferable knowledge to different clusters of tasks.
Insired by the way human beings organize knowledge, we resort to a hierarchical task clustering structure to cluster tasks. br 
As a result, the roosed aroach not only addresses the challenge via the knowledge customization to different clusters of tasks, but also reserves
knowledge generalization among a cluster of similar tasks.
To tackle the changing of task relationshi, in addition, we extend the hierarchical structure to a continual learning environment. The exerimental results show that our aroach can achieve state-of-the-art erformance in both toy-regression and few-shot image classification roblems.
"
574,2019,Toward Controlling Discrimination in Online Ad Auctions,Oral,"Online advertising latforms are thriving due to the customizable audiences they offer advertisers. However, recent studies show that the audience an ad gets shown to can be  discriminatory with resect to sensitive attributes such as gender or ethnicity, inadvertently crossing ethical andor legal boundaries. To revent this, we roose a constrained otimization framework that allows the latform to control of the fraction of each sensitive tye an advertiser's ad gets shown to while maximizing its ad revenues. Building uon Myerson's classic work, we first resent an otimal auction mechanism for a large class of fairness constraints. Finding the arameters of this otimal auction, however, turns out to be a non-convex  roblem. We show how this non-convex  roblem can be reformulated as a more structured non-convex roblem with no saddle oints or local-maxima; allowing us to develo a gradient-descent-based algorithm to solve it. Our emirical results on the A1 Yahoo! dataset demonstrate that our algorithm can obtain uniform coverage across different user attributes for each advertiser at a minor loss to the revenue of the latform, and a small change in the total number of advertisements each advertiser shows on the latform.
"
575,2019,Metropolis-Hastings Generative Adversarial Networks,Oral,"We introduce the Metroolis-Hastings generative adversarial network (MH-GAN), which combines asects of Markov chain Monte Carlo and GANs. The MH-GAN draws samles from the distribution imlicitly defined by a GAN's discriminator-generator air, as oosed to standard GANs which draw samles from the distribution defined by only the generator. It uses the discriminator from GAN training to build a wraer around the generator for imroved samling. With a erfect discriminator, this wraed generator samles from the true distribution on the data exactly even when the generator is imerfect. We demonstrate the benefits of the imroved generator on multile benchmark datasets, including CIFAR-10 and CelebA, using the DCGAN, WGAN, and rogressive GAN.
"
576,2019,Exploring interpretable LSTM neural networks over multi-variable data,Oral,"For a recurrent neural network trained on time series with target and exogenous variables, in addition to accurate rediction, it is also desired to rovide interretable insights into the data. In this aer, we exlore the structure of LSTM recurrent neural network to learn variable-wise hidden states, with the aim to cature different dynamics in multi-variable time series and distinguish the contribution of variables to the rediction. With these variable-wise hidden states, a mixture attention mechanism is roosed to model the generative rocess of the target. Then we develo the associated training method to learn network arameters, variable and temoral imortance w.r.t the rediction of the target variable. Extensive exeriments on real datasets demonstrate that by modeling dynamics of different variables, the rediction erformance is enhanced.
Meanwhile, we evaluate the interretation results both qualitatively and quantitatively. It exhibits the rosect of the develoed method as an end-to-end framework for both forecasting and knowledge extraction over multi-variable data.
"
577,2019,CAB: Continuous Adaptive Blending for Policy Evaluation and Learning,Oral,"The ability to erform offline AB-testing and off-olicy learning using logged contextual bandit feedback is highly desirable in a broad range of alications, including recommender systems, search engines, ad lacement, and ersonalized health care. Both offline AB-testing and off-olicy learning require a counterfactual estimator that evaluates how some new olicy would have erformed, if it had been used instead of the logging olicy. In this aer, we identify a family of counterfactual estimators which subsumes most such estimators roosed to date. Our analysis of this family identifies a new estimator - called Continuous Adative Blending (CAB) - which enjoys many advantageous theoretical and ractical roerties. In articular, it can be substantially less biased than clied Inverse Proensity Score (IPS) weighting and the Direct Method, and it can have less variance than Doubly Robust and IPS estimators. In addition, it is sub-differentiable such that it can be used for learning, unlike the SWITCH estimator. Exerimental results show that CAB rovides excellent evaluation accuracy and outerforms other counterfactual estimators in terms of learning erformance.
"
578,2019,Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?,Oral,"Many modern learning tasks involve fitting nonlinear models which are trained in an overarameterized regime where the arameters of the model exceed the size of the training dataset. Due to this overarameterization, the training loss may have infinitely many global minima and it is critical to understand the roerties of the solutions found by first-order otimization schemes such as (stochastic) gradient descent starting from different initializations. In this aer we demonstrate that when the loss has certain roerties over a minimally small neighborhood of the initial oint, first order methods such as (stochastic) gradient descent have a few intriguing roerties: (1) the iterates converge at a geometric rate to a global otima even when the loss is nonconvex, (2) among all global otima of the loss the iterates converge to one with a near minimal distance to the initial oint, (3) the iterates take a near direct route from the initial oint to this global otimum. As art of our roof technique, we introduce a new otential function which catures the tradeoff between the loss function and the distance to the initial oint as the iterations rogress. The utility of our general theory is demonstrated for a variety of roblem domains sanning low-rank matrix recovery to shallow neural network training.
"
579,2019,Incremental Randomized Sketching for Online Kernel Learning,Oral,"Randomized sketching has been used in offline kernel learning, but it cannot be alied directly to online kernel learning due to the lack of incremental maintenances for randomized sketches with regret guarantees. To address these issues, we roose a novel incremental randomized sketching aroach for online kernel learning, which has efficient incremental maintenances with theoretical guarantees. We construct two incremental randomized sketches using the sarse transform matrix and the samling matrix for kernel matrix aroximation, udate the incremental randomized sketches using rank-$1$ modifications, and construct an time-varying exlicit feature maing for online kernel learning. We rove that the roosed incremental randomized sketching is statistically unbiased for the matrix roduct aroximation, obtains a $1 + \esilon$ relative-error bound for the kernel matrix aroximation, enjoys a sublinear regret bound for online kernel learning, and has constant time and sace comlexities at each round for incremental maintenances. Exerimental results demonstrate that the incremental randomized sketching achieves a better learning erformance in terms of accuracy and efficiency even in adversarial environments."
580,2019,Learning Discrete and Continuous Factors of Data via Alternating Disentanglement,Oral,"We address the roblem of unsuervised disentanglement of discrete and continuous exlanatory factors of data. We first show a simle rocedure for minimizing the total correlation of the continuous latent variables without having to use a discriminator network or erform imortance samling, via cascading the information flow in the beta-VAE framework. Furthermore, we roose a method which avoids offloading the entire burden of jointly modeling the continuous and discrete factors to the variational encoder by emloying a searate discrete inference rocedure.

This leads to an interesting alternating minimization roblem which switches between finding the most likely discrete configuration given the continuous factors and udating the variational encoder based on the comuted discrete factors. Exeriments show that the roosed method clearly disentangles discrete factors and significantly outerforms current disentanglement methods based on the disentanglement score and inference network classification score.
"
581,2019,Learning Structured Decision Problems with Unawareness,Oral,"Structured models of decision making often assume an agent is aware of all ossible states and actions in advance. This assumtion is sometimes untenable. In this aer, we learn Bayesian Decision Networks from both domain exloration and exert assertions in a way which guarantees convergence to otimal behaviour, even when the agent starts unaware of actions or belief variables that are critical to success. Our exeriments show that our agent learns otimal behaviour on both small and large decision roblems, and that allowing an agent to conserve information uon making new discoveries results in faster convergence.
"
582,2019,Scalable Metropolis-Hastings for Exact Bayesian Inference with Large Datasets,Oral,"Bayesian inference via standard Markov Chain Monte Carlo (MCMC) methods such as Metroolis--Hastings is too comutationally intensive to handle large datasets, since the cost er ste usually scales like $O(n)$ in the number of data oints $n$. We roose the \emh{Scalable Metroolis--Hastings} (SMH) kernel that only requires rocessing on average $O(1)$ or even $O(1\sqrt{n})$ data oints er ste. This scheme is based on a combination of factorized accetance robabilities, rocedures for fast simulation of Bernoulli rocesses, and control variate ideas. Contrary to many MCMC subsamling schemes such as fixed ste-size Stochastic Gradient Langevin Dynamics, our aroach is exact insofar as the invariant distribution is the true osterior and not an aroximation to it. We characterise the erformance of our algorithm theoretically, and give realistic and verifiable conditions under which it is geometrically ergodic. This theory is borne out by emirical results that demonstrate overall erformance benefits over standard Metroolis-Hastings and various subsamling algorithms."
583,2019,Bridging Theory and Algorithm for Domain Adaptation,Oral,"This aer addresses the roblem of unsuervised domain adation from theoretical and algorithmic ersectives. Existing domain adatation theories naturally imly minimax otimization algorithms, which connect well with the adversarial-learning based domain adatation methods. However, several disconnections still form the ga between theory and algorithm. We extend revious theories (Ben-David et al., 2010; Mansour et al., 2009c) to multiclass classification in domain adatation, where classifiers based on scoring functions and margin loss are standard algorithmic choices. We introduce a novel measurement, margin disarity discreancy, that is tailored both to distribution comarison with asymmetric margin loss, and to minimax otimization for easier training. Using this discreancy, we derive new generalization bounds in terms of Rademacher comlexity. Our theory can be seamlessly transformed into an adversarial learning algorithm for domain adatation, successfully bridging the ga between theory and algorithm. A series of emirical studies show that our algorithm achieves the state-of-the-art accuracies on challenging domain adatation tasks.
"
584,2019,TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing,Oral,"Neural networks are difficult to interret and debug. We introduce testing techniques for neural networks that
can discover errors occurring only for rare inuts. Secifically, we develo coverage-guided fuzzing (CGF)
methods for neural networks. In CGF, random mutations of inuts are guided by a coverage metric toward the
goal of satisfying user-secified constraints. We describe how aroximate nearest neighbor (ANN) algorithms
can rovide this coverage metric for neural networks. We then combine these methods with techniques for
roerty-based testing (PBT). In PBT, one asserts roerties that a function should satisfy and the system
automatically generates tests exercising those roerties. We then aly this system to ractical goals including
(but not limited to) surfacing broken loss functions in oular GitHub reositories and making erformance
imrovements to TensorFlow. Finally, we release an oen source library called TensorFuzz that imlements the
described techniques.
"
585,2019,MetricGAN: Generative Adversarial Networks based Black-box Metric Scores Optimization for Speech Enhancement,Oral,"Adversarial loss in a conditional generative adversarial network (GAN) is not designed to directly otimize evaluation metrics of a target task, and thus, may not always guide the generator in a GAN to generate data with imroved metric scores. To overcome this issue, we roose a novel MetricGAN aroach with an aim to otimize the generator with resect to one or multile evaluation metrics. Moreover, based on MetricGAN, the metric scores of the generated data can also be arbitrarily secified by users. We tested the roosed MetricGAN on a seech enhancement task, which is articularly suitable to verify the roosed aroach because there are multile metrics measuring different asects of seech signals. Moreover, these metrics are generally comlex and could not be fully otimized by L or conventional adversarial losses.
"
586,2019,Power k-Means Clustering,Oral,"Clustering is a fundamental task in unsuervised machine learning. Lloyd's 1957 algorithm for k-means clustering remains one of the most widely used due to its seed and simlicity. As greedy aroaches, Lloyd's algorithm and its variants are sensitive to initialization and often fall short at a oor solution. This aer exlores an alternative to Lloyd's algorithm that retains its simlicity and mitigates its tendency to get traed by local minima. Called ower k-means, our method embeds the k-means roblem in a continuous class of similar, better behaved roblems with fewer local minima. Power k-means anneals its way toward the solution of ordinary k-means by way of majorization-minimization (MM), sharing the aealing descent roerty and low comlexity of Lloyd's algorithm. Further, our method comlements widely used seeding strategies, reaing marked imrovements when used in conjunction. These merits are demonstrated on a suite of simulated and real data examles
"
587,2019,Learning Optimal Fair Policies,Oral,"Systematic discriminatory biases resent in our society influence the way data is collected and stored, the way variables are defined, and the way scientific findings are ut into ractice as olicy. Automated decision rocedures and learning algorithms alied to such data may serve to eretuate existing injustice or unfairness in our society. In this aer, we consider how to make otimal but fair decisions, which “break the cycle of injustice” by correcting for the unfair deendence of both decisions and outcomes on sensitive features (e.g., variables that corresond to gender, race, disability, or other rotected attributes). We use methods from causal inference and constrained otimization to learn otimal olicies in a way that addresses multile otential biases which afflict data analysis in sensitive contexts, extending the aroach of Nabi &am; Shitser (2018). Our roosal comes equied with the theoretical guarantee that the chosen fair olicy will induce a joint distribution for new instances that satisfies given fairness constraints. We illustrate our aroach with both synthetic data and real criminal justice data.
"
588,2019,Calibrated Model-Based Deep Reinforcement Learning,Oral,"Accurate estimates of redictive uncertainty are imortant for building effective model-based reinforcement learning agents. However, redictive uncertainties --- esecially ones derived from modern neural networks --- are often inaccurate and imose a bottleneck on erformance. Here, we argue that ideal model uncertainties should be calibrated, i.e. their robabilities should match emirical frequencies of redicted events. We describe a simle way to augment any model-based reinforcement learning algorithm with calibrated uncertainties and show that doing so consistently imroves the accuracy of lanning and hels agents balance exloration and exloitation. On the HalfCheetah MuJoCo task, our system achieves state-of-the-art erformance using 50\% fewer samles than the current leading aroach. Our findings suggest that calibration can imrove the erformance and samle comlexity of model-based reinforcement learning with minimal comutational and imlementation overhead.
"
589,2019,Bit-Swap: Recursive Bits-Back Coding for Lossless Compression with Hierarchical Latent Variables,Oral,"The ""bits back"" argument (Wallace, 1990; Hinton &am; Van Cam, 1993) suggests lossless comression schemes with latent variable models. However, how to translate the ""bits back"" argument into efficient and ractical lossless comression schemes is still largely an oen roblem. Bits-Back with Asymmetric Numeral Systems (Townsend et al., 2018) makes ""bits back"" coding ractically feasible, yet when executed on hierarchical latent variable models, their algorithm becomes substantially inefficient. In this aer we roose Bit-Swa, a comression scheme that generalizes existing lossless comression techniques and results in strictly better comression rates for hierarchical latent variable models. Through exeriments we verify that the roosed technique results in lossless comression rates that are emirically suerior to existing techniques.
"
590,2019,Adaptive Scale-Invariant Online Algorithms for Learning Linear Models,Oral,"We consider online learning with linear models, where the algorithm redicts on sequentially revealed instances (feature vectors), and is comared against the best linear function (comarator) in hindsight. Poular algorithms in this framework, such as Online Gradient Descent (OGD), have arameters (learning rates), which ideally should be tuned based on the scales of the features and the otimal comarator, but these quantities only become available at the end of the learning rocess. In this aer, we resolve the tuning roblem by roosing online algorithms making redictions which are invariant under arbitrary rescaling of the features. The algorithms have no arameters to tune, do not require any rior knowledge on the scale of the instances or the comarator, and achieve regret bounds matching (u to a logarithmic factor) that of OGD with otimally tuned searate learning rates er dimension, while retaining comarable runtime erformance.
"
591,2019,Gaining Free or Low-Cost Interpretability with Interpretable Partial Substitute,Oral,"This work addresses the situation where a black-box model outerforms all its interretable cometitors. The existing solution to understanding the black-box is to use an exlainer model to generate exlanations, which can be ambiguous and inconsistent. We roose an alternative solution by finding an interretable substitute on a subset of data where the black-box model is \emh{overkill} or nearly overkill and use this interretable model to rocess this subset of data, leaving the rest to the black-box. This way, on this subset of data, the model gains comlete interretability and transarency to relace otherwise non-erfect aroximations by an external exlainer. This transarency is obtained at minimal cost or no cost of the redictive erformance. Under this framework, we develo Partial Substitute Rules (PSR)  model that uses decision rules to cature the subsace of data where the rules are as accurate or almost as accurate as the black-box rovided. PSR is agnostic to the black-box model. To train a PSR, we devise an efficient search algorithm that iteratively finds the otimal model and exloits theoretically grounded strategies to reduce comutation. Exeriments on structured and text data show that PSR obtains an effective trade-off between transarency and interretability.
"
592,2019,Graphite: Iterative Generative Modeling of Graphs,Oral,"Grahs are a fundamental abstraction for modeling relational data. However, grahs are discrete and combinatorial in nature, and learning reresentations suitable for machine learning tasks oses statistical and comutational challenges. In this work, we roose Grahite, an algorithmic framework for unsuervised learning of reresentations over nodes in large grahs using dee latent variable generative models. Our model is based on a novel combination of grah neural networks with variational autoencoders (VAE), and uses an iterative grah refinement strategy for decoding. This ermits scaling to large grahs with thousands of nodes. Theoretically, we draw novel connections of grah neural networks with aroximate inference via kernel embeddings. Emirically, Grahite outerforms cometing aroaches for the tasks of density estimation, link rediction, and node classification on synthetic and benchmark datasets. 
"
593,2019,Distributed Learning over Unreliable Networks,Oral,"Most of today's distributed machine learning systems assume {\em reliable networks}: whenever two machines exchange information (e.g., gradients or models), the network should guarantee the delivery of the message. At the same time, recent work exhibits the imressive tolerance of machine learning algorithms to errors or noise arising from relaxed communication or synchronization. In this aer, we connect these two trends, and consider the following question: {\em Can we design machine learning systems that are tolerant to network unreliability during training?} With this motivation, we focus on a theoretical roblem of indeendent interest---given a standard distributed arameter server architecture, if every communication between the worker and the server has a non-zero robability $$ of being droed, does there exist an algorithm that still converges, and at what seed? In the context of rior art, this roblem can be hrased as {\em distributed learning over random toologies}. The technical contribution of this aer is a novel theoretical analysis roving that distributed learning over random toologies can achieve comarable convergence rate to centralized or distributed learning over reliable networks. Further, we rove that the influence of the acket dro rate diminishes with the growth of the number of arameter servers. We ma this theoretical result onto a real-world scenario, training dee neural networks over an unreliable network layer, and conduct network simulation to validate the system imrovement by allowing the networks to be unreliable.
"
594,2019,Online Control with Adversarial Disturbances,Oral,"We study the control of a linear dynamical system with adversarial disturbances (as oosed to statistical noise). The objective we consider is one of regret: we desire an online control rocedure that can do nearly as well as that of a rocedure that has full knowledge of the disturbances in hindsight. Our main result is an efficient algorithm that rovides nearly tight regret bounds for this roblem. From a technical standoint, this work generalizes uon revious work in that our model allows for adversarial noise in the dynamics and allows for general convex costs.
"
595,2019,Neural Separation of Observed and Unobserved Distributions,Oral,"Searating mixed distributions is a long standing challenge for machine learning and signal rocessing. Most current methods either rely on making strong assumtions on the source distributions or rely on having training samles of each source in the mixture. In this work, we introduce a new method---Neural Egg Searation---to tackle the scenario of extracting a signal from an unobserved distribution additively mixed with a signal from an observed distribution. Our method iteratively learns to searate the known distribution from rogressively finer estimates of the unknown distribution. In some settings, Neural Egg Searation is initialization sensitive, we therefore introduce Latent Mixture Masking which ensures a good initialization. Extensive exeriments on audio and image searation tasks show that our method outerforms current methods that use the same level of suervision, and often achieves similar erformance to full suervision.
"
596,2019,Replica Conditional Sequential Monte Carlo,Oral,"We roose a Markov chain Monte Carlo (MCMC) scheme to erform state
inference in non-linear non-Gaussian state-sace models. Current state-of-the-art
methods to address this roblem rely on article MCMC techniques and
its variants, such as the iterated conditional Sequential Monte Carlo
(cSMC) scheme, which uses a Sequential Monte Carlo (SMC) tye roosal
within MCMC. A deficiency of standard SMC roosals is that they only
use observations u to time $t$ to roose states at time $t$ when
an entire observation sequence is available. More sohisticated SMC
based on lookahead techniques could be used but they can be difficult
to ut in ractice. We roose here relica cSMC where we build SMC
roosals for one relica using information from the entire observation
sequence by conditioning on the states of the other relicas. This
aroach is easily arallelizable and we demonstrate its excellent
emirical erformance when comared to the standard iterated cSMC
scheme at fixed comutational comlexity."
597,2019,Fairness-Aware Learning for Continuous Attributes and Treatments,Oral,"We address the roblem of algorithmic fairness: ensuring that the outcome of a classifier is not biased towards certain values of sensitive variables such as age, race or gender. As common fairness conditions can be exressed as (conditional) indeendence between variables, we roose to use the R\'enyi maximum correlation coefficient to generalize fairness measurement to continuous variables. We exloit the Witsenhausen's characterization of the R\'enyi coefficient to roose a differentiable imlementation linked to $f$-divergences. This allows us to generalize fairness-aware learning to continuous variables by using a enalty that uer bounds this coefficient. This  enalty can be estimated on  mini-batches allowing to use dee nets. Exeriments  show  a favorable comarison  to state of the art on binary variables and rove the ability to rotect continuous instances."
598,2019,Reinforcement Learning in Configurable Continuous Environments,Oral,"Configurable Markov Decision Processes (Conf-MDPs) have been recently introduced as an extension of the usual MDP model to account for the ossibility of configuring the environment to imrove the agent's erformance. Currently, there is still no suitable algorithm to solve the learning roblem for real-world Conf-MDPs. In this aer, we fill this ga by roosing a trust-region method, Relative Entroy Model Policy Search (REMPS), able to learn both the olicy and the MDP configuration in continuous domains without requiring the knowledge of the true model of the environment. After introducing our aroach and roviding a finite-samle analysis, we emirically evaluate REMPS on both benchmark and realistic environments by comaring our results with those of the gradient methods.
"
599,2019,Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation,Oral,"Desite the remarkable success of Dee RL in learning control olicies from raw ixels, the resulting models do not generalize. We demonstrate that a trained agent fails comletely when facing small visual changes, and that fine-tuning---the common transfer learning aradigm---fails to adat to these changes, to the extent that it is faster to re-train the model from scratch. We show that by searating the visual transfer task from the control olicy we achieve substantially better samle efficiency and transfer behavior, allowing an agent trained on the source task to transfer well to the target tasks. The visual maing from the target to the source domain is erformed using unaligned GANs, resulting in a control olicy that can be further imroved using imitation learning from imerfect demonstrations. We demonstrate the aroach on synthetic visual variants of breakout game, as well as on transfer between subsequent levels of Road Fighter, a Nintendo car-driving game. A visualization of our aroach can be seen in \url{htts:streamable.comymv8v} and \url{htts:streamable.comfvy1d}.
"
600,2019,Adversarial Online Learning with noise,Oral,"We resent and study models of adversarial online learning where the feedback observed by the learner is noisy, and the feedback is either full information feedback or bandit feedback. Secifically, we consider binary losses xored with the noise, which is a Bernoulli random variable. We consider both a constant noise rate and a variable noise rate. Our main results are tight regret bounds for learning with noise in the adversarial online learning model.
"
601,2019,Almost Unsupervised Text to Speech and Automatic Speech Recognition,Oral,"Text to seech (TTS) and automatic seech recognition (ASR) are two dual tasks in seech rocessing and both achieve imressive erformance thanks to the recent advance in dee learning and large amount of aligned seech and text data. However, the lack of aligned data oses a major ractical roblem for TTS and ASR on low-resource languages. In this aer, by leveraging the dual nature of the two tasks, we roose an almost unsuervised learning method that only leverages few hundreds of aired data and extra unaired data for TTS and ASR. Our method consists of the following comonents: (1) denoising auto-encoder, which reconstructs seech and text sequences resectively to develo the caability of language modeling both in seech and text domain; (2) dual transformation, where the TTS model transforms the text $y$ into seech $\hat{x}$, and the ASR model leverages the transformed air $(\hat{x},y)$ for training, and vice versa, to boost the accuracy of the two tasks; (3) bidirectional sequence modeling, which address the error roagation roblem esecially in the long seech and text sequence when training with few aired data; (4) a unified model structure, which combines all the above comonents for TTS and ASR based on Transformer model. Our method achieves 99.84\% in terms of word level intelligible rate and 2.68 MOS for TTS, and 11.7\% PER for ASR on LJSeech dataset, by leveraging only 200 aired seech and text data (about 20 minutes audio), together with extra unaired seech and text data. "
602,2019,Hybrid Models with Deep and Invertible Features,Oral,"Dee neural networks are owerful black-box redictors for modeling conditional distributions of the form (target|features). While they can be very successful at suervised learning roblems where the train and test distributions are the same, they can make overconfident wrong redictions when the test distribution is different. Hybrid models that include both a discriminative conditional model (target|features) and a generative model (features) can be more robust under dataset shift, as they can detect covariate shift using the generative model. Current state-of-the-art hybrid models require aroximate inference which can be comutationally exensive. We roose an hybrid model that defines a generalized linear model on to of dee invertible features (e.g. normalizing flows). An attractive roerty of our model is that  both (features),  the log density,  and  (target|features), the redictive distribution, can be comuted exactly in a single feed-forward ass.  We show that our hybrid model achieves similar redictive accuracy as urely  discriminative models on classification and regression tasks, while roviding better uncertainty quantification and the ability to detect out-of-distribution inuts. In addition, we also demonstrate that the generative comonent of the hybrid model can leverage unlabeled data for semi-suervised learning, as well as generate samles which are useful to visualize and interret the model. The availability of the exact joint density (target,features) also allows us to comute many quantities readily, making our hybrid model an useful building block for downstream alications of robabilistic dee learning, including but not limited to active learning and domain adatation.
"
603,2019,Target-Based Temporal-Difference Learning,Oral,"The use of target networks has been a oular and key comonent of recent dee Q-learning algorithms for reinforcement learning, yet little is known from the theory side. In this work, we introduce a new family of target-based temoral difference (TD) learning algorithms and rovide theoretical analysis on their convergences. In contrast to the standard TD-learning, target-based TD algorithms maintain two searate learning arameters--the target variable and online variable. Particularly, we introduce three members in the family, called the averaging TD, double TD, and eriodic TD, where the target variable is udated through an averaging, symmetric, or eriodic fashion, mirroring that used in recent dee Q-learning, resectively.

We establish an asymtotic convergence analysis for both averaging TD and double TD algorithms and a finite samle analysis for the eriodic TD algorithm. In addition, we also rovide some simulation results showing otentially suerior convergence of the target-based TD algorithms comared to the standard TD-learning. While this work is focused on linear function aroximation and olicy evaluation setting, we consider this as a meaningful ste towards the theoretical understanding of dee Q-learning variants with target networks.
"
604,2019,State-Regularized Recurrent Neural Networks,Oral,"Recurrent neural networks are a widely used class of neural architectures.  They have, however, two shortcomings. First, it is difficult to understand what exactly they learn. Second, they tend to work oorly on sequences requiring long-term memorization, desite having this caacity in rincile. We aim to address both shortcomings with a class of recurrent networks that use a stochastic state transition mechanism between cell alications. This mechanism, which we term state-regularization, makes RNNs transition between a finite set of learnable states. We evaluate state-regularized RNNs on (1) regular languages for the urose of automata extraction; (2) nonregular languages such as balanced arentheses, alindromes, and the coy task where external memory is required; and (3) real-word sequence learning tasks for sentiment analysis, visual object recognition, and language modeling. We show that state-regularization (a) simlifies the extraction of finite state automata modeling an RNN's state transition dynamics; (b) forces RNNs to oerate more like automata with external memory and less like finite state machines; (c) makes RNNs have better interretability and exlainability.
"
605,2019,Learning What and Where to Transfer,Oral,"As the alication of dee learning has exanded to real-world roblems with insufficient volume of training data, transfer learning recently has gained much attention as means of imroving the erformance in such small-data regime. However, when existing methods are alied between heterogeneous architectures and tasks, it becomes more imortant to manage their detailed configurations and often requires exhaustive tuning on them for the desired erformance. To address the issue, we roose a novel transfer learning aroach based on meta-learning that can automatically learn what knowledge to transfer from the source network to where in the target network. Given source and target networks, we roose an efficient training scheme to learn meta-networks that decide (a) which airs of layers between the source and target networks should be matched for knowledge transfer and (b) which features and how much knowledge from each feature should be transferred. We validate our meta-transfer aroach against recent transfer learning methods on various datasets and network architectures, on which our automated scheme significantly outerforms the rior baselines that find “what and where to transfer” in a hand-crafted manner.
"
606,2019,Fairness risk measures,Oral,"Ensuring that classifiers are non-discriminatory or fair with resect to a sensitive feature (e.g., race or gender) is a toical roblem. Progress in this task requires fixing a definition of fairness, and there have been several roosals in this regard over the ast few years. Several of these, however, assume either binary sensitive features (thus recluding categorical or real-valued sensitive grous), or result in non-convex objectives (thus adversely affecting the otimisation landscae).

In this aer, we roose a new definition of fairness that generalises some existing roosals, while allowing for generic sensitive features and resulting in a convex objective. The key idea is to enforce that the exected losses (or risks) across each subgrou induced by the sensitive feature are commensurate. We show how this relates to the rich literature on risk measures from mathematical finance. As a secial case, this leads to a new convex fairness-aware objective based on minimising the conditional value at risk (CVaR).
"
607,2019,A Polynomial Time MCMC Method for  Sampling from Continuous Determinantal Point Processes,Oral,"We study the Gibbs samling algorithm for discrete and continuous $k$-determinantal oint rocesses. We show that in both cases, the sectral ga of the chain is bounded by a olynomial of $k$ and it is   indeendent of the  size of the domain.
As an immediate corollary, we obtain sublinear time algorithms for samling from discrete $k$-DPPs given access to olynomially many rocessors. 
In the continuous setting, our result leads to the first class of rigorously analyzed efficient algorithms to generate random samles of continuous $k$-DPPs.
We achieve this by showing that the Gibbs samler for a large family of continuous $k$-DPPs can be simulated efficiently when the sectrum is not concentrated on the to $k$ eigenvalues."
608,2019,Escaping Saddle Points with Adaptive Gradient Methods,Oral,"Adative methods such as Adam and RMSPro are widely used in dee learning but are not well understood. In this aer, we seek a cris, clean and recise characterization of their behavior in nonconvex settings. To this end, we first rovide a novel view of adative methods as reconditioned SGD, where the reconditioner is estimated in an online manner. By studying the reconditioner on its own, we elucidate its urose: it rescales the stochastic gradient noise to be isotroic near stationary oints, which hels escae saddle oints. Furthermore, we show that adative methods can efficiently estimate the aforementioned reconditioner. By gluing together these two comonents, we rovide the first (to our knowledge) second-order convergence result for any adative method. The key insight from our analysis is that, comared to SGD, adative methods escae saddle oints faster, and can converge faster overall to second-order stationary oints.
"
609,2019,Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation,Oral,"Current methods to interret dee learning models by generating saliency mas generally rely on two key assumtions. First, they use first-order aroximations of the loss function neglecting higher-order terms such as the loss curvatures. Second, they evaluate each feature's imortance in isolation, ignoring their inter-deendencies.  In this work, we study the effect of relaxing these two assumtions. First, by characterizing a closed-form formula for the Hessian matrix of a dee ReLU network, we rove that, for a classification roblem with a large number of classes, if an inut has a high confidence classification score, the inclusion of the Hessian term has small imacts in the final solution. We rove this result by showing that in this case the Hessian matrix is aroximately of rank one and its leading eigenvector is almost arallel to the gradient of the loss function. Our emirical exeriments on ImageNet samles are consistent with our theory. This result can have imlications in other related roblems such as adversarial examles as well. Second, we comute the imortance of grou-features in dee learning interretation by introducing a sarsity regularization term. We use the $L_0-L_1$ relaxation technique along with the roximal gradient descent to have an efficient comutation of grou feature imortance scores. Our emirical results indicate that considering grou features can imrove dee learning interretation significantly."
610,2019,Iterative Linearized Control: Stable Algorithms and Complexity Guarantees,Oral,"We frame several oular iterative linearized control algorithms for nonlinear control such as ILQR and ILQG as nonlinear otimization algorithms on an objective whose first-order information can be comuted using dynamic rogramming. Our framework allows us to identify a gradient back-roagation oracle corresonding to dynamic rogramming. 
The number of calls to this oracle is arguably the relevant measure of comlexity of such algorithms in modern comuting environments. 
We also highlight several missing comonents in these algorithms and roose stable and accelerated variants that enjoy worst-case comlexity guarantees from a nonlinear otimization viewoint.
"
611,2019,AutoVC:  Zero-Shot Voice Style Transfer with Only Autoencoder Loss,Oral,"Desite the rogress in voice conversion, many-to-many voice conversion trained on non-arallel data, as well as zero-shot voice conversion, remains under-exlored. Dee style transfer algorithms, generative adversarial networks (GAN) in articular, are being alied as new solutions in this field. However, GAN training is very sohisticated and difficult, and there is no strong evidence that its generated seech is of good ercetual quality. In this aer, we roose a new style transfer scheme that involves only an autoencoder with a carefully designed bottleneck. We formally show that this scheme can achieve distribution-matching style transfer by training only on self-reconstruction loss. Based on this scheme, we roosed AutoVC, which achieves state-of-the-art results in many-to-many voice conversion with non-arallel data, and which is the first to erform zero-shot voice conversion.
"
612,2019,Adaptive Antithetic Sampling for Variance Reduction,Oral,"Variance reduction techniques are crucial in stochastic estimation and otimization roblems. Antithetic samling techniques reduce the variance of a Monte Carlo estimator by drawing correlated, rather than indeendent, samles. Designing the right correlation structure, however, is challenging and alication secific, thus limiting the ractical alicability of these methods. In this aer, we roose a general-urose adative antithetic samling framework. We leverage advances in generative models and stochastic comutation grahs to define a flexible family of antithetic samlers. We rovide gradient-based and gradient-free methods to train the samlers such that they reduce variance while ensuring that the underlying Monte Carlo estimator is rovably unbiased. We demonstrate the effectiveness of our aroach on Bayesian inference and generative model training tasks, where it reduces variance and imroves task erformance with little or no comutational overhead.
"
613,2019,Online Variance Reduction with Mixtures,Oral,"Adative imortance samling for stochastic otimization is a romising aroach that offers imroved convergence through variance reduction. In this work, we roose a new framework for variance reduction that enables the use of mixtures over redefined samling distributions, which can naturally encode rior knowledge about the data. While these samling distributions are fixed, the mixture weights are adated during the otimization rocess. We roose VRM, a novel and efficient adative scheme that asymtotically recovers the best mixture weights in hindsight and can also accommodate samling distributions over sets of oints. We emirically demonstrate the versatility of VRM in a range of alications.
"
614,2019,MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Sets,Oral,"We consider the roblem of handling missing data with dee latent variable models (DLVMs). First, we resent a simle technique to train DLVMs when the training set contains missing-at-random data. Our aroach, called MIWAE, is based on the imortance-weighted autoencoder (IWAE), and maximises a otentially tight lower bound of the log-likelihood of the observed data. Comared to the original IWAE, our algorithm does not induce any additional comutational overhead due to the missing data. We also develo Monte Carlo techniques for single and multile imutation using a DLVM trained on an incomlete data set. We illustrate our aroach by training a convolutional DLVM on a static binarisation of MNIST that contains 50% of missing ixels. Leveraging multile imutation, a convolutional network trained on these incomlete digits has a test erformance similar to one trained on comlete data. On various continuous and binary data sets, we also show that MIWAE rovides accurate single imutations, and is highly cometitive with state-of-the-art methods.
"
615,2019,$\texttt{DoubleSqueeze}$: Parallel Stochastic Gradient Descent with Double-pass Error-Compensated Compression,Oral,"A standard aroach in large scale machine learning is distributed stochastic gradient training, which requires the comutation of aggregated stochastic gradients over multile nodes on a network. Communication is a major bottleneck in such alications, and in recent years, comressed stochastic gradient methods such as QSGD (quantized SGD) and sarse SGD have been roosed to reduce communication. It was also shown that error comensation can be combined with comression to achieve better convergence in a scheme that each node comresses its local stochastic gradient and broadcast the result to all other nodes over the network in a single ass. However, such a single ass broadcast aroach is not realistic in many ractical imlementations. For examle, under the oular arameter-server model for distributed learning, the worker nodes need to send the comressed local gradients to the arameter server, which erforms the aggregation. The arameter server has to comress the aggregated stochastic gradient again before sending it back to the worker nodes. In this work, we rovide a detailed analysis on this two-ass communication model, with error-comensated comression both on the worker nodes and on the arameter server. We show that the error-comensated stochastic gradient algorithm admits three very nice roerties: 1) it is comatible with an \emh{arbitrary} comression technique; 2) it admits an imroved convergence rate than the non error-comensated stochastic gradient method such as QSGD and sarse SGD; 3) it admits linear seedu with resect to the number of workers. The emirical study is also conducted to validate our theoretical results.
"
616,2019,Accelerated Flow for Probability Distributions,Oral,"This aer resents a methodology and numerical algorithms for constructing accelerated gradient flows on the sace of robability distributions.  In articular, we extend the recent variational formulation of accelerated gradient methods in (Wibisono2016) from vector valued variables to robability distributions. The variational roblem is modeled as a mean-field otimal control roblem. The maximum rincile of otimal control theory is used to derive Hamilton's equations for the otimal gradient flow. The Hamilton's equation are shown to achieve the accelerated form of density transort from any initial robability distribution to a target robability distribution.  A quantitative estimate on the asymtotic convergence rate is rovided based on a Lyaunov function construction, when the objective functional is dislacement convex.  Two numerical aroximations are resented to imlement the Hamilton's equations as a system of $N$ interacting articles. The continuous limit of the Nesterov's algorithm is shown to be a secial case with $N=1$. The algorithm is illustrated with numerical examles and the erformance is comared with the MCMC and Hamiltonian MCMC algorithms.   "
617,2019,On the Connection Between Adversarial Robustness and Saliency Map Interpretability,Oral,"Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interretable saliency mas than their non-robust counterarts. We aim to quantify this behaviour by considering the alignment between inut image and saliency ma. We hyothesize that as the distance to the decision boundary grows, so does the alignment. This connection is strictly true in the case of linear models. We confirm these theoretical findings with exeriments based on models trained with a local Lischitz regularization and identify where the nonlinear nature of neural networks weakens the relation.
"
618,2019,Model Function Based Conditional Gradient Method with Armijo-like Line Search,Oral,"The Conditional Gradient Method is generalized to a class of non-smooth non-convex otimization roblems with many alications in machine learning. The roosed algorithm iterates by minimizing so-called model functions over the constraint set. Comlemented with an Amijo line search rocedure, we rove that subsequences converge to a stationary oint. The abstract framework of model functions rovides great flexibility in the design of concrete algorithms. As secial cases, for examle, we develo an algorithm for additive comosite roblems and an algorithm for non-linear comosite roblems which leads to a Gauss-Newton-tye algorithm. Both instances are novel in non-smooth non-convex otimization and come with numerous alications in machine learning. We erform an exeriment on a non-linear robust regression roblem and discuss the flexibility of the roosed framework in several matrix factorization formulations.
"
619,2019,Bandit Multiclass Linear Classification: Efficient Algorithms for the Separable Case,Oral,"We study the roblem of efficient online multiclass linear classification with
bandit feedback, where all examles belong to one of $K$ classes and lie in the
$d$-dimensional Euclidean sace. Previous works have left oen the challenge of
designing efficient algorithms with finite mistake bounds when the data is
linearly searable by a margin $\gamma$. In this work, we take a first ste
towards this roblem. We consider two notions of linear searability,
\emh{strong} and \emh{weak}.

1. Under the strong linear searability condition, we design an efficient
algorithm that achieves a near-otimal mistake bound of
$O\left(\frac{K}{\gamma^2} \right)$.

2. Under the more challenging weak linear searability condition, we design
an efficient algorithm with a mistake bound of $2^{\widetilde{O}(\min(K \log^2
\frac{1}{\gamma}, \sqrt{\frac{1}{\gamma}} \log K))}$ \footnote{We use the notation
$\widetilde{O}(f(\cdot)) = O(f(\cdot) \olylog(f(\cdot)))$.}. Our algorithm
is based on kernel Percetron, which is insired by the work
of \citet{Klivans-Servedio-2008} on imroerly learning intersection of halfsaces."
620,2019,Finding Options that Minimize Planning Time,Oral,"We formalize the roblem of selecting the otimal set of otions for lanning as that of  comuting the smallest set of otions so that lanning converges in less than a given maximum of value-iteration asses.
We first show that the roblem is  NP-hard, even if the task is constrained to be deterministic---the first such comlexity result for otion discovery.
We then resent the first olynomial-time boundedly subotimal aroximation algorithm for this setting, and emirically evaluate it against both the otimal otions and a reresentative collection of heuristic aroaches in simle grid-based domains including the classic four-rooms roblem.
"
621,2019,A fully differentiable beam search decoder,Oral,"We introduce a new beam search decoder that is fully differentiable, making it ossible to otimize at training time through the inference rocedure. Our decoder allows us to combine models which oerates at different granularity (e.g. acoustic and language models). It also handles an arbitrary number of target sequence candidates, making it suitable in a context where labeled data is not aligned to inut sequences. We demonstrate our aroach scales by alying it to seech recognition, jointly training acoustic and word-level language models. The system is end-to-end, with gradients flowing through the whole architecture from the word-level transcritions.  Recent research efforts have shown that  dee neural networks with attention-based mechanisms are owerful enough to successfully train an acoustic model from the final transcrition, while imlicitly learning a language model. Instead, we show that it is ossible to discriminatively train an acoustic model jointly with an \emh{exlicit} and ossibly re-trained language model.
"
622,2019,On Scalable and Efficient Computation of Large Scale Optimal Transport,Oral,"Otimal Transort (OT) naturally arises in many machine learning alications, yet the heavy comutational burden limits its wide-sread uses. To address the scalability issue, we roose an imlicit generative learning-based framework called SPOT (Scalable Push-forward of Otimal Transort). Secifically, we aroximate the otimal transort lan by a ushforward of a reference distribution, and cast the otimal transort roblem into a minimax roblem. We then can solve OT roblems efficiently using rimal dual stochastic gradient-tye algorithms. We also show that we can recover the density of the otimal transort lan using neural ordinary differential equations. Numerical exeriments on both synthetic and real datasets illustrate that SPOT is robust and has favorable convergence behavior. SPOT also allows us to efficiently samle from the otimal transort lan, which benefits downstream alications such as domain adatation.
"
623,2019,Stochastic Beams and Where To Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement,Oral,"The well-known Gumbel-Max trick for samling from a categorical distribution can be extended to samle k elements without relacement. We show how to imlicitly aly this `Gumbel-To-k' trick on a factorized distribution over sequences, allowing to draw exact samles without relacement using a Stochastic Beam Search. Even for exonentially large domains, the number of model evaluations grows only linear in k and the maximum samled sequence length. The algorithm creates a theoretical connection between samling and beam search and can be used as a rinciled intermediate alternative. In a translation task, we show that the roosed method comares favourably against alternatives to obtain diverse yet good quality translations. We show that sequences samled without relacement can be used to construct low-variance estimators for exected sentence-level BLEU score and model entroy.
"
624,2019,DBSCAN++: Towards fast and scalable density clustering,Oral,"DBSCAN is a classical density-based clustering rocedure with tremendous ractical relevance. However, DBSCAN imlicitly needs to comute the emirical density for each samle oint, leading to a quadratic  worst-case  time  comlexity, which is too slow on large datasets. We roose DBSCAN++, a simle modification of DBSCAN which only requires comuting the densities for a chosen subset of oints. We show emirically that, comared to traditional DBSCAN, DBSCAN++ can rovide not only cometitive erformance but also added robustness in the bandwidth hyerarameter while taking a fraction of the runtime. We also resent statistical consistency guarantees showing the trade-off between comutational cost and estimation rates.  Surrisingly, u to a certain oint, we can enjoy the same estimation rates while lowering comutational cost, showing that DBSCAN++ is a sub-quadratic algorithm that attains minimax otimal rates for level-set estimation, a quality that may be of indeendent interest.
"
625,2019,Proportionally Fair Clustering,Oral,"We extend the fair machine learning literature by considering the roblem of roortional centroid clustering in a metric context. For clustering n oints with k centers, we define fairness as roortionality to mean that any nk oints are entitled to form their own cluster if there is another center that is closer in distance for all nk oints. We seek clustering solutions to which there are no such justified comlaints from any subsets of agents, without assuming any a riori notion of rotected subsets. We resent and analyze algorithms to efficiently comute, otimize, and audit roortional solutions. We conclude with an emirical examination of the tradeoff between roortional solutions and the k-means objective.
"
626,2019,Scaling Up Ordinal Embedding: A Landmark Approach,Oral,"Ordinal Embedding is the roblem of lacing n objects into R^d to satisfy constraints like ""object a is closer to b than to c."" It can accommodate data that embeddings from features or distances cannot, but is a more difficult roblem. We roose a novel landmark-based method as a artial solution. At small to medium scales, we resent a novel combination of existing methods with some new theoretical justification. For very large values of n otimizing over an entire embedding breaks down, so we roose a novel method which first embeds a subset of m &lt;&lt; n objects and then embeds the remaining objects indeendently and in arallel. We rove a distance error bound for our method in terms of m and that it has O(dn log m) time comlexity, and show emirically that it is able to roduce high quality embeddings in a fraction of the time needed for any ublished method.
"
627,2019,Understanding and correcting pathologies in the training of learned optimizers,Oral,"Dee learning has shown that learned functions can dramatically outerform hand-designed functions on ercetual tasks. Analogously, this suggests that learned otimizers may similarly outerform current hand-designed otimizers, esecially for secific roblems. However, learned otimizers are notoriously difficult to train and have yet to demonstrate wall-clock seedus over hand-designed otimizers, and thus are rarely used in ractice. Tyically, learned otimizers are trained by truncated backroagation through an unrolled otimization rocess. The resulting gradients are either strongly biased (for short truncations) or have exloding norm (for long truncations). In this work we roose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on otimizer erformance. This allows us to train neural networks to erform otimization of a secific task faster than tuned first-order methods. Moreover, by training the otimizer against validation loss (as oosed to training loss), we are able to learn otimizers that train networks to generalize better than first order methods. We demonstrate these results on roblems where our learned otimizer trains convolutional networks faster in wall-clock time comared to tuned first-order methods and with an imrovement in test loss.
"
628,2019,Why do Larger Models Generalize Better? A Theoretical Perspective via the XOR Problem,Oral,"Emirical evidence suggests that neural networks with ReLU activations generalize better with over-arameterization. However, there is currently no theoretical analysis that exlains this observation. In this work, we rovide theoretical and emirical evidence that, in certain cases, overarameterized convolutional networks generalize better than small networks because of an interlay between weight clustering and feature exloration at initialization.  We demonstrate this theoretically for a 3-layer convolutional neural network with max-ooling, in a novel setting which extends the XOR roblem. We show that this interlay imlies that with overaramterization, gradient descent converges to global minima with better generalization erformance comared to global minima of small networks. Emirically, we demonstrate these henomena for a 3-layer convolutional neural network in the MNIST task.
"
629,2019,Analogies Explained: Towards Understanding Word Embeddings,Oral,"Word embeddings generated by neural network methods such as \textit{word2vec} (W2V) are well known to exhibit seemingly linear behaviour, e.g. the embeddings of analogy \emh{``queen is to woman as king is to man''} aroximately describe a arallelogram. This roerty is articularly intriguing since the embeddings are not trained to achieve it. Several exlanations have been roosed, but each introduces assumtions that do not hold in ractice.
We derive a robabilistically grounded definition of \textit{arahrasing} and show it can be re-interreted as \textit{word transformation}, a mathematical descrition of \emh{``$w_x$ is to $w_y$''}. From these concets we rove existence of the linear relationshi between W2V-tye embeddings that underlies the analogical henomenon, and identify exlicit error terms in the relationshi."
630,2019,Learning Linear-Quadratic Regulators Efficiently with only $\sqrt{T}$ Regret,Oral,"We resent the first comutationally-efficient algorithm with $\widetilde{O}(\sqrt{T})$ regret for learning in Linear Quadratic Control systems with unknown dynamics.
By that, we resolve an oen question of Abbasi-Yadkori and Szeesvari (2011) and Dean,Mania, Matni, Recht, and Tu (2018)."
631,2019,Tight Kernel Query Complexity of Kernel Ridge Regression and Kernel $k$-means Clustering,Oral,"Kernel methods generalize machine learning algorithms that only deend on the airwise inner roducts of the dataset by relacing inner roducts with kernel evaluations, a function that asses inut oints through a nonlinear feature ma before taking the inner roduct in a higher dimensional sace. In this work, we resent nearly tight lower bounds on the number of kernel evaluations required to aroximately solve kernel ridge regression (KRR) and kernel $k$-means clustering (KKMC) on $n$ inut oints. For KRR, our bound for relative error aroximation the argmin of the objective function is $\Omega(nd_{\mathrm{eff}}^\lambda\varesilon)$ where $d_{\mathrm{eff}}^\lambda$ is the effective statistical dimension, tight u to a $\log(d_{\mathrm{eff}}^\lambda\varesilon)$ factor. For KKMC, our bound for finding a $k$-clustering achieving a relative error aroximation of the objective function is $\Omega(nk\varesilon)$, tight u to a $\log(k\varesilon)$ factor. Our KRR result resolves a variant of an oen question of El Alaoui and Mahoney, asking whether the effective statistical dimension is a lower bound on the samling comlexity or not. Furthermore, for the imortant inut distribution case of mixtures of Gaussians, we rovide algorithms that byass the above lower bounds."
632,2019,Stable and Fair Classification,Oral,"Fair classification has been a toic of intense study in machine learning and several algorithms have been roosed towards this imortant task. However, in a recent study, Friedler et al.  ointed out that several fair classification algorithms are not stable with resect to variations in the training set -- a crucial consideration in several alications. Motivated by their work, we study the roblem of designing classification algorithms that are both fair and stable. We roose an extended framework based on fair classification algorithms that are formulated as otimization roblems, by introducing a stability-focused regularization term. Theoretically, we rove an additional stability guarantee, that was lacking in fair classification algorithms, and also rovide an accuracy guarantee for our extended framework. Our accuracy guarantee can be used to inform the selection of the regularization arameter in our framework. To the best of our knowledge, this is the first work that combines stability and fairness in automated decision-making tasks. We assess the benefits of our aroach emirically by extending several fair classification algorithms that are shown to achieve the best balance between fairness and accuracy over the \textbf{Adult} dataset. Our emirical results show that our extended framework indeed imroves the stability at only a slight sacrifice in accuracy.
"
633,2019,Dimensionality Reduction for Tukey Regression,Oral,"We give the first dimensionality reduction methods for the overconstrained Tukey regression roblem. The Tukey loss function $\|y\|_M = \sum_i M(y_i)$ has $M(y_i) \arox |y_i|^$ for residual errors $y_i$ smaller than a rescribed threshold $\tau$, but $M(y_i)$ becomes constant for errors $|y_i|  \tau$. Our results deend on a new structural result, roven constructively, showing that for any $d$-dimensional subsace  $L \subset \mathbb{R}^n$, there is a fixed bounded-size subset of coordinates containing, for every $y\in L$, all the large coordinates, {\it with resect to the Tukey loss function}, of~$y$. Our methods reduce a given Tukey regression roblem to a smaller weighted version, whose solution is a rovably good aroximate solution to the original roblem. Our reductions are fast, simle, and easy to imlement, and we give emirical results demonstrating their racticality, using existing heuristic solvers for the small versions. We also give exonential-time algorithms giving rovably good solutions, and hardness results suggesting that a significant seedu in the worst case is unlikely. "
634,2019,Concrete Autoencoders: Differentiable Feature Selection and Reconstruction,Oral,"We introduce the concrete autoencoder, an end-to-end differentiable method for global feature selection, which efficiently extracts a sarse set of the most informative features and simultaneously learns a neural network to reconstruct the inut data from the selected features. Our method is unsuervised, and is based on using a concrete selector layer as the encoder and using a standard neural network as the decoder. During the training hase, the temerature of the concrete selector layer is gradually decreased, which encourages a user-secified number of discrete features to be learned. During test time, the selected features can be used with the decoder network to reconstruct the remaining inut features.  We evaluate concrete autoencoders on a variety of datasets, where they significantly outerform state-of-the-art methods for feature selection and data reconstruction. In articular, on a large-scale gene exression dataset, the concrete autoencoder selects a small subset of genes whose exression levels can be use to imute the exression levels of the remaining genes. In doing so, it imroves on the current widely-used exert-curated L1000 landmark genes, otentially saving exerimental costs.  The concrete autoencoder can be imlemented by adding just a few lines of code to a standard autoencoder.
"
635,2019,Learning from Delayed Outcomes via Proxies with Applications to Recommender Systems,Oral,"Predicting delayed outcomes is an imortant roblem in recommender systems (e.g., will customers finish reading an ebook?). We formalize the roblem as an adversarial, delayed online learning roblem and consider how a roxy for the delayed outcome (e.g., if customers read a third of the book in 24 hours) can hel minimize regret, even though the roxy is not available when making a rediction. Motivated by our regret analysis, we roose two neural network architectures: Factored Forecaster (FF) which is ideal if the roxy is informative of the outcome in hindsight, and Residual Factored Forecaster (RFF)  that is robust to a non-informative roxy. Exeriments on two real world datasets for redicting human behavior show that RFF outerforms both FF and a direct forecaster that does not make use of the roxy. Our results suggest that exloiting roxies by factorization is a romising way to mitigate the imact of long delays in human behavior rediction tasks.
"
636,2019,On the Spectral Bias of Neural Networks,Oral,"Neural networks are known to be a class of highly exressive functions  able to fit even random inut-outut maings with 100% accuracy. In this work we resent roerties of neural networks that comlement this asect of exressivity. By using tools from Fourier analysis, we highlight a learning bias of dee networks towards low frequency functions -- i.e. functions that vary globally without local fluctuations -- which manifests itself as a frequency-deendent learning seed. Intuitively, this roerty is in line with the observation that over-arameterized networks rioritize learning simle atterns that generalize across data samles. We also investigate the role of the shae of the data manifold by resenting emirical and theoretical evidence that, somewhat counter-intuitively, learning higher frequencies gets easier with increasing manifold comlexity.
"
637,2019,Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs,Oral,"We study the roblem of knowledge grah (KG) embedding. A widely-established assumtion to this roblem is that similar entities are likely to have similar relational roles. However, existing related methods derive KG embeddings mainly based on trile-level learning, which lack the caability of caturing long-term relational deendencies of entities. Moreover, trile-level learning is insufficient for the roagation of semantic information among entities, esecially for the case of cross-KG embedding. In this aer, we roose recurrent skiing networks (RSNs), which emloy a skiing mechanism to bridge the gas between entities. RSNs integrate recurrent neural networks (RNNs) with residual learning to efficiently cature the long-term relational deendencies within and between KGs. We design an end-to-end framework to suort RSNs on different tasks. Our exerimental results showed that RSNs outerformed state-of-the-art embedding-based methods for entity alignment and achieved cometitive erformance for KG comletion.
"
638,2019,Learning to select for a predefined ranking,Oral,"In this aer, we formulate a novel roblem of learning to select a set of items maximizing the quality of their ordered list, where the order is redefined by some exlicit rule. Unlike the classic information retrieval roblem, in our setting, the redefined order of items in the list may not corresond to their quality in general. For examle, this is a dominant scenario in ersonalized news and social media feeds, where items are ordered by ublication time in a user interface. We roose new theoretically grounded algorithms based on direct otimization of the resulting list quality. Our offline and online exeriments with a large-scale roduct search engine demonstrate the overwhelming advantage of our methods over the baselines in terms of all key quality metrics.
"
639,2019,Parameter-Efficient Transfer Learning for NLP,Oral,"Fine-tuning large retrained models is an effective transfer mechanism in NLP. However, in the resence of many downstream tasks, fine-tuning is arameter inefficient: an entire new model is required for every task. As an alternative, we roose transfer with adater modules. Adater modules yield a comact and extensible model; they add only a few trainable arameters er task, and new tasks can be added without revisiting revious ones. The arameters of the original network remain fixed, yielding a high degree of arameter sharing. To demonstrate adater's effectiveness, we transfer the recently roosed BERT Transformer model to $26$ diverse text classification tasks, including the GLUE benchmark. Adaters attain near state-of-the-art erformance, whilst adding only a few arameters er task. On GLUE, we attain within $0.8\%$ of the erformance of full fine-tuning, adding only $3.6\%$ arameters er task. By contrast, fine-tuning trains $100\%$ of the arameters er task."
640,2019,Demystifying Dropout,Oral,"Droout is a oular technique to train large-scale dee neural networks to alleviate the overfitting roblem. To disclose the underlying reasons for its gain, numerous works have tried to exlain it from different ersectives. In this aer, unlike existing works, we exlore it from a new ersective to rovide new insight into this line of research. In detail, we disentangle the forward and backward ass of droout. Then, we find that these two asses need different levels of noise to imrove the generalization erformance of dee neural networks. Based on this observation, we roose the augmented droout which emloys different droing strategies in the forward and backward ass. Exerimental results have verified the effectiveness of our roosed method. 
"
641,2019,Flexibly Fair Representation Learning by Disentanglement,Oral,"We consider the roblem of learning reresentations that achieve grou and subgrou fairness with resect to multile sensitive attributes. Taking insiration from the disentangled reresentation learning literature, we roose an algorithm for learning comact reresentations of datasets that are useful for reconstruction and rediction, but are also \emh{flexibly fair}, meaning they can be easily modified at test time to achieve subgrou demograhic arity with resect to multile sensitive attributes and their conjunctions. We show emirically that the resulting encoder---which does not require the sensitive attributes for inference---allows for the adatation of a single reresentation to a variety of fair classification tasks with new target labels and subgrou definitions.
"
642,2019,Ladder Capsule Network,Oral,"We roose a new architecture of the casule network called the ladder casule network, which has an alternative building block to the dynamic routing algorithm in the casule network (Sabour et al., 2017). Motivated by the need for using only imortant casules during training for robust erformance, we first introduce a new layer called the runing layer, which removes irrelevant casules. Based on the selected casules, we construct higher-level casule oututs. Subsequently, to cature the art-whole satial relationshis, we introduce another new layer called the ladder layer, the oututs of which are regressed lower-level casule oututs from higher-level casules. Unlike the casule network adoting the routing-by agreement, the ladder casule network uses backroagation from a loss function to reconstruct the lower-level casule oututs from higher-level casules; thus, the ladder layer imlements the reverse directional inference of the agreementdisagreement mechanism of the casule network. The exeriments on MNIST demonstrate that the ladder casule network learns an equivariant reresentation and imroves the caability to extraolate or generalize to ose variations.
"
643,2019,Mallows ranking models: maximum likelihood estimate and regeneration,Oral,"This aer is concerned with various Mallows ranking models. We study the statistical roerties of the MLE of Mallows' $\hi$ model. We also make connections of various Mallows ranking models, encomassing recent rogress in mathematics. Motivated by the infinite to-$t$ ranking model, we roose an algorithm to select the model size $t$ automatically. The key idea relies on the renewal roerty of such an infinite random ermutation. Our algorithm shows good erformance on several data sets."
644,2019,Meta-Learning Neural Bloom Filters,Oral,"There has been a recent trend in training neural networks to relace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater comression. In this setting, a neural data structure is instantiated by training a network over many eochs of its inuts until convergence. In alications where inuts arrive at high throughut, or are ehemeral, training a network from scratch is not ractical. This motivates the need for few-shot neural data structures.
In this aer we exlore the learning of aroximate set membershi over a set of data in one-shot via meta-learning. We roose a novel memory architecture, the Neural Bloom Filter, which is able to achieve significant comression gains over classical Bloom Filters and existing memory-augmented neural networks.
"
645,2019,Adaptive Regret of Convex and Smooth Functions,Oral,"We investigate online convex otimization in changing environments, and choose the adative regret as the erformance measure. The goal is to achieve a small regret over every interval so that the comarator is allowed to change over time. Different from revious works that only utilize the convexity condition, this aer further exloits smoothness to imrove the adative regret. To this end, we develo novel adative algorithms for convex and smooth functions, and establish roblem-deendent regret bounds over any interval. Our regret bounds are comarable to existing results in the worst case, and become much tighter when the comarator has a small loss.
"
646,2019,Recursive Sketches for Modular Deep Learning,Oral,"We resent a mechanism to comute a sketch (succinct summary) of how a comlex modular dee network rocesses its inuts. The sketch summarizes essential information about the inuts and oututs of the network and can be used to quickly identify key comonents and summary statistics of the inuts. Furthermore, the sketch is recursive and can be unrolled to identify sub-comonents of these comonents and so forth, caturing a otentially comlicated DAG structure. These sketches erase gracefully; even if we erase a fraction of the sketch at random, the remainder still retains the codehigh-weight'' information resent in the original sketch. The sketches can also be organized in a reository to imlicitly form acodeknowledge grah''; it is ossible to quickly retrieve sketches in the reository that are related to a sketch of interest. Arranged in this fashion, the sketches can also be used to learn emerging concets by looking for new clusters in sketch sace. Finally, in the scenario where we want to learn a ground truth dee network, we show that augmenting inutoutut airs with these sketches can theoretically make it easier to do so.
"
647,2019,Gromov-Wasserstein Learning for Graph Matching and Node Embedding,Oral,"A novel Gromov-Wasserstein learning framework is roosed to jointly match (align) grahs and learn embedding vectors for the associated grah nodes. 
Using Gromov-Wasserstein discreancy, we measure the dissimilarity between two grahs and find their corresondence, according to the learned otimal transort. 
The node embeddings associated with the two grahs are learned under the guidance of the otimal transort, the distance of which not only reflects the toological structure of each grah but also yields the corresondence across the grahs. 
These two learning stes are mutually-beneficial, and are unified here by minimizing the Gromov-Wasserstein discreancy with structural regularizers. This framework leads to an otimization roblem that is solved by a roximal oint method.
We aly the roosed method to matching roblems in real-world networks, and demonstrate its suerior erformance comared to alternative aroaches.
"
648,2019,Efficient Full-Matrix Adaptive Regularization,Oral,"Adative regularization methods re-multily a descent direction by a reconditioning matrix. Due to the large number of arameters of machine learning roblems, full-matrix reconditioning methods are rohibitively exensive. We show how to modify full-matrix adative regularization in order to make it ractical and effective. We also rovide novel theoretical analysis for adative regularization in non-convex otimization settings. The core of our algorithm, termed GGT, consists of efficient inverse comutation of square roots of low-rank matrices. Our reliminary exeriments underscore imroved convergence rate of GGT across a variety of synthetic tasks and standard dee learning benchmarks.
"
649,2019,Efficient On-Device Models using Neural Projections,Oral,"Many alications involving visual and language understanding can be effectively solved using dee neural networks. Even though these techniques achieve state-of-the-art results, it is very challenging to aly them on devices with limited memory and comutational caacity such as mobile hones, smart watches and IoT. We roose a neural rojection aroach for training comact on-device neural networks. We introduce ""rojection"" network that uses locality-sensitive rojections to generate comact binary reresentations and learn small neural networks with comutationally efficient oerations. We design a joint otimization framework where the rojection network can be trained from scratch or leverage existing larger neural networks such as feed-forward NNs, CNNs or RNNs. The trained neural rojection network can be directly used for inference on device at low memory and comutation cost. We demonstrate the effectiveness of this as a general-urose aroach for significantly shrinking the memory requirements of different tyes of neural networks while reserving good accuracy on various visual recognition and text classification tasks. We also discuss novel extensions of the aroach and derive rojection models for other learning scenarios and real-world on-device alications.
"
650,2019,CoT: Cooperative Training for Generative Modeling of Discrete Data,Oral,"To tackle the distribution shifting roblem inherent in Maximum Likelihood Estimation, a.k.a. exosure bias, researchers mainly focused on introducing auxiliary adversarial training to enalize the unrealistic generated samles. To exloit the suervision signal from the discriminator, most revious models, tyically language GANs, leverage REINFORCE to address the non-differentiable roblem of discrete sequential data. In this aer, we roose a novel aroach called Cooerative Training to imrove the training of sequence generative models. Our algorithm transforms the minimax game of GANs into the form of a joint maximization roblem and manages to exlicitly estimate and otimize Jensen-Shannon divergence. In the exeriments, comared to existing state-of-the-art methods, our model shows suerior erformance in both samle quality and diversity, as well as training stability. Unlike revious methods, our aroach works without the necessity of re-training via Maximum Likelihood Estimation, which is crucial to the success of revious methods.
"
651,2019,Zero-Shot Knowledge Distillation in Deep Networks,Oral,"Knowledge distillation deals with the roblem of training a smaller model from a high caacity model so as to retain most of its erformance. The source and target model are generally referred to as Teacher and Student model resectively. Existing aroaches use either the training data or meta-data extracted from it in order to train the Student. However, accessing the dataset on which the Teacher has been trained may not always be feasible if the dataset is very large or it oses rivacy or safety concerns (e.g., biometric or medical data). Therefore, in this aer, we roose a novel data-free method to train the Student from the Teacher. Without even utilizing any meta-data, we extract the Data Imressions from the arameters of the Teacher model and utilize these as surrogate for the original training data samles to transfer its learning to Student via knowledge distillation. Hence we dub our method ""Zero-shot Knowledge Distillation"". We demonstrate that our framework results in cometitive generalization erformance as achieved by the actual training data samles on multile benchmark datasets.
"
652,2019,Online Adaptive Principal Component Analysis and Its extensions,Oral,"We roose algorithms for online rincial comonent analysis (PCA)
and variance minimization for adative settings.
Previous literature has focused on uer bounding the static adversarial regret,
whose comarator is the otimal fixed action in hindsight.
However, static regret is not an aroriate metric when the underlying environment is changing.
Instead, we adot the adative regret metric from the revious literature 
and roose online adative algorithms for PCA and variance minimization, 
that have sub-linear adative regret guarantees.
We demonstrate both theoretically and exerimentally that
the roosed algorithms can adat to the changing environments.
"
653,2019, Fair Regression: Quantitative Definitions and Reduction-Based Algorithms,Oral,"In this aer, we study the rediction of a real-valued target, such as a risk score or recidivism rate, while guaranteeing a quantitative notion of fairness with resect to a rotected attribute such as gender or race. We call this class of roblems fair regression. We roose general schemes for fair regression under two notions of fairness: (1) statistical arity, which asks that the rediction be statistically indeendent of the rotected attribute, and (2) bounded grou loss, which asks that the rediction error restricted to any rotected grou remain below some re-determined level. While we only study these two notions of fairness, our schemes are alicable to arbitrary Lischitz-continuous losses, and so they encomass least-squares regression, logistic regression, quantile regression, and many other tasks. Our schemes only require access to standard risk minimization algorithms (such as standard classification or least-squares regression) while roviding theoretical guarantees on the otimality and fairness of the obtained solutions. In addition to analyzing theoretical roerties of our schemes, we emirically demonstrate their ability to uncover fairness--accuracy frontiers on several standard datasets.
"
654,2019,Unreproducible Research is Reproducible,Oral,"The aarent contradiction of the title is a wordlay on the different meanings attributed to the word \emh{reroducible} across different scientific fields. What we imly is that unreroducible \emh{findings} can be built uon reroducible \emh{methods}. Without denying the imortance of facilitating the reroduction of \emh{methods}, we deem imortant to reassert that reroduction of \emh{findings} is a fundamental ste of the scientific inquiry. We argue that the commendable quest towards easy deterministic reroducibility of methods and numerical results, should not have us forget the even more imortant necessity of ensuring the reroducibility of emirical conclusions and findings by roerly accounting for essential sources of variations. We rovide exeriments to exemlify the limitations of current evaluation methods of models in the field of dee learning, showing that even if the results could be reroduced, a slightly different exeriment would not suort the findings. This work is an attemt to romote the use of more rigorous and diversified methodologies. It is not an attemt to imose a new methodology and it is not a critique on the nature of exloratory research. We hoe to hel clarify the distinction between exloratory and emirical research in the field of dee learning and believe more energy should be devoted to roer emirical research in our community.
"
655,2019,Deep Residual Output Layers for Neural Language Generation,Oral,"Many tasks, including language generation, benefit from learning the structure of the outut sace, articularly when the sace of outut labels is large and the data is sarse. State-of-the-art neural language models indirectly cature the outut sace structure in their classifier weights since they lack arameter sharing across outut labels. Learning shared outut label maings hels, but existing methods have limited exressivity and are rone to overfitting. In this aer, we investigate the usefulness of more owerful shared maings for outut labels, and roose a dee residual outut maing with droout between layers to better cature the structure of the outut sace and avoid overfitting. Evaluations on three language generation tasks show that our outut label maing can match or imrove state-of-the-art recurrent and self-attention architectures, and suggest that the classifier does not necessarily need to be high-rank to better model natural language if it is better at caturing the structure of the outut sace.
"
656,2019,Breaking the gridlock in Mixture-of-Experts: Consistent and Efficient Algorithms,Oral,"Mixture-of-Exerts (MoE) is a widely oular model for ensemble learning and is a basic building block of highly successful modern neural networks as well as a comonent in Gated Recurrent Units (GRU) and Attention networks. However, resent algorithms for learning MoE, including the EM algorithm and gradient descent, are known to get stuck in local otima. From a theoretical viewoint, finding an efficient and rovably consistent algorithm to learn the arameters remains a long standing oen roblem for more than two decades. In this aer, we introduce the first algorithm that learns the true arameters of a MoE model for a wide class of non-linearities with global consistency guarantees. While existing algorithms jointly or iteratively estimate the exert arameters and the gating arameters in the MoE, we roose a novel algorithm that breaks the deadlock and can directly estimate the exert arameters by sensing its echo in a  carefully designed cross-moment tensor between the inuts and the outut. Once the exerts are known, the recovery of gating arameters still requires an EM algorithm; however, we show that the EM algorithm for this simlified roblem, unlike the joint EM algorithm, converges to the true arameters. We emirically validate our algorithm on both the synthetic and real data sets in a variety of settings, and show suerior erformance to standard baselines.
"
657,2019,Spectral Clustering of Signed Graphs via Matrix Power Means,Oral,"Signed grahs can be used to encode ositive (attractive) and negative (reulsive) relations between nodes. We roose to merge the information from ositive and negative edges via the one-arameter family of signed ower mean Lalacians defined as the matrix ower mean of standard and signless Lalacians.  We analyze the signed ower mean Lalacian in the stochastic block model in exectation and show that it outerforms the state of the art in terms of conditions under which it recovers the ground truth clusters. Moreover, under the stochastic block model we show concentration around its exectation of eigenvalues and eigenvectors of the signed ower mean Lalacian. Finally, we rovide an extensive comarison to existing methods on real world datasets.
"
658,2019,Fast and Stable Maximum Likelihood Estimation for Incomplete Multinomial Models,Oral,"We roose a fixed-oint iteration aroach to the maximum likelihood estimation of the incomlete multinomial model, which rovides a unified framework for analyzing ranking data. Incomlete observations cannot be distinguished as belonging to a unique category, but instead they fall in a subset of categories. We develo an minorize-maximization (MM) tye of algorithm, which requires relatively fewer iterations and better time efficiency to achieve convergence. Under such a general framework, incomlete multinomial models can be reformulated to include several well-known ranking models as secial cases, such as the Bradley--Terry, Plackett--Luce models and their variants. Exerimental results show that our algorithm runs faster than existing methods on synthetic data and real data. The simle form of iteratively udating equations in our algorithm involves only basic matrix oerations, which makes it easy to imlement with large data.
"
659,2019,Efficient Nonconvex Regularized Tensor Completion with Structure-aware Proximal Iterations,Oral,"Nonconvex regularizers have been successfully used in low-rank matrix learning. In this aer, we extend this to the more challenging roblem of low-rank tensor comletion. Based on the roximal average algorithm,
we develo an efficient solver that avoids exensive tensor folding and unfolding. A secial ``sarse lus low-rank"" structure, which is essential for fast comutation of individual roximal stes,  is maintained
throughout the iterations. We also incororate adative momentum to further seed u emirical convergence. Convergence results to critical oints are rovided under smoothness and Kurdyka-Lojasiewicz conditions. Exerimental results on a number of synthetic and real-world data sets show that the roosed algorithm is more efficient in both time and sace, and is also more accurate than existing aroaches.
"
660,2019,Fairness without Harm: Decoupled Classifiers with Preference Guarantees,Oral,"Should we train different classifiers for grous defined by sensitive attributes, such as gender and ethnicity? In a domain such as medicine, it may be ethical to allow classifiers to vary by grou membershi -- so long as treatment disarity is aligned with the rinciles of beneficence (codedo the best"") and non-maleficence (codedo no harm""). We argue that classifiers should satisfy {\em reference guarantees} for individuals who are subjected to disarate treatment:  (i) the majority of individuals in each grou should refer their classifier in comarison to (i) a ooled classifier that makes no use of sensitive attributes ({\em rationality}, resonsive to non-maleficence) and (ii) the classifier assigned to any other grou ({\em envy-freeness}, resonsive to beneficence). Standard decouled training, which fits a searate classifier for each grou, may fail (i) or (ii) due to data disarities or heterogeneity in the data generating distributions between grous. We introduce a {\em recursive decouling rocedure} that adatively chooses grou attributes for decouling, and resent formal conditions for achieving these reference guarantees.  We illustrate the benefits of our aroach through exeriments on real-world datasets, showing that it can safely imrove the grous defined by multile sensitive attributes without violating reference guarantees on test data.
"
661,2019,Fast Algorithm for Generalized Multinomial Models with Ranking Data,Oral,"We develo a framework of the generalized multinomial model, which includes both the oular Plackett--Luce model and Bradley--Terry model as secial cases. We theoretically rove that the maximum likelihood estimator (MLE) under the generalized multinomial model corresonds to the stationary distribution of an inhomogeneous Markov chain uniquely. Based on this Markov chain, we roose an iterative algorithm that is easy to imlement and interret and certain to converge. Numerical exeriments on synthetic data and real data demonstrate the advantages of our Markov chain based algorithm over existing ones that it converges to the MLE with fewer iterations and faster convergence rate. The new algorithm is readily alicable for roblems such as age ranking, sorts ranking data.
"
662,2019,Non-Monotonic Sequential Text Generation,Oral,"Standard sequential generation methods assume a re-secified generation order, such as text generation methods which generate words from left to right. In this work, we roose a framework for training models of text generation that oerate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework oerates by generating a word at an arbitrary osition, and then recursively generating words to its left and then words to its right, yielding a binary tree. Learning is framed as imitation learning, including a coaching method which moves from imitating an oracle to reinforcing the olicy's own references. Exerimental results demonstrate that using the roosed method, it is ossible to learn olicies which generate text without re-secifying a generation order, while achieving cometitive erformance with conventional left-to-right generation.
"
663,2019,Coresets for Ordered Weighted Clustering,Oral,"We design coresets for ordered k-median, a generalization of classical clustering roblems such as k-median and k-center, that offers a more flexible data analysis, like easily combining multile objectives (e.g., to increase fairness or for Pareto otimization). Its objective function is defined via the Ordered Weighted Averaging (OWA) aradigm of Yager (1988), where data oints are weighted according to a redefined weight vector, but in order of their contribution to the objective (distance from the centers).

A owerful data-reduction technique, called a coreset, is to summarize a oint set X in R^d into a small (weighted) oint set X', such that for every set of k otential centers, the objective value of the coreset X' aroximates that of X within factor $1\m \esilon$. When there are multile objectives (weights), the above standard coreset might have limited usefulness, and we therefore introduce the notion of a \emh{simultaneous} coreset, where the above aroximation holds for all weights (in addition to all centers). Our main result is a construction of a simultaneous coreset of size $O_{\esilon, d}(k^2 \log^2 |X|)$ for ordered k-median. We believe that simultaneous coresets can be alicable more broadly in data analysis. 

To validate the efficacy of our coreset construction we ran exeriments on a real geograhical data set. We find that our algorithm roduces a small coreset, which translates to a massive seedu of clustering comutations, while maintaining high accuracy for a range of weights. "
664,2019,POLITEX: Regret Bounds for Policy Iteration using Expert Prediction,Oral,"We resent POLITEX (POLicy ITeration using EXerts), a model-free reinforcement learning (RL) algorithm that uses linear function aroximation for continuing RL roblems. POLITEX can be thought of as a “soft” variant of olicy iteration, where the olicy in each iteration corresonds to a Boltzmann distribution over the
sum of revious action-value functions. We show that in uniformly mixing Markov Decision Processes (MDPs), for a time-horizon of T and a worst-case value function aroximation error ε where linear function aroximation is used with d-dimensional features, the regret of POLITEX scales as O˜(d^(12)T^(34) + εT). 
Under a uniform mixing assumtion, we rovide the first regret result for a ractical model-free method that uses function aroximation and where the regret does
not scale with the size of the underlying MDP.  We also rovide a new finite samle analysis of the LSPE algorithm, used by POLITEX to estimate the value functions, which may be of indeendent interest. Exerimental results
on a queuing roblem confirm that POLITEX is cometitive with some of its alternatives, while reliminary results on Ms Pacman (one of the standard Atari benchmark roblems) confirm the viability of POLITEX beyond linear function aroximation.
"
665,2019,A Convergence Theory for Deep Learning via Over-Parameterization,Oral,"Dee neural networks (DNNs) have demonstrated dominating erformance in many fields; since AlexNet, the neural networks used in ractice are going wider and deeer. On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer networks remains somewhat unsettled.

In this work, we rove why simle algorithms such as stochastic gradient descent (SGD) can find GLOBAL MINIMA on the training objective of DNNs in POLYNOMIAL TIME. We only make two assumtions: the inuts do not degenerate and the network is over-arameterized. The latter means the number of hidden neurons is sufficiently large: olynomial in $L$, the number of DNN layers and in $n$, the number of training samles.

  As concrete examles, on the training set and starting from randomly initialized weights, we show that SGD attains 100\% accuracy in classification tasks, or minimizes regression loss in linear convergence seed $\varesilon \roto e^{-\Omega(T)}$, with a number of iterations that only scales olynomial in $n$ and $L$. Our theory alies to the widely-used but non-smooth ReLU activation, and to any smooth and ossibly non-convex loss functions. In terms of network architectures, our theory at least alies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).
"
666,2019,Improving Neural Language Modeling via Adversarial Training,Oral,"Recently, substantial rogress has been made in language modeling by using dee neural networks. However, in ractice, large scale neural language models have been shown to be rone to overfitting. In this aer, we resent a simle yet highly effective adversarial training mechanism for regularizing neural language models. The idea is to introduce adversarial noise to the outut embedding layer while training the models. We show that the otimal adversarial noise yields a simle closed form solution, thus allowing us to develo a simle and time efficient algorithm. Theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, heling to increase the robustness of models. Emirically, we show that our method imroves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test erlexity scores of 46.42 and 38.65, resectively. 
When alied to machine translation,  our method imroves over various transformer-based translation baselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English tasks.
"
667,2019,Geometric Scattering for Graph Data Analysis,Oral,"We exlore the generalization of scattering transforms from traditional (e.g., image or audio) signals to grah data, analogous to the generalization of ConvNets in geometric dee learning, and the utility of extracted grah features in grah data analysis. In articular, we focus on the caacity of these features to retain informative variability and relations in the data (e.g., between individual grahs, or in aggregate), while relating our construction to revious theoretical results that establish the stability of similar transforms to families of grah deformations. We demonstrate the alication the our geometric scattering features in grah classification of social network data, and in data exloration of biochemistry data.
"
668,2019,Fair k-Center Clustering for Data Summarization,Oral,"In data summarization we want to choose k rototyes in order to summarize a data set. We study a setting where the data set comrises several demograhic grous and we are restricted to choose k_i rototyes belonging to grou i. A common aroach to the roblem without the fairness constraint is to otimize a centroid-based clustering objective such as k-center. A natural extension then is to incororate the fairness constraint into the clustering objective. Existing algorithms for doing so run in time suer-quadratic in the size of the data set. This is in contrast to the standard k-center objective that can be aroximately otimized in linear time. In this aer, we resolve this ga by roviding a simle aroximation algorithm for the k-center roblem under the fairness constraint with running time linear in the size of the data set and k. If the number of demograhic grous is small, the aroximation guarantee of our algorithm only incurs a constant-factor overhead. We demonstrate the alicability of our algorithm on both synthetic and real data sets.
"
669,2019,Graph Resistance and Learning from Pairwise Comparisons,Oral,"We consider the roblem of learning the qualities of a collection of items by erforming noisy comarisons among them. Following the standard aradigm, we assume there is a fixed ``comarison grah'' and every neighboring air of items in this grah is comared k times according to the Bradley-Terry-Luce model (where the robability than an item wins a comarison is roortional the item quality). We are interested in how the relative error in quality estimation scales with the comarison grah in the regime where k is large. We show that, asymtotically, the relevant grah-theoretic quantity is the square root of the resistance of the comarison grah. Secifically, we rovide an algorithm with relative error decay that scales with the square root of the grah resistance, and rovide a lower bound showing that (u to log factors) a better scaling is imossible. The erformance guarantee of our algorithm, both in terms of the grah and the skewness of the item quality distribution, significantly outerforms earlier results. 
"
670,2019,"Anytime Online-to-Batch, Optimism and Acceleration",Oral,"A standard way to obtain convergence guarantees in stochastic convex otimization is to run an online learning algorithm and then outut the average of its iterates: the actual iterates of the online learning algorithm do not come with individual guarantees. We close this ga by introducing a black-box modification to any online learning algorithm whose iterates converge to the otimum in stochastic scenarios. We then consider the case of smooth losses, and show that combining our aroach with otimistic online learning algorithms immediately yields a fast convergence rate of $O(LT^{32}+\sigma\sqrt{T})$ on $L$-smooth roblems with $\sigma^2$ variance in the gradients. Finally, we rovide a reduction that converts any adative online algorithm into one that obtains the otimal accelerated rate of $\tilde O(LT^2 + \sigma\sqrt{T})$, while still maintaining $\tilde O(1\sqrt{T})$ convergence in the non-smooth setting. Imortantly, these algorithms adat to $L$ and $\sigma$ automatically: they do not need to know either to obtain these rates."
671,2019,Robust Estimation of Tree Structured Gaussian Graphical Models,Oral,"Consider jointly Gaussian random variables whose conditional indeendence structure is secified by a grahical model. If we observe realizations of the variables, we can comute the covariance matrix, and it is well known that the suort of the inverse covariance matrix corresonds to the edges of the grahical model. Instead, suose we only have noisy observations. If the noise at each node is indeendent, we can comute the sum of the covariance matrix and an unknown diagonal. The inverse of this sum is (in general) dense. We ask: can the original indeendence structure be recovered? We address this question for tree structured grahical models. We rove that this roblem is unidentifiable, but show that this unidentifiability is limited to a small class of candidate trees. We further resent additional constraints under which the roblem is identifiable. Finally, we rovide an O(n^3) algorithm to find this equivalence class of trees.
"
672,2019,Robust Inference via Generative Classifiers for Handling Noisy Labels,Oral,"Large-scale datasets may contain significant roortions of noisy (incorrect) class labels, and it is well-known that modern dee neural networks (DNNs) oorly generalize from such noisy training datasets. To mitigate the issue, we roose a novel inference method, termed Robust Generative classifier (RoG), alicable to any discriminative (e.g., softmax) neural classifier re-trained on noisy datasets. In articular, we induce a generative classifier on to of hidden feature saces of the re-trained DNNs, for obtaining a more robust decision boundary. By estimating the arameters of generative classifier using the minimum covariance determinant estimator, we significantly imrove the classification accuracy with neither re-training of the dee model nor changing its architectures. With the assumtion of Gaussian distribution for features, we rove that RoG generalizes better than baselines under noisy labels. Finally, we roose the ensemble version of RoG to imrove its erformance by investigating the layer-wise characteristics of DNNs. Our extensive exerimental results demonstrate the sueriority of RoG given different learning models otimized by several training techniques to handle diverse scenarios of noisy labels. 
"
673,2019,Insertion Transformer: Flexible Sequence Generation via Insertion Operations,Oral,"We resent the Insertion Transformer, an iterative, artially autoregressive model for sequence generation based on insertion oerations. Unlike tyical autoregressive models which rely on a fixed left-to-right ordering of the outut, our aroach accommodates arbitrary orderings by allowing for tokens to be inserted anywhere in the sequence during decoding. This flexibility confers a number of advantages: for instance, not only can our model be trained to follow secific orderings such as left-to-right generation or a binary tree traversal, but it can also be trained to maximize entroy over all valid insertions for robustness. In addition, our model seamlessly accommodates both fully autoregressive generation (one insertion at a time) and artially autoregressive generation (simultaneous insertions at multile locations). We validate our aroach by analyzing its erformance on the WMT 2014 English-German machine translation task under various settings for training and decoding. We find that the Insertion Transformer outerforms many rior non-autoregressive aroaches to translation at comarable or better levels of arallelism, and successfully recovers the erformance of the original Transformer while requiring significantly fewer iterations during decoding.
"
674,2019,Mixture Models for Diverse Machine Translation: Tricks of the Trade,Oral,"Mixture models trained via EM are among the simlest, most widely used and well understood latent variable models in the machine learning literature. Surrisingly, these models have been hardly exlored in text generation alications such as machine translation. In rincile, they rovide a latent variable to control generation and roduce a diverse set of hyotheses. In ractice, however, mixture models are rone to degeneracies---often only one comonent gets trained or the latent variable is simly ignored. We find that disabling droout noise in resonsibility comutation is critical to successful training. In addition, the design choices of arameterization, rior distribution, hard versus soft EM and online versus offline assignment can dramatically affect model erformance. We develo an evaluation rotocol to assess both quality and diversity of generations against multile references, and rovide an extensive emirical study of several mixture model variants. Our analysis shows that certain tyes of mixture models are more robust and offer the best trade-off between translation quality and diversity comared to variational models and diverse decoding aroaches.\footnote{Our code will be made ublicly available after the review rocess.}
"
675,2019,A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks,Oral,"The gradient noise (GN) in the stochastic gradient descent (SGD) algorithm is often considered to be Gaussian in the large data regime by assuming that the classical central limit theorem (CLT) kicks in. This assumtion is often made for mathematical convenience, since it enables SGD to be analyzed as a stochastic differential equation (SDE) driven by a Brownian motion. We argue that the Gaussianity assumtion might fail to hold in dee learning settings and hence render the Brownian motion-based analyses inaroriate. Insired by non-Gaussian natural henomena, we consider the GN in a more general context and invoke the generalized CLT (GCLT), which suggests that the GN converges to a heavy-tailed $\alha$-stable random variable. Accordingly, we roose to analyze SGD as an SDE driven by a L\'{e}vy motion. Such SDEs can incur `jums', which force the SDE transition from narrow minima to wider minima, as roven by existing metastability theory. To validate the $\alha$-stable assumtion, we conduct exeriments on common dee learning scenarios and show that in all settings, the GN is highly non-Gaussian and admits heavy-tails. We investigate the tail behavior in varying network architectures and sizes, loss functions, and datasets. Our results oen u a different ersective and shed more light on the belief that SGD refers wide minima."
676,2019,Differentially Private Fair Learning,Oral,"Motivated by settings in which redictive models may be required to be non-discriminatory with resect to certain attributes (such as race), but even collecting the sensitive attribute may be forbidden or restricted, we initiate the study of fair learning under the constraint of differential rivacy. Our first algorithm is a rivate imlementation of the equalized odds ost-rocessing aroach of (Hardt et al., 2016). This algorithm is aealingly simle, but must be able to use rotected grou membershi exlicitly at test time, which can be viewed as a form of “disarate treatment”. Our second algorithm is a differentially rivate version of the oracle-efficient in-rocessing aroach of (Agarwal et al., 2018). This algorithm is more comlex but need not have access to rotected grou membershi at test time. We identify new tradeoffs between fairness, accuracy, and rivacy that emerge only when requiring all three roerties, and show that these tradeoffs can
be milder if grou membershi may be used at test time.
"
677,2019,Learning Context-dependent Label Permutations for Multi-label Classification,Oral,"A key roblem in multi-label classification is to utilize deendencies among the labels. Chaining classifiers are a simle technique for addressing this roblem but current algorithms all assume a fixed, static label ordering. In this work, we roose a multi-label classification aroach which allows to choose a dynamic, context-deendent label ordering. Our roosed aroach consists of two sub-comonents: a simle EM-like algorithm which bootstras the learned model, and a more elaborate aroach based on reinforcement learning. Our exeriments on three ublic multi-label classification benchmarks show that our roosed dynamic label ordering aroach based on reinforcement learning outerforms recurrent neural networks with fixed label ordering across both biartition and ranking measures on all the three datasets. As a result, we obtain a owerful sequence rediction-based algorithm for multi-label classification, which is able to efficiently and exlicitly exloit label deendencies.
"
678,2019,LIT: Learned Intermediate Representation Training for Model Compression,Oral,"Researchers have roosed a range of model comression techniques to reduce the comutational
and memory footrint of dee neural networks (DNNs). In this work, we introduce Learned
Intermediate reresentation Training (LIT), a novel model comression technique that outer-
forms a range of recent model comression techniques by leveraging the highly reetitive struc-
ture of modern DNNs (e.g., ResNet). LIT uses a teacher DNN to train a student DNN of reduced
deth by leveraging two key ideas: 1) LIT directly comares intermediate reresentations of
the teacher and student model and 2) LIT uses the intermediate reresentation from the teacher
model’s revious block as inut to the current student block during training, imroving stability
of intermediate reresentations in the student network. We show that LIT can substantially reduce
network size without loss in accuracy on a range of DNN architectures and datasets. For examle,
LIT can comress ResNet on CIFAR10 by 3.7× comared to 1.5× and 2.6× by network slimming
and FitNets resectively (by weight). Furthermore, LIT can comress, by deth, ResNeXt 5.5×
on CIFAR10 (image classification), VDCNN by 1.7× on Amazon Reviews (sentiment analysis),
and StarGAN by 1.8× on CelebA (style transfer, i.e., GANs).
"
679,2019,MASS: Masked Sequence to Sequence Pre-training for Language Generation,Oral,"Pre-training and fine-tuning, e.g., BERT~\cite{devlin2018bert}, have achieved great success in language understanding by transferring knowledge from rich-resource re-training task to the lowzero-resource downstream tasks. Insired by the success of BERT, we roose MAsked Sequence to Sequence re-training (MASS) for the encoder-decoder based language generation tasks. MASS adots the encoder-decoder framework to reconstruct a sentence fragment given the remaining art of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as inut, and its decoder tries to redict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develo the caability of reresentation extraction and language modeling. By further fine-tuning on a variety of zerolow-resource language generation tasks, including neural machine translation, text summarization and conversational resonse generation (3 tasks and totally 8 datasets), MASS achieves significant imrovements over the baselines without re-training or with other re-training methods. Esecially, we achieve the state-of-the-art accuracy (30.02 in terms of BLEU score) on the unsuervised English-French translation, even beating the early attention-based suervised model~\cite{bahdanau2015neural}.
"
680,2019,Cautious Regret Minimization: Online Optimization with Long-Term Budget Constraints,Oral,"We study a class of online convex otimization roblems with long-term budget constraints that arise naturally as reliability guarantees or total consumtion constraints. In this general setting, rior work by Mannor et al. (2009) has shown that achieving no regret is imossible if the functions defining the agent's budget are chosen by an adversary. To overcome this obstacle, we refine the agent's regret metric by introducing the notion of a ""K-benchmark"", i.e., a comarator which meets the roblem's allotted budget over any window of length K. The imossibility analysis of Mannor et al. (2009) is recovered when K=T; however, for K=o(T), we show that it is ossible to minimize regret while still meeting the roblem's long-term budget constraints. We achieve this via an online learning olicy based on Cautious Online Lagrangiant Descent (COLD) for which we derive exlicit bounds, in terms of both the incurred regret and the residual budget violations.
"
681,2019,Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models,Oral,"Beam search is the most oular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for non-greedy local decisions that can otentially lead to a sequence with a higher overall robability. However, revious work on a number of alications found that the quality of the highest robability hyothesis found by beam search degrades with large beam widths. We erform an emirical study of the behavior of beam search across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disroortionately based on early very low robability tokens that are followed by a sequence of tokens with higher (conditional) robability. We show that, emirically, such sequences are more likely to have a lower evaluation score than lower robability sequences without this attern. Using the notion of search discreancies from heuristic search, we hyothesize that large discreancies are the cause of the erformance degradation. We show that this hyothesis generalizes the revious ones in machine translation and image cationing. To validate our hyothesis, we show that constraining beam search to avoid large discreancies eliminates the erformance degradation.
"
682,2019,A Better k-means++ Algorithm via Local Search,Oral,"In this aer, we develo a new variant of k-means++ seeding that in exectation achieves a constant aroximation guarantee. We obtain this result by a simle combination of k-means++ samling with a local search strategy.

We evaluate our algorithm emirically and show that it also imroves the quality of a solution in ractice.
"
683,2019,Obtaining Fairness using Optimal Transport Theory,Oral,"In the fair classification setu, we recast the links between fairness and redictability in terms of robability metrics. We analyze reair methods based on maing conditional distributions to the Wasserstein barycenter. We roose a Random Reair which yields a tradeoff between minimal information loss and a certain amount of fairness.
"
684,2019,Approximation and non-parametric estimation of ResNet-type convolutional neural networks,Oral,"Convolutional neural networks (CNNs) have been shown to achieve otimal aroximation and estimation error rates (in minimax sense) in several function classes. However, revious known otimal CNNs are unrealistically wide and difficult to obtain via otimization due to sarse constraints in imortant function classes, including the H\""older class. We show a ResNet-tye CNN can attain the minimax otimal error rates in these classes in more lausible situations -- it can be dense, and its width, channel size, and filter size are constant with resect to samle size. The key idea is that we can relicate the learning ability of Fully-connected neural networks (FNNs) by tailored CNNs, as long as the FNNs have \textit{block-sarse} structures. Our theory is general in that we can automatically translate any aroximation rate achieved by block-sarse FNNs into that by CNNs. As an alication, we derive aroximation and estimation error rates of the aformentioned tye of CNNs for the Barron and H\""older classes with the same strategy.
"
685,2019,Spectral Approximate Inference,Oral,"Grahical models (GMs) have been successfully alied to various alications of machine learning. Given a GM, comuting its artition function is the most essential inference task, but it is comutationally intractable in general. To address the issue, iterative aroximation algorithms exloring certain local structureconsistency of GM have been investigated as oular choices in ractice. However, due to their localiterative nature, they often outut oor aroximations or even do not converge, e.g., in low-temerature regimes (hard instances of large arameters). To overcome the limitation, we roose a novel aroach utilizing the global sectral feature of GM. Our contribution is two-fold: (a) we first roose a fully olynomial-time aroximation scheme (FPTAS) for aroximating the artition function of GM associating with a low-rank couling matrix; (b) for general high-rank GMs, we design a sectral mean-field scheme utilizing (a) as a subroutine, where it aroximates a high-rank GM into a roduct of rank-1 GMs for an efficient aroximation of the artition function. The roosed algorithm is more robust in its running time and accuracy than rior methods, i.e., neither suffers from the convergence issue nor deends on hard local structures. Our exeriments demonstrate that it indeed outerforms baselines, in articular, significantly in the low-temerature regimes.
"
686,2019,Optimal Kronecker-Sum Approximation of Real Time Recurrent Learning,Oral,"One of the central goals of Recurrent Neural Networks (RNNs) is to learn long-term deendencies in sequential data. 
Nevertheless, the most oular training method, Truncated Backroagation through Time (TBPTT), categorically forbids learning deendencies beyond the truncation horizon.
In contrast, the online training algorithm Real Time Recurrent Learning (RTRL) rovides untruncated gradients, with the disadvantage of imractically large comutational costs. 
Recently ublished aroaches reduce these costs by roviding noisy aroximations of RTRL. 
We resent a new aroximation algorithm of RTRL, Otimal Kronecker-Sum Aroximation (OK).
We rove that OK is otimal for a class of aroximations of RTRL, which includes all aroaches ublished so far. 
Additionally, we show that OK has emirically negligible noise: Unlike revious algorithms it matches TBPTT in a real world task (character-level Penn TreeBank) and can exloit online arameter udates to outerform TBPTT in a synthetic string memorization task.
"
687,2019,Humor in Word Embeddings: Cockamamie Gobbledegook for Nincompoops,Oral,"We study humor in Word Embeddings, a oular AI tool that associates each word with a Euclidean vector. We find that: (a) the word vectors cature multile asects of humor discussed in theories of humor; and (b) each individual's sense of humor can be reresented by a vector, and that these sense-of-humor vectors accurately redict differences in eole's sense of humor on new, unrated, words. The fact that single-word humor seems to be relatively easy for AI has imlications for the study of humor in language. Humor ratings are taken from the work of Englethaler and Hills (2017) as well as our own crowdsourcing study of 120,000 words. 
"
688,2019,Analyzing and Improving Representations with the Soft Nearest Neighbor Loss,Oral,"We introduce the Soft Nearest Neighbor Loss that measures the entanglement of class manifolds in reresentation sace: i.e., how close airs of oints from the same class are relatively to airs of oints from different classes. We demonstrate several use cases of the loss. As an analytical tool, it rovides insights into the evolution of similarity structures during learning. Surrisingly, we find that maximizing the entanglement of reresentations of different classes in the hidden layers is beneficial for discriminating, ossibly because it encourages reresentations to identify class-indeendent similarity structures.  Maximizing the soft nearest neighbor loss  in the hidden layers leads not only to imroved generalization but also to better-calibrated estimates of uncertainty on outlier data. Data that is not from the training distribution can be recognized by observing that in the hidden layers, it has fewer than the normal number of neighbors from the redicted class.
"
689,2019,Kernel Normalized Cut: a Theoretical Revisit,Oral,"In this aer, we study about theoretical roerties of clustering based on the kernel normalized cut. Our first contribution is to derive a nonasymtotic uer bound on the exected distortion rate of the kernel normalized cut. From this result, we show that the solution of the kernel normalized cut converges to that of the oulation-level weighted k-means clustering on a certain reroducing kernel Hilbert sace (RKHS). Our second contribution is to discover an interesting fact that the oulation-level weighted k-means clustering in the RKHS is equivalent to the oulation-level normalized cut. Combining these results, we can see that the kernel normalized cut converges to the oulation-level normalized cut. The criterion of the oulation-level normalized cut can be considered as an indivisibility of the oulation distribution, and this criterion lays an imortant role in the theoretical analysis of sectral clustering in Schiebinger et al. (2015). We believe that our results will rovide dee insights into behavior of both normalized cut and sectral clustering.
"
690,2019,Global Convergence of Block Coordinate Descent in Deep Learning,Oral,"Dee learning has aroused extensive attention due to its great emirical success. The efficiency of the block coordinate descent (BCD) methods has been recently demonstrated in dee neural network (DNN) training. However, theoretical studies on their convergence roerties are limited due to the highly nonconvex nature of DNN training. In this aer, we aim at roviding a general methodology for rovable convergence guarantees for this tye of methods. In articular, for most of the commonly used DNN training models involving both two- and three-slitting schemes, we establish the global convergence to a critical oint at a ${\cal O}(1k)$ rate, where $k$ is the number of iterations. The results extend to general loss functions which have Lischitz continuous gradients and dee residual networks (ResNets). Our key develoment adds several new elements to the Kurdyka-{\L}ojasiewicz inequality framework that enables us to carry out the global convergence analysis of BCD in the general scenario of dee learning."
691,2019,Repairing without Retraining: Avoiding Disparate Impact with Counterfactual Distributions,Oral,"When the average erformance of a rediction model varies significantly with resect to a sensitive attribute (e.g., race or gender), the erformance disarity can be exressed in terms of the robability distributions of inut and outut variables for each sensitive grou. In this aer, we exloit this fact to exlain and reair the erformance disarity of a fixed classification model over a oulation of interest. Given a black-box classifier that erforms unevenly across sensitive grous, we aim to eliminate the erformance ga by erturbing the distribution of inut features  for the disadvantaged grou. We refer to the erturbed distribution as a counterfactual distribution, and characterize its roerties for oular fairness criteria (e.g., redictive arity, equal FPR, equal oortunity). We then design a descent algorithm to efficiently learn a counterfactual distribution given the black-box classifier and samles drawn from the underlying oulation. We use the estimated counterfactual distribution to build a data rerocessor that reduces disarate imact without training a new model. We illustrate these use cases through exeriments on real-world datasets, showing that we can reair different kinds of disarate imact without a large dro in accuracy. 
"
692,2019,Partially Linear Additive Gaussian Graphical Models,Oral,"We roose a artially linear additive Gaussian grahical model (PLA-GGM) for the estimation of associations between random variables distorted by observed confounders. Model arameters are estimated  using an $L_1$-regularized maximal seudo-rofile likelihood estimator (MaPPLE) for which we rove a $\sqrt{n}$-sarsistency. Imortantly, our aroach avoids arametric constraints on the effects of confounders on the estimated grahical model structure. Emirically, the PLA-GGM is alied to both synthetic and real-world datasets, demonstrating suerior erformance comared to cometing methods."
693,2019,Trainable Decoding of Sets of Sequences for Neural Sequence Models,Oral,"Many structured rediction tasks admit multile correct oututs and so, it is often useful to decode a set of oututs that maximize some task-secific set-level metric. However, retooling standard sequence rediction rocedures tailored towards redicting the single best outut leads to the decoding of sets containing very similar sequences; failing to cature the variation in the outut sace. To address this, we roose $\nabla$BS, a trainable decoding rocedure that oututs a set of sequences, highly valued according to the metric. Our method tightly integrates the training and decoding hases and further allows for the otimization of the task-secific metric addressing the shortcomings of standard sequence rediction. Further, we discuss the trade-offs of commonly used set-level metrics and motivate a new set-level metric that naturally evaluates the notion of ``caturing the variation in the outut sace''. Finally, we show results on the image cationing task and find that our model outerforms standard techniques and natural ablations."
694,2019,Discovering Context Effects from Raw Choice Data,Oral,"Many alications in reference learning assume that decisions come from the maximization of a stable utility function. Yet a large exerimental literature shows that individual choices and judgements can be affected by ""irrelevant"" asects of the context in which they are made. An imortant class of such contexts is the comosition of the choice set. In this work, our goal is to discover such choice set effects from raw choice data. We introduce an extension of the Multinomial Logit (MNL) model, called the context deendent random utility model (CDM) which allows for a articular class of choice-set effects. We show that the CDM can be thought of as a second-order aroximation to a general choice system, can be inferred otimally using maximum likelihood and, imortantly, is easily interretable. We aly the CDM to both real and simulated choice data to erform rinciled exloratory analyses for the resence of choice set effects.
"
695,2019,Adaptive Sensor Placement for Continuous Spaces,Oral,We consider the roblem of adatively lacing sensors along an interval to detect stochastically-generated events. We resent a new formulation of the roblem as a continuum-armed bandit roblem with feedback in the form of artial observations of realisations of an inhomogeneous Poisson rocess. We design a solution method by combining Thomson samling with nonarametric inference via increasingly granular Bayesian histograms and derive an $\tilde{O}(T^{23})$ bound on the Bayesian regret in $T$ rounds. This is couled with the design of an efficent otimisation aroach to select actions in olynomial time. In simulations we demonstrate our aroach to have substantially lower and less variable regret than cometitor algorithms.
696,2019,What is the Effect of Importance Weighting in Deep Learning?,Oral,"Imortance-weighted risk minimization is a key ingredient in many machine learning algorithms for causal inference, domain adatation, class imbalance, and off-olicy reinforcement learning. While the effect of imortance weighting is well-characterized for low-caacity missecified models, little is known about how it imacts over-arameterized, dee neural networks. This work is insired by recent theoretical results showing that on (linearly) searable data, dee linear networks otimized by SGD learn weight-agnostic solutions, romting us to ask, for realistic dee networks, for which many ractical datasets are searable, what is the effect of imortance weighting? We resent the surrising finding that while imortance weighting imacts models early in training, its effect diminishes over successive eochs. Moreover, while L2 regularization and batch normalization (but not droout), restore some of the imact of imortance weighting, they exress the effect via (seemingly) the wrong abstraction: why should ractitioners tweak the L2 regularization, and by how much, to roduce the correct weighting effect? Our exeriments confirm these findings across 
a range of architectures and datasets.
"
697,2019,Guarantees for Spectral Clustering with Fairness Constraints,Oral,"Given the widesread oularity of sectral clustering (SC) for artitioning grah data, we study a version of constrained SC in which we try to incororate the fairness notion roosed by Chierichetti et al. (2017). According to this notion, a clustering is fair if every demograhic grou is aroximately roortionally reresented in each cluster. To this end, we develo variants of both normalized and unnormalized  constrained SC and show that they hel find fairer clusterings on both synthetic and real data. We also rovide a rigorous theoretical analysis of our algorithms. While there have been efforts to incororate various constraints into the SC framework, theoretically analyzing them is a challenging roblem. We overcome this by roosing a natural variant of the stochastic block model where h grous have strong inter-grou connectivity, but also exhibit a ""natural"" clustering structure which is fair. We rove that our algorithms can recover this fair clustering with high robability.
"
698,2019,"On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference",Oral,"Our goal is for agents to otimize the right reward function, desite how difficult it is for us to secify what that is. Inverse Reinforcement Learning (IRL) enables us to infer reward functions from demonstrations, but it usually assumes that the exert is noisily otimal. Real eole, on the other hand, often have systematic biases: risk-aversion, myoia, etc. One otion is to try to characterize these biases and account for them exlicitly during learning. But in the era of dee learning, a natural suggestion researchers make is to avoid mathematical models of human behavior that are fraught with secific assumtions, and instead use a urely data-driven aroach. We decided to ut this to the test -- rather than relying on assumtions about which secific bias the demonstrator has when lanning, we instead learn the demonstrator's lanning algorithm that they use to generate demonstrations, as a differentiable lanner. Our exloration yielded mixed findings: on the one hand, learning the lanner can lead to better reward inference than relying on the wrong assumtion; on the other hand, this benefit is dwarfed by the loss we incur by going from an exact to a differentiable lanner. This suggest that at least for the foreseeable future, agents need a middle ground between the flexibility of data-driven methods and the useful bias of known human biases.
"
699,2019,On the Long-term Impact of Algorithmic Decision Policies: Effort Unfairness and Feature Segregation through Social Learning,Oral,"Most existing notions of algorithmic fairness are static: they ensure some form of allocative equality at the time of decision making, but do not account for the adverse imact of the algorithmic decisions today on the long-term welfare and roserity of certain segments of the oulation. 
We take a broader ersective on algorithmic fairness. We roose an effort-based measure of fairness and resent a data-driven framework for characterizing the long-term imact of algorithmic olicies on reshaing the underlying oulation. Motivated by the sychological literature on social learning and the economic literature on equality of oortunity, we roose a micro-scale model of how individuals resond to decision making algorithms. We emloy existing measures of segregation from sociology and economics to quantify the resulting macro-scale oulation-level change. Imortantly, we observe that different models may shift the grou-conditional distribution of qualifications in different directions. Our findings raise a number of imortant questions regarding the formalization of fairness for decision-making models.
"
700,2019,Measurements of Three-Level Hierarchical Structure in the Outliers in the Spectrum of Deepnet Hessians,Oral,"We consider dee classifying neural networks. We exose a structure in the derivative of the logits with resect to the arameters of the model, which is used to exlain the existence of outliers in the sectrum of the Hessian. Previous works decomosed the Hessian into two comonents, attributing the outliers to one of them, the so-called Covariance of gradients. We show this term is not a Covariance but a second moment matrix, i.e., it is influenced by means of gradients. These means ossess an additive two-way structure that is the source of the outliers in the sectrum. This structure can be used to aroximate the rincial subsace of the Hessian using certain ""averaging"" oerations, avoiding the need for high-dimensional eigenanalysis. We corroborate this claim across different datasets, architectures and samle sizes.
"
701,2019,MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization,Oral,"Abstractive summarization has been studied using neural sequence transduction methods with datasets of large, aired document-summary examles. However, such datasets are rare and the models trained from them do not generalize to other domains. Recently, some rogress has been made in learning sequence-to-sequence maings with only unaired examles. In our work, we consider the setting where there are only documents (roduct or business reviews) with no summaries rovided, and roose an end-to-end, neural model architecture to erform unsuervised abstractive summarization. Our roosed model consists of an auto-encoder where the mean of the reresentations of the inut reviews decodes to a reasonable summary-review. We consider variants of the roosed architecture and erform an ablation study to show the imortance of secific comonents. We show through metrics and human evaluation that the generated summaries are highly abstractive, fluent, relevant, and reresentative of the average sentiment of the inut reviews. Finally, we collect a ground-truth evaluation dataset and show that our model outerforms a strong extractive baseline.
"
702,2019,DAG-GNN: DAG Structure Learning with Graph Neural Networks,Oral,"Learning a faithful directed acyclic grah (DAG) from samles of a joint distribution is a challenging combinatorial roblem, owing to the intractable search sace suerexonential in the number of grah nodes. A recent breakthrough formulates the roblem as a continuous otimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors aly the aroach to the linear structural equation model (SEM) and the least-squares loss function that are statistically well justified but nevertheless limited. Motivated by the widesread success of dee learning that is caable of caturing comlex nonlinear maings, in this work we roose a nonlinear generative model and aly a variant of the structural constraint to learn the DAG. At the heart of the generative model is a variational autoencoder arameterized by a novel grah neural network architecture, which we coin DAG-GNN. In addition to the richer caacity, an advantage of the roosed model is that it naturally handles discrete variables as well as vector-valued variables. We demonstrate that on synthetic data sets, the roosed method learns more accurate grahs for nonlinearly generated samles; and on benchmark data sets with discrete variables, the learned grahs are reasonably close to the global otima.
"
703,2019,Learning to Generalize from Sparse and Underspecified Rewards,Oral,"We consider the roblem of learning from sarse and undersecified
rewards.  This task structure arises in interretation roblems where
an agent receives a comlex inut, such as a natural language command,
and needs to generate a comlex resonse, such as an action sequence,
but only receives binary success-failure feedback.  Rewards of this
kind are usually undersecified because they do not distinguish
between uroseful and accidental success.  To learn in these
scenarios, effective exloration is critical to find successful
trajectories, but generalization also deends on discounting surious
trajectories that achieve accidental success.  We address exloration
by using a mode covering direction of KL divergence to collect a
diverse set of successful trajectories, followed by a mode seeking KL
divergence to train a robust olicy.  We address reward
undersecification by using Meta-Learning and Bayesian Otimization to
construct an auxiliary reward function, which rovides more accurate
feedback for learning.  The arameters of the auxiliary reward
function are otimized with resect to the validation erformance of
the trained olicy.  Without using exert demonstrations, ground
truth rograms, our Meta Reward-Learning (MeRL) achieves
state-of-the-art results on weakly-suervised semantic arsing,
imroving uon rior work by 1.3% and 2.6%
on WikiTablesQuestions and WikiSQL.
"
704,2019,Random Walks on Hypergraphs with Edge-Dependent Vertex Weights,Oral,"Hyergrahs are used in many machine learning methods to model higher-order relationshis in data. While sectral methods for grahs are well-established, sectral theory for hyergrahs remains an active area of research. In this aer, we use random walks to develo a sectral theory for hyergrahs with edge-deendent vertex weights: hyergrahs where every vertex v has a weight $\gamma_e(v)$ for each incident hyeredge e, describing the contribution of v to the hyeredge e. We derive a random walk-based hyergrah Lalacian, and bound the mixing time of random walks on such hyergrahs. Moreover, we give conditions under which random walks on such hyergrahs are equivalent to random walks on grahs. As a corollary, we show that current machine learning methods that rely on Lalacians derived from random walks on hyergrahs with edge-indeendent vertex weights do not utilize higher-order relationshis in the data. Finally, we demonstrate the effectiveness of hyergrahs with edge-deendent vertex weights on ranking alications using both synthetic and real-world datasets."
705,2019,Efficient Training of BERT by Progressively Stacking,Oral,"Unsuervised re-training is oularly used in natural language rocessing. By designing roer unsuervised rediction tasks, a dee neural network can be trained and shown to be effective in many downstream tasks. As the data is usually adequate, the model for re-training is generally huge and contains millions of arameters. Therefore, the training efficiency becomes a critical issue even when using high-erformance hardware. In this aer, we exlore an efficient training method for the state-of-the-art bidirectional Transformer (BERT) model. By visualizing the self-attention distribution of different layers at different ositions in a well-trained BERT model, we find that in most layers, the self-attention distribution will concentrate locally around its osition and the start-of-sentence token. Motivating from this, we roose the stacking algorithm to transfer knowledge from a shallow model to a dee model; then we aly stacking rogressively to accelerate BERT training. The exerimental results showed that the models trained by our training strategy achieve similar erformance to models trained from scratch, but our algorithm is much faster.
"
706,2019,Learning Distance for Sequences by Learning a Ground Metric,Oral,"Most existing metric learning methods are develoed for vector reresentations. Learning distances that oerate directly on multi-dimensional sequences is challenging because such distances are structural by nature and the vectors in sequences are not indeendent. Generally, distances for sequences heavily deend on the ground metric between the vectors in sequences. We roose to learn the distance for sequences through learning a ground Mahalanobis metric for the vectors in sequences. The learning samles are sequences of vectors for which how the ground metric between vectors induces the overall distance is given. The objective is that the distance induced by the learned ground metric affects large values for sequences from different classes and small values for those from the same class. We formulate the metric as a arameter of the distance and bring closer each sequence to an associated virtual sequence w.r.t. the distance to reduce the number of constraints. We develo a general iterative solution for any ground-metric-based sequence distance. Exeriments on several sequence datasets demonstrate the effectiveness and efficiency of our method.
"
707,2019,Supervised Hierarchical Clustering with Exponential Linkage,Oral,"In suervised clustering, standard techniques for learning a
  dissimilarity function often resent a mismatch between the training
  and clustering objectives.  This mismatch leads to oor erformance,
  which we demonstrate in the case where training maximizes rediction
  accuracy on all within- and across-cluster airs and clustering is
  erformed with agglomerative clustering with single linkage.  We resent a
  training rocedure tailored secifically to single linkage clustering that
  results in imroved erformance. Since designing secialized
  training rocedures is cumbersome, we introduce the arametric
  family of exonential linkage functions, which smoothly interolates
  between single, average and comlete linkage, and give a training
  rocedure that jointly selects a linkage from the family and learns
  a dissimilarity function suited for that linkage.  In exeriments on
  four datasets, our training rocedure leads to imrovements of u to
  6\% dendrogram urity over all airs training and consistently
  matches or outerforms the next best linkagetraining-rocedure air
  on three out of four datasets.
"
708,2019,Scale-free adaptive planning for deterministic dynamics & discounted rewards,Oral,"We address the roblem of lanning in an environment with deterministic dynamics and stochastic discounted rewards under a limited numerical budget where the ranges of both rewards and noise are unknown.  We introduce \latyoos, an adative, robust and efficient alternative to the \OLOP (oen-loo otimistic lanning) algorithm.  Whereas \OLOP requires ariori knowledge of the ranges of both rewards and noise, \latyoos dynamically adats its behavior to both.  This allows \latyoos to be immune to two vulnerabilities of \OLOP: failure when given underestimated ranges of noise and rewards and inefficiency when these are overestimated. \Platyoos additionally adats to the global smoothness of the value function.  We assess \latyoos’s erformance in terms of the simle regret, the exected loss resulting from choosing our algorithm’s recommended action rather than an otimal one.  We show that \latyoos acts in a rovably more efficient manner vs \OLOP when \OLOP is given an overestimated reward and show that in the case of no noise, \latyoos learns exonentially faster than \OLOP. 
"
709,2019,Making Decisions that Reduce Discriminatory Impacts,Oral,"As machine learning algorithms move into real-world settings, it is crucial to ensure they are aligned with societal values.  There has been much work on one asect of this, namely unfairness in the rediction roblem: How can we reduce discrimination in the redictions themselves? While an imortant question, solutions to this roblem only aly in a restricted setting because we have full control over the redictions. Often we care about the non-discrimination of quantities we do not have full control over. Thus, we describe another key asect of this challenge, the imact roblem: How can we reduce discrimination arising from the real-world imact of decisions? To address this, we describe causal methods that model the relevant arts of the real-world system in which the decisions are made. Unlike revious aroaches, these  models not only allow us to ma the causal athway of a single decision, but also to model the effect of interference--how the imact on an individual deends on decisions made about other eole. Often, the goal of decision olicies is to maximize a beneficial imact overall. To reduce the discrimination of these benefits, we devise a constraint insired by recent work in counterfactual fairness, and give an efficient rocedure to solve the constrained otimization roblem. We demonstrate our aroach with an examle: how to increase students taking college entrance exams in New York City ublic schools.
"
710,2019,On the Limitations of Representing Functions on Sets,Oral,"Recent work on the reresentation of functions on sets has considered the use of summation in a latent sace to enforce ermutation invariance. In articular, it has been conjectured that the dimension of this latent sace may remain fixed as the cardinality of the sets under consideration increases. However, we demonstrate that the analysis leading to this conjecture requires maings which are highly discontinuous and argue that this is only of limited ractical use. Motivated by this observation, we rove that an imlementation of this model via continuous maings (as rovided by e.g. neural networks or Gaussian rocesses) actually imoses a constraint on the dimensionality of the latent sace. Practical universal function reresentation for set inuts can only be achieved with a latent dimension at least the size of the maximum number of inut elements.
"
711,2019,Similarity of Neural Network Representations Revisited,Oral,"Recent work has sought to understand the behavior of neural networks by comaring reresentations between layers and between different trained models. We examine methods for comaring neural network reresentations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between reresentations of higher dimension than the number of available data oints. We introduce a similarity index that measures the relationshi between reresentational similarity matrices and does not suffer from this limitation. We show that this similarity index is equivalent to centered kernel-tangent alignment (KTA) and is also closely connected to CCA. Unlike other methods, KTA can reliably identify corresondences between reresentations of layers in networks trained from different initializations.
"
712,2019,CHiVE: Varying Prosody in Speech Synthesis with a Linguistically Driven Dynamic Hierarchical Conditional Variational Network,Oral,"The rosodic asects of seech signals roduced by current text-to-seech systems are tyically averaged over training material, and as such lack the variety and liveliness found in natural seech. To avoid monotony and averaged rosody contours, it is desirable to have a way of modeling the variation in the rosodic asects of seech, so audio signals can be synthesized in multile ways for a given text. We resent a new, hierarchically structured conditional variational auto-encoder to generate rosodic features (F0, c0 and duration) suit- able for use with a vocoder or a generative model like WaveNet. At inference time, an embedding reresenting the rosody of a sentence may be samled from the variational layer to allow for rosodic variation. To efficiently cature the hierarchical nature of the linguistic inut (words, syllables and hones), both the encoder and decoder arts of the auto-encoder are hierarchical, in line with the linguistic structure, with layers being clocked dynamically at the resective rates. We show in our exeriments that our dynamic hierarchical network outerforms a non-hierarchical state-of-the-art baseline. Additionally, we show that rosody transfer across sentences is ossible by emloying the rosody embedding of one sentence to generate the seech signal of another.
"
713,2019,Rates of Convergence for Sparse Variational Gaussian Process Regression,Oral,"Excellent variational aroximations to Gaussian rocess osteriors have been develoed which avoid the $\mathcal{O}\left(N^3\right)$ scaling with dataset size $N$. They reduce the comutational cost to $\mathcal{O}\left(NM^2\right)$, with $M\ll N$ the number of \emh{inducing variables}, which summarise the rocess. While the comutational cost seems to be linear in $N$, the true comlexity of the algorithm deends on how $M$ must increase to ensure a certain quality of aroximation. We show that with high robability the KL divergence can be made arbitrarily small by growing $M$ more slowly than $N$. A articular case is that for regression with normally distributed inuts in D-dimensions with the Squared Exonential kernel, $M=\mathcal{O}(\log^D N)$ suffices. Our results show that as datasets grow, Gaussian rocess osteriors can be aroximated chealy, and rovide a concrete rule for how to increase $M$ in continual learning scenarios."
714,2019,Decentralized Exploration in Multi-Armed Bandits,Oral,"We consider the decentralized exloration roblem: a set of layers collaborate to identify the best arm by asynchronously interacting with the same stochastic environment. The objective is to insure rivacy in the best arm identification roblem between asynchronous, collaborative, and thrifty layers. In the context of a digital service, we advocate that this decentralized aroach allows a good balance between conflicting interests: the roviders otimize their services, while
rotecting rivacy of users and saving resources. We define the rivacy level as the amount of information an adversary could infer by interceting
all the messages concerning a single user. We rovide a generic algorithm DECENTRALIZED ELIMINATION, which uses any best arm identification
algorithm as a subroutine. We rove that this algorithm insures rivacy, with a low communication cost, and that in comarison to the lower bound of the best arm identification roblem, its samle comlexity suffers from a enalty deending on the inverse of the robability of the most frequent layers. Then, thanks to the genericity of the aroach, we extend the roosed algorithm to the non-stationary bandits. Finally, exeriments illustrate and comlete the analysis.
"
715,2019,Probabilistic Neural Symbolic Models for Interpretable Visual Question Answering,Oral,"We roose a new class of robabilistic neural-symbolic models, that has symbolic functional rograms as a latent, stochastic variable. Instantiated in the context of visual question answering, our robabilistic formulation offers two key concetual advantages over rior neural-symbolic models for VQA. Firstly, the rograms generated by our model are more understandable while requiring lesser number of teaching examles. Secondly, we show that one can ose counterfactual scenarios to the model, to robe its beliefs on the questions or rograms that could lead to a secified answer given an image. Our results on a dataset of comositional questions about SHAPES verify our hyotheses, showing that the model gets better rogram (and answer) rediction accuracy even in the low data regime, and allows one to robe the coherence and consistency of reasoning erformed.
"
716,2019,COMIC: Multi-view Clustering Without Parameter Selection,Oral,"In this aer, we study two challenges in clustering analysis, namely, how to cluster multi-view data and how to erform clustering without arameter selection on cluster size. To this end, we roose a novel objective function to roject raw data into one sace in which the rojection embraces the geometric consistency (GC) and the cluster assignment consistency (CAC). To be secific, the GC aims to learn a connection grah from a rojection sace wherein the data oints are connected if and only if they belong to the same cluster. The CAC aims to minimize the discreancy of airwise connection grahs induced from different views based on the view-consensus assumtion, \textit{i.e.}, different views could roduce the same cluster assignment structure as they are different ortraits of the same object. Thanks to the view-consensus derived from the connection grah, our method could achieve romising  erformance in learning view-secific reresentation and eliminating the heterogeneous gas across different views. Furthermore, with the roosed objective, it could learn almost all arameters including the cluster number from data without labor-intensive arameter selection. Extensive exerimental results show the romising erformance achieved by our method on five datasets comaring with nine state-of-the-art multi-view clustering aroaches. 
"
717,2019,Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations,Oral,"Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be reresented by dense matrix-vector multilication, yet each has a secialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and imlementations is necessary, what structural rior they encode, and how much knowledge is required to automatically learn a fast algorithm for a rovided structured transform. Motivated by a characterization of fast matrix-vector multilication as roducts of sarse matrices, we introduce a arameterization of divide-and-conquer methods that is caable of reresenting a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many imortant transforms; for examle, it recovers the $O(N \log N)$ Cooley-Tukey FFT algorithm to machine recision, for dimensions $N$ u to $1024$. Furthermore, our method can be incororated as a lightweight relacement of generic matrices in machine learning ielines to learn efficient and comressible transformations. On a standard task of comressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 oints---the first time a structured aroach has done so---with 4X faster inference seed and 40X fewer arameters."
718,2019,Communication-Constrained Inference and the Role of Shared Randomness,Oral,"A central server needs to erform statistical inference based on
samles that are distributed over multile users 
who can each send a message of limited length to the center. 
We study  roblems of distribution learning and identity testing
in this distributed inference setting and examine the role of shared
randomness as a resource. We roose a general urose
\textit{simulate-and-infer} strategy that  uses only rivate-coin
communication rotocols and is
samle-otimal for distribution learning. This general strategy turns
out to be samle-otimal even for distribution testing among
rivate-coin rotocols. Interestingly, we roose a ublic-coin
rotocol that outerforms simulte-and-infer for distribution testing
and is, in fact, samle-otimal. Underlying our ublic-coin rotocol
is a random hash that when alied to the samles minimally contracts the
chi-squared distance of their distribution from the uniform distribution. 
"
719,2019,"Submodular Maximization beyond Non-negativity: Guarantees, Fast Algorithms, and Applications",Oral,"It is generally believed that submodular functions--and the more general class of 
$\gamma$-weakly submodular functions--may only be otimized under the non-negativity assumtion $f(S) \geq 0$. In this aer, we show that once the function is exressed as the difference $f = g - c$, where $g$ is monotone, non-negative, and $\gamma$-weakly submodular and $c$ is non-negative modular, then strong aroximation guarantees may be obtained. We resent an algorithm for maximizing $g - c$ under a $k$-cardinality constraint which roduces a random feasible set $S$ such that $\mathbb{E}[g(S) -c(S)]  \geq  (1  -  e^{-\gamma}  - \esilon) g(\ot) - c(\ot)$, 
whose running time is $O (\frac{n}{\esilon} \log^2 \frac{1}{\esilon})$, indeendent of $k$. We extend these results to the unconstrained setting by describing an algorithm with the same aroximation guarantees and faster $O(n \frac{1}{\esilon} \log\frac{1}{\esilon})$ runtime. The main techniques underlying our algorithms are two-fold: the use of a surrogate objective which varies the relative imortance between $g$ and $c$ throughout the algorithm, and a geometric swee over ossible $\gamma$ values. Our algorithmic guarantees are comlemented by a hardness result showing that no olynomial-time algorithm which accesses $g$ through a value oracle can do better. We emirically demonstrate the success of our algorithms by alying them to exerimental design on the Boston Housing dataset and directed vertex cover on the Email EU dataset."
720,2019,"Distributed, Egocentric Representations of Graphs for Detecting Critical Structures",Oral,"We study the roblem of detecting critical structures using a grah embedding model. Existing grah embedding models lack the ability to recisely detect critical structures that are secific to a task at the global scale. In this aer, we roose a novel grah embedding model, called the Ego-CNNs, that detects recise critical structures efficiently. An Ego-CNN can be jointly trained with a task model and hel exlaindiscover knowledge for the task. We conduct extensive exeriments and the results show that Ego-CNNs (1) can lead to comarable task erformance as the state-of-the-art grah embedding models, (2) works nicely with CNN visualization techniques to illustrate the detected structures, and (3) is efficient and can incororate with scale-free riors, which commonly occurs in social network datasets, to further imrove the training efficiency.
"
721,2019,Nonparametric Bayesian Deep Networks with Local Competition,Oral,"The aim of this work is to enable inference of dee networks that retain high accuracy for the least ossible model comlexity, with the latter deduced from the data during inference.  To this end, we revisit dee networks that comrise cometing linear units, as oosed to nonlinear units that do not entail any form of (local) cometition. In this context, our main technical innovation consists in an inferential setu that leverages solid arguments from Bayesian nonarametrics. We infer both the needed set of connections or locally cometing sets of units, as well as the required floating-oint recision for storing the network arameters. Secifically, we introduce auxiliary discrete latent variables reresenting which initial network comonents are actually needed for modeling the data at hand, and erform Bayesian inference over them by imosing aroriate stick-breaking riors.  As we exerimentally show using benchmark datasets, our aroach yields networks with less comutational footrint than the state-of-the-art, and with no comromises in redictive accuracy.
"
722,2019,Warm-starting Contextual Bandits: Robustly Combining Supervised and Bandit Feedback,Oral,"We investigate the feasibility of learning from both fully-labeled suervised data and contextual bandit data. We secifically consider settings in which the underlying learning signal may be different between these two data sources. Theoretically, we state and rove no-regret algorithms for learning that is robust to divergences between the two sources. Emirically, we evaluate some of these algorithms on a large selection of datasets, showing that our aroaches are feasible, and helful in ractice.
"
723,2019,Online Algorithms for Rent-Or-Buy with Expert Advice,Oral,"We study the use of redictions by multile exerts (such as machine learning algorithms) to imrove the erformance of online algorithms. In articular, we consider the classical rent-or-buy roblem (also called ski rental), and obtain algorithms that rovably imrove their erformance over the adversarial scenario by using these redictions. We also rove matching lower bounds to show that our algorithms are the best ossible, and erform exeriments to emirically validate their erformance in ractice
"
724,2019,The Wasserstein Transform,Oral,"We introduce the Wasserstein transform, a method for enhancing and denoising datasets defined on general metric saces. The construction draws insiration from Otimal Transortation ideas. We establish the stability of our method under data erturbation and, when the dataset is assumed to be Euclidean, we also exhibit a recise connection between the Wasserstein transform and the mean shift family of algorithms. We then use this connection to rove that mean shift also inherits stability under erturbations. We study the erformance of the Wasserstein transform method on different datasets as a rerocessing ste rior to clustering and classification tasks.
"
725,2019,Learning and Data Selection in Big Datasets,Oral,"Finding a dataset of minimal cardinality to characterize the otimal arameters of a model is of aramount imortance in machine learning and distributed otimization over a network. This aer investigates the comressibility of large datasets. More secifically, we roose a framework that jointly learns the inut-outut maing
as well as the most reresentative samles of the dataset (sufficient dataset). Our analytical results show that the cardinality of the sufficient dataset increases sub-linearly with resect to the original dataset size. Numerical evaluations of real
datasets reveal a large comressibility, u to 95%, without a noticeable dro in the learnability erformance, measured by the generalization error.
"
726,2019,Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query Complexity,Oral,"As a general otimization roblem, submodular maximization has a wide range of alications in machine learning (e.g., active learning, clustering, and feature selection).  In large-scale otimization, the arallel running time of an algorithm is governed by its adativity, which measures the number of sequential rounds needed if the algorithm can execute olynomially-many indeendent oracle queries in arallel.  While low adativity is ideal, it is not sufficient for an algorithm to be efficient in ractice---there are many alications of distributed submodular otimization where the number of function evaluations becomes rohibitively exensive.  Motivated by these alications, we study the adativity and query comlexity of submodular maximization.  In this aer, we give the first constant-aroximation algorithm for maximizing a non-monotone submodular function subject to a cardinality constraint~$k$ that runs in $O(\log(n))$ adative rounds.  Additionally, our algorithm makes only $O(n \log(k))$ oracle queries in exectation.  In our emirical study, we use three real-world alications to comare our algorithm with several benchmarks for non-monotone submodular maximization, and the results show that our algorithm finds cometitive solutions using \emh{significantly fewer rounds and queries}."
727,2019,Sublinear quantum algorithms for training linear and kernel-based classifiers,Oral,"We investigate quantum algorithms for classification, a fundamental roblem in machine learning, with rovable guarantees. Given $n$ $d$-dimensional data oints, the state-of-the-art (and otimal) classical algorithm for training classifiers with constant margin by Clarkson et al. runs in $\tilde{O}(n +d)$, which is also otimal in its inutoutut model. We design sublinear quantum algorithms for the same task running in $\tilde{O}(\sqrt{n} +\sqrt{d})$, a quadratic imrovement in both $n$ and $d$. Moreover, our algorithms use the standard quantization of the classical inut and generate the same classical outut, suggesting minimal overheads when used as subroutines for end-to-end alications. We also demonstrate a tight lower bound (u to oly-log factors) and discuss the ossibility of imlementation on near-term quantum machines."
728,2019,Breaking the Softmax Bottleneck via Learnable Monotonic Pointwise Non-linearities,Oral,"The softmax function on to of a final linear layer is the de facto method to outut robability distributions in neural networks. In many alications such as language models or text generation, these models have to roduce distributions over large outut vocabularies. Recently, this has been shown to have limited reresentational caacity due to its connection with the rank bottleneck in matrix factorization. However, little is known about the limitations of linear-softmax for quantities of ractical interest such as cross entroy or mode estimation, direction theoretically and emirically exlored in this aer. As an efficient and effective solution to alleviate this issue, we roose to learn arametric monotonic functions on to of the logits. Theoretically, we show that such monotonic functions are likely to increase the rank of a matrix to its full rank. Emirically, our method imroves over the traditional softmax-linear layer both in synthetic and real language model exeriments with negligible time or memory overhead, while being comarable to the more comutationally exensive mixture of softmax distributions.
"
729,2019,Sequential Facility Location: Approximate Submodularity and Greedy Algorithm,Oral,"We develo and analyze a novel utility function and a fast otimization algorithm for subset selection in sequential data that incororates the underlying dynamic model of data. We roose a caacitated sequential facility location function that finds a fixed number of reresentatives that well encode the data, where the sequence of reresentatives are comatible with the dynamic model of data. As maximizing this new objective function is NP-hard, we develo a fast greedy algorithm based on submodular maximization. Unlike the conventional facility location, the comutation of the marginal gain in our case cannot be done by oerations on each item indeendently. We exloiting the sequential structure of the roblem and develo an efficient dynamic rogramming-based algorithm that exactly comutes the marginal gain. We investigate conditions on the dynamic transition model of data, under which our utility function is submodular or $\esilon$-aroximately submodualr, hence, the greedy algorithm comes with erformance guarantees. By exeriments on synthetic data and the real-world roblem of rocedure learning from instructional videos, we show that our framework not only significantly imroves the comutational time, but also achieves better objective function values and obtains more coherent summaries. "
730,2019,Exploiting structure of uncertainty for efficient matroid semi-bandits,Oral,"We imrove the efficiency of algorithms for stochastic \emh{combinatorial semi-bandits}.
In most interesting roblems, state-of-the-art algorithms take advantage of structural roerties of rewards, such as \emh{indeendence}. However, while 
being minimax otimal in terms of regret, these algorithms are intractable. In our aer, we first reduce their imlementation to a secific \emh{submodular maximization}. Then, in case of \emh{matroid} constraints,  
we design adated aroximation routines, thereby roviding the first efficient algorithms that exloit reward structure. In articular, we imrove the state-of-the-art efficient ga-free regret bound by a factor $\sqrt{k}$, where $k$ is the maximum action size.
Finally, we show how our imrovement translates to more general \emh{budgeted combinatorial semi-bandits}."
731,2019,Good Initializations of Variational Bayes for Deep Models,Oral,"Stochastic variational inference is an established way to carry out aroximate Bayesian inference for dee models. While there have been effective roosals for good initializations for loss minimization in dee learning, far less attention has been devoted to the issue of initialization of stochastic variational inference. We address this by roosing a novel layer-wise initialization strategy based on Bayesian linear models. The roosed method is extensively validated on regression and classification tasks, including Bayesian DeeNets and ConvNets, showing faster and better convergence comared to alternatives insired by the literature on initializations for loss minimization.
"
732,2019,Multi-Object Representation Learning with Iterative Variational Inference,Oral,"Human ercetion is structured around objects which form the basis for our higher-level cognition and imressive systematic generalization abilities.
Yet most work on reresentation learning focuses on feature learning without even considering multile objects, or treats segmentation as an (often suervised) rerocessing ste.
Instead, we argue for the imortance of learning to segment and reresent objects jointly.
Starting from the simle assumtion that a scene is comosed of entities with common features, we demonstrate that it is ossible to learn to segment images into interretable objects with disentangled reresentations.
Our method learns - without suervision - to inaint occluded arts, and extraolates to objects with novel feature combinations. 
We also show that, because our method is based on iterative variational inference, our system is able to learn multi-modal osteriors for ambiguous inuts and extends naturally to sequential data.
"
733,2019,Dropout as a Structured Shrinkage Prior,Oral,"Droout regularization of dee neural networks has been a mysterious yet effective tool to revent overfitting.  Exlanations for its success range from the revention of ""co-adated"" weights to it being a form of chea Bayesian inference.  We roose a novel framework for understanding multilicative noise in neural networks, considering continuous distributions as well as Bernoulli noise (i.e. droout).  We show that multilicative noise induces structured shrinkage riors on a network's weights.  We derive the equivalence through rearametrization roerties of scale mixtures and without invoking any aroximations.  Given the equivalence, we then show that droout's Monte Carlo training objective aroximates marginal MAP estimation.  We extend this framework to ResNets, terming the rior ""automatic deth determination"" as it is the natural analog of ""automatic relevance determination"" for network deth.  Lastly, we investigate two inference strategies that imrove uon the aforementioned MAP aroximation in regression benchmarks.
"
734,2019,Agnostic Federated Learning,Oral,"A key learning scenario in large-scale alications is that of federated learning, where a centralized model is trained based on data originating from a large number of clients. We argue that, with the existing training and inference, federated models can be biased towards different clients. Instead, we roose a new framework of agnostic federated learning, where the centralized model is otimized for any target distribution formed by a mixture of the client distributions. We further show that this framework naturally yields a notion of fairness. We resent data-deendent Rademacher comlexity guarantees for learning with this objective, which guide the definition of an algorithm for agnostic federated learning. We also give a fast stochastic otimization algorithm for solving the corresonding otimization roblem, for which we rove convergence bounds, assuming a convex  loss function and hyothesis set. We further emirically demonstrate  the benefits of our aroach in several datasets. Beyond federated learning, our framework and algorithm can be of interest to other  learning scenarios such as cloud comuting, domain adatation,  drifting, and other contexts where the training and test distributions do not coincide.
"
735,2019,Neural Collaborative Subspace Clustering,Oral,"We introduce the Neural Collaborative Subsace Clustering, a neural model that discovers clusters of data oints drawn from a union of low-dimensional subsaces. In contrast to revious attemts, our model runs without the aid of sectral clustering. This makes our algorithm one of the kinds that can gracefully scale to large datasets. At its heart, our neural model benefits from a classifier which determines whether a air of oints lies on the same subsace or not. Essential to our model is the construction of two affinity matrices, one from the classifier and the other from a notion of subsace self-exressiveness, to suervise training in a collaborative scheme. We thoroughly assess and contrast the erformance of our model against various state-of-the-art  clustering algorithms including dee subsace-based ones. 
"
736,2019,PAC Identification of Many Good Arms in Stochastic Multi-Armed Bandits,Oral,"We consider the roblem of identifying any k out of the best m arms in an n-armed stochastic multi-armed bandit. Framed in the PAC setting, this articular roblem
generalises both the roblem of codebest subset selection'' (Kalyanakrishnan &am; Stone, 2010) and that of
selectingcodeone out of the best m'' arms (Roy Chaudhuri &am; Kalyanakrishnan, 2017). In alications such as crowd-sourcing and drug-designing, identifying a single good solution is often not sufficient. Moreover, finding the best subset might be hard due to the resence of many indistinguishably close solutions. Our generalisation of identifying exactly k arms out of the best m, where 1 ≤ k ≤ m, serves as a more effective alternative. We resent a lower bound on the worst-case samle comlexity for general k, and
a fully sequential PAC algorithm, LUCB-k-m, which is more samle-efficient on easy instances. 
Also, extending our analysis to infinite-armed bandits, we resent a PAC algorithm that is indeendent of n, 
which identifies an arm from the best ρ fraction of arms using at most an additive oly-log number of samles than
comared to the lower bound, 
thereby imroving over (Roy Chaudhuri &am; Kalyanakrishnan, 2017) and Aziz et al. (2018). 
The roblem of identifying k &gt; 1 distinct arms from the best ρ fraction is not always well-defined; 
for a secial class of this roblem, we resent lower and uer bounds. Finally, through a reduction, we establish a relation between uer bounds for the codeone out of the best ρ'' roblem for infinite instances and thecodeone out of the best m'' roblem for finite instances. We conjecture that it is more efficient to solve ``small"" finite instances using the latter formulation, rather than going through the former.
"
737,2019,Categorical Feature Compression via Submodular Optimization,Oral,"In the era of big data, learning from categorical features with very large vocabularies (e.g., 28 million for the Criteo click rediction dataset) has become a ractical challenge for machine learning researchers and ractitioners.  We design a highly-scalable vocabulary comression algorithm that seeks to maximize the mutual information between the comressed categorical feature and the target binary labels and we furthermore show that its solution is guaranteed to be within a $1-1e \arox 63\%$ factor of the global otimal solution. Although in some settings, entroy-based set functions are known to be submodular, this is not the case for the mutual information objective we consider (mutual information with resect to the target labels).  To address this, we introduce a novel re-arametrization of the mutual information objective, which we rove is submodular, and also design a data structure to query the submodular function in amortized $O(\log n )$ time (where $n$ is the inut vocabulary size). Our comlete algorithm is shown to oerate in $O(n \log n )$ time. Additionally, we design a distributed imlementation in which the query data structure is decomosed across $O(k)$ machines such that each machine only requires $O(\frac n k)$ sace, while still reserving the aroximation guarantee and using only logarithmic rounds of comutation.  We also rovide analysis of simle alternative heuristic comression methods to demonstrate they cannot achieve any aroximation guarantee.  Using the large-scale Criteo learning task, we demonstrate better erformance in retaining mutual information and also verify cometitive learning erformance comared to other baseline methods."
738,2019,Contextual Multi-armed Bandit Algorithm for Semiparametric Reward Model,Oral,"Contextual multi-armed bandit (MAB) algorithms have been shown romising for maximizing cumulative rewards in sequential decision tasks such as news article recommendation systems, web age ad lacement algorithms, and mobile health. However, most of the roosed contextual MAB algorithms assume linear relationshis between the reward and the context of the action. This aer rooses a new contextual MAB algorithm for a relaxed, semiarametric reward model that suorts nonstationarity. The roosed method is less restrictive, easier to imlement and faster than two alternative algorithms that consider the same model, while achieving a tight regret uer bound. We rove that the high-robability uer bound of the regret incurred by the roosed algorithm has the same order as the Thomson samling algorithm for linear reward models. The roosed and existing algorithms are evaluated via simulation and also alied to Yahoo! news article recommendation log data.
"
739,2019,Multi-Frequency Phase Synchronization,Oral,"We roose a novel formulation for hase synchronization---the statistical roblem of jointly estimating alignment angles from noisy airwise comarisons---as a nonconvex otimization roblem that enforces consistency among the airwise comarisons in multile frequency channels. Insired by harmonic retrieval in signal rocessing, we develo a simle yet efficient two-stage algorithm that leverages the
multi-frequency information. We demonstrate in theory and ractice that the roosed algorithm significantly outerforms state-of-the-art hase synchronization algorithms, at a mild comutational costs incurred by using the extra frequency channels. We also extend our algorithmic framework to general synchronization roblems over comact Lie grous.
"
740,2019,Discovering Conditionally Salient Features with Statistical Guarantees,Oral,"The goal of feature selection is to identify imortant features that are relevant to exlain a outcome variable. Most of the work in this domain has focused on identifying \emh{globally} relevant features, which are features that are related to the outcome using evidence across the entire dataset. We study a more fine-grained statistical roblem: \emh{conditional feature selection}, where a feature may be relevant deending on the values of the other features. For examle in genetic association studies, variant $A$ could be associated with the henotye in the entire dataset, but conditioned on variant $B$ being resent it might be indeendent of the henotye. In this sense, variant $A$ is globally relevant, but conditioned on $B$ it is  no longer locally relevant in that region of the feature sace.  We resent a generalization of the knockoff rocedure that erforms \emh{conditional feature selection} while controlling a generalization of the false discovery rate (FDR) to the conditional setting. By exloiting the featureresonse model-free framework of the knockoffs, the quality of the statistical FDR guarantee is not degraded even when we erform conditional feature selections. We imlement this method and resent an algorithm that automatically artitions the feature sace such that it enhances the differences between selected sets in different regions, and validate the statistical theoretical results with exeriments."
741,2019,Cross-Domain 3D Equivariant Image Embeddings,Oral,"Sherical convolutional networks have been introduced recently as tools to learn owerful feature reresentations of 3D shaes. Sherical CNNs are equivariant to 3D rotations making them ideally suited for alications where 3D data may be observed in arbitrary orientations. In this aer we learn 2D image embeddings with a similar equivariant structure: embedding the image of a 3D object should commute with rotations of the object.  We introduce a cross-domain embedding from 2D images into a sherical CNN latent sace. Our model is suervised only by target embeddings obtained from a sherical CNN retrained for 3D shae classification. The trained model learns to encode images with 3D shae roerties and is equivariant to 3D rotations of the observed object. We show that learning only a rich embedding for images with aroriate geometric structure is in and of itself sufficient for tackling numerous alications. Evidence from two different alications, relative ose estimation and novel view synthesis, demonstrates that equivariant embeddings are sufficient for both alications without requiring any task-secific suervised training.
"
742,2019,Unsupervised Deep Learning by Neighbourhood Discovery,Oral,"Dee convolutional neural networks (CNNs) have demonstrated remarkable success in comuter vision by suervisedly learning strong visual feature reresentations. However, training CNNs relies heavily on the availability of exhaustive training data annotations, limiting significantly their deloyment and scalability in many alication scenarios. In this work, we introduce a generic unsuervised dee learning aroach to training dee models without the need for any manual label suervision. Secifically, we rogressively discover samle anchoredcentred neighbourhoods to reason and learn the underlying class decision boundaries iteratively and accumulatively. Every single neighbourhood is secially formulated so that all the member samles can share the same unseen class labels at high robability for facilitating the extraction of class discriminative feature reresentations during training. Exeriments on image classification show the erformance advantages of the roosed method over the state-of-the-art unsuervised learning models on CIFAR10 and CIFAR100, SVHN, and ImageNet
benchmarks.
"
743,2019,ARSM: Augment-REINFORCE-Swap-Merge Estimator for Gradient Backpropagation Through Categorical Variables,Oral,"To address the challenge of backroagating the gradient through categorical variables, we roose the augment-REINFORCE-swa-merge (ARSM) gradient estimator that is unbiased and has low variance. ARSM first uses variable augmentation, REINFORCE, and Rao-Blackwellization to re-exress the gradient as an exectation under the Dirichlet distribution, then uses variable swaing to construct differently exressed but equivalent exectations, and finally shares common random numbers between these exectations to achieve significant variance reduction. Exerimental results show ARSM closely resembles the erformance of the true gradient for otimization in univariate settings; outerforms existing estimators by a large margin when alied to categorical variational auto-encoders; and rovides a ""try-and-see self-critic"" variance reduction method for discrete-action olicy gradient, which removes the need of estimating baselines by generating a random number of seudo actions and estimating their action-value functions. 
"
744,2019,On Variational Bounds of Mutual Information,Oral,"Estimating, minimizing, andor maximizing Mutual Information (MI) is core to many objectives in machine learning, but tractably bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds arameterized by neural networks (Alemi et al., 2016, Belghazi et al., 2018, van den Oord et al., 2018). However, the relationshis and tradeoffs between these bounds remains unclear. In this work, we unify these recent develoments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this roblem, we introduce a continuum of lower bounds that encomasses revious bounds and flexibly trades off bias and variance. On a suite of high-dimensional, controlled roblems, we emirically characterize the bias and variance of both the bounds and their gradients and demonstrate the effectiveness of these new bounds for estimation and reresentation learning.
"
745,2019,Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning,Oral,"When observing the actions of others, humans make inferences about why the others acted as they did, and what this imlies about their view of the world. Humans also use the fact that their actions will be interreted in this manner when observed by others, allowing them to act informatively and thereby communicate efficiently with others. Although learning algorithms have recently achieved suerhuman erformance in a number of two-layer, zero-sum games, scalable multi-agent reinforcement learning algorithms that can discover effective strategies and conventions in comlex, artially observable settings have roven elusive. We resent the \emh{Bayesian action decoder} (BAD), a new multi-agent learning method that uses an aroximate Bayesian udate to obtain a ublic belief that conditions on the actions taken by all agents in the environment. Together with the ublic belief, this Bayesian udate effectively defines a new Markov decision rocess, the \emh{ublic belief MDP}, in which the action sace consists of deterministic artial olicies, arameterised by neural networks, that can be samled for a given ublic state. BAD exloits the fact that an agent acting only on this ublic belief state can still learn to use its rivate information if the action sace is augmented to be over artial olicies maing rivate information into environment actions. The Bayesian udate is also closely related to the \emh{theory of mind} reasoning that humans carry out when observing others' actions. We first validate BAD on a roof-of-rincile two-ste matrix game, where it outerforms  olicy gradient methods. We then evaluate BAD on the challenging, cooerative artial-information card game Hanabi, where in the two-layer setting the method surasses all reviously ublished learning and hand-coded aroaches.
"
746,2019,Faster Algorithms for Binary Matrix Factorization,Oral,"We give faster aroximation algorithms for 
well-studied variants of Boolean Matrix Factorization, 
where we are given a binary matrix $A \in \mathbb{R}^{m \times n}$
and would like to find binary matrices $U \in \{0,1\}^{m \times k}$
and $V \in \{0,1\}^{k \times n}$ so as to minimize $\|U \cdot V - A\|_F^2$. 
In the first setting, 
$U \cdot V$ denotes multilication over the integers, and 
We give an algorithm oututting
a constant factor aroximation in $2^{O(k^2 \log k)} \textrm{oly}(mn)$ time, imroving the revious $\min(2^{2^k}, 2^n) \textrm{oly}(mn)$ time. 

Our techniques generalize to finding 
$U \in \{0,1\}^{m \times k}$
and $V \in \{0,1\}^{k \times n}$ so as to aroximately 
minimize $\|U \cdot V - A\|_^$,
where $ \geq 1$ is any real number, in $2^{O(k^{\lceil 2 \rceil + 1}\log k)} \textrm{oly}(mn)$ time.
For $ = 1$, our results have a grah-theoretic
interretation. Namely, 
given an unweighted biartite grah $G$ with $m$ vertices in the left art and $n$ vertices in the right art, 
how well can one aroximate
$G$ as a disjoint union $H$ of $k$ biartite cliques (bicliques)? A natural notion of aroximation is to find a disjoint union $H$ of $k$ bicliques 
so as to 
minimize the number of edges in the symmetric difference, i.e., 
to find $H$ with
$$|E(G)\setminus E(H)| + |E(H)\setminus E(G)| 
\leq C \min_{\textrm{a disjoint union }H' \textrm{ of } k \textrm{ cliques}} 
|E(G)\setminus E(H')| + |E(H') \setminus E(G)|,$$
where $C  1$ is a constant aroximation factor.
It is known
that deciding if a biartite grah is equal to the disjoint union of $k$
bicliques requires $2^{\Omega(k)} \cdot \textrm{oly}(mn)$ time under standard comlexity-theoretic
assumtions, and so this lower bound also alies to the aroximate case.
While there are algorithms to decide if $G$
can be exressed as the disjoint union of $k$ bicliques in $2^{O(k^2)} \cdot \textrm{oly}(mn)$ time, 
surrisingly, no algorithms were known for the aroximate version
of the roblem. We give the first constant factor aroximation
algorithm for this roblem, running in $2^{O(k^2 \log k)} \textrm{oly}(mn)$ time.

Finally, we give the fastest known bicriteria constant factor aroximation 
when the multilication $U \cdot V$ is over the finite field $GF(2)$. 
We achieve $2^{O(k^3)} \oly(mn)$ time
to outut binary rank $O(k \log m)$ matrices $U$ and $V$ whose cost is
as good as the best rank-$k$ aroximation, imroving the revious 
$\min(2^{2^k}mn, \min(m,n)^{O(\oly(k))} \textrm{oly}(mn))$ time."
747,2019,A Theoretical Analysis of Contrastive Unsupervised Representation Learning,Oral,"Recent emirical works successfully use unlabeled data to learn feature reresentations that are broadly useful in downstream classification tasks.
Several are reminiscent of the well-known word2vec embedding algorithm: leveraging availability of airs of semantically codesimilar"" data oints andcodenegative samles"", the learner forces the inner roduct of reresentations of similar airs with each other to be higher on average than with negative samles.
The current aer uses the term {\em contrastive learning} for such algorithms and resents a theoretical framework for understanding it, by introducing {\em latent classes} and hyothesizing that semantically similar oints are samled from the same {\em latent class}. This concetual framework allows us to show rovable guarantees on the erformance of the learnt reresentation on downstream classification tasks, whose classes are assumed to be random samles from the same set of latent classes. Our generalization bound also shows that learnt reresentations can reduce (labeled) samle comlexity on downstream tasks.
Controlled exeriments are erformed in NLP and image domains to suort the theory.
"
748,2019,Loss Landscapes of Regularized Linear Autoencoders,Oral,"Autoencoders are a dee learning model for reresentation learning. When trained to minimize the distance between the data and its reconstruction, linear autoencoders (LAEs) learn the subsace sanned by the to rincial directions but cannot learn the rincial directions themselves. In this aer, we rove that L2-regularized LAEs are symmetric at all critical oints and learn the rincial directions as the left singular vectors of the decoder. We smoothly arameterize the critical manifold and relate the minima to the MAP estimate of robabilistic PCA. We illustrate these results emirically and consider imlications for PCA algorithms, comutational neuroscience, and the algebraic toology of learning.
"
749,2019,Autoregressive Energy Machines,Oral,"Neural density estimators are flexible families of arametric models which have seen widesread use in unsuervised machine learning in recent years. Maximum-likelihood training tyically dictates that these models be constrained to secify an exlicit density. However, this limitation can be overcome by instead using a neural network to secify an energy function, or unnormalized density, which can subsequently be normalized to obtain a valid distribution. The challenge with this aroach lies in accurately estimating the normalizing constant of the high-dimensional energy function. We roose the Autoregressive Energy Machine, an energy-based model which simultaneously learns an unnormalized density and comutes an imortance-samling estimate of the normalizing constant for each conditional in an autoregressive decomosition. The Autoregressive Energy Machine achieves state-of-the-art erformance on a suite of density-estimation tasks.
"
750,2019,Hyperbolic Disk Embeddings for Directed Acyclic Graphs,Oral,"Obtaining continuous reresentations of structural data such as directed acyclic grahs (DAGs) has gained attention in machine learning and artificial intelligence. However, embedding comlex DAGs in which both ancestors and descendants of nodes are exonentially increasing is difficult. Tackling in this roblem, we develo Disk Embeddings, which is a framework for embedding DAGs into quasi-metric saces. Existing state-of-the-art methods, Order Embeddings and Hyerbolic Entailment Cones, are instances of Disk Embedding in Euclidean sace and sheres resectively. Furthermore, we roose a novel method Hyerbolic Disk Embeddings to handle exonential growth of relations. The results of our exeriments show that our Disk Embedding models outerform existing methods esecially in comlex DAGs other than trees.
"
751,2019,Greedy Orthogonal Pivoting Algorithm for Non-Negative Matrix Factorization,Oral,"Non-negative matrix factorization is a owerful tool for learning useful reresentations in the data and has been widely alied in many roblems such as data mining and signal rocessing.  Orthogonal NMF, which  can imrove the locality of decomosition, has drawn considerable interest in solving clustering roblems in recent years. However, imosing simultaneous non-negative and orthogonal structure can be quite difficult, and so existing algorithms can only solve it aroximately. To address this challenge, we roose an innovative rocedure called Greedy Orthogonal Pivoting Algorithm (GOPA). The GOPA algorithm fully exloits the sarsity of non-negative orthogonal solutions to break the global roblem into a series of local otimizations, in which an adative subset of coordinates are udated in a greedy, closed-form manner. The biggest advantage of GOPA is that it romotes exact orthogonality and rovides solid emirical evidence that stronger orthogonality does contribute favorably to better  clustering erformance. On the other hand, we further design randomized and arallel version of GOPA, which can further reduce the comutational cost and imrove accuracy, making it suitable for large data.
"
752,2019,Partially Exchangeable Networks and Architectures for Learning Summary Statistics in Approximate Bayesian Computation,Oral,"We resent a novel family of dee neural architectures, named artially exchangeable networks (PENs) that leverage robabilistic symmetries. By design, PENs are invariant to block-switch transformations, which characterize the artial exchangeability roerties of conditionally Markovian rocesses. Moreover, we show that any block-switch invariant function has a PEN-like reresentation. The DeeSets architecture is a secial case of PEN and we can therefore also target fully exchangeable data. We emloy PENs to learn summary statistics in aroximate Bayesian comutation (ABC). When comaring PENs to revious dee learning methods for learning summary statistics, our results are highly cometitive, both considering time series and static models. Indeed, PENs rovide more reliable osterior samles even when using less training data.
"
753,2019,TarMAC: Targeted Multi-Agent Communication,Oral,"We roose a targeted communication architecture for multi-agent reinforcement learning, where agents learn both \emh{what} messages to send and \emh{whom} to address them to while erforming cooerative tasks in artially-observable environments. This targeting behavior is learnt solely from downstream task-secific reward without any communication suervision. We additionally augment this with a multi-round communication aroach where agents coordinate via multile rounds of communication before taking actions in the environment.

We evaluate our aroach on a diverse set of cooerative multi-agent tasks, of varying difficulties, with varying number of agents, in a variety of environments ranging from 2D grid layouts of shaes and simulated traffic junctions to 3D indoor environments, and demonstrate the benefits of targeted and multi-round communication. Moreover, we show that the targeted communication strategies learned by agents are interretable and intuitive.

Finally, we show that our architecture can be easily extended to mixed and cometitive environments, leading to imroved erformance and samle comlexity over recent state-of-the-art aroaches.
"
754,2019,The information-theoretic value of unlabeled data in semi-supervised learning,Oral,"We quantify the searation between the numbers of labeled examles required to
learn in two settings: Settings with and without the knowledge of
the distribution of the unlabeled data. More secifically, we rove a searation
by $\Theta(\log n)$ multilicative factor for the class of rojections over
the Boolean hyercube of dimension $n$.

Learning with the knowledge of the distribution (a.k.a. fixed-distribution
learning) can be viewed as an idealized scenario of semi-suervised learning
where the number of unlabeled data oints is so great that the unlabeled
distribution is known exactly. For this reason, we call the searation the
value of unlabeled data."
755,2019,Noise2Self: Blind Denoising by Self-Supervision,Oral,"We roose a general framework for denoising high-dimensional measurements which requires no rior on the signal, no estimate of the noise, and no clean training data. The only assumtion is that the noise exhibits statistical indeendence across different dimensions of the measurement. Moreover, our framework is not restricted to a articular denoising model. We show how it can be used to calibrate any arameterised denoising algorithm, from the single hyerarameter of a median filter to the millions of weights of a dee neural network. We demonstrate this on natural image and microscoy data, where we exloit the indeendence of noise between ixels, and on single-cell gene exression data, where we exloit the indeendence between detections of individual molecules. Finally, we rove a theoretical lower bound on the erformance of an otimal denoiser. This framework generalizes recent work on training neural nets from noisy images and on cross-validation for matrix factorization.
"
756,2019,Hierarchical Importance Weighted Autoencoders,Oral,"Imortance weighted variational inference (Burda et al., 2016) uses multile i.i.d. samles to have a tighter variational lower bound. We believe a joint roosal has the otential of reducing the number of redundant samles, and introduce a hierarchical structure to induce correlation.  The hoe is that the roosals would coordinate to make u for the error made by one another, and reduce the variance as a whole.  Theoretically, we analyze the condition under which convergence of the estimator variance can be connected to convergence of the lower bound. Emirically, we confirm that maximization of the lower bound does imlicitly minimize variance.  Further analysis shows that this is a result of negative correlation induced by the roosed hierarchical meta samling scheme, and erformance of inference also imroves when number of samles increases.
"
757,2019,Unsupervised Label Noise Modeling and Loss Correction,Oral,"Desite being robust to small amounts of label noise, convolutional neural networks trained with stochastic gradient methods have been shown to easily fit random labels. 
When there are a mixture of correct and mislabelled targets, networks tend to fit the former before the latter. 
This suggests using a suitable two-comonent mixture model as an unsuervised generative model of samle loss values during training to allow online estimation of the robability that a samle is mislabelled.
Secifically, we roose a beta mixture to estimate this robability and correct the loss by relying on the network rediction (the so-called bootstraing loss). 
We further adat mixu augmentation to drive our aroach a ste further. 
Exeriments on CIFAR-10100 and TinyImageNet demonstrate a robustness to label noise that substantially outerforms recent state-of-the-art.
"
758,2019,QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning,Oral,"We exlore value-based solutions for multi-agent reinforcement learning (MARL) tasks in the centralized training with decentralized execution (CTDE) regime oularized recently. However, VDN and QMIX are reresentative examles that use the idea of factorization of the joint action-value function into individual ones for decentralized execution. VDN and QMIX address only a fraction of factorizable MARL tasks due to their structural constraint in factorization such as additivity and monotonicity. In this aer, we roose a new factorization method for MARL, QTRAN, which is free from such structural constraints and takes on a new aroach to transforming the original joint action-value function into an easily factorizable one, with the same otimal actions. QTRAN guarantees successful factorization of any factorizable task, thus covering a much wider class of MARL tasks than does VDN or QMIX. Our exeriments for the tasks of multi-domain Gaussian-squeeze and modified redator-rey demonstrate  QTRAN's suerior erformance with esecially larger margins in games whose ayoffs enalize non-cooerative behavior more aggressively. 
"
759,2019,Guided evolutionary strategies: augmenting random search with surrogate gradients,Oral,"Many alications in machine learning require otimizing a function whose true gradient is unknown or comutationally exensive, but where surrogate gradient information, directions that may be correlated with the true gradient, is chealy available. For examle, this occurs when an aroximate gradient is easier to comute than the full gradient (e.g. in meta-learning or unrolled otimization), or when a true gradient is intractable and is relaced with a surrogate (e.g. in reinforcement learning or training networks with discrete variables). We roose Guided Evolutionary Strategies (GES), a method for otimally using surrogate gradient directions to accelerate random search. GES defines a search distribution for evolutionary strategies that is elongated along a subsace sanned by the surrogate gradients and estimates a descent direction which can then be assed to a first-order otimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subsace and use this to derive a setting of the hyerarameters that works well across roblems. We evaluate GES on several examle roblems, demonstrating an imrovement over both standard evolutionary strategies and first-order methods that directly follow the surrogate gradient.
"
760,2019,LatentGNN: Learning Efficient Non-local Relations for Visual Recognition,Oral,"Caturing long-range deendencies in feature reresentations is crucial for many visual recognition tasks. Desite recent successes of dee convolutional networks, it remains challenging to model non-local context relations between visual features. A romising strategy is to model the feature context by a fully-connected grah neural network (GNN), which augments traditional convolutional features with an estimated non-local context reresentation. However, most GNN-based aroaches require comuting a dense grah affinity matrix and hence have difficulty in scaling u to tackle comlex real-world visual roblems. In this work, we roose an efficient and yet flexible non-local relation reresentation based on a novel class of grah neural networks. Our key idea is to introduce a latent sace to reduce the comlexity of grah, which allows us to use a low-rank reresentation for the grah affinity matrix and to achieve a linear comlexity in comutation. Extensive exerimental evaluations on three major visual recognition tasks show that our method outerforms the rior works with a large margin while maintaining a low comutation cost.   
"
761,2019,Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces,Oral,"Bayesian otimization is known to be difficult to scale to high dimensions, because the acquisition ste requires solving a non-convex otimization roblem in the same search sace. In order to scale the method and kee its benefits, we roose an algorithm (LineBO) that restricts the roblem to a sequence of iteratively chosen one-dimensional sub-roblems. We show that our algorithm converges globally and obtains a fast local rate when the function is strongly convex. Further, if the objective has an invariant subsace, our method automatically adats to the effective dimension without changing the algorithm. Our method scales well to high dimensions and makes use of a global Gaussian rocess model. When combined with the SafeOt algorithm to solve the sub-roblems, we obtain the first safe Bayesian otimization algorithm with theoretical guarantees alicable in high-dimensional settings. We evaluate our method on multile synthetic benchmarks, where we obtain cometitive erformance. Further, we deloy our algorithm to otimize the beam intensity of a free electron laser with u to 40 arameters while satisfying the safe oeration constraints.
"
762,2019,Learning Dependency Structures for Weak Supervision Models,Oral,"Labeling training data is a key bottleneck in the modern machine learning ieline. Recent weak suervision aroaches combine labels from multile noisy sources by estimating their accuracies without access to ground truth; however, estimating the deendencies among these sources is a critical challenge. We focus on a robust PCA-based algorithm for learning these deendency structures, establish imroved theoretical recovery rates, and outerform existing methods on various real-world tasks. Under certain conditions, we show that the amount of unlabeled data needed can scale sublinearly or even logarithmically with the number of sources m, imroving over revious efforts that ignore the sarsity attern in the deendency structure and scale linearly in m. We rovide an information-theoretic lower bound on the minimum samle comlexity of the weak suervision setting. Our method outerforms weak suervision aroaches that assume conditionally-indeendent sources by u to 4.64 F1 oints and revious structure learning aroaches by u to 4.41 F1 oints on real-world relation extraction and image classification tasks.
"
763,2019,Actor-Attention-Critic for Multi-Agent Reinforcement Learning,Oral,"Reinforcement learning in multi-agent scenarios is imortant for real-world alications but resents challenges beyond those seen in single-agent settings. We resent an actor-critic algorithm that trains decentralized olicies in multi-agent settings, using centrally comuted critics that share an attention mechanism which selects relevant information for each agent at every timeste. This attention mechanism enables more effective and scalable learning in comlex multi-agent environments, when comared to recent aroaches. Our aroach is alicable not only to cooerative settings with shared rewards, but also individualized reward settings, including adversarial settings, as well as settings that do not rovide global states, and it makes no assumtions about the action saces of the agents. As such, it is flexible enough to be alied to most multi-agent learning roblems.
"
764,2019,Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness,Oral,"The ability to learn disentangled reresentations that slit underlying sources of variation in high dimensional, unstructured data is imortant for data efficient and robust use of neural networks. While various aroaches aiming towards this goal have been roosed in recent times, a commonly acceted definition and validation rocedure is missing. We rovide a causal ersective on reresentation learning which covers disentanglement and domain shift robustness as secial cases. Our causal framework allows us to introduce a new metric for the quantitative evaluation of dee latent variable models. We show how this metric can be estimated from labeled observational data and further rovide an efficient estimation algorithm that scales linearly in the dataset size. 
"
765,2019,Faster Attend-Infer-Repeat with Tractable Probabilistic Models,Oral,"The recent attend-infer-reeat (AIR) framework marks a milestone in Bayesian scene understanding and in the romising avenue of structured robabilistic modeling.
The AIR model exresses the comosition of visual scenes from individual objects, and uses variational autoencoders to model the aearance of those objects.
However, inference in the overall model is highly intractable, which hamers its learning seed and makes it rone to sub-otimal solutions.
In this aer, we show that inference and learning in AIR can be considerably accelerated by relacing the intractable object reresentations with tractable robabilistic models.
In articular, we ot for sum-roduct (SP) networks, an exressive dee robabilistic model with a rich set of tractable inference routines.
As our emirical evidence shows, the resulting model, called SPAIR, achieves a higher object detection accuracy than the original AIR system, while reducing the learning time by an order of magnitude.
Moreover, SPAIR allows one to treat object occlusions in a consistent manner and to include a background noise model, imroving the robustness of Bayesian scene understanding.
"
766,2019,Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment,Oral,"Domain adatation addresses the common roblem when the target distribution generating our test  data  drifts  from  the source (training)  distribution.    While  absent  assumtions,  domain adatation  is  imossible,  strict  conditions,  e.g. covariate or label shift, enable rinciled algorithms.   Recently-roosed domain-adversarial aroaches consist of aligning source and target encodings, often motivating this aroach as minimizing two (of three) terms in a theoretical bound on target error. Unfortunately, this minimization can cause arbitrary increases in the third term, e.g., these methods are unrinciled under label distribution shift. We roose asymmetrically-relaxed distribution alignment, a new aroach that overcomes some limitations of standard domain-adversarial algorithms. We characterize recise assumtions under which our algorithm is theoretically rinciled and demonstrate emirical benefits on both synthetic and real datasets.
"
767,2019,Pareto Optimal Streaming Unsupervised Classification,Oral,"We study an online and streaming unsuervised classification system. Our setting consists of a collection of classifiers (with unknown confusion matrices) each of which can classify one samle er unit time, and which are accessed by a stream of unlabeled samles. Each samle is disatched to one or more classifiers, and deending on the labels collected from these classifiers, may be sent to other classifiers to collect additional labels. The labels are continually aggregated. Once the aggregated label has high enough accuracy (a re-secified threshold for accuracy) or the samle is sent to all the classifiers, the now labeled samle is ejected from the system. For any given re-secified threshold for accuracy, the objective is to sustain the maximum ossible rate of arrival of new samles, such that the number of samles in memory does not grow unbounded. In this aer, we characterize the Pareto-otimal region of accuracy and arrival rate, and develo an algorithm that can oerate at any oint within this region. Our algorithm uses queueing-based routing and scheduling aroaches combined with novel online tensor decomosition method to learn the hidden arameters, to Pareto-otimality guarantees. We finally verify our theoretical results through simulations on various synthetic and real ensembles, where our real ensembles are formed using dee image classifiers, e.g.  AlexNet, VGG, and ResNet.
"
768,2019,Understanding Priors in Bayesian Neural Networks at the Unit Level,Oral,"We investigate dee Bayesian neural networks with Gaussian riors on the weights and a class of ReLU-like nonlinearities. Bayesian neural networks with Gaussian riors are well known to induce an L2,  ``weight decay'', regularization. Our results indicate a more intricate regularization effect at the level of the unit activations. Our main result  establishes that the induced rior distribution on the units before and after activation becomes increasingly heavy-tailed with the deth of the layer. 
We show that first layer units are Gaussian, second layer units are sub-exonential, and units in deeer layers are characterized by  sub-Weibull distributions. 
Our results  rovide new theoretical insight on dee Bayesian neural networks, which we corroborate  with exerimental simulation results on convolutional  networks. 
"
769,2019,Lorentzian Distance Learning for Hyperbolic Representations,Oral,"We introduce an aroach to learn reresentations based on the Lorentzian distance in hyerbolic geometry. Hyerbolic geometry is esecially suited to hierarchically-structured datasets, which are revalent in the real world. Current hyerbolic reresentation learning methods comare examles with the Poincar\'e distance. They try to minimize the distance of each node in a hierarchy with its descendants while maximizing its distance with other nodes. This formulation roduces node reresentations close to the centroid of their descendants. To obtain efficient and interretable algorithms, we exloit the fact that the centroid w.r.t the squared Lorentzian distance can be written in closed-form. We show that the Euclidean norm of such a centroid decreases as the curvature of the hyerbolic sace decreases. This roerty makes it aroriate to reresent hierarchies where arent nodes minimize the distances to their descendants and have smaller Euclidean norm than their children. Our aroach obtains state-of-the-art results in retrieval and classification tasks on different datasets. 
"
770,2019,Semi-Cyclic Stochastic Gradient Descent,Oral,"We consider convex SGD udates with a blockcyclic structure, i.e. where each cycle consists of a small number of blocks, each with many samles from a ossibly different, block-secific, distribution. This situation arises, e.g., in Federated Learning where the mobile devices available for udates at different times during the day have different characteristics. We show that such block-cyclic structure can significantly deteriorate the erformance of SGD, but roose a simle correction aroach that allows rediction with the same erformance guarantees as for i.i.d., non-cyclic, samling.
"
771,2019,Finite-Time Analysis of Distributed TD(0) with Linear Function Approximation on Multi-Agent Reinforcement Learning,Oral,"We study the olicy evaluation roblem in multi-agent reinforcement learning. In this roblem, a grou of agents work cooeratively to evaluate the value function for the global discounted accumulative reward roblem, which is comosed of local rewards observed by the agents. Over a series of time stes, the agents act, get rewarded, udate their local estimate of the value function, then communicate with their neighbors.  The local udate at each agent can be interreted as a distributed consensus-based variant of the oular temoral difference learning algorithm TD(0). 

While distributed reinforcement learning algorithms have been resented in the literature, almost nothing is known about their convergence rate.  Our main contribution is roviding a finite-time analysis for the convergence of the distributed TD(0) algorithm. We do this when the communication network between the agents is time-varying in general. We obtain an exlicit uer bound on the rate of convergence of this algorithm as a function of the network toology and the discount factor. Our results mirror what we would exect from using distributed stochastic gradient descent for solving convex otimization roblems. 
"
772,2019,Geometry and Symmetry in Short-and-Sparse Deconvolution,Oral,"We study the Short-and-Sarse (SaS) deconvolution roblem of recovering a short signal a0 and a sarse signal x0 from their convolution. We roose a method based on nonconvex otimization, which under certain conditions recovers the target short and sarse signals, u to a signed shift symmetry which is intrinsic to this model. This symmetry lays a central role in shaing the otimization landscae for
deconvolution. We give a regional analysis, which characterizes this landscae geometrically, on a union of subsaces. Our geometric characterization holds when the length-0 short signal a0 has shift coherence µ, and x0 follows a random sarsity model with sarsity rate θ ∈ [c10, c2(0\sqrt{\mu}+\sqrt{0})]  (log^2(0)) . Based on this geometry, we give a rovable method that successfully solves SaS deconvolution with high robability.
"
773,2019,SelectiveNet: A Deep Neural Network with an Integrated Reject Option,Poster,"We consider the roblem of selective rediction (also known as reject otion) in dee neural networks, and introduce SelectiveNet, a dee neural architecture with an integrated reject otion. Existing rejection mechanisms are based mostly on a threshold over the rediction confidence of a re-trained network. In contrast, SelectiveNet is trained to otimize both classification (or regression) and rejection simultaneously, end-to-end.  The result is a dee neural network that is otimized over the covered domain. In our exeriments, we show a consistently imroved risk-coverage trade-off over several well-known classification and regression datasets, thus reaching new state-of-the-art results for dee selective classification.
"
774,2019,Manifold Mixup: Better Representations by Interpolating Hidden States,Poster,"Dee neural networks excel at learning the training data, but often rovide incorrect and confident redictions when evaluated on slightly different test examles.  This includes distribution shifts, outliers, and adversarial examles.  To address these issues, we roose \manifoldmixu{}, a simle regularizer that encourages neural networks to redict less confidently on interolations of hidden reresentations.  \manifoldmixu{} leverages semantic interolations as additional training signal, obtaining neural networks with smoother decision boundaries at multile levels of reresentation.  As a result, neural networks trained with \manifoldmixu{} learn flatter class-reresentations, that is, with fewer directions of variance.  We rove theory on why this flattening haens under ideal conditions, validate it emirically on ractical situations, and connect it to the revious works on information theory and generalization.  In site of incurring no significant comutation and being imlemented in a few lines of code, \manifoldmixu{} imroves strong baselines in suervised learning, robustness to single-ste adversarial attacks, and test log-likelihood.
"
775,2019,Processing Megapixel Images with Deep Attention-Sampling Models,Poster,"Existing dee architectures cannot oerate on very large signals such
as megaixel images due to comutational and memory constraints. To
tackle this limitation, we roose a fully differentiable end-to-end
trainable model that samles and rocesses only a fraction of the full
resolution inut image.
The locations to rocess are samled from an attention distribution
comuted from a low resolution view of the inut. We refer to our
method as attention samling and it can rocess images of
several megaixels with a standard single GPU setu.
We show that samling from the attention distribution results in an
unbiased estimator of the full model with minimal variance, and we
derive an unbiased estimator of the gradient that we use to train our
model end-to-end with a normal SGD rocedure.
This new method is evaluated on three classification tasks, where we
show that it allows to reduce comutation and memory footrint by
an order of magnitude for the same accuracy as classical
architectures. We also show the consistency of the samling that
indeed focuses on informative arts of the inut images.
"
776,2019,TapNet: Neural Network Augmented with Task-Adaptive Projection for Few-Shot Learning,Poster,"Handling reviously unseen tasks after given only a few training examles continues to be a tough challenge in machine learning. We roose TaNets, neural networks augmented with task-adative rojection for imroved few-shot learning. Here, emloying a meta-learning strategy with eisode-based training, a network and a set of er-class reference vectors are learned across widely varying tasks. At the same time, for every eisode, features in the embedding sace are linearly rojected into a new sace as a form of quick task-secific conditioning. The training loss is obtained based on a distance metric between the query and the reference vectors in the rojection sace. Excellent generalization results in this way. When tested on the Omniglot, miniImageNet and tieredImageNet datasets, we obtain state of the art classification accuracies under various few-shot scenarios.
"
777,2019,Online Meta-Learning,Poster,"A central caability of intelligent systems is the ability to continuously build uon revious exeriences to seed u and enhance learning of new tasks. Two distinct research aradigms have studied this question. Meta-learning views this roblem as learning a rior over model arameters that is amenable for fast adatation on a new task, but tyically assumes the tasks are available together as a batch. In contrast, online (regret based) learning considers a setting where tasks are revealed one after the other, but conventionally trains a single model without task-secific adatation.
This work introduces an online meta-learning setting, which merges ideas from both aradigms to better cature the sirit and ractice of continual lifelong learning. We roose the follow the meta leader (FTML) algorithm which extends the MAML algorithm to this setting. Theoretically, this work rovides an O(log T) regret guarantee with one additional higher order smoothness assumtion (in comarison to the standard online setting). Our exerimental evaluation on three different large-scale roblems suggest that the roosed algorithm significantly outerforms alternatives based on traditional online learning aroaches.
"
778,2019,Training Neural Networks with Local Error Signals,Poster,"Suervised training of neural networks for classification is tyically erformed with a global loss function. The loss function rovides a gradient for the outut layer, and this gradient is back-roagated to hidden layers to dictate an udate direction for the weights. An alternative aroach is to train the network with layer-wise loss functions. In this aer we demonstrate, for the first time, that layer-wise training can aroach the state-of-the-art on a variety of image datasets. We use single-layer sub-networks and two different suervised loss functions to generate local error signals for the hidden layers, and we show that the combination of these losses hel with otimization in the context of local learning. Using local errors could be a ste towards more biologically lausible dee learning because the global error does not have to be transorted back to hidden layers. A comletely backro free variant outerforms reviously reorted results among methods aiming for higher biological lausibility.
"
779,2019,GMNN: Graph Markov Neural Networks,Poster,"This aer studies semi-suervised object classification in relational data, which is a fundamental roblem in relational data modeling. The roblem has been extensively studied in the literature of both statistical relational learning (e.g. relational Markov networks) and grah neural networks (e.g. grah convolutional networks). Statistical relational learning methods can effectively model the deendency of object labels through conditional random fields for collective classification, whereas grah neural networks learn effective object reresentations for classification through end-to-end training. In this aer, we roose the Grah Markov Neural Network (GMNN) that combines the advantages of both worlds. A GMNN models the joint distribution of object labels with a conditional random field, which can be effectively trained with the variational EM algorithm. In the E-ste, one grah neural network learns effective object reresentations for aroximating the osterior distributions of object labels. In the M-ste, another grah neural network is used to model the local label deendency. Exeriments on object classification, link classification, and unsuervised node reresentation learning show that GMNN achieves state-of-the-art results. 
"
780,2019,Self-Attention Graph Pooling,Poster,"Advanced methods of alying dee learning to structured data such as grahs have been roosed in recent years. In articular, studies have focused on generalizing convolutional neural networks to grah data, which includes redefining the convolution and the downsamling (ooling) oerations for grahs. The method of generalizing the convolution oeration to grahs has been roven to imrove erformance and is widely used. However, the method of alying downsamling to grahs is still difficult to erform and has room for imrovement. In this aer, we roose a grah ooling method based on self-attention. Self-attention using grah convolution allows our ooling method to consider both node features and grah toology. To ensure a fair comarison, the same training rocedures and model architectures were used for the existing ooling methods and our method. The exerimental results demonstrate that our method achieves suerior grah classification erformance on the benchmark datasets using a reasonable number of arameters.
"
781,2019,Combating Label Noise in Deep Learning using Abstention,Poster,"We introduce a novel method to combat label noise when training dee neural networks for classification. We roose a loss function that ermits abstention during training thereby allowing the DNN to abstain on confusing samles while continuing to learn and imrove classification erformance on the non-abstained samles. We show how such a dee abstaining classifier (DAC) can be used for robust learning in the resence of different tyes of label noise. In the case of structured or systematic label noise – where noisy training labels or confusing examles are correlated with underlying features of the data– training with abstention enables reresentation learning for features that are associated with unreliable labels. In the case of unstructured (arbitrary) label noise, abstention during training enables the DAC to be used as an effective data cleaner by identifying samles that are likely to have label noise. We rovide analytical results on the loss function behavior that enable dynamic adation of abstention rates based on learning rogress during training. We demonstrate the utility of the dee abstaining classifier for various image classification tasks under different tyes of label noise; in the case of arbitrary label noise, we show significant im- rovements over reviously ublished results on multile image benchmarks.
"
782,2019,LGM-Net: Learning to Generate Matching Networks for Few-Shot Learning,Poster,"In this work, we roose a novel meta-learning aroach for few-shot classification, which learns transferable rior knowledge across tasks and directly roduces network arameters for similar unseen tasks with training samles. Our aroach, called LGM-Net, includes two key modules, namely, TargetNet and MetaNet. The TargetNet module is a neural network for solving a secific task and the MetaNet module aims at learning to generate functional weights for TargetNet by observing training samles. We also resent an intertask normalization strategy for the training rocess to leverage common information shared across different tasks. The exerimental results on Omniglot and miniImageNet datasets demonstrate that LGM-Net can effectively adat to similar unseen tasks and achieve cometitive erformance, and the results on synthetic datasets show that transferable rior knowledge is learned by the MetaNet module via maing training data to functional weights. LGM-Net enables fast learning and adatation since no further tuning stes are required comared to other meta-learning aroaches
"
783,2019,Self-Attention Generative Adversarial Networks,Poster,"In this aer, we roose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range deendency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only satially local oints in lower-resolution feature mas. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant ortions of the image are consistent with each other.
Furthermore, recent work has shown that generator conditioning affects GAN erformance. Leveraging this insight, we aly sectral normalization to the GAN generator and find that this imroves training dynamics. The roosed SAGAN erforms better than rior work, boosting the best ublished Incetion score from 36.8 to 52.52 and reducing Fr\'echet Incetion distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that corresond to object shaes rather than local regions of fixed shae.
"
784,2019,Multivariate-Information Adversarial Ensemble for Scalable Joint Distribution Matching,Poster,"A broad range of cross-$m$-domain generation researches boil down to matching a joint distribution by dee generative models (DGMs). Hitherto algorithms excel in airwise domains while as $m$ increases, remain struggling to scale themselves to ﬁt a joint distribution. In this aer, we roose a domain-scalable DGM, i.e., MMI-ALI for $m$-domain joint distribution matching. As an $m$-domain ensemble model of ALIs (Dumoulin et al., 2016), MMI-ALI is adversarially trained with maximizing Multivariate Mutual Information (MMI) w.r.t. joint variables of each air of domains and their shared feature. The negative MMIs are uer bounded by a series of feasible losses rovably leading to matching $m$-domain joint distributions. MMI-ALI linearly scales as $m$ increases and thus, strikes a right balance between efﬁcacy and scalability. We evaluate MMI-ALI in diverse challenging $m$-domain scenarios and verify its sueriority."
785,2019,High-Fidelity Image Generation With Fewer Labels,Poster,"Dee generative models are becoming a cornerstone of modern machine learning. Recent work on conditional generative adversarial networks has shown that learning comlex, high-dimensional distributions over natural images is within reach. While the latest models are able to generate high-fidelity, diverse natural images at high resolution, they rely on a vast quantity of labeled data. In this work we demonstrate how one can benefit from recent work on self- and semi-suervised learning to outerform the state of the art on both unsuervised ImageNet synthesis, as well as in the conditional setting. In articular, the roosed aroach is able to match the samle quality (as measured by FID) of the current state-of-the-art conditional model BigGAN on ImageNet using only 10% of the labels and outerform it using 20% of the labels.
"
786,2019,Revisiting precision recall definition for generative modeling,Poster,"In this article we revisit the definition of Precision-Recall (PR) curves for generative models roosed by (Sajjadi et al., 2018). Rather than roviding a scalar for generative quality, PR curves distinguish mode-collase (oor recall) and bad quality (oor recision). We first generalize their formulation to arbitrary measures hence removing any restriction to finite suort. We also exose a bridge between PR curves and tye I and tye II error (a.k.a. false detection and rejection) rates of likelihood ratio classifiers on the task of discriminating between samles of the two distributions. Building uon this new ersective, we roose a novel algorithm to aroximate recision-recall curves, that shares some interesting methodological roerties with the hyothesis testing technique from (Loez-Paz  &am;  Oquab,  2017). We demonstrate the interest of the roosed formulation over the original aroach on controlled multi-modal datasets.
"
787,2019,Wasserstein of Wasserstein Loss for Learning Generative Models,Poster,"The Wasserstein distance serves as a loss function for unsuervised learning which deends on the choice of a ground metric on samle sace. We roose to use the Wasserstein distance itself as the ground metric on the samle sace of images. This ground metric is known as an effective distance for image retrieval, that correlates with human ercetion. We derive the Wasserstein ground metric on ixel sace and define a Riemannian Wasserstein gradient enalty to be used in the Wasserstein Generative Adversarial Network (WGAN) framework. The new gradient enalty is comuted efficiently via convolutions on the $L^2$ gradients with negligible additional comutational cost. The new formulation is more robust to the natural variability of the data and rovides for a more continuous discriminator in samle sace.
"
788,2019,Flat Metric Minimization with Applications in Generative Modeling,Poster,"We take the novel ersective to view data not as a robability distribution but rather as a current. Primarily studied in the field of geometric measure theory, k-currents are continuous linear functionals acting on comactly suorted smooth differential forms and can be understood as a generalized notion of oriented k-dimensional manifold. By moving from distributions (which are 0-currents) to k-currents, we can exlicitly orient the data by attaching a k-dimensional tangent lane to each samle oint. Based on the flat metric which is a fundamental distance between currents, we derive FlatGAN, a formulation in the sirit of generative adversarial networks but generalized to k-currents. In our theoretical contribution we rove that the flat metric between a arametrized current and a reference current is Lischitz continuous in the arameters. In exeriments, we show that the roosed shift to k0 leads to interretable and disentangled latent reresentations which behave equivariantly to the secified oriented tangent lanes.
"
789,2019,Entropic GANs meet VAEs: A Statistical Approach to Compute Sample Likelihoods in GANs,Poster,"Building on the success of dee learning, two modern aroaches to learn a robability model from the data are Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs). VAEs consider an exlicit robability model for the data and comute a generative distribution by maximizing a variational lower-bound on the log-likelihood function. GANs, however, comute a generative model by minimizing a distance between observed and generated robability distributions without considering an exlicit model for the observed data. The lack of having exlicit robability models in GANs rohibits comutation of samle likelihoods in their frameworks and limits their use in statistical inference roblems. In this work, we resolve this issue by constructing an exlicit robability model that can be used to comute samle likelihood statistics in GANs. In articular, we rove that under this robability model, a family of Wasserstein GANs with an entroy regularization can be viewed as a generative model that maximizes a variational lower-bound on average samle log likelihoods, an aroach that VAEs are based on. This result makes a rinciled connection between two modern generative models, namely GANs and VAEs. In addition to the aforementioned theoretical results, we comute likelihood statistics for GANs trained on Gaussian, MNIST, SVHN, CIFAR-10 and LSUN datasets. Our numerical results validate the roosed theory.
"
790,2019,Non-Parametric Priors For Generative Adversarial Networks,Poster,"The advent of generative adversarial networks (GAN) has enabled new caabilities in synthesis, interolation, and data augmentation heretofore considered very challenging. However, one of the common assumtions in most GAN architectures is the assumtion of simle arametric latent-sace distributions. While easy to imlement, a simle latent-sace distribution can be roblematic for uses such as interolation. This is due to distributional mismatches when samles are interolated in the latent sace. We resent a straightforward formalization of this roblem; using basic results from robability theory and off-the-shelf-otimization tools, we develo ways to arrive at aroriate non-arametric riors. The obtained rior exhibits unusual qualitative roerties in terms of its shae, and quantitative benefits in terms of lower divergence with its mid-oint distribution. We demonstrate that our designed rior hels imrove image generation along any Euclidean straight line during interolation, both qualitatively and quantitatively, without any additional training or architectural modifications. The roosed formulation is quite flexible, aving the way to imose newer constraints on the latent-sace statistics.
"
791,2019,Lipschitz Generative Adversarial Nets,Poster,"In this aer we show that generative adversarial networks (GANs) without restriction on the discriminative function sace commonly suffer from the roblem that the gradient roduced by the discriminator is uninformative to guide the generator. By contrast, Wasserstein GAN (WGAN), where the discriminative function is restricted to 1-Lischitz, does not suffer from such a gradient uninformativeness roblem. We further show in the aer that the model with a comact dual form of Wasserstein distance, where the Lischitz condition is relaxed, may also theoretically suffer from this issue. This imlies the imortance of Lischitz condition and motivates us to study the general formulation of GANs with Lischitz constraint, which leads to a new family of GANs that we call Lischitz GANs (LGANs). We show that LGANs guarantee the existence and uniqueness of the otimal discriminative function as well as the existence of a unique Nash equilibrium. We rove that LGANs are generally caable of eliminating the gradient uninformativeness roblem. According to our emirical analysis, LGANs are more stable and generate consistently higher quality samles comared with WGAN.
"
792,2019,HexaGAN: Generative Adversarial Nets for Real World Classification,Poster,"Most dee learning classification studies assume clean data. However, when dealing with the real world data, we encounter three roblems such as 1) missing data, 2) class imbalance, and 3) missing label roblems. These roblems undermine the erformance of a classifier. Various rerocessing techniques have been roosed to mitigate one of these roblems, but an algorithm that assumes and resolves all three roblems together has not been roosed yet. In this aer, we roose HexaGAN, a generative adversarial network framework that shows romising classification erformance for all three roblems. We interret the three roblems from a single ersective to solve them jointly. To enable this, the framework consists of six comonents, which interact with each other. We also devise novel loss functions corresonding to the architecture. The designed loss functions allow us to achieve state-of-the-art imutation erformance, with u to a 14% imrovement, and to generate high-quality class-conditional data. We evaluate the classification erformance (F1-score) of the roosed method with 20% missingness and confirm u to a 5% imrovement in comarison with the erformance of combinations of state-of-the-art methods.
"
793,2019,Graph Matching Networks for Learning the Similarity of Graph Structured Objects,Poster,"This aer addresses the challenging roblem of retrieval and matching of grah structured objects, and makes two key contributions. First, we demonstrate how Grah Neural Networks (GNN), which have emerged as an effective model for various suervised rediction roblems defined on structured data, can be trained to roduce embedding of grahs in vector saces that enables efficient similarity reasoning. Second, we roose a novel Grah Matching Network model that, given a air of grahs as inut, comutes a similarity score between them by jointly reasoning on the air through a new cross-grah attention-based matching mechanism. We demonstrate the effectiveness of our models on different domains including the challenging roblem of control-flow grah based function similarity search that lays an imortant role in the detection of vulnerabilities in software systems. The exerimental analysis demonstrates that our models are not only able to exloit structure in the context of similarity learning but they can also outerform domain secific baseline systems that have been carefully hand-engineered for these roblems.
"
794,2019,BayesNAS: A Bayesian Approach for Neural Architecture Search,Poster,"One-Shot Neural Architecture Search (NAS) is a romising method to significantly reduce search time without any searate training. It can be treated as a Network Comression roblem on the architecture arameters from an over-arameterized network. However, there are two issues associated with most one-shot NAS methods.  First, deendencies between a node and its redecessors and successors are often disregarded which result in imroer treatment over zero oerations.  Second, architecture arameters runing based on their magnitude is questionable. In this aer, we emloy the classic Bayesian learning aroach to alleviate these two issues by modeling architecture arameters using hierarchical automatic relevance determination (HARD) riors. Unlike other NAS methods, we train the over-arameterized network for only one eoch then udate the architecture. Imressively, this enabled us to find the architecture in both roxy and roxyless tasks on CIFAR-10 within only 0.2 GPU days using a single GPU. As a byroduct, our aroach can be transferred directly to comress convolutional neural networks by enforcing structural sarsity which achieves extremely sarse networks without accuracy deterioration.
"
795,2019,Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks,Poster,"Many machine learning tasks such as multile instance learning, 3D shae recognition, and few-shot image classification are defined on sets of instances. Since solutions to such roblems do not deend on the order of elements of the set, models used to address them should be ermutation invariant. We resent an attention-based neural network module, the Set Transformer, secifically designed to model interactions among elements in the inut set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce comutational comlexity, we introduce an attention scheme insired by inducing oint methods from sarse Gaussian rocess literature. It reduces the comutation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating the state-of-the-art erformance comared to recent methods for set-structured data.
"
796,2019,Shallow-Deep Networks: Understanding and Mitigating Network Overthinking,Poster,"We characterize a revalent weakness of dee neural networks (DNNs), 'overthinking', which occurs when a DNN can reach correct redictions before its final layer. Overthinking is comutationally wasteful, and it can also be destructive when, by the final layer, a correct rediction changes into a misclassification. Understanding overthinking requires studying how each rediction evolves during a DNN's forward ass, which conventionally is oaque. For rediction transarency, we roose the Shallow-Dee Network (SDN), a generic modification to off-the-shelf DNNs that introduces internal classifiers. We aly SDN to four modern architectures, trained on three image classification tasks, to characterize the overthinking roblem. We show that SDNs can mitigate the wasteful effect of overthinking with confidence-based early exits, which reduce the average inference cost by more than 50% and reserve the accuracy. We also find that the destructive effect occurs for 50% of misclassifications on natural inuts and that it can be induced, adversarially, with a recent backdooring attack. To mitigate this effect, we roose a new confusion metric to quantify the internal disagreements that will likely to lead to misclassifications.
"
797,2019,Graph U-Nets,Poster,"We consider the roblem of reresentation learning for grah data. Convolutional neural networks can naturally oerate on images, but have significant challenges in dealing with grah data. Given images are secial cases of grahs with nodes lie on 2D lattices, grah embedding tasks have a natural corresondence with image ixel-wise rediction tasks such as segmentation. While encoder-decoder architectures like U-Nets have been successfully alied on many image ixel-wise rediction tasks, similar methods are lacking for grah data. This is due to the fact that ooling and u-samling oerations are not natural on grah data. To address these challenges, we roose novel grah ooling (gPool) and unooling (gUnool) oerations in this work. The gPool layer adatively selects some nodes to form a smaller grah based on their scalar rojection values on a trainable rojection vector. We further roose the gUnool layer as the inverse oeration of the gPool layer. The gUnool layer restores the grah into its original structure using the osition information of nodes selected in the corresonding gPool layer. Based on our roosed gPool and gUnool layers, we develo an encoder-decoder model on grah, known as the grah U-Nets. Our exerimental results on node classification and grah classification tasks demonstrate that our methods achieve consistently better erformance than revious models.
"
798,2019,SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver,Poster,"Integrating logical reasoning within dee learning architectures has been a major goal of modern AI systems. In this aer, we roose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loo of larger dee learning systems. Our (aroximate) solver is based uon a fast coordinate descent aroach to solving the semidefinite rogram (SDP) associated with the MAXSAT roblem. We show how to analytically differentiate through the solution to this SDP and efficiently solve the associated backward ass. We demonstrate that by integrating this solver into end-to-end learning systems, we can learn the logical structure of challenging roblems in a minimally suervised fashion. In articular, we show that we can learn the arity function using single-bit suervision (a traditionally hard task for dee networks) and learn how to lay 9x9 Sudoku solely from examles. We also solve a ``visual Sudoku'' roblem that mas images of Sudoku uzzles to their associated logical solutions by combining our MAXSAT solver with a traditional convolutional architecture. Our aroach thus shows romise in integrating logical structures within dee learning.
"
799,2019,Area Attention,Poster,"Existing attention mechanisms are trained to attend to individual items in a collection (the memory) with a redefined, fixed granularity, e.g., a word token or an image grid. We roose area attention: a way to attend to areas in the memory, where each area contains a grou of items that are structurally adjacent, e.g., satially for a 2D memory such as images, or temorally for a 1D memory such as natural language sentences. Imortantly, the shae and the size of an area are dynamically determined via learning, which enables a model to attend to information with varying granularity. Area attention can easily work with existing model architectures such as multi-head attention for simultaneously attending to multile areas in the memory. We evaluate area attention on two tasks: neural machine translation (both character and token-level) and image cationing, and imrove uon strong (state-of-the-art) baselines in all the cases. These imrovements are obtainable with a basic form of area attention that is arameter free.
"
800,2019,The Evolved Transformer,Poster,"Recent works have highlighted the strength of the Transformer architecture on sequence tasks while, at the same time, neural architecture search (NAS) has begun to outerform human-designed models. Our goal is to aly NAS to search for a better alternative to the Transformer. We first construct a large search sace insired by the recent advances in feed-forward sequence models and then run evolutionary architecture search with warm starting by seeding our initial oulation with the Transformer. To directly search on the comutationally exensive WMT 2014 English-German translation task, we develo the Progressive Dynamic Hurdles method, which allows us to dynamically allocate more resources to more romising candidate models. The architecture found in our exeriments -- the Evolved Transformer -- demonstrates consistent imrovement over the Transformer on four well-established language tasks: WMT 2014 English-German, WMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size, the Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8 on WMT'14 English-German; at smaller sizes, it achieves the same quality as the original ""big"" Transformer with 37.6% less arameters and outerforms the Transformer by 0.7 BLEU at a mobile-friendly model size of 7M arameters.
"
801,2019,Jumpout : Improved Dropout for Deep Neural Networks with ReLUs,Poster,"We discuss three novel insights about droout for DNNs with ReLUs: 1) droout encourages each local linear iece of a DNN to be trained on data oints from nearby regions; 2) the same droout rate results in different (effective) deactivation rates for layers with different ortions of ReLU-deactivated neurons; and 3) the rescaling factor of droout causes a normalization inconsistency between training and test when used together with batch normalization. The above leads to three simle but nontrivial modifications resulting in our method ``jumout.'' Jumout samles the droout rate from a monotone decreasing distribution (e.g., the right half of a Gaussian), so each local linear iece is trained, with high robability, to work better for data oints from nearby than more distant regions. Jumout moreover adatively normalizes the droout rate at each layer and every training batch, so the effective deactivation rate on the activated neurons is ket the same. Furthermore, it rescales the oututs for a better trade-off that kees both the variance and mean of neurons more consistent between training and test hases, thereby mitigating the incomatibility between droout and batch normalization. Jumout significantly imroves the erformance of different neural nets on CIFAR10, CIFAR100, Fashion-MNIST, STL10, SVHN, ImageNet-1k, etc., while introducing negligible additional memory and comutation costs.
"
802,2019,Stochastic Deep Networks,Poster,"Machine learning is increasingly targeting areas where inut data cannot be accurately described by a single vector, but can be modeled instead using the more flexible concet of random vectors, namely robability measures or more simly oint clouds of varying cardinality. Using dee architectures on measures oses, however, many challenging issues. Indeed, dee architectures are originally designed to handle fixed-length vectors, or, using recursive mechanisms, ordered sequences thereof. In shar contrast, measures describe a varying number of weighted observations with no articular order. We roose in this work a dee framework designed to handle crucial asects of measures, namely ermutation invariances, variations in weights and cardinality. Architectures derived from this ieline can (i) ma measures to measures - using the concet of ush-forward oerators; (ii) bridge the ga between measures and Euclidean saces - through integration stes. This allows to design discriminative networks (to classify or reduce the dimensionality of inut measures), generative architectures (to synthesize measures) and recurrent ielines (to redict measure dynamics). We rovide a theoretical analysis of these building blocks, review our architectures' aroximation abilities and robustness w.r.t. erturbation, and try them on various discriminative and generative tasks.
"
803,2019,ELF OpenGo: an analysis and open reimplementation of AlphaZero,Poster,"The AlhaGo, AlhaGo Zero, and AlhaZero series of algorithms are remarkable demonstrations of dee reinforcement learning's caabilities, achieving suerhuman erformance in the comlex game of Go with rogressively increasing autonomy. However, many obstacles remain in the understanding of and usability of these romising aroaches by the research community. Toward elucidating unresolved mysteries and facilitating future research, we roose ELF OenGo, an oen-source reimlementation of the AlhaZero algorithm. ELF OenGo is the first oen-source Go AI to convincingly demonstrate suerhuman erformance with a erfect (20:0) record against global to rofessionals. We aly ELF OenGo to conduct extensive ablation studies, and to identify and analyze numerous interesting henomena in both the model training and in the gamelay inference rocedures. Our code, models, selflay datasets, and auxiliary data are ublicly available.
"
804,2019,Making Deep Q-learning methods robust to time discretization,Poster,"Desite remarkable successes, Dee Reinforce-
ment Learning (DRL) is not robust to hyeraram-
eterization, imlementation details, or small envi-
ronment changes (Henderson et al. 2017, Zhang
et al. 2018). Overcoming such sensitivity is key
to making DRL alicable to real world roblems.
In this aer, we identify sensitivity to time dis-
cretization in near continuous-time environments
as a critical factor; this covers, e.g., changing
the number of frames er second, or the action
frequency of the controller. Emirically, we find
that Q-learning-based aroaches such as Dee Q-
learning (Mnih et al., 2015) and Dee Determinis-
tic Policy Gradient (Lillicra et al., 2015) collase
with small time stes. Formally, we rove that
Q-learning does not exist in continuous time. We
detail a rinciled way to build an off-olicy RL
algorithm that yields similar erformances over
a wide range of time discretizations, and confirm
this robustness emirically.
"
805,2019,Nonlinear Distributional Gradient Temporal-Difference Learning,Poster,"We devise a distributional  variant of gradient temoral-difference (TD) learning.   Distributional reinforcement learning has been demonstrated to outerform the regular one in the recent study \cite{bellemare2017distributional}. In the olicy evaluation setting, we  design two new algorithms called  distributional GTD2  and distributional TDC  using the Cram{\'e}r distance on the distributional version of the Bellman error objective function, which inherits  advantages of both the nonlinear gradient TD algorithms and the distributional RL aroach. In the control setting, we roose the distributional Greedy-GQ using  similar derivation. We rove the asymtotic almost-sure convergence of distributional GTD2 and TDC to a local otimal solution for   general smooth function aroximators, which includes neural networks that have been widely used in recent study to solve the real-life RL roblems. In each ste, the comutational comlexity of above three algorithms is linear w.r.t.\ the number of the arameters of the function aroximator, thus can be imlemented efficiently for neural networks.
"
806,2019,Composing Entropic Policies using Divergence Correction,Poster,"Comosing skills mastered in one task to solve novel tasks romises dramatic imrovements in the data efficiency of reinforcement learning. Here, we analyze two recent works comosing behaviors reresented in the form of action-value functions and show that they erform oorly in some situations. As art of this analysis, we extend an imortant generalization of olicy imrovement to the maximum entroy framework and introduce an algorithm for the ractical imlementation of successor features in continuous action saces. Then we roose a novel aroach which addresses the failure cases of rior work and, in rincile, recovers the otimal olicy during transfer. This method works by exlicitly learning the (discounted, future) divergence between base olicies. We study this aroach in the  tabular case and on non-trivial continuous control roblems with comositional structure and show that it outerforms or matches existing methods across all tasks considered.
"
807,2019,TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning,Poster,"One of the challenges to reinforcement learning (RL) is scalable transferability among comlex tasks. Incororating a grahical model (GM), along with the rich family of related methods, as a basis for RL frameworks rovides otential to address issues such as transferability, generalisation and exloration. Here we roose a flexible GM-based RL framework which leverages efficient inference rocedures to enhance generalisation and transfer ower. In our roosed transferable and information-based grahical model framework ‘TibGM’, we show the equivalence between our mutual information-based objective in the GM, and an RL consolidated objective consisting of a standard reward maximisation target and a generalisationtransfer objective. In settings where there is a sarse or decetive reward signal, our TibGM framework is flexible enough to incororate exloration bonuses deicting intrinsic rewards. We emirically verify imroved erformance and exloration ower. 
"
808,2019,Multi-Agent Adversarial Inverse Reinforcement Learning,Poster,"Reinforcement learning agents are rone to undesired behaviors due to reward mis-secification. Finding a set of reward functions to roerly guide agent behaviors is articularly challenging in multi-agent scenarios. Inverse reinforcement learning rovides a framework to automatically acquire suitable reward functions from exert demonstrations. Its extension to multi-agent settings, however, is difficult due to the more comlex notions of rational behaviors. In this aer, we roose MA-AIRL, a new framework for multi-agent inverse reinforcement learning, which is effective and scalable for Markov games with high-dimensional state-action sace and unknown dynamics. We derive our algorithm based on a new solution concet and maximum seudolikelihood estimation within an adversarial reward learning framework. In the exeriments, we demonstrate that MA-AIRL can recover reward functions that are highly correlated with the ground truth rewards, while significantly outerforms rior methods in terms of olicy imitation.
"
809,2019,Policy Consolidation for Continual Reinforcement Learning,Poster,"We roose a method for tackling catastrohic forgetting in dee reinforcement learning that is \textit{agnostic} to the timescale of changes in the distribution of exeriences, does not require knowledge of task boundaries and can adat in \textit{continuously} changing environments. In our \textit{olicy consolidation} model, the olicy network interacts with a cascade of hidden networks that simultaneously remember the agent's olicy at a range of timescales and regularise the current olicy by its own history, thereby imroving its ability to learn without forgetting. We find that the model imroves continual learning relative to baselines on a number of continuous control tasks in single-task, alternating two-task, and multi-agent cometitive self-lay settings.
"
810,2019,Off-Policy Deep Reinforcement Learning without Exploration,Poster,"Many ractical alications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further ossibility for data collection. In this aer, we demonstrate that due to errors introduced by extraolation, standard off-olicy dee reinforcement learning algorithms, such as DQN and DDPG, are incaable of learning with data uncorrelated to the distribution under the current olicy, making them ineffective for this fixed batch setting. We introduce a novel class of off-olicy algorithms, batch-constrained reinforcement learning, which restricts the action sace in order to force the agent towards behaving close to on-olicy with resect to a subset of the given data. We resent the first continuous control dee reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and emirically demonstrate the quality of its behavior in several tasks.
"
811,2019,Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation,Poster,"We consider the roblem of imitation learning from a finite set of exert trajectories, without access to reinforcement signals. The classical aroach of extracting the exert's reward function via inverse reinforcement learning, followed by reinforcement learning is indirect and may be comutationally exensive. Recent generative adversarial methods based on matching the olicy distribution between the exert and the agent could be unstable during training. We roose a new framework for imitation learning by estimating the suort of the exert olicy to comute a fixed reward function, which allows us to re-frame imitation learning within the standard reinforcement learning setting. We demonstrate the efficacy of our reward function on both discrete and continuous domains, achieving comarable or better erformance than the state of the art under different reinforcement learning algorithms.
"
812,2019,Revisiting the Softmax Bellman Operator: New Benefits and New Perspective,Poster,"The imact of softmax on the value function itself in reinforcement learning (RL) is often viewed as roblematic because it leads to sub-otimal value (or Q) functions and interferes with the contraction roerties of the Bellman oerator. Surrisingly, desite these concerns, and indeendent of its effect on exloration, the softmax Bellman oerator when combined with Dee Q-learning, leads to Q-functions with suerior olicies in ractice, even outerforming its double Q-learning counterart. To better understand how and why this occurs, we revisit theoretical roerties of the softmax Bellman oerator, and rove that (i) it converges to the standard Bellman oerator exonentially fast in the inverse temerature arameter, and (ii) the distance of its Q function from the otimal one can be bounded. These alone do not exlain its suerior erformance, so we also show that the softmax oerator can reduce the overestimation error, which may give some insight into why a sub-otimal oerator leads to better erformance in the resence of value function aroximation. A comarison among different Bellman oerators is then resented, showing the trade-offs when selecting them.
"
813,2019,An Investigation of Model-Free Planning,Poster,"The field of reinforcement learning (RL) is facing increasingly challenging domains with combinatorial comlexity. For an RL agent to address these challenges, it is essential that it can lan effectively. Prior work has tyically utilized an exlicit model of the environment, combined with a secific lanning algorithm (such as tree search). More recently, a new family of methods have been roosed that learn how to lan, by roviding the structure for lanning via an inductive bias in the function aroximator (such as a tree structured neural network), trained end-to-end by a model-free RL algorithm. In this aer, we go even further, and demonstrate emirically that an entirely model-free aroach, without secial structure beyond standard neural network comonents such as convolutional networks and LSTMs, can learn to exhibit many of the characteristics tyically associated with a model-based lanner. We measure our agent's effectiveness at lanning in terms of its ability to generalize across a combinatorial and irreversible state sace, its data efficiency, and its ability to utilize additional thinking time. We find that our agent has many of the characteristics that one might exect to find in a lanning algorithm. Furthermore, it exceeds the state-of-the-art in challenging combinatorial domains such as Sokoban and outerforms other model-free aroaches that utilize strong inductive biases toward lanning.
"
814,2019,CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning,Poster,"In oen-ended environments, autonomous learning agents must set their own goals and build their own curriculum through an intrinsically motivated exloration. They may consider a large diversity of goals, aiming to discover what is controllable in their environments, and what is not. Because some goals might rove easy and some imossible, agents must actively select which goal to ractice at any moment, to maximize their overall mastery on the set of learnable goals. This aer rooses CURIOUS , an algorithm that leverages 1) a modular Universal Value Function Aroximator with hindsight learning to achieve a diversity of goals of different kinds within a unique olicy and 2) an automated curriculum learning mechanism that biases the attention of the agent towards goals maximizing the absolute learning rogress. Agents focus sequentially on goals of increasing comlexity, and focus back on goals that are being forgotten. Exeriments conducted in a new modular-goal robotic environment show the resulting develomental self-organization of a learning curriculum, and demonstrate roerties of robustness to distracting goals, forgetting and changes in body roerties.
"
815,2019,Task-Agnostic Dynamics Priors for Deep Reinforcement Learning,Poster,"While model-based dee reinforcement learning (RL) holds great romise for samle efficiency and generalization, learning an accurate dynamics model is often challenging and requires substantial interaction with the environment. A wide variety of domains have dynamics that share common foundations like the laws of classical mechanics, which are rarely exloited by existing algorithms. In fact, humans continuously acquire and use such dynamics riors to easily adat to oerating in new environments. In this work, we roose an aroach to learn task-agnostic dynamics riors from videos and incororate them into an RL agent. Our method involves re-training a frame redictor on task-agnostic hysics videos to initialize dynamics models (and fine-tune them) for unseen target environments. Our frame rediction architecture, SatialNet, is designed secifically to cature localized hysical henomena and interactions. Our aroach allows for both faster olicy learning and convergence to better olicies, outerforming cometitive aroaches on several different environments. We also demonstrate that incororating this rior allows for more effective transfer between environments.
"
816,2019,Diagnosing Bottlenecks in Deep Q-learning Algorithms,Poster,"Q-learning methods are a common class of algorithms used in reinforcement learning (RL). However, their behavior with function aroximation, esecially with neural networks, is oorly understood theoretically and emirically. In this work, we aim to exerimentally investigate otential issues in Q-learning, by means of a ""unit testing"" framework where we can utilize oracles to disentangle sources of error. 
Secifically, we investigate questions related to function aroximation, samling error and nonstationarity, and where available, verify if trends found in oracle settings hold true with dee RL methods. We find that large neural network architectures have many benefits with regards to learning stability; offer several ractical comensations for overfitting; and develo a novel samling method based on exlicitly comensating for function aroximation error that yields fair imrovement on high-dimensional continuous control domains. 
"
817,2019,Collaborative Evolutionary Reinforcement Learning,Poster,"Dee reinforcement learning algorithms have been successfully alied to a range of challenging control tasks. However, these methods tyically struggle with achieving effective exloration and are extremely sensitive to the choice of hyerarameters. One reason is that most aroaches use a noisy version of their oerating olicy to exlore - thereby limiting the range of exloration. In this aer, we introduce Collaborative Evolutionary Reinforcement Learning (CERL), a scalable framework that comrises a ortfolio of olicies that simultaneously exlore and exloit diverse regions of the solution sace. A collection of learners - tyically roven algorithms like TD3 - otimize over varying time-horizons leading to this diverse ortfolio. All learners contribute to and use a shared relay buffer to achieve greater samle efficiency. Comutational resources are dynamically distributed to favor the best learners as a form of online algorithm selection. Neuroevolution binds this entire rocess to generate a single emergent learner that exceeds the caabilities of any individual learner. Exeriments in a range of continuous control benchmarks demonstrate that the emergent learner significantly outerforms its comosite learners while remaining overall more samle-efficient - notably solving the Mujoco Humanoid benchmark where all of its comosite learners (TD3) fail entirely in isolation.
"
818,2019,EMI: Exploration with Mutual Information,Poster,"Reinforcement learning algorithms struggle when the reward signal is very sarse. In these cases, naive random exloration methods essentially rely on a random walk to stumble onto a rewarding state. Recent works utilize intrinsic motivation to guide the exloration via generative models, redictive forward models, or discriminative modeling of novelty. We roose EMI, which is an exloration method that constructs embedding reresentation of states and actions that does not rely on generative decoding of the full observation but extracts redictive signals that can be used to guide exloration based on forward rediction in the reresentation sace. Our exeriments show cometitive results on challenging locomotion tasks with continuous control and on image-based exloration tasks with discrete actions on Atari. The source code is available at htts:github.comsnu-mllabEMI.
"
819,2019,Imitation Learning from Imperfect Demonstration,Poster,"Imitation learning (IL) aims to learn an otimal olicy from demonstrations. However, such demonstrations are often imerfect since collecting otimal ones is costly. To effectively learn from imerfect demonstrations, we roose a novel aroach that utilizes confidence scores, which describe the quality of demonstrations. More secifically, we roose two confidence-based IL methods, namely two-ste imortance weighting IL (2IWIL) and generative adversarial IL with imerfect demonstration and confidence (IC-GAIL). We show that confidence scores given only to a small ortion of sub-otimal demonstrations significantly imrove the erformance of IL both theoretically and emirically.
"
820,2019,Curiosity-Bottleneck: Exploration By Distilling Task-Specific Novelty ,Poster,"Exloration based on state novelty has brought great success in challenging reinforcement learning roblems with sarse rewards. However, existing novelty-based strategies become inefficient in real-world roblems where observation contains not only task-deendent state novelty of our interest but also task-irrelevant information that should be ignored. We introduce an information- theoretic exloration strategy named Curiosity-Bottleneck that distills task-relevant information from observation. Based on the information bottleneck rincile, our exloration bonus is quantified as the comressiveness of observation with resect to the learned reresentation of a comressive value network. With extensive exeriments on static image classification, grid-world and three hard-exloration Atari games, we show that Curiosity-Bottleneck learns an effective exloration strategy by robustly measuring the state novelty in distractive environments where state-of-the-art exloration methods often degenerate.
"
821,2019,Dynamic Weights in Multi-Objective Deep Reinforcement Learning,Poster,"Many real-world decision roblems are characterized by multile conflicting objectives which must be balanced based on their relative imortance. In the dynamic weights setting the relative imortance changes over time and secialized algorithms that deal with such change, such as a tabular Reinforcement Learning (RL) algorithm by Natarajan and Tadealli (2005), are required. However, this earlier work is not feasible for RL settings that necessitate the use of function aroximators. We generalize across weight changes and high-dimensional inuts by roosing a multi-objective Q-network whose oututs are conditioned on the relative imortance of objectives and we introduce Diverse Exerience Relay (DER) to counter the inherent non-stationarity of the Dynamic Weights setting. We erform an extensive exerimental evaluation and comare our methods to adated algorithms from Dee Multi-TaskMulti-Objective Reinforcement Learning and show that our roosed network in combination with DER dominates these adated algorithms across weight change scenarios and roblem domains.
"
822,2019,Fingerprint Policy Optimisation for Robust Reinforcement Learning,Poster,"Policy gradient methods ignore the otential value of adjusting environment variables: unobservable state features that are randomly determined by the environment in a hysical setting, but are controllable in a simulator. This can lead to slow learning, or convergence to subotimal olicies, if the environment variable has a large imact on the transition dynamics. In this aer, we resent fingerrint olicy otimisation (FPO), which finds a olicy that is otimal in exectation across the distribution of environment variables. The central idea is to use Bayesian otimisation (BO) to actively select the distribution of the environment variable that maximises the imrovement generated by each iteration of the olicy gradient method. To make this BO ractical, we contribute two easy-to-comute low-dimensional fingerrints of the current olicy. Our exeriments show that FPO can efficiently learn olicies that are robust to significant rare events, which are unlikely to be observable under random samling, but are key to learning good olicies.
"
823,2019,An Investigation into Neural Net Optimization via Hessian Eigenvalue Density,Poster,"To understand the dynamics of training in dee neural networks, we study the evolution of the Hessian eigenvalue density throughout the otimization rocess. In non-batch normalized networks, we observe the raid aearance of large isolated eigenvalues in the sectrum, along with a surrising concentration of the gradient in the corresonding eigensaces. In a batch normalized network, these two effects are almost absent. We give a theoretical rationale to artially exlain these henomena. As art of this work, we adat advanced tools from numerical linear algebra that allow scalable and accurate estimation of the entire Hessian sectrum of ImageNet-scale neural networks; this technique may be of indeendent interest in other alications. 
"
824,2019,Differentiable Linearized ADMM,Poster,"Recently, a number of learning-based otimization methods that combine data-driven architectures with the classical otimization algorithms have been roosed and exlored, showing suerior emirical erformance in solving various ill-osed inverse roblems, but there is still a scarcity of rigorous analysis about the convergence behaviors of learning-based otimization. In articular, most existing analyses are secific to unconstrained roblems but cannot aly to the more general cases where some variables of interest are subject to certain constraints. In this aer, we roose Differentiable Linearized ADMM (D-LADMM) for solving the roblems with linear constraints. Secifically, D-LADMM is a K-layer LADMM insired dee neural network, which is obtained by firstly introducing some learnable weights in the classical Linearized ADMM algorithm and then generalizing the roximal oerator to some learnable activation function. Notably, we rigorously rove that there exist a set of learnable arameters for D-LADMM to generate globally converged solutions, and we show that those desired arameters can be attained by training D-LADMM in a roer way. To the best of our knowledge, we are the first to rovide the convergence analysis for the learning-based otimization method on constrained roblems.
"
825,2019,Adaptive Stochastic Natural Gradient Method for One-Shot Neural Architecture Search,Poster,"High sensitivity of neural architecture search (NAS) methods against their inut such as ste-size (i.e., learning rate) and search sace revents ractitioners from alying them out-of-the-box to their own roblems, albeit its urose is to automate a art of tuning rocess. Aiming at a fast, robust, and widely-alicable NAS, we develo a generic otimization framework for NAS. We turn a couled otimization of connection weights and neural architecture into a differentiable otimization by means of stochastic relaxation. It accets arbitrary search sace (widely-alicable) and enables to emloy a gradient-based simultaneous otimization of weights and architecture (fast). We roose a stochastic natural gradient method with an adative ste-size mechanism built uon our theoretical investigation (robust). Desite its simlicity and no roblem-deendent arameter tuning, our method exhibited near state-of-the-art erformances with low comutational budgets both on image classification and inainting tasks.
"
826,2019,A Quantitative Analysis of the Effect of Batch Normalization on Gradient Descent,Poster,"Desite its emirical success and recent theoretical rogress, there generally lacks a quantitative analysis of the effect of batch normalization (BN) on the convergence and stability of gradient descent. In this aer, we rovide such an analysis on the simle roblem of ordinary least squares (OLS), where the recise dynamical roerties of gradient descent (GD) is comletely known, thus allowing us to isolate and comare the additional effects of BN. More recisely, we show that unlike GD, gradient descent with BN (BNGD) converges for arbitrary learning rates for the weights, and the convergence remains linear under mild conditions. Moreover, we quantify two different sources of acceleration of BNGD over GD -- one due to over-arameterization which imroves the effective condition number and another due having a large range of learning rates giving rise to fast descent. These henomena set BNGD aart from GD and could account for much of its robustness roerties. These findings are confirmed quantitatively by numerical exeriments, which further show that many of the uncovered roerties of BNGD in OLS are also observed qualitatively in more comlex suervised learning roblems.
"
827,2019,The Effect of Network Width on Stochastic Gradient Descent and Generalization: an Empirical Study,Poster,"We investigate how the final arameters found by stochastic gradient descent are influenced by over-arameterization. We generate families of models by increasing the number of channels in a base network, and then erform a large hyer-arameter search to study how the test error deends on learning rate, batch size, and network width. We find that the otimal SGD hyer-arameters are determined by a ""normalized noise scale,"" which is a function of the batch size, learning rate, and initialization conditions. In the absence of batch normalization, the otimal normalized noise scale is directly roortional to width. Wider networks, with their higher otimal noise scale, also achieve higher test accuracy. These observations hold for MLPs, ConvNets, and ResNets, and for two different arameterization schemes (""Standard"" and ""NTK""). We observe a similar trend with batch normalization for ResNets. Surrisingly, since the largest stable learning rate is bounded, the largest batch size consistent with the otimal normalized noise scale decreases as the width increases.
"
828,2019,AdaGrad stepsizes: sharp convergence over nonconvex landscapes,Poster,"Adative gradient methods such as AdaGrad and its variants udate the stesize in stochastic gradient descent on the fly according to the gradients received along the way; such methods have gained widesread use in large-scale otimization for their ability to converge robustly, without the need to fine-tune
arameters such as the stesize schedule. Yet, the theoretical guarantees to date for AdaGrad are for online and convex otimization.   We bridge this ga by roviding strong theoretical guarantees for the convergence of AdaGrad over smooth, nonconvex landscaes. We show that the norm version of AdaGrad (AdaGrad-Norm) converges to a stationary oint at the $\mathcal{O}(\log(N)\sqrt{N})$ rate in the stochastic setting, and at the otimal $\mathcal{O}(1N)$ rate in the batch (non-stochastic) setting -- in this sense, our convergence guarantees are ``shar''. In articular, both our theoretical results and extensive numerical exeriments imly that AdaGrad-Norm is robust to the \emh{unknown Lischitz constant and level of stochastic noise on the gradient}."
829,2019,Beyond Backprop: Online Alternating Minimization with Auxiliary Variables,Poster,"Desite significant recent advances in dee neural networks,  training them remains a challenge due to the highly non-convex nature of the objective function.  State-of-the-art methods rely on error backroagation, which suffers from   several well-known issues, such as vanishing and exloding gradients, inability to handle non-differentiable nonlinearities and to arallelize weight-udates across layers, and biological imlausibility. These limitations continue to motivate exloration of alternative training algorithms,   including several recently roosed auxiliary-variable methods  which break the comlex nested objective function into local subroblems. However, those techniques are mainly offline (batch), which limits their alicability to   extremely large datasets, as well as to online, continual or reinforcement learning.  The main contribution of our work is  a    novel online (stochasticmini-batch) alternating minimization (AM) aroach  for training dee neural networks, together with the first theoretical convergence guarantees for AM in stochastic settings and romising emirical results  on a variety of architectures  and datasets.
"
830,2019,SWALP : Stochastic Weight Averaging in Low Precision Training,Poster,"Low recision oerations can rovide scalability, memory savings, ortability, and energy efficiency. This aer rooses SWALP, an aroach to low recision training that averages low-recision SGD iterates with a modified learning rate schedule. SWALP is easy to imlement and can match the erformance of full-recision SGD even with all numbers quantized down to 8 bits, including the gradient accumulators. Additionally, we show that SWALP converges arbitrarily close to the otimal solution for quadratic objectives, and to a noise ball asymtotically smaller than low recision SGD in strongly convex settings. 
"
831,2019,Efficient optimization of loops and limits with randomized telescoping sums,Poster,"We consider otimization roblems in which the objective requires an inner loo with many stes or is the limit of a sequence of increasingly costly aroximations.
Meta-learning, training recurrent neural networks, and otimization of the solutions to differential equations are all examles of otimization roblems with this character.
In such roblems, it can be exensive to comute the objective function value and its gradient, but truncating the loo or using less accurate aroximations can induce biases that damage the overall solution.
We roose \emh{randomized telescoe} (RT) gradient estimators, which reresent the objective as the sum of a telescoing series and samle linear combinations of terms to rovide chea unbiased gradient estimates.
We identify conditions under which RT estimators achieve otimization convergence rates indeendent of the length of the loo or the required accuracy of the aroximation.
We also derive a method for tuning RT estimators online to maximize a lower bound on the exected decrease in loss er unit of comutation.
We evaluate our adative RT estimators on a range of alications including meta-otimization of learning rates, variational inference of ODE arameters, and training an LSTM to model long sequences.
"
832,2019,Self-similar Epochs: Value in arrangement,Poster,"Otimization of machine learning models is commonly erformed through stochastic gradient udates on randomly ordered training examles. This ractice means that each fraction of an eoch comrises an indeendent random samle of the training data that may not reserve informative structure resent in the full data.  We hyothesize that the training can be more effective with {\it self-similar}  arrangements that otentially allow each eoch to  rovide benefits of multile ones. We study this for  ``matrix factorization'' -- the common task of learning metric embeddings of entities such as queries, videos,  or words from examle airwise associations. We construct arrangements that reserve the weighted Jaccard similarities of  rows and columns and exerimentally observe training acceleration of 3\%-37\% on synthetic and recommendation datasets.  Princiled arrangements of training examles emerge as a novel and otentially owerful enhancement to SGD that merits further exloration.
"
833,2019,Adversarial Attacks on Node Embeddings via Graph Poisoning,Poster,"The goal of network reresentation learning is to learn low-dimensional node embeddings that cature the grah structure and are useful for solving downstream tasks. However, desite the roliferation of such methods, there is currently no study of their robustness to adversarial attacks. We rovide the first adversarial vulnerability analysis on the widely used family of methods based on random walks. We derive efficient adversarial erturbations that oison the network structure and have a negative effect on both the quality of the embeddings and the downstream tasks. We further show that our attacks are transferable since they generalize to many models and are successful even when the attacker is restricted.
"
834,2019,First-Order Adversarial Vulnerability of Neural Networks and Input Dimension,Poster,"Over the ast few years, neural networks were roven vulnerable to adversarial images: targeted but imercetible image erturbations lead to drastically different redictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inuts. Surrisingly, vulnerability does not deend on network toology: for many standard network architectures, we rove that at initialization, the L1-norm of these gradients grows as the square root of the inut dimension, leaving the networks increasingly vulnerable with growing image size. We emirically show that this dimension-deendence ersists after either usual or robust training, but gets attenuated with higher regularization.
"
835,2019,On Certifying Non-Uniform Bounds against Adversarial Attacks,Poster,"This work studies the robustness certification roblem of neural network models,
which aims to find certified adversary-free regions as large as ossible around data oints.
In contrast to the existing aroaches that seek regions bounded uniformly along all inut features,
we consider non-uniform bounds and use it to study the decision boundary of neural network models.
We formulate our target as an otimization roblem with nonlinear constraints.
Then, a framework alicable for general feedforward neural networks is roosed to bound the outut logits
so that the relaxed roblem can be solved by the augmented Lagrangian method.
Our exeriments show the non-uniform bounds have larger volumes than uniform ones.
Comared with normal models, the robust models have even larger non-uniform bounds and better interretability.
Further, the geometric similarity of the non-uniform bounds gives a quantitative, data-agnostic metric of inut features' robustness.
"
836,2019,Improving Adversarial Robustness via Promoting Ensemble Diversity,Poster,"Though dee neural networks have achieved significant rogress on various tasks, often enhanced by model ensemble, existing high-erformance models can be vulnerable to adversarial attacks. Many efforts have been devoted to enhancing the robustness of individual networks and then constructing a straightforward ensemble, e.g., by directly averaging the oututs, which ignores the interaction among networks. This aer resents a new method that exlores the interaction among individual networks to imrove robustness for ensemble models. Technically, we define a new notion of ensemble diversity in the adversarial setting as the diversity among non-maximal redictions of individual members, and resent an adative diversity romoting (ADP) regularizer to encourage the diversity, which leads to globally better robustness for the ensemble by making adversarial examles difficult to transfer among individual members. Our method is comutationally efficient and comatible with the defense methods acting on individual networks. Emirical results on various datasets verify that our method can imrove adversarial robustness while maintaining state-of-the-art accuracy on normal examles.
"
837,2019,Adversarial camera stickers: A physical camera-based attack on deep learning systems,Poster,"Recent work has documented the suscetibility of dee learning systems to adversarial examles, but most such attacks directly maniulate the digital inut to a classifier. Although a smaller line of work considers hysical adversarial attacks, in all cases these involve maniulating the object of interest, e.g., utting a hysical sticker on an object to misclassify it, or manufacturing an object secifically intended to be misclassified. In this work, we consider an alternative question: is it ossible to fool dee classifiers, over all erceived objects of a certain tye, by hysically maniulating the camera itself? We show that by lacing a carefully crafted and mainly-translucent sticker over the lens of a camera, one can create universal erturbations of the observed images that are inconsicuous, yet misclassify target objects as a different (targeted) class. To accomlish this, we roose an iterative rocedure for both udating the attack erturbation (to make it adversarial for a given classifier), and the threat model itself (to ensure it is hysically realizable). For examle, we show that we can achieve hysically-realizable attacks that fool ImageNet classifiers in a targeted fashion 49.6% of the time. This resents a new class of hysically-realizable threat models to consider in the context of adversarially robust machine learning. Our demo video can be viewed at: htts:youtu.bewUVmL33Fx54
"
838,2019,Adversarial examples from computational constraints,Poster,"Why are classifiers in high dimension vulnerable to “adversarial” erturbations? We show that it is likely not due to information theoretic limitations, but rather it could be due to comutational constraints.
First we rove that, for a broad set of classification tasks, the mere existence of a robust classifier imlies that it can be found by a ossibly exonential-time algorithm with relatively few training examles. Then we give two articular classification tasks where learning a robust classifier is comutationally intractable. More recisely we construct two binary classifications task in high dimensional sace which are (i) information theoretically easy to learn robustly for large erturbations, (ii) efficiently learnable (non-robustly) by a simle linear searator,
(iii) yet are not efficiently robustly learnable, even for small erturbations. Secifically, for the first task hardness holds for any efficient algorithm in the statistical query (SQ) model, while for the second task we rule out any efficient algorithm under a crytograhic assumtion. These examles give an exonential searation between classical learning and robust learning in the statistical query model or under a crytograhic assumtion. It suggests that adversarial examles may be an unavoidable byroduct of comutational limitations of learning algorithms. 
"
839,2019,POPQORN: Quantifying Robustness of Recurrent Neural Networks,Poster,"The vulnerability to adversarial attacks has been a critical issue for dee neural networks. Addressing this issue requires a reliable way to evaluate the robustness of a network. Recently, several methods have been develoed to comute robustness quantification for neural networks, namely, certified lower bounds of the minimum adversarial erturbation. Such methods, however, were devised for feed-forward networks, e.g. multi-layer ercetron or convolutional networks. It remains an oen roblem to quantify robustness for recurrent networks, esecially LSTM and GRU. 
For such networks, there exist additional challenges in comuting the robustness quantification, such as handling the inuts at multile stes and the interaction between gates and states. In this work, we roose POPQORN (Proagated-outut Quantified Robustness for RNNs), a general algorithm to quantify robustness of RNNs, including vanilla RNNs, LSTMs, and GRUs. We demonstrate its effectiveness on different network architectures and show that the robustness quantification on individual stes can lead to new insights.
"
840,2019,Using Pre-Training Can Improve Model Robustness and Uncertainty,Poster,"He et al. (2018) have called into question the utility of re-training by showing that training from scratch can often yield similar erformance to re-training. We show that although re-training may not imrove erformance on traditional classification metrics, it imroves model robustness and uncertainty estimates. Through extensive exeriments on label corrution, class imbalance, adversarial examles, out-of-distribution detection, and confidence calibration, we demonstrate large gains from re-training and comlementary effects with task-secific methods. We show aroximately a 10% absolute imrovement over the revious state-of-the-art in adversarial robustness. In some cases, using re-training without task-secific methods also surasses the state-of-the-art, highlighting the need for re-training when evaluating future methods on robustness and uncertainty tasks.
"
841,2019,Generalized No Free Lunch Theorem for Adversarial Robustness,Poster,"This manuscrit resents some new imossibility results on adversarial robustness
in machine learning, a very imortant yet largely oen roblem. We show that if conditioned on a class label the data distribution satisfies the $W_2$ Talagrand
transortation-cost inequality (for examle, this condition is satisfied if the conditional distribution has density which is log-concave; is the uniform measure on a comact Riemannian manifold with ositive Ricci curvature, any classifier can be adversarially fooled with high robability once the erturbations are slightly greater than the natural noise level in the roblem. We call this result The Strong ""No Free Lunch"" Theorem as some recent results (Tsiras et al. 2018, Fawzi et al. 2018, etc.) on the subject can be immediately recovered as very articular cases. Our theoretical bounds are demonstrated on both simulated and real data (MNIST). We conclude the manuscrit with some seculation on ossible future research directions."
842,2019,PROVEN: Verifying Robustness of Neural Networks with a Probabilistic Approach,Poster," We roose a novel framework PROVEN to \textbf{PRO}babilistically \textbf{VE}rify \textbf{N}eural network's robustness with statistical guarantees. PROVEN rovides robability certificates of neural network robustness when the inut erturbation follow distributional characterization. Notably, PROVEN is derived from current state-of-the-art worst-case neural network robustness verification frameworks, and therefore it can rovide robability certificates with little comutational overhead on to of existing  methods such as Fast-Lin, CROWN and CNN-Cert. Exeriments on small and large MNIST and CIFAR neural network models demonstrate our robabilistic aroach can tighten u robustness certificate to around $1.8 \times$ and $3.5 \times$ with at least a $99.99\%$ confidence comared with the worst-case robustness certificate by CROWN and CNN-Cert.  "
843,2019,On Learning Invariant Representations for Domain Adaptation,Poster,"recodeDue to the ability of dee neural nets to learn rich reresentations, recent advances in unsuervised domain adatation have focused on learning domain-invariant features that achieve a small error on the source domain. The hoe is that the learnt reresentation, together with the hyothesis learnt from the source domain, can generalize to the target domain. In this aer, we first construct a simle counterexamle showing that, contrary to common belief, the above conditions are not sufficient to guarantee successful domain adatation. In articular, the counterexamle exhibits \emh{conditional shift}: the class-conditional distributions of inut features change between source and target domains. To give a sufficient condition for domain adatation, we roose a natural and interretable generalization uer bound that exlicitly takes into account the aforementioned shift. Moreover, we shed new light on the roblem by roving an information-theoretic lower bound on the joint error of \emh{any} domain adatation method that attemts to learn invariant reresentations. Our result characterizes a fundamental tradeoff between learning invariant reresentations and achieving small joint error on both domains when the marginal label distributions differ from source to target. Finally, we conduct exeriments on real-world datasets that corroborate our theoretical findings. We believe these insights are helful in guiding the future design of domain adatation and reresentation learning algorithms.
codere
"
844,2019,Lexicographic and Depth-Sensitive Margins in Homogeneous and Non-Homogeneous Deep Models,Poster,"With an eye toward understanding comlexity control in dee learning, we study how infinitesimal regularization or gradient descent otimization lead to margin maximizing solutions in both homogeneous and non homogeneous models, extending revious work that focused on infinitesimal regularization only in homogeneous models.  To this end we study the limit of loss minimization with a diverging norm constraint (the codeconstrained ath''), relate it to the limit of acodemargin ath'' and characterize the resulting solution.  For non-homogeneous ensemble models, which outut is a sum of homogeneous sub-models, we show that this solution discards the shallowest sub-models if they are unnecessary. For homogeneous models, we show convergence to a ``lexicograhic max-margin solution'', and rovide conditions under which max-margin solutions are also attained as the limit of unconstrained gradient descent.
"
845,2019,Adversarial Generation of Time-Frequency Features with application in audio synthesis,Poster,"Time-frequency (TF) reresentations rovide owerful and intuitive features for the analysis of time series such as audio. But still, generative modeling of audio in the TF domain is a subtle matter. Consequently, neural audio synthesis widely relies on directly modeling the waveform and revious attemts at unconditionally synthesizing audio from neurally generated invertible TF features still struggle to roduce audio at satisfying quality. In this article, focusing on the short-time Fourier transform, we discuss the challenges that arise in audio synthesis based on generated invertible TF features and how to overcome them. We demonstrate the otential of deliberate generative TF modeling by training a generative adversarial network (GAN) on short-time Fourier features. We show that by alying our guidelines, our TF-based network was able to outerform a state-of-the-art GAN generating waveforms directly, desite the similar architecture in the two networks. 
"
846,2019,On the Universality of Invariant Networks,Poster,"Constraining linear layers in neural networks to resect symmetry transformations from a grou $G$ is a common design rincile for invariant networks that has found many alications in machine learning. 		
In this aer, we consider a fundamental question that has received very little attention to date: Can these networks aroximate any (continuous) invariant function? 		
We tackle the rather general case where $G\leq S_n$ (an arbitrary subgrou of the symmetric grou) that acts on $\R^n$ by ermuting coordinates. This setting includes several recent oular invariant networks. We resent two main results: First, $G$-invariant networks are universal if high-order tensors are allowed. Second, there are grous $G$ for which higher-order tensors are unavoidable for obtaining universality. 		
$G$-invariant networks consisting of only first-order tensors are of secial interest due to their ractical value. We conclude the aer by roving a necessary condition for the universality of $G$-invariant networks that incororate only first-order tensors. Lastly, we roose a conjecture stating that this condition is also sufficient. "
847,2019,Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks,Poster,"Recent works have cast some light on the mystery of why dee nets fit any data and generalize desite being very overarametrized. This aer analyzes training and generalization for a simle 2-layer ReLU net with random initialization, and rovides the following imrovements over recent works:
(i) Using a tighter characterization of training seed than recent aers, an exlanation for why training a neural net with random labels leads to slower training, as originally observed in [Zhang et al. ICLR'17].
(ii) Generalization bound indeendent of network size, using a data-deendent comlexity measure. Our measure distinguishes clearly between random labels and true labels on MNIST and CIFAR, as shown by exeriments. Moreover, recent aers require samle comlexity to increase (slowly) with the size, while our samle comlexity is comletely indeendent of the network size.
(iii) Learnability of a broad class of smooth functions by 2-layer ReLU nets trained via gradient descent.
The key idea is to track dynamics of training and generalization via roerties of a related kernel.
"
848,2019,Gauge Equivariant Convolutional Networks and the Icosahedral CNN,Poster,"The rincile of equivariance to symmetry transformations enables a theoretically grounded aroach to neural network architecture design. Equivariant networks have shown excellent erformance and data efficiency on vision and medical imaging roblems that exhibit symmetries. Here we show how this rincile can be extended beyond global symmetries to local gauge transformations. This enables the develoment of a very general class of convolutional neural networks on manifolds that deend only on the intrinsic geometry, and which includes many oular methods from equivariant and geometric dee learning.

We imlement gauge equivariant CNNs for signals defined on the surface of the icosahedron, which rovides a reasonable aroximation of the shere. By choosing to work with this very regular manifold, we are able to imlement the gauge equivariant convolution using a single conv2d call, making it a highly scalable and ractical alternative to Sherical CNNs. Using this method, we demonstrate substantial imrovements over revious methods on the task of segmenting omnidirectional images and global climate atterns.
"
849,2019,Feature-Critic Networks for Heterogeneous Domain Generalization,Poster,"The well known domain shift issue causes model erformance to degrade when deloyed to a new target domain with different statistics to training. Domain adatation techniques alleviate this, but need some instances from the target domain to drive adatation. Domain generalisation is the recently toical roblem of learning a model that generalises to unseen domains out of the box, and various aroaches aim to train a domain-invariant feature extractor, tyically by adding some manually designed losses. In this work, we roose a learning to learn aroach, where the auxiliary loss that hels generalisation is itself learned. Beyond conventional domain generalisation, we consider a more challenging setting of heterogeneous domain generalisation, where the unseen domains do not share label sace with the seen ones, and the goal is to train a feature reresentation that is useful off-the-shelf for novel data and novel categories. Exerimental evaluation demonstrates that our method outerforms state-of-the-art solutions in both settings.
"
850,2019,Learning to Convolve: A Generalized Weight-Tying Approach,Poster,"Recent work (Cohen &am; Welling, 2016) has shown that generalizations of convolutions, based on grou theory, rovide owerful inductive biases for learning. In these generalizations, filters are not only translated but can also be rotated, flied, etc. However, coming u with exact models of how to rotate a 3x3 filter on a square ixel-grid is difficult. In this aer, we learn how to transform filters for use in the grou convolution, focussing on roto-translation. For this, we learn a filter basis and all rotated versions of that filter basis. Filters are then encoded by a set of rotation invariant coefficients. To rotate a filter, we switch the basis. We demonstrate we can roduce feature mas with low sensitivity to inut rotations, while achieving high erformance on MNIST and CIFAR-10.
"
851,2019,On Dropout and Nuclear Norm Regularization,Poster,"We give a formal and comlete characterization of the exlicit regularizer induced by droout in dee linear networks with squared loss. We show that (a) the exlicit regularizer is comosed of an $\ell_2$-ath regularizer and other terms that are also re-scaling invariant, (b) the convex enveloe of the induced regularizer is the squared nuclear norm of the network ma, and (c) for a sufficiently large droout rate, we characterize the global otima of the droout objective. We validate our theoretical findings with emirical results."
852,2019,Gradient Descent Finds Global Minima of Deep Neural Networks,Poster,"Gradient descent finds a global minimum in training dee neural networks desite the objective function being non-convex. The current aer roves gradient descent achieves zero training loss in olynomial time for a dee over-arameterized neural network with residual connections (ResNet). Our analysis relies on the articular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training rocess and this stability imlies the global otimality of the gradient descent algorithm. We further extend our analysis to dee residual convolutional neural networks and obtain a similar convergence result.
"
853,2019,Composable Core-sets for Determinant Maximization: A Simple Near-Optimal Algorithm,Poster,"``Comosable core-sets'' are an efficient framework for solving otimization roblems in massive data models. In this work, we consider efficient construction of comosable core-sets for the determinant maximization roblem.
This can also be cast as the MAP inference task for ``determinantal oint rocesses"", that have recently gained a lot of interest for modeling diversity and fairness. The roblem was recently studied in \cite{indyk2018comosable}, where they designed comosable core-sets with the otimal aroximation bound of $O(k)^k$. On the other hand, the more ractical ``Greedy"" algorithm has been reviously used in similar contexts. In this work, first we rovide a theoretical aroximation guarantee of $C^{k^2}$ for the Greedy algorithm in the context of comosable core-sets; Further, we roose to use a ``Local Search"" based algorithm that while being still ractical, achieves a nearly otimal aroximation bound of $O(k)^{2k}$; Finally, we imlement all three algorithms and show the effectiveness of our roosed algorithm on standard data sets."
854,2019,Sublinear Time Nearest Neighbor Search over Generalized Weighted Space,Poster,"Nearest Neighbor Search (NNS) over generalized weighted sace is a fundamental roblem which has many alications in various fields. However, to the best of our knowledge, there is no sublinear time solution to this roblem. Based on the idea of Asymmetric Locality-Sensitive Hashing (ALSH), we introduce a novel sherical asymmetric transformation and roose the first two novel weight-oblivious hashing schemes SL-ALSH and S2-ALSH accordingly. We further show that both schemes enjoy a quality guarantee and can answer the NNS queries in sublinear time. Evaluations over three real datasets demonstrate the suerior erformance of the two roosed schemes.
"
855,2019,Compressing Gradient Optimizers via Count-Sketches,Poster,"Many oular first-order otimization methods accelerate the convergence rate of dee learning models. However, these algorithms require auxiliary variables, which cost additional memory roortional to the number of arameters in the model. The roblem is becoming more severe as models grow larger to learn from comlex, large-scale datasets. Our roosed solution is to maintain a linear sketch to comress the auxiliary variables. Our aroach has the same erformance as the full-sized baseline, while using less sace for the auxiliary variables. Theoretically, we rove that count-sketch otimization maintains the SGD convergence rate, while gracefully reducing memory usage for large-models. We show a rigorous evaluation on oular architectures such as ResNet-18 and Transformer-XL. On the 1-Billion Word dataset, we save 25% of the memory used during training (7.7 GB instead of 10.8 GB) with minimal accuracy and erformance loss. For an Amazon extreme classification task with over 49.5 million classes, we also reduce the training time by 38%, by increasing the mini-batch size 3.5x using our count-sketch otimizer.
"
856,2019,Scalable Fair Clustering,Poster,"We study the fair variant of the classic k-median roblem introduced by (Chierichetti et al., NeurIPS 2017) in which the oints are colored, and the goal is to minimize the same average distance objective as in the standard $k$-median roblem while ensuring that all clusters have an ``aroximately equal'' number of oints of each color. 
(Chierichetti et al., NeurIPS 2017) roosed a two-hase algorithm for fair $k$-clustering. In the first ste, the ointset is artitioned into subsets called fairlets that satisfy the fairness requirement and aroximately reserve the k-median objective. In the second ste, fairlets are merged into k clusters by one of the existing k-median algorithms. The running time of this algorithm is dominated by the first ste, which takes suer-quadratic time. 
In this aer, we resent a ractical aroximate fairlet decomosition algorithm that runs in nearly linear time. "
857,2019,Conditional Gradient Methods via Stochastic Path-Integrated Differential Estimator,Poster,"We roose a class of variance-reduced stochastic conditional gradient methods. By adoting the recent stochastic ath-integrated differential estimator technique (SPIDER) of Fang et. al. (2018) for the classical Frank-Wolfe (FW) method, we introduce SPIDER-FW for finite-sum minimization as well as the more general exectation minimization roblems. SPIDER-FW enjoys suerior comlexity guarantees in the non-convex setting, while matching the best known FW variants in the convex case. We also extend our framework a la conditional gradient sliding (CGS) of Lan &am; Zhou. (2016), and roose SPIDER-CGS.
"
858,2019, Fault Tolerance in Iterative-Convergent Machine Learning,Poster,"Machine learning (ML) training algorithms often ossess an inherent self-correcting behavior due to their iterative- convergent nature. Recent systems exloit this roerty to achieve adatability and efficiency in unreliable comuting environments by relaxing the consistency of execution and allowing calculation errors to be self-corrected during training. However, the behavior of such systems are only well understood for secific tyes of calculation errors, such as those caused by staleness, reduced recision, or asynchronicity, and for secific algorithms, such as stochastic gradient descent. In this aer, we develo a general framework to quantify the effects of calculation errors on iterative-convergent algorithms. We then use this framework to derive a worst-case uer bound on the cost of arbitrary erturbations to model arameters during training and to design new strategies for checkoint-based fault tolerance. Our system, SCAR, can reduce the cost of artial failures by 78%–95% when comared with traditional checkoint-based fault tolerance across a variety of ML models and training algorithms, roviding near-otimal erformance in recovering from failures.
"
859,2019,Static Automatic Batching In TensorFlow,Poster,"Dynamic neural networks are becoming increasingly common, and yet it is hard to imlement them efficiently. On-the-fly oeration batching for such models is sub-otimal and suffers from run time overheads, while writing manually batched versions can be hard and error-rone. To address this we extend TensorFlow with for, a arallel-for loo otimized using static loo vectorization. With for, users can exress comutation using nested loos and conditional constructs, but get erformance resembling that of a manually batched version. Benchmarks demonstrate seedus of one to two orders of magnitude on range of tasks, from jacobian comutation, to Grah Neural Networks. 
"
860,2019,Improving Neural Network Quantization without Retraining using Outlier Channel Splitting,Poster,"Quantization can imrove the execution latency and energy efficiency of neural networks on both commodity GPUs and secialized accelerators. The majority of existing literature focuses on training quantized DNNs, while this work examines the less-studied toic of quantizing a floating-oint model without (re)training. DNN weights and activations follow a bell-shaed distribution ost-training, while ractical hardware uses a linear quantization grid. This leads to challenges in dealing with outliers in the distribution. Prior work has addressed this by cliing the outliers or using secialized hardware. In this work, we roose outlier channel slitting (OCS), which dulicates channels containing outliers, then halves the channel values. The network remains functionally identical, but affected outliers are moved toward the center of the distribution. OCS requires no additional training and works on commodity hardware. Exerimental evaluation on ImageNet classification and language modeling shows that OCS can outerform state-of-the-art cliing techniques with only minor overhead.
"
861,2019,Memory-Optimal Direct Convolutions for Maximizing Classification Accuracy in Embedded Applications,Poster,"In the age of Internet of Things (IoT), embedded devices ranging from ARM Cortex M0s with hundreds of KB of RAM to Arduinos with 2KB RAM are exected to erform increasingly sohisticated classification tasks, such as voice and gesture recognition, activity tracking, and biometric security. While convolutional neural networks (CNNs), together with sectrogram rerocessing, are a natural solution to many of these classification tasks, storage of the network's activations often exceeds the hard memory constraints of embedded latforms. This aer resents memory-otimal direct convolutions as a way to ush classification accuracy as high as ossible given strict hardware memory constraints at the exense of extra comute. We therefore exlore the oosite end of the comute-memory trade-off curve from standard aroaches that minimize latency. We validate the memory-otimal CNN technique with an Arduino imlementation of the 10-class MNIST classification task, fitting the network secification, weights, and activations entirely within 2KB SRAM and achieving a state-of-the-art classification accuracy for small-scale embedded systems of 99.15%.
"
862,2019,DL2: Training and Querying Neural Networks with Logic,Poster,"We resent DL2, a system for training and querying neural networks with logical constraints. Using DL2, one can declaratively secify domain knowledge constraints to be enforced during training, as well as ose queries on the model to find inuts that satisfy a set of constraints. DL2 works by translating logical constraints into a loss function with desirable mathematical roerties. The loss is then minimized with standard gradient-based methods. We evaluate DL2 by training networks with interesting constraints in unsuervised, semi-suervised and suervised settings. Our exerimental evaluation demonstrates that DL2 is more exressive than rior aroaches combining logic and neural networks, and its loss functions are better suited for otimization. Further, we show that for a number of queries, DL2 can find the desired inuts in seconds (even for large models such as ResNet-50 on ImageNet).
"
863,2019,PA-GD: On the Convergence of Perturbed Alternating Gradient Descent to Second-Order Stationary Points for Structured Nonconvex Optimization,Poster,"Alternating gradient descent (A-GD) is a simle but oular algorithm in machine learning, which udates two blocks of variables in an alternating manner using gradient descent stes. In this aer, we consider a smooth unconstrained nonconvex otimization roblem, and roose a erturbed A-GD (PA-GD) which is able to converge (with high robability) to the second-order stationary oints (SOSPs) with a global sublinear rate. Existing analysis on A-GD tye algorithm either only guarantees convergence to first-order solutions, or converges to second-order solutions asymtotically (without rates). To the best of our knowledge, this is the first alternating tye algorithm that takes $\mathcal{O}(\text{olylog}(d)\esilon^2)$ iterations to achieve an ($\esilon,\sqrt{\esilon}$)-SOSP with high robability, where olylog$(d)$ denotes the olynomial of the logarithm with resect to roblem dimension $d$."
864,2019,Improved Zeroth-Order Variance Reduced Algorithms and Analysis for Nonconvex Optimization,Poster,"Two tyes of zeroth-order stochastic algorithms have recently been designed for nonconvex otimization resectively based on the first-order techniques SVRG and SARAHSPIDER. This aer addresses several imortant issues that are still oen in these methods. First, all existing SVRG-tye zeroth-order algorithms suffer from worse function query comlexities than either zeroth-order gradient descent (ZO-GD) or stochastic gradient descent (ZO-SGD). In this aer, we roose a new algorithm ZO-SVRG-Coord-Rand and develo a new analysis for an existing ZO-SVRG-Coord algorithm roosed in Liu et al. 2018b, and show that both ZO-SVRG-Coord-Rand and ZO-SVRG-Coord (under our new analysis) outerform other exiting SVRG-tye zeroth-order methods as well as ZO-GD and ZO-SGD. Second, the existing SPIDER-tye algorithm SPIDER-SZO (Fang et al., 2018) has suerior theoretical erformance, but suffers from the generation of a large number of Gaussian random variables as well as a $\sqrt{\esilon}$-level stesize in ractice. In this aer, we develo a new algorithm ZO-SPIDER-Coord, which is free from Gaussian variable generation and allows a large constant stesize while maintaining the same convergence rate and query comlexity, and we further show that ZO-SPIDER-Coord automatically achieves a linear convergence rate as the iterate enters into a local PL region without restart and algorithmic modification."
865,2019,Faster Stochastic Alternating Direction Method of Multipliers for Nonconvex Optimization,Poster,"In this aer, we roose a faster stochastic alternating direction method of multiliers (ADMM) for nonconvex otimization by using a new stochastic ath-integrated differential estimator (SPIDER), called as SPIDER-ADMM. Moreover, we rove that the SPIDER-ADMM achieves a record-breaking incremental first-order oracle (IFO) comlexity for finding an ϵ-aroximate solution. As one of major contribution of this aer, we rovide a new theoretical analysis framework for nonconvex stochastic ADMM methods with roviding the otimal IFO comlexity. Based on this new analysis framework, we study the unsolved otimal IFO comlexity of the existing non-convex SVRG-ADMM and SAGA-ADMM methods, and rove their the otimal IFO comlexity. Thus, the SPIDER-ADMM imroves the existing stochastic ADMM methods. Moreover, we extend SPIDER-ADMM to the online setting, and roose a faster online SPIDER-ADMM. Our theoretical analysis also derives the IFO comlexity of the online SPIDER-ADMM. Finally, the exerimental results on benchmark datasets validate that the roosed algorithms have faster convergence rate than the existing ADMM algorithms for nonconvex otimization.
"
866,2019,Lower Bounds for Smooth Nonconvex Finite-Sum Optimization,Poster,"Smooth finite-sum otimization has been widely studied in both convex and nonconvex settings. However, existing lower bounds for finite-sum otimization are mostly limited to the setting where each comonent function is (strongly) convex, while the lower bounds for nonconvex finite-sum otimization remain largely unsolved. In this aer, we study the lower bounds for smooth nonconvex finite-sum otimization, where the objective function is the average of $n$ nonconvex comonent functions. We rove tight lower bounds for the comlexity of finding $\esilon$-subotimal oint and $\esilon$-aroximate stationary oint in different settings, for a wide regime of the smallest eigenvalue of the Hessian of the objective function (or each comonent function). Given our lower bounds, we can show that existing algorithms including {KatyushaX} \cite{allen2018katyushax}, {Natasha} \cite{allen2017natasha} and {StagewiseKatyusha} \cite{yang2018does} have achieved otimal {Incremental First-order Oracle} (IFO) comlexity (i.e., number of IFO calls) u to logarithm factors for nonconvex finite-sum otimization. We also oint out otential ways to further imrove these comlexity results, in terms of making stronger assumtions or by a different convergence analysis."
867,2019,Nonconvex Variance Reduced Optimization with Arbitrary Sampling,Poster,"We rovide the first imortance samling variants of variance reduced algorithms for emirical risk minimization with non-convex loss functions. In articular, we analyze non-convex versions of \texttt{SVRG}, \texttt{SAGA} and \texttt{SARAH}. Our methods have the caacity to seed u the training rocess by  an order of magnitude comared to the state of the art on real datasets. Moreover, we also imrove uon current mini-batch analysis of these methods by roosing  imortance samling for minibatches in this setting. Surrisingly, our aroach can in some regimes lead to suerlinear seedu with resect to the minibatch size, which is not usually resent in stochastic otimization. All the above results follow from a general analysis of the methods which works with {\em arbitrary samling}, i.e., fully general randomized strategy for the selection of subsets of examles to be samled in each iteration. Finally, we also erform a novel imortance samling analysis of \texttt{SARAH} in the convex setting.
"
868,2019,Error Feedback Fixes SignSGD and other Gradient Compression Schemes,Poster,"Sign-based algorithms (e.g. signSGD) have been roosed as a biased gradient comression technique to alleviate the communication bottleneck in training large neural networks across multile workers. We show simle convex counter-examles where signSGD does not converge to the otimum.
Further, even when it does converge, signSGD may generalize oorly when comared with SGD.  These issues arise because of the biased nature of the sign comression oerator.

We then show that using error-feedback, i.e. incororating the error made by the comression oerator into the next ste, overcomes these issues. We rove that our algorithm (EF-SGD) with arbitrary comression oerator achieves the same rate of convergence as SGD without any additional assumtions. Thus EF-SGD achieves gradient comression for free. Our exeriments thoroughly substantiate the theory.
"
869,2019,A Composite Randomized Incremental Gradient Method,Poster,"We consider the roblem of minimizing the comosition of a smooth function (which can be nonconvex) and a smooth vector maing, where both of them can be exress as the average of a large number of comonents. We roose a comosite randomized incremental gradient method by extending the SAGA framework. The gradient samle comlexity of our method matches that of several recently develoed methods based on SVRG in the general case. However, for structured roblems where linear convergence rates can be obtained, our method can be much better for ill-conditioned roblems. In addition, when the finite-sum structure only aear for the inner maing, the samle comlexity of our method is the same as that of SAGA for minimizing finite sum of smooth nonconvex functions, desite the additional outer comosition and the stochastic comosite gradients being biased in our case.
"
870,2019,Optimal Continuous DR-Submodular Maximization and Applications to Provable Mean Field Inference,Poster,"Mean field inference for discrete grahical models is generally a highly nonconvex roblem, which also holds for the class of robabilistic log-submodular models. Existing otimization methods, e.g., coordinate ascent  algorithms, tyically only find local otima.
In this work we roose rovable mean filed  methods for robabilistic  log-submodular models and its osterior agreement (PA) with strong aroximation guarantees. The main algorithmic technique is a new Double Greedy scheme, termed DR-DoubleGreedy, for continuous DR-submodular maximization with box-constraints. It is a one-ass algorithm with linear time comlexity, reaching the otimal 12 aroximation ratio, which may be of indeendent interest. We validate the suerior erformance of our algorithms against baselines on both synthetic and real-world datasets.
"
871,2019,Multiplicative Weights Updates as a distributed constrained optimization algorithm: Convergence to second-order stationary points almost always,Poster,"Non-concave maximization has been the subject of much recent study in the otimization and machine learning communities, secifically in dee learning.
Recent aers  ([Ge et al. 2015, Lee et al 2017] and references therein) indicate that first order methods work well and avoid saddles oints.  Results as in [Lee \etal 2017], however, are limited to the \textit{unconstrained} case or for cases where the critical oints are in the interior of the feasibility set, which fail to cature some of the most interesting alications. In this aer we focus on \textit{constrained} non-concave maximization. We analyze a variant of a well-established algorithm in machine learning called Multilicative Weights Udate (MWU) for the maximization roblem $\max_{\mathbf{x} \in D} P(\mathbf{x})$, where $P$ is non-concave, twice continuously differentiable and $D$ is a roduct of simlices. We show that MWU converges almost always for small enough stesizes to critical oints that satisfy the second order KKT conditions,
by combining techniques from dynamical systems as well as taking advantage of a recent connection between Baum Eagon inequality and MWU [Palaioanos et al 2017]."
872,2019,Katalyst: Boosting Convex Katayusha for  Non-Convex Problems with a  Large Condition Number,Poster,"An imortant class of non-convex objectives that has wide alications in machine learning consists of  a sum of $n$ smooth functions and a non-smooth convex function. Tremendous studies have been devoted to conquering these roblems by leveraging one of the two tyes of variance reduction techniques, i.e., SVRG-tye that comutes a full gradient occasionally and SAGA-tye that maintains $n$ stochastic gradients at every iteration.  In ractice, SVRG-tye is referred to SAGA-tye due to its otentially less memory costs. 
An interesting question that has been largely ignored is how to imrove the comlexity of variance reduction methods for roblems with a large condition number that measures the degree to which the objective is close to a convex function.  In this aer, we resent a simle but non-trivial boosting of a  state-of-the-art SVRG-tye method for convex roblems (namely Katyusha) to enjoy an imroved comlexity for solving non-convex roblems with a large condition number (that is close to a convex function). To the best of our knowledge, its comlexity has the best deendence on $n$ and the degree of non-convexity, and also matches that of a recent SAGA-tye accelerated stochastic algorithm for a constrained non-convex smooth otimization roblem.  
"
873,2019,Safe Policy Improvement with Baseline Bootstrapping,Poster,"This aer considers Safe Policy Imrovement (SPI) in Batch Reinforcement Learning (Batch RL): from a fixed dataset and without direct access to the true environment, train a olicy that is guaranteed to erform at least as well as the baseline olicy used to collect the data. 
	    Our aroach, called SPI with Baseline Bootstraing (SPIBB), is insired by the knows-what-it-knows aradigm: it bootstras the trained olicy with the baseline when the uncertainty is high. 
	    Our first algorithm, $\Pi_b$-SPIBB, comes with SPI theoretical guarantees. 
	    We also imlement a variant, $\Pi_{\leq b}$-SPIBB, that is even more efficient in ractice. 
	    We aly our algorithms to a motivational stochastic gridworld domain and further demonstrate on randomly generated MDPs the sueriority of SPIBB with resect to existing algorithms, not only in safety but also in mean erformance. 
	    Finally, we imlement a model-free version of SPIBB and show its benefits on a navigation task with dee RL imlementation called SPIBB-DQN, which is, to the best of our knowledge, the first RL algorithm relying on a neural network reresentation able to train efficiently and reliably from batch data, without any interaction with the environment."
874,2019,Distributional Reinforcement Learning for Efficient Exploration,Poster,"In distributional reinforcement learning (RL), the estimated distribution of value functions model both the arametric and intrinsic uncertainties. We roose a novel and efficient exloration method for dee RL that has two comonents. The first is a decaying schedule to suress the intrinsic uncertainty. The second is an exloration bonus calculated from the uer quantiles of the learned distribution. In Atari 2600 games, our method achieves 483 % average gain across 49 games in cumulative rewards over QR-DQN. We also comared our algorithm with QR-DQN in a challenging 3D driving simulator (CARLA). Results show that our algorithm achieves nearotimal safety rewards twice faster than QRDQN.
"
875,2019,Optimistic Policy Optimization via Multiple Importance Sampling,Poster,"Policy Search (PS) is an effective aroach to Reinforcement Learning (RL) for solving
control tasks with continuous state-action saces. In this aer, we address the exloration-exloitation trade-off in PS by roosing an aroach based on Otimism in the Face of Uncertainty. We cast the PS roblem as a suitable Multi Armed Bandit (MAB) roblem, defined over the olicy arameter sace, and we roose a class of algorithms that effectively exloit the roblem structure, by leveraging Multile Imortance Samling to erform an off-olicy estimation of the exected return.
We show that the regret of the roosed aroach is bounded by $\widetilde{\mathcal{O}}(\sqrt{T})$ for both discrete and continuous arameter saces. Finally, we evaluate our algorithms on tasks of varying difficulty, comaring them with existing MAB and RL algorithms."
876,2019,Neural Logic Reinforcement Learning,Poster,"Dee reinforcement learning (DRL) has achieved significant breakthroughs in various tasks. However, most DRL algorithms suffer a roblem of generalising the learned olicy, which makes the olicy erformance largely affected even by minor modifications of the training environment. Excet that, the use of dee neural networks makes the learned olicies hard to be interretable. To address these two challenges, we roose a novel algorithm named Neural Logic Reinforcement Learning (NLRL) to reresent the olicies in reinforcement learning by first-order logic. NLRL is based on olicy gradient methods and differentiable inductive logic rogramming that have demonstrated significant advantages in terms of interretability and generalisability in suervised tasks. Extensive exeriments conducted on cliff-walking and blocks maniulation tasks demonstrate that NLRL can induce interretable olicies achieving near-otimal erformance while showing good generalisability to environments of different initial states and roblem sizes.
"
877,2019,Learning to Collaborate in Markov Decision Processes,Poster,"We consider a two-agent MDP framework where agents reeatedly solve a task in a collaborative setting. We study the roblem of designing a learning algorithm for the first agent (A1) that facilitates a successful collaboration even in cases when the second agent (A2) is adating its olicy in an unknown way. The key challenge in our setting is that the first agent faces non-stationarity in rewards and transitions because of the adative behavior of the second agent.

We design novel online learning algorithms for agent A1 whose regret decays as $O(T^{1-\frac{3}{7} \cdot \alha})$ with $T$ learning eisodes rovided that the magnitude of agent A2's olicy changes between any two consecutive eisodes are uer bounded by $O(T^{-\alha})$. Here, the arameter $\alha$ is assumed to be strictly greater than $0$, and we show that this assumtion is necessary rovided that the {\em learning arity with noise} roblem is comutationally hard. We show that sub-linear regret of agent A1 further imlies near-otimality of the agents' joint return for MDPs that manifest the roerties of a {\em smooth} game. "
878,2019,Predictor-Corrector Policy Optimization,Poster,"We resent a redictor-corrector framework, called PicCoLO, that can transform a first-order model-free reinforcement or imitation learning algorithm into a new hybrid method that leverages redictive models to accelerate olicy learning. The new ``PicCoLOed'' algorithm otimizes a olicy by recursively reeating two stes: In the Prediction Ste, the learner uses a model to redict the unseen future gradient and then alies the redicted estimate to udate the olicy; in the Correction Ste, the learner runs the udated olicy in the environment, receives the true gradient, and then corrects the olicy using the gradient error. Unlike revious algorithms, PicCoLO corrects for the mistakes of using imerfect redicted gradients and hence does not suffer from model bias. The develoment of PicCoLO is made ossible by a novel reduction from redictable online learning to adversarial online learning,  which rovides a systematic way to modify existing first-order algorithms to achieve the otimal regret with resect to redictable information. We show, in both theory and simulation, that the convergence rate of several first-order model-free algorithms can be imroved by PicCoLO. 
"
879,2019,Learning a Prior over Intent via Meta-Inverse Reinforcement Learning,Poster,"A significant challenge for the ractical alication of reinforcement learning to real world roblems is the need to secify an oracle reward function that correctly defines a task. Inverse reinforcement learning (IRL) seeks to avoid this challenge by instead inferring a reward function from exert demonstrations. While aealing, it can be imractically exensive to collect datasets of demonstrations that cover the variation common in the real world (e.g. oening any tye of door). Thus in ractice, IRL must commonly be erformed with only a limited set of demonstrations where it can be exceedingly difficult to unambiguously recover a reward function. In this work, we exloit the insight that demonstrations from other tasks can be used to constrain the set of ossible reward functions by learning a ""rior"" that is secifically otimized for the ability to infer exressive reward functions from limited numbers of demonstrations. We demonstrate that our method can efficiently recover rewards from images for novel tasks and rovide intuition as to how our aroach is analogous to learning a rior.
"
880,2019,DeepMDP: Learning Continuous Latent Space Models for Representation Learning,Poster,"Many reinforcement learning (RL) tasks rovide the agent with high-dimensional observations that can be simlified into low-dimensional continuous states. To formalize this rocess, we introduce the concet of a \texit{DeeMDP}, a arameterized latent sace model that is trained via the minimization of two tractable latent sace losses: rediction of rewards and rediction of the distribution over next latent states. We show that the otimization of these objectives guarantees (1) the quality of the embedding function as a reresentation of the state sace and (2) the quality of the DeeMDP as a model of the environment. Our theoretical findings are substantiated by the exerimental result that a trained DeeMDP recovers the latent structure underlying high-dimensional observations on a synthetic environment. Finally, we show that learning a DeeMDP as an auxiliary task in the Atari 2600 domain leads to large erformance imrovements over model-free RL.
"
881,2019,Importance Sampling Policy Evaluation with an Estimated Behavior Policy,Poster,"We consider the roblem of off-olicy evaluation in Markov decision rocesses. Off-olicy evaluation is the task of evaluating the exected return of one olicy with data generated by a different, behavior olicy. Imortance samling is a technique for off-olicy evaluation that re-weights off-olicy returns to account for differences in the likelihood of the returns between the two olicies. In this aer, we study imortance samling with an estimated behavior olicy where the behavior olicy estimate comes from the same set of data used to comute the imortance samling estimate. We find that this estimator often lowers the mean squared error of off-olicy evaluation comared to imortance samling with the true behavior olicy or using a behavior olicy that is estimated from a searate data set. Intuitively, estimating the behavior olicy in this way corrects for error due to samling in the action-sace. Our emirical results also extend to other oular variants of imortance samling and show that estimating a non-Markovian behavior olicy can further lower large-samle mean squared error even when the true behavior olicy is Markovian.
"
882,2019,Learning from a Learner,Poster,"In this aer, we roose a novel setting for Inverse Reinforcement Learning (IRL), namely ""Learning from a Learner"" (LfL). As oosed to standard IRL, it does not consist in learning a reward by observing an otimal agent but from observations of another learning (and thus sub-otimal) agent. To do so, we leverage the fact that the observed agent's olicy is assumed to imrove over time. The ultimate goal of this aroach is to recover the actual environment's reward and to allow the observer to outerform the learner. To recover that reward in ractice, we roose methods based on the entroy-regularized olicy iteration framework. We discuss different aroaches to learn solely from trajectories in the state-action sace. We demonstrate the genericity of our method by observing agents imlementing various reinforcement learning algorithms. Finally, we show that, on both discrete and continuous stateaction tasks, the observer's erformance (that otimizes the recovered reward) can surass those of the observed agent. 
"
883,2019,Separable value functions across time-scales,Poster,"In many finite horizon eisodic reinforcement learning (RL) settings, it is desirable to otimize for the undiscounted return - in settings like Atari, for instance, the goal is to collect the most oints while staying alive in the long run. Yet, it may be difficult (or even intractable) mathematically to learn with this target. As such, temoral discounting is often alied to otimize over a shorter effective lanning horizon. This comes at the cost of otentially biasing the otimization target away from the undiscounted goal. In settings where this bias is unaccetable - where the system must otimize for longer horizons at higher discounts - the target of the value function aroximator may increase in variance leading to difficulties in learning. We resent an extension of temoral difference (TD) learning, which we call TD($\Delta$), that breaks down a value function into a series of comonents based on the differences between value functions with smaller discount factors. The searation of a longer horizon value function into these comonents has useful roerties in scalability and erformance. We discuss these roerties and show theoretic and emirical imrovements over standard TD learning in certain settings.
"
884,2019,Learning Action Representations for Reinforcement Learning,Poster,"Most model-free reinforcement learning methods leverage state reresentations (embeddings) for generalization, but either ignore structure in the sace of actions or assume the structure is rovided a riori.  We show how a olicy can be decomosed into a comonent that acts in a low-dimensional sace of action reresentations and a comonent that transforms these reresentations into actual actions. These reresentations imrove generalization over large, finite action sets by allowing the agent to infer the outcomes of actions similar to actions already taken.  We rovide an algorithm to both learn and use action reresentations and rovide conditions for its convergence. The efficacy of the roosed method is demonstrated on large-scale real-world roblems.
"
885,2019,Bayesian Counterfactual Risk Minimization,Poster,"We resent a Bayesian view of counterfactual risk minimization (CRM) for offline learning from logged bandit feedback. Using PAC-Bayesian analysis, we derive a new generalization bound for the truncated inverse roensity score estimator. We aly the bound to a class of Bayesian olicies, which motivates a novel, otentially data-deendent, regularization technique for CRM. Exerimental results indicate that this technique outerforms standard $L_2$ regularization, and that it is cometitive with variance regularization while being both simler to imlement and more comutationally efficient."
886,2019,Per-Decision Option Discounting,Poster,"In order to solve comlex roblems an agent must be able to reason over a sufficiently long horizon. Temoral abstraction, commonly modeled through otions, offers the ability to reason at many timescales, but the horizon length is still determined by the discount factor of the underlying Markov Decision Process. We roose a modification to the otions framework that naturally scales the agent's horizon with otion length. We show that the roosed otion-ste discount controls a bias-variance trade-off, with larger discounts (counter-intuitively) leading to less estimation variance.
"
887,2019,Tighter Problem-Dependent Regret Bounds in Reinforcement Learning without Domain Knowledge using Value Function Bounds,Poster,"Strong worst-case erformance bounds for eisodic reinforcement learning exist 
but fortunately in ractice RL algorithms erform much better than 
such bounds would redict. Algorithms and theory that rovide strong 
roblem-deendent bounds could hel illuminate the key features of what 
makes a RL roblem hard and reduce the barrier to using RL algorithms 
in ractice. As a ste towards this
we derive an algorithm and analysis for finite horizon discrete MDPs br 
with state-of-the-art worst-case regret bounds and substantially tighter bounds if the RL 
environment has secial features but without ariori 
knowledge of the environment from the algorithm. As a result of our analysis, 
we also hel address an oen learning theory question~\cite{jiang2018oen} 
about eisodic MDPs with a constant uer-bound on the sum of rewards, 
roviding a regret bound function of the number of eisodes with no 
deendence on the horizon.
"
888,2019,A Theory of Regularized Markov Decision Processes,Poster,"Many recent successful (dee) reinforcement learning algorithms make use of regularization, generally based on entroy or Kullback-Leibler divergence. We roose a general theory of regularized Markov Decision Processes that generalizes these aroaches in two directions: we consider a larger class of regularizers, and we consider the general modified olicy iteration aroach, encomassing both olicy iteration and value iteration. The core building blocks of this theory are a notion of regularized Bellman oerator and the Legendre-Fenchel transform, a classical tool of convex otimization. This aroach allows for error roagation analyses of general algorithmic schemes of which (ossibly variants of) classical algorithms such as Trust Region Policy Otimization, Soft Q-learning, Stochastic Actor Critic or Dynamic Policy Programming are secial cases. This also draws connections to roximal convex otimization, esecially to Mirror Descent.
"
889,2019,Discovering Options for Exploration by Minimizing Cover Time,Poster,"One of the main challenges in reinforcement learning is solving tasks with sarse reward. We show that the difficulty of discovering a distant rewarding state in an MDP is bounded by the exected cover time of a random walk over the grah induced by the MDP's transition dynamics. We therefore roose to accelerate exloration by constructing otions that minimize cover time. We introduce a new otion discovery algorithm that diminishes the exected cover time by connecting the most distant states in the state-sace grah with otions. We show emirically that the roosed algorithm imroves learning in several domains with sarse rewards.
"
890,2019,Policy Certificates: Towards Accountable Reinforcement Learning,Poster,"The erformance of a reinforcement learning algorithm can vary drastically during learning because of exloration. Existing algorithms rovide little information about the quality of their current olicy before executing it, and thus have limited use in high-stakes alications like healthcare. We address this lack of accountability by roosing that algorithms outut olicy certificates. These certificates bound the sub-otimality and return of the olicy in the next eisode, allowing humans to intervene when the certified quality is not satisfactory. We further introduce two new algorithms with certificates and resent a new  framework for theoretical analysis that guarantees the quality of their olicies and certificates. For tabular MDPs, we show that comuting certificates can even imrove the samle-efficiency of otimism-based exloration. As a result, one of our algorithms is the first to achieve minimax-otimal PAC bounds u to lower-order terms, and this algorithm also matches (and in some settings slightly imroves uon) existing minimax regret bounds.
"
891,2019,The Value Function Polytope in Reinforcement Learning,Poster,"We establish geometric and toological roerties of the sace of value functions in finite state-action Markov decision rocesses. 
Our main contribution is the characterization of the nature of its shae: a general olytoe (Aigner et al., 2010). To demonstrate this result, we exhibit several roerties of the structural relationshi between olicies and value functions including the line theorem, which shows that the value functions of olicies constrained on all but one state describe a line segment. Finally, we use this novel ersective and introduce visualizations to enhance the understanding of the dynamics of reinforcement learning algorithms.
"
892,2019,Data Shapley:  Equitable Valuation of Data for Machine Learning,Poster,"As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic redictions and decisions. 
For examle, in healthcare and consumer markets, it has been suggested that individuals should be comensated for the data that they generate, but it is not clear what is an equitable valuation for individual data.
In this work, we develo a rinciled framework to address data valuation in the context of suervised machine learning. Given a learning algorithm trained on $n$ data oints to roduce a redictor, we roose data Shaley as a metric to quantify the value of each training datum to the redictor erformance. Data Shaley uniquely satisfies several natural roerties of equitable data valuation. We develo Monte Carlo and gradient-based methods to efficiently estimate data Shaley values in ractical settings where comlex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive exeriments across biomedical, image and synthetic data demonstrate that data Shaley has several other benefits: 1) it is more owerful than the oular leave-one-out or leverage score in roviding insight on what data is more valuable for a given learning task; 2)  low Shaley value data effectively cature outliers and corrutions; 3)  high Shaley value data inform what tye of new data to acquire to imrove the redictor.  "
893,2019,Feature Grouping as a Stochastic Regularizer for High-Dimensional Structured Data,Poster,"In many alications where collecting data is exensive, for examle neuroscience or medical imaging, the samle size is tyically small comared to the feature dimension. These datasets call for intelligent regularization that exloits known structure, such as correlations between the features arising from the measurement device. However, existing structured regularizers need secially crafted solvers, which are difficult to aly to comlex models. We roose a new regularizer secifically designed to leverage structure in the data in a way that can be alied efficiently to comlex models. Our aroach relies on feature grouing, using a fast clustering algorithm inside a stochastic gradient descent loo: given a
family of feature grouings that cature feature covariations, we randomly select these grous at each iteration. Exeriments on two real-world datasets demonstrate that the roosed aroach roduces models that generalize better than those trained with conventional regularizers, and also imroves convergence seed, and has a linear comutational cost.
"
894,2019,Metric-Optimized Example Weights,Poster,"Real-world machine learning alications often have comlex test metrics, and may have training and test data that are not identically distributed. Motivated by known connections between comlex test metrics and cost-weighted learning, we roose addressing these issues by using a weighted loss function with a standard loss, where the weights on the training examles are learned to otimize the test metric on a validation set. These metric-otimized examle weights can be learned for any test metric, including black box and customized ones for secific alications. We illustrate the erformance of the roosed method on diverse ublic benchmark datasets and real-world alications. We also rovide a generalization bound for the method.
"
895,2019,Improving Model Selection by Employing the Test Data,Poster,"Model selection and evaluation are usually strictly searated by means of data slitting to enable an unbiased estimation and a simle statistical inference for the unknown generalization erformance of the final rediction model. We investigate the roerties of novel evaluation strategies, namely when the final model is selected based on emirical erformances on the test data. To guard against selection induced overotimism, we emloy a arametric multile test correction based on the aroximate multivariate distribution of erformance estimates. Our numerical exeriments involve training common machine learning algorithms (EN, CART, SVM, XGB) on various artificial classification tasks. At its core, our roosed aroach imroves model selection in terms of the exected final model erformance without introducing overotimism. We furthermore observed a higher robability for a successful evaluation study, making it easier in ractice to emirically demonstrate a sufficiently high redictive erformance.
"
896,2019,Topological Data Analysis of Decision Boundaries with Application to Model Selection,Poster,"We roose the labeled Cech comlex, the lain labeled Vietoris-Ris comlex, and the locally scaled labeled Vietoris-Ris comlex to erform ersistent homology inference of decision boundaries in classification tasks. We rovide theoretical conditions and analysis for recovering the homology of a decision boundary from samles. Our main objective is quantification of dee neural network comlexity to enable matching of datasets to re-trained models to facilitate the functioning of AI marketlaces; we reort results for exeriments using MNIST, FashionMNIST, and CIFAR10.
"
897,2019,Contextual Memory Trees,Poster,"We design and study a  Contextual Memory Tree (CMT), a learning memory controller that inserts new memories into an exerience store of unbounded size. It oerates online and is designed to efficiently query for memories from that store, suorting logarithmic time insertion and retrieval oerations. Hence CMT can be integrated into existing statistical learning algorithms as an augmented memory unit without substantially increasing training and inference comutation.  Furthermore CMT oerates as a reduction to classification, allowing it to benefit from advances in reresentation or architecture.  We demonstrate the efficacy of CMT by augmenting existing multi-class and multi-label classification algorithms with CMT and observe statistical imrovement. We also test CMT learning on several image-cationing tasks to demonstrate that it erforms comutationally better than a simle nearest neighbors memory system while benefitting from reward learning.
"
898,2019,Sparse Extreme Multi-label Learning with Oracle Property,Poster,"The ioneering work of sarse local embeddings for extreme classification (SLEEC) (Bhatia et al., 2015) has shown great romise in multi-label learning. Unfortunately, the statistical rate of convergence and oracle roerty of SLEEC are still not well understood. To fill this ga, we resent a unified framework for SLEEC with nonconvex enalty. Theoretically, we rigorously rove that our roosed estimator enjoys oracle roerty (i.e., erforms as well as if the underlying model were known beforehand), and obtains a desirable statistical convergence rate. Moreover, we show that under a mild condition on the magnitude of the entries in the underlying model, we are able to obtain an imroved convergence rate. Extensive numerical exeriments verify our theoretical findings and the sueriority of our roosed estimator.
"
899,2019,Shape Constraints for Set Functions,Poster,"Set functions redict a label from a ermutation-invariant variable-size collection of feature vectors. We roose making set functions more understandable and regularized by caturing domain knowledge through shae constraints. We show how rior work in monotonic constraints can be adated to set functions, and then roose two new shae constraints designed to generalize the conditioning role of weights in a weighted mean. We show how one can train standard functions and set functions that satisfy these shae constraints with a dee lattice network. We roose a nonlinear estimation strategy we call the semantic feature engine that uses set functions with the roosed shae constraints to estimate labels for comound sarse categorical features. Exeriments on real-world data show the achieved accuracy is similar to dee sets or dee neural networks, but rovides guarantees on the model behavior, which makes it easier to exlain and debug. 
"
900,2019,On The Power of Curriculum Learning in Training Deep Networks,Poster,"Training neural networks is traditionally done by roviding a sequence of random mini-batches samled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform samling of mini-batches, on the training of dee networks, and secifically CNNs trained for image recognition. To emloy curriculum learning, the training algorithm must resolve 2 roblems: (i) sort the training examles by difficulty; (ii) comute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some cometitive ``teacher"" network, and bootstraing. In our emirical evaluation, both methods show similar benefits in terms of increased learning seed and imroved final erformance on test data. We address challenge (ii) by investigating different acing functions to guide the samling. The emirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the otimization landscae. We then define the concet of an ideal curriculum, and show that under mild conditions it does not change the corresonding global minimum of the otimization function.
"
901,2019,Voronoi Boundary Classification: A High-Dimensional Geometric Approach via Weighted Monte Carlo Integration,Poster,"Voronoi cell decomositions rovide a classical avenue to classification. Tyical aroaches however only utilize oint-wise cell-membershi information by means of nearest neighbor queries and do not utilize further geometric information about Voronoi cells since the comutation of Voronoi diagrams is rohibitively exensive in high dimensions. We roose a Monte-Carlo integration based aroach that instead comutes a weighted integral over the boundaries of Voronoi cells, thus incororating additional information about the Voronoi cell structure. We demonstrate the scalability of our aroach in u to 3072 dimensional saces and analyze convergence based on the number of Monte Carlo samles and choice of weight functions. Exeriments comaring our aroach to Nearest Neighbors, SVM and Random Forests indicate that while our aroach erforms similarly to Random Forests for large data sizes, the algorithm exhibits non-trivial data-deendent erformance characteristics for smaller datasets and can be analyzed in terms of a geometric confidence measure, thus adding to the reertoire of geometric aroaches to classification while having the benefit of not requiring any model changes or retraining as new training samles or classes are added.
"
902,2019,Robust Decision Trees Against Adversarial Examples,Poster,"Although adversarial examles and model robust-ness have been extensively studied in the context of neural networks, research on this issue in tree-based models and how to make tree-based models robust against adversarial examles is still limited. In this aer, we show that tree-based models are also vulnerable to adversarial examles and develo a novel algorithm to learn robust trees. At its core, our method aims to otimize the erformance under the worst-case erturbation of inut features, which leads to a max-min saddle oint roblem. Incororating this saddle oint objective into the decision tree building rocedure is non-trivial due to the discrete nature of trees—a naive aroach to finding the best slit according to this saddle oint objective will take exonential time. To make our aroach ractical and scalable, we roose efficient tree building algorithms by aroximating the inner minimizer in the saddleoint roblem, and resent efficient imlementations for classical information gain based trees as well as state-of-the-art tree boosting systems such as XGBoost.  Exerimental results on real world datasets demonstrate that the roosed algorithms can significantly imrove the robustness of tree-based models against adversarial examles.
"
903,2019,Automatic Classifiers as Scientific Instruments: One Step Further Away from Ground-Truth,Poster,"Automatic machine learning-based detectors of various sychological and social henomena (e.g., emotion, stress, engagement) have great otential to advance basic science. However, when a detector d is trained to aroximate an existing measurement tool (e.g., a questionnaire, observation rotocol), then care must be taken when interreting measurements collected using d since they are one ste further removed from the under- lying construct. We examine how the accuracy of d, as quantified by the correlation q of d’s out- uts with the ground-truth construct U, imacts the estimated correlation between U (e.g., stress) and some other henomenon V (e.g., academic erformance). In articular: (1) We show that if the true correlation between U and V is r, then the exected samle correlation, over all vectors T n whose correlation with U is q, is qr. (2) We derive a formula for the robability that the samle correlation (over n subjects) using d is ositive given that the true correlation is negative (and vice-versa); this robability can be substantial (around 20 − 30%) for values of n and q that have been used in recent affective comuting studies. (3) With the goal to reduce the variance of correlations estimated by an automatic detector, we show that training multile neural networks d(1) , . . . , d(m) using different training architectures and hyerarameters for the same detection task rovides only limited “coverage” of T^n.
"
904,2019,"Look Ma, No Latent Variables: Accurate Cutset Networks via Compilation",Poster,"Tractable robabilistic models obviate the need for unreliable aroximate inference aroaches and as a result often yield accurate query answers in ractice. However, most tractable models that achieve state-of-the-art generalization erformance (measured using test set likelihood score) use latent variables. Such models admit oly-time marginal (MAR) inference but do not admit oly-time (full) maximum-a-osteriori (MAP) inference. To address this roblem, in this aer, we roose a novel aroach for inducing cutset networks, a well-known tractable, highly interretable reresentation that does not use latent variables and admits linear time MAR as well as MAP inference. Our aroach addresses a major limitation of existing techniques that learn cutset networks from data in that their accuracy is quite low as comared to latent variable models such as ensembles of cutset networks and sum-roduct networks. The key idea in our aroach is to construct dee cutset networks by not only learning them from data but also comiling them from a more accurate latent tractable model. We show exerimentally that our new aroach yields more accurate MAP estimates as comared with existing aroaches and significantly imroves the test set log-likelihood score of cutset networks bringing them closer in terms of generalization erformance to latent variable models.
"
905,2019,Optimal Transport for structured data with application on graphs,Poster,"This work considers the roblem of comuting distances between structured
objects such as undirected grahs, seen as robability distributions in a
secific metric sace. We consider a new transortation distance (
i.e. that minimizes a total cost of transorting robability masses) that unveils
the geometric nature of the structured objects sace. Unlike Wasserstein or
Gromov-Wasserstein metrics that focus solely and resectively on features (by
considering a metric in the feature sace) or structure (by seeing structure as
a metric sace), our new distance exloits jointly both information, and is
consequently called Fused Gromov-Wasserstein  (FGW). After discussing its
roerties and comutational asects, we show results on a grah classification
task, where our method outerforms both grah kernels and
dee grah convolutional networks.  Exloiting further on the metric roerties
of FGW, interesting geometric objects such as Fr{\'e}chet means or barycenters
of grahs are illustrated and discussed in a clustering context.
"
906,2019,Learning Optimal Linear Regularizers,Poster,"We resent algorithms for efficiently learning regularizers that imrove generalization.  Our aroach is based on the insight that regularizers can be viewed as uer bounds on the generalization ga, and that reducing the slack in the bound can imrove erformance on test data.  For a broad class of regularizers, the hyerarameters that give the best uer bound can be comuted using linear rogramming.  Under certain Bayesian assumtions, solving the LP lets us ""jum"" to
the otimal hyerarameters given very limited data.  This suggests a natural algorithm for tuning regularization hyerarameters, which we show to be effective on both real and synthetic data.
"
907,2019,On Symmetric Losses for Learning from Corrupted Labels,Poster,"This aer aims to rovide a better understanding of a symmetric loss. 
First, we emhasize that using a symmetric loss is advantageous in the balanced error rate (BER) minimization and area under the receiver oerating characteristic curve (AUC) maximization from corruted labels. 
Second, we rove general theoretical roerties of symmetric losses, including a classification-calibration condition, excess risk bound, conditional risk minimizer, and AUC-consistency condition. 
Third, since all nonnegative symmetric losses are non-convex, we roose a convex barrier hinge loss that benefits significantly from the symmetric condition, although it is not symmetric everywhere. 
Finally, we conduct exeriments to validate the relevance of the symmetric condition. 
"
908,2019,AUCµ: A Performance Metric for Multi-Class Machine Learning Models,Poster,"The area under the receiver oerating characteristic curve (AUC) is arguably the most common metric in machine learning for assessing the quality of a two-class classification model. As the number and comlexity of machine learning alications grows, so too does the need for measures that can gracefully extend to classification models trained for more than two classes. Prior work in this area has roven comutationally intractable andor inconsistent with known roerties
of AUC, and thus there is still a need for an imroved multi-class efficacy metric. We rovide in this work a multi-class extension of AUC that we call AUCµ that is derived from first rinciles of the binary class AUC. AUCµ has similar comutational comlexity to AUC and maintains the roerties of AUC critical to its interretation and use.
"
909,2019,Regularization in directable environments with application to Tetris,Poster,"Learning from small data sets is difficult in the absence of secific domain knowledge. We resent a regularized linear model called STEW that benefits from a generic and revalent form of rior knowledge:  feature directions. STEW shrinks weights toward each other, converging to an equal-weights solution in the limit of infinite regularization. We rovide theoretical results on the equal-weights solution that exlains how STEW can roductively trade-off bias and variance. Across a wide range of learning roblems, including Tetris,  STEW outerformed existing linear models, including ridge regression, the Lasso, and the non-negative Lasso, when feature directions were known. The model roved to be robust to unreliable (or absent) feature directions, still outerforming alternative models under diverse conditions. Our results in Tetris were obtained by using a novel aroach to learning in sequential decision environments based on multinomial logistic regression.
"
910,2019,Improved Dynamic Graph Learning through Fault-Tolerant Sparsification,Poster,"Grah sarsification has been used to imrove the comutational cost of learning over grahs, e.g., Lalacian-regularized estimation and grah semi-suervised learning (SSL). However, when grahs vary over time, reeated sarsification requires olynomial order comutational cost er udate. We roose a new tye of grah sarsification namely fault-tolerant (FT) sarsification to significantly reduce the cost to only a constant. Then the comutational cost of subsequent grah learning tasks can be significantly imroved with limited loss in their accuracy. In articular, we give theoretical analyze to uer bound the loss in the accuracy of the subsequent Lalacian-regularized estimation and grah SSL, due to the FT sarsification. In addition, FT sectral sarsification can be generalized to FT cut sarsification, for cut-based grah learning. Extensive exeriments have confirmed the comutational efficiencies and accuracies of the roosed methods for learning on dynamic grahs.
"
911,2019,Heterogeneous Model Reuse via Optimizing Multiparty Multiclass Margin,Poster,"Nowadays, many roblems require learning a model from data owned by different articiants who are restricted to share their examles due to rivacy concerns, which is referred to as multiarty learning in the literature. In conventional multiarty learning, a global model is usually trained from scratch via a communication rotocol, ignoring the fact that each arty may already have a local model trained on her own dataset. In this aer, we define a multiarty multiclass margin to measure the global behavior of a set of heterogeneous local models, and roose a general learning method called HMR (Heterogeneous Model Reuse) to otimize the margin. Our method reuses local models to aroximate a global model, even when data are non-i.i.d distributed among arties, by exchanging few examles under redefined budget. Exeriments on synthetic and real-world data covering different multiarty scenarios show the effectiveness of our roosal.
"
912,2019,Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff,Poster,"Lossy comression algorithms are tyically designed and analyzed through the lens of Shannon's rate-distortion theory, where the goal is to achieve the lowest ossible distortion (e.g., low MSE or high SSIM) at any given bit rate. However, in recent years, it has become increasingly acceted that ""low distortion"" is not a synonym for ""high ercetual quality"", and in fact otimization of one often comes at the exense of the other. In light of this understanding, it is natural to seek for a generalization of rate-distortion theory which takes ercetual quality into account. In this aer, we adot the mathematical definition of ercetual quality recently roosed by Blau &am; Michaeli (2018), and use it to study the three-way tradeoff between rate, distortion, and ercetion. We show that restricting the ercetual quality to be high, generally leads to an elevation of the rate-distortion curve, thus necessitating a sacrifice in either rate or distortion. We rove several fundamental roerties of this trile-tradeoff, calculate it in closed form for a Bernoulli source, and illustrate it visually on a toy MNIST examle.
"
913,2019,Collaborative Channel Pruning for Deep Networks,Poster,"Dee networks have achieved imressive erformance in various domains, but their alications
are largely limited by the rohibitive comutational overhead. In this aer, we roose a novel
algorithm, namely collaborative channel runing
(CCP), to reduce the comutational overhead with
negligible erformance degradation. The joint
imact of runedreserved channels on the loss
function is quantitatively analyzed, and such interchannel deendency is exloited to determine
which channels to be runed. The channel selection roblem is then reformulated as a constrained 0-1 quadratic otimization roblem, and the Hessian matrix, which is essential in constructing the above otimization, can be efficiently aroximated. Emirical evaluation on two benchmark data sets indicates that our roosed CCP algorithm achieves higher classification accuracy with similar comutational comlexity than other stateof-the-art channel runing algorithms
"
914,2019,"Same, Same But Different: Recovering Neural Network Quantization Error Through Weight Factorization",Poster,"Quantization of neural networks has become common ractice, driven by the need for efficient imlementations of dee neural networks on embedded devices. In this aer, we exloit an oft-overlooked degree of freedom in most networks - for a given layer, individual outut channels can be scaled by any factor rovided that the corresonding weights of the next layer are inversely scaled. Therefore, a given network has many factorizations which change the weights of the network without changing its function. We resent a concetually simle and easy to imlement method that uses this roerty and show that roer factorizations significantly decrease the degradation caused by quantization. We show imrovement on a wide variety of networks and achieve state-of-the-art degradation results for MobileNets. While our focus is on quantization, this tye of factorization is alicable to other domains such as network-runing, neural nets regularization and network interretability.
"
915,2019,GDPP: Learning Diverse Generations using Determinantal Point Processes,Poster,"Generative models have roven to be an outstanding tool for reresenting high-dimensional robability distributions and generating realistic looking images. An essential characteristic of generative models is their ability to roduce multi-modal oututs. However, while training, they are often suscetible to mode collase, that is models are limited in maing inut noise to only a few modes of the true data distribution. In this work, we draw insiration from Determinantal Point Process (DPP) to roose an unsuervised enalty loss that alleviates mode collase while roducing higher quality samles. DPP is an elegant robabilistic measure used to model negative correlations within a subset and hence quantify its diversity. We use DPP kernel to model the diversity in real data as well as in synthetic data. Then, we devise an objective term that encourages generator to synthesize data with a similar diversity to real data. In contrast to revious state-of-the-art generative models that tend to use additional trainable arameters or comlex training aradigms, our method does not change the original training scheme. Embedded in an adversarial training and variational autoencoder, our Generative DPP aroach shows a consistent resistance to mode-collase on a wide-variety of synthetic data and natural image datasets including MNIST, CIFAR10, and CelebA, while outerforming state-of-the-art methods for data-efficiency, generation quality, and convergence-time whereas being 5.8x faster than its closest cometitor.
"
916,2019,Co-Representation Network for Generalized Zero-Shot Learning,Poster,"Generalized zero-shot learning is a significant toic but faced with bias roblem, which leads to unseen classes being easily misclassified into seen classes. Hence we roose a embedding model called co-reresentation network to learn a more uniform visual embedding sace that effectively alleviates the bias roblem and hels with classification. We mathematically analyze our model and find it learns a rojection with high local linearity, which is roved to cause less bias roblem. The network consists of a cooeration module for reresentation and a relation module for classification, it is simle in structure and can be easily trained in an end-to-end manner. Exeriments show that our method outerforms existing generalized zero-shot learning methods on several  benchmark datasets.
"
917,2019,GEOMetrics: Exploiting Geometric Structure for Graph-Encoded Objects,Poster,"Mesh models are a romising aroach for encoding the structure of 3D objects. Current mesh reconstruction systems redict uniformly distributed vertex locations of a redetermined grah through a series of grah convolutions, leading to comromises with resect to erformance or resolution. In this aer, we argue that the grah reresentation of geometric objects allows for additional structure, which should be leveraged for enhanced reconstruction. Thus, we roose a system which roerly benefits from the advantages of the geometric structure of grah-encoded objects by introducing (1) a grah convolutional udate reserving vertex information; (2) an adative slitting heuristic allowing detail to emerge; and (3) a training objective oerating both on the local surfaces defined by vertices as well as the global structure defined by the mesh. Our roosed method is evaluated on the task of 3D object reconstruction from images with the ShaeNet dataset, where we demonstrate state of the art erformance, both visually and numerically, while having far smaller sace requirements by generating adative meshes.
"
918,2019,EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,Poster,"Convolutional Neural Networks (ConvNets) are commonly develoed at a fixed resource budget, and then scaled u for better accuracy if more resources are given. In this aer, we systematically study model scaling and identify that carefully balancing network deth, width, and resolution can lead to better erformance. Based on this observation, we roose a new scaling method that uniformly scales all dimensions of dethwidthresolution using a simle yet highly effective comound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it u to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than revious ConvNets. In articular, our EfficientNet-B7 achieves stateof-the-art 84.4% to-1  97.1% to-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flower (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer arameters.
"
919,2019,Geometry Aware Convolutional Filters for Omnidirectional Images Representation,Poster,"Due to their wide field of view, omnidirectional cameras are frequently used by autonomous vehicles, drones and robots for navigation and other comuter vision tasks. The images catured by such cameras, are often analyzed and classified with techniques designed for lanar images that unfortunately fail to roerly handle the native geometry of such images and therefore results in subotimal erformance. In this aer we aim at imroving oular dee convolutional neural networks so that they can roerly take into account the secific roerties of omnidirectional data. In articular we roose an algorithm that adats convolutional layers, which often serve as a core building block of a CNN, to the roerties of omnidirectional images. Thus, our filters have a shae and size that adat to the location on the omnidirectional image. We show that our method is not limited to sherical surfaces and is able to incororate the knowledge about any kind of rojective geometry inside the dee learning network. As deicted by our exeriments, our method outerforms the existing dee neural network techniques for omnidirectional image classification and comression tasks.
"
920,2019,A Personalized Affective Memory Model for Improving Emotion Recognition,Poster,"Recent models of emotion recognition strongly rely on suervised dee learning solutions for the distinction of general emotion exressions. However, they are not reliable when recognizing online and ersonalized facial exressions, e.g., for erson-secific affective understanding. In this aer, we resent a neural model based on a conditional adversarial autoencoder to learn how to reresent and edit general emotion exressions. We then roose Grow-When-Required networks as ersonalized affective memories to learn individualized asects of emotional exressions. Our model achieves state-of-the-art erformance on emotion recognition when evaluated on in-the-wild datasets. Furthermore, our exeriments include ablation studies and neural visualizations in order to exlain the behavior of our model.
"
921,2019,Temporal Gaussian Mixture Layer for Videos,Poster,"We introduce a new convolutional layer named the Temoral Gaussian Mixture (TGM) layer and resent how it can be used to efficiently cature longer-term temoral information in continuous activity videos. The TGM layer is a temoral convolutional layer governed by a much smaller set of arameters (e.g., locationvariance of Gaussians) that are fully differentiable. We resent our fully convolutional video models with multile TGM layers for activity detection. The extensive exeriments on multile datasets, including Charades and MultiTHUMOS, confirm the effectiveness of TGM layers, significantly outerforming the state-of-the-arts.
"
922,2019,Regret Circuits: Composability of Regret Minimizers,Poster,"Regret minimization is a owerful tool for solving large-scale roblems; it was recently used in breakthrough results for large-scale extensive-form game solving. This was achieved by comosing simlex regret minimizers into an overall regret-minimization framework for extensive-form game strategy saces. In this aer we study the general comosability of regret minimizers. We derive a calculus for constructing regret minimizers for comosite convex sets that are obtained from convexity-reserving oerations on simler convex sets. We show that local regret minimizers for the simler sets can be combined with additional regret minimizers into an aggregate regret minimizer for the comosite set. As one alication, we show that the CFR framework can be constructed easily from our framework. We also show ways to include curtailing (constraining) oerations into our framework. For one, they enable the construction of CFR generalization for extensive-form games with general convex strategy constraints that can cut across decision oints.
"
923,2019,Game Theoretic Optimization via Gradient-based Nikaido-Isoda Function,Poster,"Comuting Nash equilibrium (NE) of multi-layer games has witnessed renewed interest due to recent advances in generative adversarial networks. However, comuting equilibrium efficiently is challenging. To this end, we introduce the Gradient-based Nikaido-Isoda (GNI) function which serves: (i) as a merit function, vanishing only at the first-order stationary oints of each layer's otimization roblem, and (ii) rovides error bounds to a stationary Nash oint. Gradient descent is shown to converge sublinearly to a first-order stationary oint of the GNI function. For the articular case of bilinear min-max games and multi-layer quadratic games, the GNI function is convex.  Hence, the alication of gradient descent in this case yields linear convergence to an NE (when one exists). In our numerical exeriments, we observe that the GNI formulation always converges to the first-order stationary oint of each layer's otimization roblem.
"
924,2019,Stable-Predictive Optimistic Counterfactual Regret Minimization,Poster,"The CFR framework has been a owerful tool for solving large-scale extensive-form games in ractice. However, the theoretical rate at which ast CFR-based algorithms converge to the Nash equilibrium is on the order of $O(T^{-12})$, where $T$ is the number of iterations. In contrast, first-order methods can be used to achieve a $O(T^{-1})$ deendence on iterations, yet these methods have been less successful in ractice. In this work we resent the first CFR variant that breaks the square-root deendence on iterations. By combining and extending recent advances on redictive and stable regret minimizers for the matrix-game setting we show that it is ossible to leverage ``otimistic'' regret minimizers to achieve a $O(T^{-34})$ convergence rate within CFR. This is achieved by introducing a new notion of stable-redictivity, and by setting the stability of each counterfactual regret minimizer relative to its location in the decision tree. Exeriments show that this method is faster than the original CFR algorithm, although not as fast as newer variants, in site of their worst-case $O(T^{-12})$ deendence on iterations."
925,2019,When Samples Are Strategically Selected,Poster,"In standard classification roblems, the assumtion is that the entity making the decision (the {\em rincial}) has access to {\em all} the samles.  However, in many contexts, she either does not have direct access to the samles, or can insect only a limited set of samles and does not know which are the most relevant ones.  In such cases, she must rely on another arty (the {\em agent}) to either rovide the samles or oint out the most relevant ones.  If the agent has a different objective, then the rincial cannot trust the submitted samles to be reresentative.  She must set a {\em olicy} for how she makes decisions, keeing in mind the agent's incentives.  In this aer, we introduce a theoretical framework for this roblem and rovide key structural and comutational results.
"
926,2019,Statistical Foundations of Virtual Democracy,Poster,"Virtual democracy is an aroach to automating decisions, by learning models of the references of individual eole, and, at runtime, aggregating the redicted references of those eole on the dilemma at hand. One of the key questions is which aggregation method -- or voting rule -- to use; we offer a novel statistical viewoint that rovides guidance. Secifically, we seek voting rules that are robust to rediction errors, in that their outut on eole's true references is likely to coincide with their outut on noisy estimates thereof. We rove that the classic Borda count rule is robust in this sense, whereas any voting rule belonging to the wide family of airwise-majority consistent rules is not. Our emirical results further suort, and more recisely measure, the robustness of Borda count. 
"
927,2019,Optimal Auctions through Deep Learning,Poster,"Designing an incentive comatible auction that maximizes exected revenue is an intricate task. The single-item case was resolved in a seminal iece of work by Myerson in 1981. Even after 30-40 years of intense research the roblem remains unsolved for seemingly simle multi-bidder, multi-item settings. In this work, we initiate the exloration of the use of tools from dee learning for the automated design of otimal auctions. We model an auction as a multi-layer neural network, frame otimal auction design as a constrained learning roblem, and show how it can be solved using standard ielines. We rove generalization bounds and resent extensive exeriments, recovering essentially all known analytical solutions for  multi-item settings, and obtaining novel mechanisms for settings in which the otimal mechanism is unknown.
"
928,2019,Learning to Clear the Market,Poster,"The roblem of market clearing is to set a rice for an item such that quantity demanded equals quantity sulied. In this work, we cast the roblem of redicting clearing rices into a learning framework and use the resulting models to erform revenue otimization in auctions and markets with contextual information. The economic intuition behind market clearing allows us to obtain fine-grained control over the aggressiveness of the resulting ricing olicy, grounded in theory. To evaluate our aroach, we fit a model of clearing rices over a massive dataset of bids in dislay ad auctions from a major ad exchange. The learned rices outerform other modeling techniques in the literature in terms of revenue and efficiency trade-offs. Because of the convex nature of the clearing loss function, the convergence rate of our method is as fast as linear regression.
"
929,2019,Learning to bid in revenue-maximizing auctions,Poster,"We consider the roblem of the otimization of bidding strategies in rior-deendent revenue-maximizing auctions, when the seller fixes the reserve rices based on the bid distributions. Our study is done in the setting where one bidder is strategic. Using a variational aroach, we study the comlexity of the original objective and we introduce a relaxation of the objective functional in  order to use gradient descent methods. Our aroach is simle, general and can be alied to various value distributions and revenue-maximizing mechanisms. The new strategies we derive yield  massive ulifts comared to the traditional truthfully bidding strategy. 
"
930,2019,Open-ended learning in symmetric zero-sum games,Poster,"Zero-sum games such as chess and oker are, abstractly, functions that evaluate airs of agents, for examle labeling them codewinner' andcodeloser'. If the game is aroximately transitive, then self-lay generates sequences of agents of increasing strength. However, nontransitive games, such as rock-aer-scissors, can exhibit strategic cycles, and there is no longer a clear objective -- we want agents to increase in strength, but against whom is unclear. In this aer, we introduce a geometric framework for formulating agent objectives in zero-sum games, in order to construct adative sequences of objectives that yield oen-ended learning. The framework allows us to reason about oulation erformance in nontransitive games, and enables the develoment of a new algorithm (rectified Nash resonse, PSROemrN) that uses game-theoretic niching to construct diverse oulations of effective agents, roducing a stronger set of  agents than existing algorithms. We aly PSROemrN to two highly nontransitive resource allocation games and find that PSRO_rN consistently outerforms the existing alternatives.
"
931,2019,Deep Counterfactual Regret Minimization,Poster,"Counterfactual Regret Minimization (CFR) is the leading algorithm for solving large imerfect-information games. It converges to an equilibrium by iteratively traversing the game tree. In order to deal with extremely large games, abstraction is tyically alied before running CFR. The abstracted game is solved with tabular CFR, and its solution is maed back to the full game. This rocess can be roblematic because asects of abstraction are often manual and domain secific, abstraction algorithms may miss imortant strategic nuances of the game, and there is a chicken-and-egg roblem because determining a good abstraction requires knowledge of the equilibrium of the game. This aer introduces \emh{Dee Counterfactual Regret Minimization}, a form of CFR that obviates the need for abstraction by instead using dee neural networks to aroximate the behavior of CFR in the full game. We show that Dee CFR is rinciled and achieves strong erformance in large oker games. This is the first non-tabular variant of CFR to be successful in large games.
"
932,2019,Generalized Approximate Survey Propagation for High-Dimensional Estimation,Poster,"In Generalized Linear Estimation (GLE) roblems, we seek to estimate a signal that is observed through a linear transform followed by a comonent-wise, ossibly nonlinear and noisy, channel. In the Bayesian otimal setting, Generalized Aroximate Message Passing (GAMP) is known to achieve otimal erformance for GLE. However, its erformance can significantly deteriorate whenever there is a mismatch between the assumed and the true generative model, a situation frequently encountered in ractice. In this aer, we roose a new algorithm, named Generalized Aroximate Survey Proagation (GASP), for solving GLE in the resence of rior or model missecifications. As a rototyical examle, we consider the hase retrieval roblem, where we show that GASP outerforms the corresonding GAMP, reducing the reconstruction threshold and, for certain choices of its arameters, aroaching Bayesian otimal erformance. Furthermore, we resent a set of state evolution equations that can recisely characterize the erformance of GASP in the high-dimensional limit.
"
933,2019,Boosted Density Estimation Remastered,Poster,"    There has recently been a steady increase in the number iterative
    aroaches to density estimation. However, an accomanying burst
    of formal convergence guarantees has not followed; all results ay
    the rice of heavy assumtions which are often unrealistic or hard
    to check. The \emh{Generative Adversarial Network (GAN)}
    literature --- seemingly orthogonal to the aforementioned ursuit
    --- has had the side effect of a renewed interest in variational
    divergence minimisation (notably $f$-GAN). We show how to combine
    this latter aroach and the classical boosting theory in suervised
    learning to get the first density estimation algorithm that rovably
    achieves geometric convergence under very weak assumtions. We do so by a trick allowing to combine
    \textit{classifiers} as the sufficient statistics of an
    exonential family. Our analysis includes an imroved variational characterisation of $f$-GAN."
934,2019,Inference and Sampling of $K_{33}$-free Ising Models,Poster,"We call an Ising model tractable when it is ossible to comute its artition function value (statistical inference) in olynomial time. The tractability also imlies an ability to samle configurations of this model in olynomial time. The notion of tractability extends the basic case of lanar zero-field Ising models. Our starting oint is to describe algorithms for the basic case, comuting artition function and samling efficiently. Then, we extend our tractable inference and samling algorithms to models whose triconnected comonents are either lanar or grahs of $O(1)$ size. In articular, it results in a olynomial-time inference and samling algorithms for $K_{33}$ (minor)-free toologies of zero-field Ising models---a generalization of lanar grahs with a otentially unbounded genus."
935,2019,Random Matrix Improved Covariance Estimation for a Large Class of Metrics,Poster,"Relying on recent advances in statistical estimation of covariance distances based on random matrix theory, this article rooses an imroved covariance and recision matrix estimation for a wide family of metrics. 
The method is shown to largely outerform the samle covariance matrix estimate and to comete with state-of-the-art methods, while at the same time being comutationally simler and faster. Alications to linear and quadratic discriminant analyses also show significant gains, therefore suggesting ractical interest to statistical machine learning.
"
936,2019,Dual Entangled Polynomial Code: Three-Dimensional Coding for Distributed Matrix Multiplication,Poster,"Matrix multilication is a fundamental building block in various machine learning algorithms. When the matrix comes from a large dataset, the multilication can be slit into multile tasks which calculate the multilication of submatrices on different nodes. As some nodes may be stragglers, coding schemes have been roosed to tolerate stragglers in such distributed matrix multilication. However, existing coding schemes tyically slit the matrices in only one or two dimensions, limiting their caabilities to handle large-scale matrix multilication. Three-dimensional coding, however, does not have any code construction that achieves the otimal number of tasks required for decoding, with the best result achieved by entangled olynomial (EP) codes. In this aer, we roose dual entangled olynomial (DEP) codes that require around 25% fewer tasks than EP codes by executing two matrix multilications on each task. With exeriments in a real cloud environment, we show that DEP codes can also save the decoding overhead and memory consumtion of tasks.
"
937,2019,Neural Joint Source-Channel Coding,Poster,"For reliable transmission across a noisy communication channel, classical results from information theory show that it is asymtotically otimal to searate out the source and channel coding rocesses. However, this decomosition can fall short in the finite bit-length regime, as it requires non-trivial tuning of hand-crafted codes and assumes infinite comutational ower for decoding. In this work, we roose to jointly learn the encoding and decoding rocesses using a new discrete variational autoencoder model. By adding noise into the latent codes to simulate the channel during training, we learn to both comress and error-correct given a fixed bit-length and comutational budget. We obtain codes that are not only cometitive against several searation schemes, but also learn useful robust reresentations of the data for downstream tasks such as classification. Finally, inference amortization yields an extremely fast neural decoder, almost an order of magnitude faster comared to standard decoding methods based on iterative belief roagation.
"
938,2019,Doubly-Competitive Distribution Estimation,Poster,"Distribution estimation is a statistical-learning cornerstone. Its classical \emh{min-max} formulation minimizes the estimation error for the worst distribution, hence under-erforms for ractical distributions that, like ower-law, are often rather simle. Modern research has therefore focused on two frameworks: \emh{structural} estimation that imroves learning accuracy by assuming a simle structure of the underlying distribution; and \emh{cometitive}, or \emh{instance-otimal}, estimation that achieves the erformance of a genie aided estimator u to a small excess error that vanishes as the samle size grows, regardless of the distribution. This aer combines and strengthens the two frameworks. It designs a single estimator whose excess error vanishes both at a universal rate as the samle size grows, as well as when the (unknown) distribution gets simler. We show that the resulting algorithm significantly imroves the erformance guarantees for numerous cometitive- and structural-estimation results. The algorithm runs in near-linear time and is robust to model missecification and domain-symbol ermutations. 
"
939,2019,Homomorphic Sensing,Poster,"A recent line of research termed ""unlabeled sensing"" and ""shuffled linear regression"" has been exloring under great generality the recovery of signals from subsamled and ermuted measurements; a challenging roblem in diverse fields of data science and machine learning. In this aer we introduce an abstraction of this roblem which we call ""homomorhic sensing"". Given a linear subsace and a finite set of linear transformations we develo an algebraic theory which establishes conditions guaranteeing that oints in the subsace are uniquely determined from their homomorhic image under some transformation in the set. As a secial case, we recover known conditions for unlabeled sensing, as well as new results and extensions. On the algorithmic level we exhibit two dynamic rogramming based algorithms, which to the best of our knowledge are the first working solutions for the unlabeled sensing roblem for small dimensions. One of them, additionally based on branch-and-bound, when alied to image registration under affine transformations, erforms on ar with or outerforms state-of-the-art methods on benchmark datasets.
"
940,2019,Phaseless PCA: Low-Rank Matrix Recovery from Column-wise Phaseless Measurements,Poster,"This work rooses the first set of simle, ractically useful, and rovable algorithms for two inter-related roblems. (i) The first is low-rank matrix recovery from magnitude-only (haseless) linear rojections of each of its columns. This finds imortant alications in haseless dynamic imaging, e.g., Fourier Ptychograhic imaging of live biological secimens. Our guarantee shows that, in the regime of small ranks, the samle comlexity required is only a little larger than the order-otimal one, and much smaller than what standard (unstructured) hase retrieval methods need. %Moreover our algorithm is fast and memory-efficient if only the minimum required number of measurements is used (ii) The second roblem we study is a dynamic extension of the above: it allows the low-dimensional subsace from which each imagesignal (each column of the low-rank matrix) is generated to change with time. We introduce a simle algorithm that is rovably correct as long as the subsace changes are iecewise constant.
"
941,2019,Rate Distortion For Model Compression:From Theory To Practice,Poster,"The enormous size of modern dee neural net-works makes it challenging to deloy those models in memory and communication limited scenarios. Thus, comressing a trained model without a significant loss in erformance has become an increasingly imortant task. Tremendous advances has been made recently, where the main technical building blocks are runing, quantization, and low-rank factorization. In this aer, we roose rinciled aroaches to imrove uon the common heuristics used in those building blocks, by studying the fundamental limit for model comression via the rate distortion theory. We rove a lower bound for the rate distortion function for model comression and rove its achievability for linear models. Although this achievable comression scheme is intractable in ractice, this analysis motivates a novel objective function for model comression, which can be used to imrove classes of model comressor such as runing or quantization. Theoretically, we rove that the roosed scheme is otimal for comressing one-hidden-layer ReLU neural networks. Emirically,we show that the roosed scheme imroves uon the baseline in the comression-accuracy tradeoff.
"
942,2019,Formal Privacy for Functional Data with Gaussian Perturbations,Poster,"Motivated by the raid rise in statistical tools in {\it Functional Data Analysis}, we consider the Gaussian mechanism for achieving differential rivacy (DP) with arameter estimates taking values in a, otentially infinite-dimensional, searable Banach sace.  Using classic results from robability theory, we show how densities over function saces can be utilized to achieve the desired DP bounds.  This extends rior results of Hall et al (2013) to a much broader class of statistical estimates and summaries, including ``ath level"" summaries, nonlinear functionals, and full function releases.  By focusing on Banach saces, we rovide a deeer icture of the challenges for rivacy with comlex data, esecially the role regularization lays in balancing utility and rivacy.  Using an alication to enalized smoothing, we highlight this balance in the context of mean function estimation.  Simulations and an  alication to {diffusion tensor imaging} are briefly resented, with extensive additions included in a sulement.
"
943,2019,Graphical-model based estimation and inference for differential privacy,Poster,"Many rivacy mechanisms reveal high-level information about a data distribution through noisy measurements. It is common to use this information to estimate the answers to new queries. In this work, we rovide an aroach to solve this estimation roblem efficiently using grahical models, which is articularly effective when the distribution is high-dimensional but the measurements are over low-dimensional marginals. We show that our aroach is far more efficient than existing
estimation techniques from the rivacy literature and that it can imrove the accuracy and scalability of many state-of-the-art mechanisms.
"
944,2019,White-box vs Black-box: Bayes Optimal Strategies for Membership Inference,Poster,"Membershi inference determines, given a samle and trained arameters of a machine learning model, whether the samle was art of the training set. In this aer, we derive the otimal strategy for membershi inference with a few assumtions on the distribution of the arameters. We show that otimal attacks only deend on the loss function, and thus black-box attacks are as good as white-box attacks. As the otimal strategy is not tractable, we rovide aroximations of it leading to several inference methods, and show that existing membershi inference methods are coarser aroximations of this otimal strategy. Our membershi attacks outerform the state of the art in various settings, ranging from a simle logistic regression to more comlex architectures and datasets, such as ResNet-101 and Imagenet. 
"
945,2019,An Optimal Private Stochastic-MAB Algorithm based on Optimal Private Stopping Rule,Poster,"We resent a rovably otimal differentially rivate algorithm for the stochastic multi-arm bandit roblem, as oosed to the rivate analogue of the UCB-algorithm (Mishra and Thakurta, 2015; Tossou and Dimitrakakis, 2016) which doesn't meet the recently discovered lower-bound of $\Omega \left(\frac{K\log(T)}{\esilon} \right)$ (Shariff and Sheffet, 2018). Our construction is based on a different algorithm, Successive Elimination (Even-Dar et al., 2002), that reeatedly ulls all remaining arms until an arm is found to be subotimal and is then eliminated. In order to devise a rivate analogue of Successive Elimination we visit the roblem of rivate \emh{stoing rule}, that takes as inut a stream of i.i.d samles from an unknown distribution and returns a \emh{multilicative} $(1 \m \alha)$-aroximation of the distribution's mean, and rove the otimality of our rivate stoing rule. We then resent the rivate Successive Elimination algorithm which meets both the non-rivate lower bound (Lai and Robbins, 1985) and the above-mentioned rivate lower bound. We also comare emirically the erformance of our algorithm with the rivate UCB algorithm."
946,2019,Sublinear Space Private Algorithms Under the Sliding Window Model,Poster,"The Differential rivacy overview of Ale states, ``Ale retains the collected data for a maximum of three months."" Analysis of recent data is formalized by the {\em sliding window model}. This begs the question: what is the rice of rivacy in the sliding window model? In this aer, we study heavy hitters in the sliding window model with window size $w$. Previous works of Chan et al. (2012) estimates heavy hitters with an error of order $\theta w$ for a constant $\theta 0$. In this aer, we give an efficient differentially rivate algorithm to estimate heavy hitters in the sliding window model with $\widetilde O(w^{34})$ additive error and using $\widetilde O(\sqrt{w})$ sace.
"
947,2019,Locally Private Bayesian Inference for Count Models,Poster,"We resent a general and modular method for rivacy-reserving
Bayesian inference for Poisson factorization, a broad class of models
that includes some of the most widely used models in the social
sciences. Our method satisfies limited-recision local rivacy, a
generalization of local differential rivacy that we introduce to
formulate aroriate rivacy guarantees for sarse count data. We
resent an MCMC algorithm that aroximates the osterior distribution
over the latent variables conditioned on data that has been locally
rivatized by the geometric mechanism. Our method is based on two
insights: 1) a novel reinterretation of the geometric mechanism in
terms of the Skellam distribution and 2) a general theorem that
relates the Skellam and Bessel distributions. We demonstrate our
method's utility using two case studies that involve real-world email
data. We show that our method consistently outerforms the commonly
used naive aroach, wherein inference roceeds as usual, treating
the locally rivatized data as if it were not rivatized.
"
948,2019,Low Latency Privacy Preserving Inference,Poster,"When alying machine learning to sensitive data, one has to find a balance between accuracy, information security, and comutational-comlexity. Recent studies combined Homomorhic Encrytion with neural networks to make inferences while rotecting against information leakage. However, these methods are limited by the width and deth of neural networks that can be used (and hence the accuracy) and exhibit high latency even for relatively simle networks. In this study we rovide two solutions that address these limitations. In the first solution, we resent more than 10\times imrovement in latency and enable inference on wider networks comared to rior attemts with the same level of security. The imroved erformance is achieved by novel methods to reresent the data during the comutation. In the second solution, we aly the method of transfer learning to rovide rivate inference services using dee networks with latency of \sim0.16 seconds. We demonstrate the efficacy of our methods on several comuter vision tasks.
"
949,2019,Communication Complexity in Locally Private Distribution Estimation and Heavy Hitters,Poster,"We consider the roblems of distribution estimation, and heavy hitter (frequency) estimation under rivacy, and communication constraints. While the  constraints have been studied searately, otimal schemes for one are sub-otimal for the other. We roose a samle-otimal $\es$-locally differentially rivate (LDP) scheme for distribution estimation, where each user communicates one bit, and requires \emh{no} ublic randomness. We also show that Hadamard Resonse, a recently roosed scheme for $\es$-LDP distribution estimation is also utility-otimal for heavy hitters estimation. Our final result shows that unlike distribution estimation, without ublic randomness, any utility-otimal heavy hitter estimation algorithm must require $\Omega(\log n)$ bits of communication er user."
950,2019,Poission Subsampled R\'enyi Differential Privacy,Poster,"We consider the roblem of rivacy-amlification by under the Renyi Differential Privacy framework. This is the main technique underlying the moments accountants (Abadi et al., 2016) for differentially rivate dee learning. 
Unlike revious attemts on this roblem which deals with Samling with Relacement, we consider the Poisson subsamling scheme which selects each data oint indeendently with a coin toss. This allows us to significantly simlify and tighten the bounds for the RDP of subsamled mechanisms and derive numerically stable aroximation schemes.  In articular, for subsamled Gaussian mechanism and subsamled Lalace mechanism, we rove an analytical formula of their RDP that exactly matches the lower bound. The result is the first of its kind and we numerically demonstrate an order of magnitude imrovement in the rivacy-utility tradeoff. 
"
951,2019,Benefits and Pitfalls of the Exponential Mechanism with Applications to Hilbert Spaces and Functional PCA,Poster,"The exonential mechanism is a fundamental tool of Differential Privacy (DP) due to its strong rivacy guarantees and flexibility. We study its extension to settings with summaries based on infinite dimensional oututs such as with functional data analysis, shae analysis, and nonarametric statistics. We show that the mechanism must be designed with resect to a secific base measure over the outut sace, such as a Gaussian rocess. We rovide a ositive result that establishes a Central Limit Theorem for the exonential mechanism quite broadly. We also rovide a negative result, showing that the  magnitude of noise introduced for rivacy is asymtotically non-negligible relative to the statistical estimation error. We develo an $\e$-DP mechanism for functional rincial comonent analysis, alicable in searable Hilbert saces, and demonstrate its erformance via simulations and alications to two datasets."
952,2019,Refined Complexity of PCA with Outliers,Poster,"Princial comonent analysis (PCA) is one of the most fundamental rocedures in exloratory data analysis and is the basic ste in alications ranging from    quantitative finance and bioinformatics to image analysis and neuroscience. However, it is well-documented that the alicability of PCA in many real scenarios could be  constrained by an ""immune deficiency"" to outliers such as corruted observations. We consider the following algorithmic question about the PCA with outliers. For a set of $n$ oints in $\mathbb{R}^{d}$, how to learn a subset of  oints, say 1% of the total number of oints, such that the remaining  art of the oints is best fit into some unknown  $r$-dimensional subsace? We rovide a rigorous algorithmic analysis of the roblem. We show that the roblem is solvable in time $n^{O(d^2)}$. In articular, for constant dimension the roblem is solvable in olynomial time. We comlement the algorithmic result by the lower bound, showing that unless Exonential Time Hyothesis fails, in time $f(d)n^{o(d)}$, for any function $f$ of $d$, it is imossible not only to solve the roblem exactly but even to aroximate it within a constant factor."
953,2019,On Efficient Optimal Transport: An Analysis of Greedy and Accelerated Mirror Descent Algorithms,Poster,"We rovide theoretical analyses for two algorithms that solve the regularized otimal transort (OT) roblem between two discrete robability measures with at most $n$ atoms. We show that a greedy variant of the classical Sinkhorn algorithm, known as the \emh{Greenkhorn algorithm}, can be imroved to $\bigOtil\left(n^2\varesilon^2\right)$, imroving on the best known comlexity bound of $\bigOtil\left(n^2\varesilon^3\right)$. This matches the best known comlexity bound for the Sinkhorn algorithm and hels exlain why the Greenkhorn algorithm outerforms the Sinkhorn algorithm in ractice. Our roof technique is based on a rimal-dual formulation and rovide a \textit{tight} uer bound for the dual solution, leading to a class of \emh{adative rimal-dual accelerated mirror descent} (APDAMD) algorithms.  We rove that the comlexity of these algorithms is $\bigOtil\left(n^2\sqrt{\gamma}\varesilon\right)$ in which $\gamma \in (0, n]$ refers to some constants in the Bregman divergence. Exerimental results on synthetic and real datasets demonstrate the favorable erformance of the Greenkhorn and APDAMD algorithms in ractice."
954,2019,Passed & Spurious: Descent Algorithms and Local Minima in Spiked Matrix-Tensor Models,Poster,"In this work we analyse quantitatively the interlay between the loss landscae and erformance of descent algorithms in a rototyical inference roblem, the siked matrix-tensor model. We study a loss function that is the negative log-likelihood of the model. We analyse the number of local minima at a fixed distance from the signalsike with the Kac-Rice formula, and locate trivialization of the landscae at large signal-to-noise ratios. We evaluate analytically the erformance of a gradient flow algorithm using integro-differential PDEs as develoed in hysics of disordered systems for the Langevin dynamics.
We analyze the erformance of an aroximate message assing algorithm estimating the maximum likelihood configuration via its state evolution. We conclude by comaring the above results: while we observe a
drastic slow down of the gradient flow dynamics even in the region
where the landscae is trivial, both the analyzed algorithms are shown
to erform well even in the art of the region of arameters where
surious local minima are resent. 
"
955,2019,Teaching a black-box learner,Poster,"One widely-studied model of {\it teaching} calls for a teacher to rovide the minimal set of labeled examles that uniquely secifies a target concet. The assumtion is that the teacher knows the learner's hyothesis class, which is often not true of real-life teaching scenarios. We consider the roblem of teaching a learner whose reresentation and hyothesis class are unknown---that is, the learner is a black box. We show that a teacher who does not interact with the learner can do no better than roviding random examles. We then rove, however, that with interaction, a teacher can efficiently find a set of teaching examles that is a rovably good aroximation to the otimal set. As an illustration, we show how this scheme can be used to {\it shrink} training sets for any family of classifiers: that is, to find an aroximately-minimal subset of training instances that yields the same classifier as the entire set.
"
956,2019,PAC Learnability of Node Functions in Networked Dynamical Systems,Poster,"We consider the PAC learnability of the local functions at the vertices of a discrete networked dynamical system, assuming that the underlying network is known. Our focus is on the learnability of threshold functions. We show that several variants of threshold functions are PAC learnable and rovide tight bounds on the samle comlexity. In general, when the inut consists of ositive and negative examles, we show that the concet class of threshold functions is not efficiently PAC learnable, unless NP = RP. Using a dynamic rogramming aroach, we show efficient PAC learnability when the number of negative examles is small. We also resent an efficient learner which is consistent with all the ositive examles and at least (1-1e) fraction of the negative examles. This algorithm is based on maximizing a submodular function under matroid constraints. By erforming exeriments on both synthetic and real-world networks, we study how the network structure and samle comlexity influence the quality of the inferred system.
"
957,2019,Online learning with kernel losses,Poster,"We resent a generalization of the adversarial linear bandits framework, where the underlying losses are kernel functions (with an associated reroducing kernel Hilbert sace) rather than linear functions. We study a version of the exonential weights algorithm and bound its regret in this setting. Under conditions on the eigen-decay of the kernel we rovide a shar characterization of the regret for this algorithm. When we have olynomial eigen-decay ($\mu_j \le \mathcal{O}(j^{-\beta})$), we find that the regret is bounded by $\mathcal{R}_n \le \mathcal{O}(n^{\beta2(\beta-1)})$. While under the assumtion of exonential eigen-decay ($\mu_j \le \mathcal{O}(e^{-\beta j })$) we get an even tighter bound on the regret $\mathcal{R}_n \le \tilde{\mathcal{O}}(n^{12})$. When the eigen-decay is olynomial we also show a \emh{non-matching} minimax lower bound on the regret of $\mathcal{R}_n \ge \Omega(n^{(\beta+1)2\beta})$ and a lower bound of $\mathcal{R}_n \ge \Omega(n^{12})$ when the decay in the eigen-values is exonentially fast.

We also study the full information setting when the underlying losses are kernel functions and resent an adated exonential weights algorithm and a conditional gradient descent algorithm."
958,2019,Nearest Neighbor and Kernel Survival Analysis: Nonasymptotic Error Bounds and Strong Consistency Rates,Poster,"We establish the first nonasymtotic error bounds for Kalan-Meier-based nearest neighbor and kernel survival robability estimators where feature vectors reside in metric saces. Our bounds imly rates of strong consistency for these nonarametric estimators and, u to a log factor, match an existing lower bound for conditional CDF estimation. Our roof strategy also yields nonasymtotic guarantees for nearest neighbor and kernel variants of the Nelson-Aalen cumulative hazards estimator. We exerimentally comare these methods on four datasets.
We find that for the kernel survival estimator, a good choice of kernel is one learned using random survival forests.
"
959,2019,Fast Rates for a kNN Classifier Robust to Unknown Asymmetric Label Noise,Poster,"We consider classification in the resence of class-deendent asymmetric label noise with unknown noise robabilities. In this setting, identifiability conditions are known, but additional assumtions were shown to be required for finite samle rates, and so far only the arametric rate has been obtained. Assuming these identifiability conditions, together with a measure-smoothness condition on the regression function and Tsybakov’s margin condition, we show that the Robust kNN classifier of Gao et al. attains, the mini-max otimal rates of the noise-free setting, u to a log factor, even when trained on data with unknown asymmetric label noise. Hence, our results rovide a solid theoretical backing for this emirically successful algorithm. By contrast the standard kNN is not even consistent in the setting of asymmetric label noise. A key idea in our analysis is a simle kNN based method for estimating the maximum of a function that requires far less assumtions than existing mode estimators do, and which may be of indeendent interest for noise roortion estimation and randomised otimisation roblems.
"
960,2019,Uniform Convergence Rate of the Kernel Density Estimator Adaptive to Intrinsic Volume Dimension,Poster,"We derive concentration inequalities for the suremum norm of the difference between a kernel density estimator (KDE) and its oint-wise exectation that hold uniformly over the selection of the bandwidth and under weaker conditions on the kernel and the data generating distribution than reviously used in the literature. We first roose a novel concet, called the volume dimension, to measure the intrinsic dimension of the suort of a robability distribution based on the rates of decay of the robability of vanishing Euclidean balls. Our bounds deend on the volume dimension and generalize the existing bounds derived in the literature. In articular, when the data-generating distribution has a bounded Lebesgue density or is suorted on a sufficiently well-behaved lower-dimensional manifold, our bound recovers the same convergence rate deending on the intrinsic dimension of the suort as ones known in the literature. At the same time, our results aly to more general cases, such as the ones of distribution with unbounded densities or suorted on a mixture of manifolds with different dimensions. Analogous bounds are derived for the derivative of the KDE, of any order. Our results are generally alicable but are esecially useful for roblems in geometric inference and toological data analysis, including level set estimation, density-based clustering, modal clustering and mode hunting, ridge estimation and ersistent homology.
"
961,2019,Maximum Likelihood Estimation for  Learning Populations of Parameters,Poster,"Consider a setting with $N$ indeendent individuals, each with an unknown arameter, $_i \in [0, 1]$ drawn from some unknown distribution $P^\star$. After observing the outcomes of $t$ indeendent Bernoulli trials, i.e., $X_i \sim \text{Binomial}(t, _i)$ er individual, our objective is to accurately estimate $P^\star$ in the sarse regime, namely when $t \ll N$. This roblem arises in numerous domains, including the social sciences, sychology, health-care, and biology, where the size of the oulation under study is usually large yet the number of observations er individual is often limited. 

Our main result shows that, in this sarse regime where $t \ll N$, the maximum likelihood estimator (MLE) is both statistically minimax otimal and efficiently comutable. Precisely, for sufficiently large $N$, the MLE achieves the information theoretic otimal error bound of $\mathcal{O}(\frac{1}{t})$ for $t  c\log{N}$, with regards to the earth mover's distance (between the estimated and true distributions). More generally, in an exonentially large interval of $t$ beyond $c \log{N}$, the MLE achieves the minimax error bound of $\mathcal{O}(\frac{1}{\sqrt{t\log N}})$. In contrast, regardless of how large $N$ is, the naive ""lug-in"" estimator for this roblem only achieves the sub-otimal error of $\Theta(\frac{1}{\sqrt{t}})$. Emirically, we also demonstrate the MLE erforms well on both synthetic as well as real datasets."
962,2019,Projection onto Minkowski Sums with Application to Constrained Learning,Poster,"We introduce block descent algorithms for rojecting onto Minkowski sums of sets. Projection onto such sets is a crucial ste in many statistical learning roblems, and may regularize comlexity of solutions to an otimization roblem or arise in dual formulations of enalty methods. We show that rojecting onto the Minkowski sum admits simle, efficient algorithms when comlications such as overlaing constraints ose challenges to existing methods. We rove that our algorithm converges linearly when sets are strongly convex or satisfy an error bound condition, and extend the theory and methods to encomass non-convex sets as well. We demonstrate emirical advantages in runtime and accuracy over cometitors in alications to $\ell_{1,}$-regularized learning, constrained lasso, and overlaing grou lasso."
963,2019,Blended Conditonal Gradients,Poster,"We resent a blended conditional gradient aroach for minimizing a smooth convex function over a olytoe P, combining the Frank–Wolfe algorithm (also called conditional gradient) with gradient-based stes, different from away stes and airwise stes, but still achieving linear convergence for strongly convex functions, along with good ractical erformance. Our aroach retains all favorable roerties of conditional gradient algorithms, notably avoidance of rojections onto P and maintenance of iterates as sarse convex combinations of a limited number of extreme oints of P. The algorithm is lazy, making use of inexensive inexact solutions of the linear rogramming subroblem that characterizes the conditional gradient aroach. It decreases measures of otimality (rimal and dual gas) raidly, both in the number of iterations and in wall-clock time, outerforming even the lazy conditional gradient algorithms of Braun et al. 2017. We also resent a streamlined version of the algorithm that alies when P is the robability simlex.
"
964,2019,Acceleration of SVRG and Katyusha X by Inexact Preconditioning,Poster,"Emirical risk minimization is an imortant class of otimization roblems with many oular machine learning alications, and stochastic variance reduction methods are oular choices for solving them. Among these methods, SVRG and Katyusha X (a Nesterov accelerated SVRG) achieve fast convergence without substantial memory requirement. In this aer, we roose to accelerate these two algorithms by \textit{inexact reconditioning}, the roosed methods emloy \textit{fixed} reconditioners, although the subroblem in each eoch becomes harder, it suffices to aly \textit{fixed} number of simle subroutines to solve it inexactly, without losing the overall convergence. As a result, this inexact reconditioning strategy gives rovably better iteration comlexity and gradient comlexity over SVRG and Katyusha X. We also allow each function in the finite sum to be nonconvex while the sum is strongly convex. In our numerical exeriments, we observe an on average $8\times$ seedu on the number of iterations and $7\times$ seedu on runtime."
965,2019,Characterization of Convex Objective Functions and Optimal Expected Convergence Rates for SGD,Poster,"We study Stochastic Gradient Descent (SGD) with diminishing ste sizes for convex objective functions. We introduce a definitional framework and theory that defines and characterizes a core roerty, called curvature, of convex objective functions. In terms of curvature we can derive a new inequality that can be used to comute an otimal sequence of diminishing ste sizes by solving a differential equation. Our exact solutions confirm known results in literature and allows us to fully characterize a new regularizer with its corresonding exected convergence rates.
"
966,2019,A Conditional-Gradient-Based Augmented Lagrangian Framework,Poster,"This aer considers a generic convex minimization temlate with affine constraints over a comact domain, which covers key semidefinite rogramming alications. The existing conditional gradient methods either do not aly to our temlate or are too slow in ractice. To this end, we roose a new conditional gradient method, based on a unified treatment of smoothing and augmented Lagrangian frameworks. The roosed method maintains favorable roerties of the classical conditional gradient method, such as chea linear minimization oracle calls and sarse reresentation of the decision variable.  We rove $O(1\sqrt{k})$ convergence rate for our method in the objective residual and the feasibility ga. This rate is essentially the same as the state of the art CG-tye methods for our roblem temlate, but the roosed method is arguably suerior in ractice comared to existing methods in various alications."
967,2019,SGD: General Analysis and Improved Rates,Poster,"We roose a  general yet simle theorem describing the convergence of SGD under the arbitrary samling aradigm.  Our theorem describes the convergence of an infinite array of variants of SGD, each of which is associated with a secific robability law governing the data selection rule used to form  minibatches. This is the first time such an analysis is erformed, and most of our variants of SGD were never exlicitly considered in the literature before.  Our analysis relies on the recently introduced notion of exected smoothness and does not rely on a uniform bound on the variance of the stochastic gradients. By secializing our theorem to different mini-batching strategies, such as samling with relacement and indeendent samling, we derive exact exressions for the stesize as a function of the mini-batch size. With this we can also determine the mini-batch size that otimizes the total comlexity, and show exlicitly that as the variance of the stochastic gradient evaluated at the minimum grows, so does the otimal mini-batch size. For zero variance, the otimal mini-batch size is one. Moreover, we rove insightful  stesize-switching rules  which describe when one should switch from a constant to a decreasing stesize regime. 
"
968,2019,Curvature-Exploiting Acceleration of Elastic Net Computations,Poster,"This aer introduces an efficient second-order method for solving the elastic net roblem. Its key innovation is a comutationally efficient technique for injecting curvature information in the otimization rocess which admits a strong theoretical erformance guarantee. In articular, we show imroved run time over oular first-order methods and quantify the seed-u in terms of statistical measures of the data matrix. The imroved time comlexity is the result of an extensive exloitation of the roblem structure and a careful combination of second-order information, variance reduction techniques, and momentum acceleration. Beside theoretical seed-u, exerimental results demonstrate great ractical erformance benefits of curvature information, esecially for ill-conditioned data sets.
"
969,2019,Decentralized Stochastic Optimization and Gossip Algorithms with Compressed Communication,Poster,"We consider decentralized stochastic otimization with the objective function (e.g. data samles for machine learning tasks) being distributed over n machines that can only communicate to their neighbors on a fixed communication grah. To address the communication bottleneck,  the nodes comress (e.g. quantize or sarsify) their model udates. We cover both unbiased and biased comression oerators with quality denoted by \delta &lt;= 1 (\delta=1 meaning no comression).
We (i) roose a novel gossi-based stochastic gradient descent algorithm, CHOCO-SGD, that converges at rate O(1(nT) + 1(T \rho^2 \delta)^2) for strongly convex objectives, where T denotes the number of iterations and \rho the eigenga of the connectivity matrix. We (ii) resent a novel gossi algorithm, CHOCO-GOSSIP, for the average consensus roblem that converges in time O(1(\rho^2\delta) \log (1\esilon)) for accuracy \esilon &gt; 0. This is (u to our knowledge) the first gossi algorithm that suorts arbitrary comressed messages for \delta &gt; 0 and still exhibits linear convergence. We (iii) show in exeriments that both of our algorithms do outerform the resective state-of-the-art baselines and CHOCO-SGD can reduce communication by at least two orders of magnitudes.
"
970,2019,Safe Grid Search with Optimal Complexity,Poster,"Poular machine learning estimators involve regularization arameters that can be challenging to tune, and standard strategies rely on grid search for this task.
In this aer, we revisit the techniques of aroximating the regularization  ath u to redefined tolerance $\esilon$ in a unified framework and show that its comlexity is $O(1\sqrt[d]{\esilon})$ for uniformly convex loss of order $d \geq 2$ and $O(1\sqrt{\esilon})$ for Generalized Self-Concordant functions. This framework encomasses least-squares but also logistic regression, a case that as far as we know was not handled as recisely in revious works. We leverage our technique to rovide refined bounds on the validation error as well as a ractical algorithm for hyerarameter tuning. The latter has global convergence guarantee when targeting a rescribed accuracy on the validation set. Last but not least, our aroach hels relieving the ractitioner from the (often neglected) task of selecting a stoing criterion when otimizing over the training set: our method automatically calibrates this criterion based on the targeted accuracy on the validation set."
971,2019,SAGA with Arbitrary Sampling,Poster,"We study the roblem of minimizing the average of a very large number of smooth functions, which is of key imortance in  training suervised  learning models.  One of the most celebrated methods in this context is the SAGA algorithm of Defazio et al. (2014). Desite years of research on the toic, a general-urose version of SAGA---one that would include arbitrary imortance samling and minibatching schemes---does not exist.  We remedy this situation and roose a general and flexible variant of SAGA following the arbitrary samling aradigm. We erform an iteration comlexity analysis of the method, largely ossible due to the construction of  new stochastic Lyaunov functions. We establish linear convergence rates in the smooth and  strongly convex regime, and  under certain error bound conditions also in a  regime without strong convexity.  Our rates match those of the rimal-dual method Quartz (Qu et al., 2015) for which an arbitrary samling analysis is available, which makes a significant ste towards closing the ga in our understanding of comlexity of rimal and dual methods for finite sum roblems. Finally, we show through exeriments that secific variants of our general SAGA method can erform better in ractice than other cometing methods.
"
972,2019,Natural Analysts in Adaptive Data Analysis,Poster,"Adative data analysis is frequently criticized for its essimistic generalization guarantees. The source of these essimistic bounds is a model that ermits arbitrary, ossibly adversarial analysts that otimally use information to bias results. While being a central issue in the field, still lacking are notions of natural analysts that allow for more otimistic bounds faithful to the reality that tyical analysts aren't adversarial. In this work, we roose notions of natural analysts that smoothly interolate between the otimal non-adative bounds and the best-known adative generalization bounds. To accomlish this, we model the analyst's knowledge as evolving according to the rules of an unknown dynamical system that takes in revealed information and oututs new statistical queries to the data. This allows us to restrict the analyst through different natural control-theoretic notions. One such notion corresonds to a recency bias, formalizing an inability to arbitrarily use distant information. Another comlementary notion formalizes an anchoring bias, a tendency to weight initial information more strongly. Both notions come with quantitative arameters that smoothly interolate between the non-adative case and the fully adative case, allowing for a rich sectrum of intermediate analysts that are neither non-adative nor adversarial. Natural not only from a cognitive ersective, we show that our notions also cature standard otimization methods, like gradient descent in various settings. This gives a new interretation to the fact that gradient descent tends to overfit much less than its adative nature might suggest.
"
973,2019,CapsAndRuns: An Improved Method for Approximately Optimal Algorithm Configuration,Poster,"We consider the roblem of configuring general-urose solvers to run efficiently on roblem instances drawn from an unknown distribution, a roblem of major interest in solver autoconfiguration. Following revious work, we focus on designing algorithms that find a configuration with near-otimal exected caed runtime while doing the least amount of work, with the ca chosen in a configuration-secific way so that most instances are solved. In this aer we resent a new algorithm, CasAndRuns, which finds a near-otimal configuration while using time that scales (in a roblem deendent way) with the otimal exected caed runtime, significantly strengthening revious results which could only guarantee a bound that scaled with the otentially much larger otimal exected uncaed runtime. The new algorithm is simler and more intuitive than the revious methods: first it estimates the otimal runtime ca for each configuration, then it uses a Bernstein race to find a near otimal configuration given the cas. Exeriments verify that our method can significantly outerform its cometitors.
"
974,2019,Leveraging Low-Rank Relations Between Surrogate Tasks in Structured Prediction,Poster,"We study the interlay between surrogate methods for structured rediction and techniques from multitask learning designed to leverage relationshis between surrogate oututs. br 
We roose an efficient algorithm based on trace norm regularization which, differently from revious methods, does not require exlicit knowledge of the codingdecoding functions of the surrogate framework. 
As a result, our algorithm can be alied to the broad class of roblems in which the surrogate sace is large or even infinite dimensional. We study excess risk bounds for trace norm regularized structured rediction roving the consistency and learning rates for our estimator. We also identify relevant regimes in which our aroach can enjoy better generalization erformance than revious methods. 
Numerical exeriments on ranking roblems indicate that enforcing low-rank relations among surrogate oututs may indeed rovide a significant advantage in ractice.
"
975,2019,Training Well-Generalizing Classifiers for Fairness Metrics and Other Data-Dependent Constraints,Poster,"Classifiers can be trained with data-deendent constraints to satisfy fairness goals, reduce churn, achieve a targeted false ositive rate, or other olicy goals. We study the generalization erformance for such constrained otimization roblems, in terms of how well the constraints are satisfied at evaluation time, given that they are satisfied at training time. To imrove generalization, we frame the roblem as a two-layer game where one layer otimizes the model arameters on a training dataset, and the other layer enforces the constraints on an indeendent validation dataset. We build on recent work in two-layer constrained otimization to show that if one uses this two-dataset aroach, then constraint generalization can be significantly imroved. As we illustrate exerimentally, this aroach works not only in theory, but also in ractice.
"
976,2019,Optimality Implies Kernel Sum Classifiers are Statistically Efficient,Poster,"We roose a novel combination of otimization tools with learning theory bounds in order to analyze the samle comlexity of otimal kernel sum classifiers. This contrasts the tyical learning theoretic results which hold for all (otentially subotimal) classifiers. Our work also justifies assumtions made in rior work on multile kernel learning. As a byroduct of our analysis, we also rovide a new form of Rademacher comlexity for hyothesis classes containing only otimal classifiers.
"
977,2019,The Implicit Fairness Criterion of Unconstrained Learning,Poster,"We clarify what fairness guarantees we can and cannot exect to follow from unconstrained machine learning. Secifically, we show that in many settings, unconstrained learning on its own imlies grou calibration, that is, the outcome variable is conditionally indeendent of grou membershi given the score. 
A lower bound confirms the otimality of our uer bound. Moreover, we rove that as the excess risk of the learned score decreases, the more strongly it violates searation and indeendence, two other standard fairness criteria. Our results challenge the view that grou calibration necessitates an active intervention, suggesting that often we ought to think of it as a byroduct of unconstrained machine learning. 
"
978,2019,Weak Detection of Signal in the Spiked Wigner Model,Poster,"We consider the roblem of detecting the resence of the signal in a rank-one signal-lus-noise data matrix. In case the signal-to-noise ratio is under the threshold below which a reliable detection is imossible, we roose a hyothesis test based on the linear sectral statistics of the data matrix. When the noise is Gaussian, the error of the roosed test is otimal as it matches the error of the likelihood ratio test that minimizes the sum of the Tye-I and Tye-II errors. The test is data-driven and does not deend on the distribution of the signal or the noise. If the density of the noise is known, it can be further imroved by an entrywise transformation to lower the error of the test.
"
979,2019,Rademacher Complexity for Adversarially Robust Generalization,Poster,"Many machine learning models are vulnerable to adversarial attacks; for examle, adding adversarial erturbations that are imercetible to humans can often make machine learning models roduce wrong redictions with high confidence; moreover, although we may obtain robust models on the training dataset via adversarial training, in some roblems the learned models cannot generalize well to the test data.
In this aer, we focus on $\ell_\infty$ attacks, and study the adversarially robust generalization roblem through the lens of Rademacher comlexity.
For binary linear classifiers, we rove tight bounds for the adversarial Rademacher comlexity, and show that the adversarial Rademacher comlexity is never smaller than its natural counterart, and it has an unavoidable dimension deendence, unless the weight vector has bounded $\ell_1$ norm, and our results also extend to multi-class linear classifiers; in addition, for (nonlinear) neural networks, we show that the dimension deendence in the adversarial Rademacher comlexity also exists.
We further consider a surrogate adversarial loss for one-hidden layer ReLU network and rove margin bounds for this setting.
Our results indicate that having $\ell_1$ norm constraints on the weight matrices might be a otential way to imrove generalization in the adversarial setting.
We demonstrate exerimental results that validate our theoretical findings."
980,2019,Provably efficient RL with Rich Observations via Latent State Decoding,Poster,"We study the exloration roblem in eisodic MDPs with rich observations generated from a small number of latent states. Under certain identifiability assumtions, we demonstrate how to estimate a maing from the observations to latent states inductively through a sequence of regression and clustering stes---where reviously decoded latent states rovide labels for later regression roblems---and use it to construct good exloration olicies. We rovide finite-samle guarantees on the quality of the learned state decoding function and exloration olicies, and comlement our theory with an emirical evaluation on a class of hard exloration roblems. Our method exonentially imroves over $Q$-learning with na\""ive exloration, even when $Q$-learning has cheating access to latent states."
981,2019,Information-Theoretic Considerations in Batch Reinforcement Learning,Poster,"Value-function aroximation methods that oerate in batch mode have foundational imortance to reinforcement learning (RL). Finite samle guarantees for these methods often crucially rely on two tyes of assumtions: (1) mild distribution shift, and (2) reresentation conditions that are stronger than realizability. However, the necessity (“why do we need them?”) and the naturalness (“when do they hold?”) of such assumtions have largely eluded the literature. In this aer, we revisit these assumtions and rovide theoretical results towards answering the above questions, and make stes towards a deeer understanding of value-function aroximation.
"
982,2019,A Contrastive Divergence for Combining Variational Inference and MCMC,Poster,"We develo a method to combine Markov chain Monte Carlo (MCMC) and variational inference (VI), leveraging the advantages of both inference aroaches. Secifically, we imrove the variational distribution by running a few MCMC stes. To make inference tractable, we introduce the variational contrastive divergence (VCD), a new divergence that relaces the standard Kullback-Leibler (KL) divergence used in VI. The VCD catures a notion of discreancy between the initial variational distribution and its imroved version (obtained after running the MCMC stes), and it converges asymtotically to the symmetrized KL divergence between the variational distribution and the osterior of interest. The VCD objective can be otimized efficiently with resect to the variational arameters via stochastic otimization. We show exerimentally that otimizing the VCD leads to better redictive erformance on two latent variable models: logistic matrix factorization and variational autoencoders (VAEs).
"
983,2019,Calibrated Approximate Bayesian Inference,Poster,"We give a general urose comutational framework for estimating the bias in coverage resulting from making aroximations in Bayesian inference. Coverage is the robability credible sets cover true arameter values. We show how to estimate the actual coverage an aroximation scheme achieves when the ideal observation model and the rior can be simulated, but have been relaced, in the Monte Carlo, with aroximations as they are intractable.  Coverage estimation rocedures given in Lee et al. (2018) work well on simle roblems, but are biased, and do not scale well, as those authors note.   For examle, the methods of Lee et al. (2018) fail for calibration of an aroximate comletely collased MCMC algorithm for artition structure in a Dirichlet rocess for clustering grou labels in a hierarchical model. By exloiting the symmetry of the coverage error under ermutation of low level grou labels and smoothing with Bayesian Additive Regression Trees, we are able to show that the original aroximate inference had oor coverage and should not be trusted. 
"
984,2019,Moment-Based Variational Inference for Markov Jump Processes,Poster,"We roose moment-based variational inference as a flexible framework for aroximate smoothing of latent Markov jum rocesses. The main ingredient of our aroach is to artition the set of all transitions of the latent rocess into classes. This allows to exress the Kullback-Leibler divergence from the aroximate to the osterior rocess in terms of a set of moment functions that arise naturally from the chosen artition. To illustrate ossible choices of the artition, we consider secial classes of jum rocesses that frequently occur in alications. We then extend the results to latent arameter inference and demonstrate the method on several examles.
"
985,2019,Understanding MCMC Dynamics as Flows on the Wasserstein Space,Poster,"It is known that the Langevin dynamics used in MCMC is the gradient flow of the KL divergence on the Wasserstein sace, which hels convergence analysis and insires recent article-based variational inference methods (ParVIs). But no more MCMC dynamics is understood in this way. In this work, by develoing novel concets, we roose a theoretical framework that recognizes a general MCMC dynamics as the fiber-gradient Hamiltonian flow on the Wasserstein sace of a fiber-Riemannian Poisson manifold. The ""conservation + convergence"" structure of the flow gives a clear icture on the behavior of general MCMC dynamics. The framework also enables ParVI simulation of MCMC dynamics, which enriches the ParVI family with more efficient dynamics, and also adats ParVI advantages to MCMCs. We develo two ParVI methods for a articular MCMC dynamics and demonstrate the benefits in exeriments.
"
986,2019,LR-GLM: High-Dimensional Bayesian Inference Using Low-Rank Data Approximations,Poster,"Due to the ease of modern data collection, alied statisticians often have access to a large set of covariates that they wish to relate to some observed outcome. 
 Generalized linear models (GLMs) offer a articularly interretable framework for such an analysis.  In these high-dimensional roblems, the number of covariates is often large relative to the number of observations, so we face non-trivial inferential uncertainty;  a Bayesian aroach allows coherent quantification of this uncertainty. Unfortunately, existing methods for Bayesian inference in GLMs require running times roughly cubic in arameter dimension,  and so are limited to settings with at most tens of thousand arameters.  We roose to reduce time and memory costs with a low-rank aroximation of the data in an aroach we call LR-GLM. When used with the Lalace aroximation or Markov chain Monte Carlo, LR-GLM rovides a full Bayesian osterior aroximation  and admits running times reduced by a full factor of the arameter dimension.  We rigorously establish the quality of our aroximation and show how the choice of rank allows a tunable  comutational--statistical trade-off.  Exeriments suort our theory and demonstrate the efficacy of LR-GLM on real large-scale datasets.
"
987,2019,Amortized Monte Carlo Integration,Poster,"Current aroaches to amortizing Bayesian inference focus solely on aroximating the osterior distribution. Tyically, this aroximation is, in turn, used to calculate exectations for one or more target functions—a comutational ieline which is inefficient when the target function(s) are known ufront. In this aer, we address this inefficiency by introducing AMCI, a method for amortizing Monte Carlo integration directly. AMCI oerates similarly to amortized inference but roduces three distinct amortized roosals, each tailored to a different comonent of the overall exectation calculation. At runtime, samles are roduced searately from each amortized roosal, before being combined to an overall estimate of the exectation. We show that while existing aroaches are fundamentally limited in the level of accuracy they can achieve, AMCI can theoretically roduce arbitrarily small errors for any integrable target function using only a single samle from each roosal at runtime. We further show that it is able to emirically outerform the theoretically otimal selfnormalized imortance samler on a number of examle roblems. Furthermore, AMCI allows not only for amortizing over datasets but also amortizing over target functions.
"
988,2019,Stein Point Markov Chain Monte Carlo,Poster,"An imortant task in machine learning and statistics is the aroximation of a robability measure by an emirical measure suorted on a discrete oint set. Stein Points are a class of algorithms for this task, which roceed by sequentially minimising a Stein discreancy between the emirical measure and the target and, hence, require the solution of a non-convex otimisation roblem to obtain each new oint. This aer removes the need to solve this otimisation roblem by, instead, selecting each new oint based on a Markov chain samle ath. This significantly reduces the comutational cost of Stein Points and leads to a suite of algorithms that are straightforward to imlement. The new algorithms are illustrated on a set of challenging Bayesian inference roblems, and rigorous theoretical guarantees of consistency are established.
"
989,2019,Fast and Simple Natural-Gradient Variational Inference with Mixture of Exponential-family Approximations,Poster,"Natural-gradient methods enable fast and simle algorithms for variational inference, but due to comutational difficulties, their use is mostly limited to minimal exonential-family (EF) aroximations.
In this aer, we extend their alication to estimate structured aroximations such as mixtures of EF distributions.
Such aroximations can fit comlex, multimodal osterior distributions and are generally more accurate than unimodal EF aroximations.
By using a minimal conditional-EF reresentation of such aroximations, we derive simle natural-gradient udates. 
Our emirical results demonstrate a faster convergence of our natural-gradient method comared to black-box gradient-based methods. Our work exands the scoe of natural gradients for Bayesian inference and makes them more widely alicable than before.
"
990,2019,Particle Flow Bayes' Rule,Poster,"We resent a article flow realization of Bayes' rule, where an ODE-based neural oerator is used to transort articles from a rior to its osterior after a new observation. We rove that such an ODE oerator exists. Its neural arameterization can be trained in a meta-learning framework, allowing this oerator to reason about the effect of an individual observation on the osterior, and thus generalize across different riors, observations and to sequential Bayesian inference. We demonstrated the generalization ability of our article flow Bayes oerator in several canonical and high dimensional examles.
"
991,2019,Correlated Variational Auto-Encoders,Poster,"Variational Auto-Encoders (VAEs) are caable of learning latent reresentations for high dimensional data. However, due to the i.i.d. assumtion, VAEs only otimize the singleton variational distributions and fail to account for the correlations between data oints, which might be crucial for learning latent reresentations from dataset where a riori we know correlations exist. We roose Correlated Variational Auto-Encoders (CVAEs) that can take the correlation structure into consideration when learning latent reresentations with VAEs. CVAEs aly a rior based on the correlation structure. To address the intractability introduced by the correlated rior, we develo an aroximation by average of a set of tractable lower bounds over all maximal acyclic subgrahs of the undirected correlation grah. Exerimental results on matching and link rediction on ublic benchmark rating datasets and sectral clustering on a synthetic dataset show the effectiveness of the roosed method over baseline algorithms.
"
992,2019,Towards a Unified Analysis of Random Fourier Features,Poster,"Random Fourier features is a widely used, simle, and effective technique for scaling u kernel methods. The existing theoretical analysis of the aroach, however, remains focused on secific learning tasks and tyically gives essimistic bounds which are at odds with the emirical results. We tackle these roblems and rovide the first unified risk analysis of learning with random Fourier features using the squared error and Lischitz continuous loss functions. In our bounds, the trade-off between the comutational cost and the exected risk convergence rate is roblem secific and exressed in terms of the regularization arameter and the \emh{number of effective degrees of freedom}. We study both the standard random Fourier features method for which we imrove the existing bounds on the number of features required to guarantee the corresonding minimax risk convergence rate of kernel ridge regression, as well as a data-deendent modification which samles features roortional to \emh{ridge leverage scores} and further reduces the required number of features. As ridge leverage scores are exensive to comute, we devise a simle aroximation scheme which rovably reduces the comutational cost without loss of statistical efficiency.
"
993,2019,Learning deep kernels for exponential family densities,Poster,"The kernel exonential family is a rich class of distributions, which can be fit efficiently and with statistical guarantees by score matching. Being required to choose a riori a simle kernel such as the Gaussian, however, limits its ractical alicability. We rovide a scheme for learning a kernel arameterized by a dee network, which can find comlex location-deendent local features of the data geometry. This gives a very rich class of density models, caable of fitting comlex structures on moderate-dimensional roblems. Comared to dee density models fit via maximum likelihood, our aroach rovides a comlementary set of strengths and tradeoffs: in emirical studies, the former can yield higher likelihoods, whereas the latter gives better estimates of the gradient of the log density, the score, which describes the distribution's shae.
"
994,2019,Bayesian Deconditional Kernel Mean Embeddings,Poster,"Conditional kernel mean embeddings form an attractive nonarametric framework for reresenting conditional means of functions, describing the observation rocesses for many comlex models. However, the recovery of the original underlying function of interest whose conditional mean was observed is a challenging inference task. We formalize deconditional kernel mean embeddings as a solution to this inverse roblem, and show that it can be naturally viewed and used as a nonarametric Bayes' rule. Critically, we introduce the notion of task transformed Gaussian rocesses and establish deconditional kernel means embeddings as their osterior redictive mean. This connection rovides Bayesian interretations and uncertainty estimates for deconditional kernel means, exlains their regularization hyerarameters, and rovides a marginal likelihood for kernel hyerarameter learning. They further enable ractical alications such as learning sarse reresentations for big data and likelihood-free inference.
"
995,2019,A Kernel Perspective for Regularizing Deep Neural Networks,Poster,"We roose a new oint of view for regularizing dee neural networks by using the norm of a reroducing kernel Hilbert sace (RKHS). Even though this norm cannot be comuted, it admits uer and lower aroximations leading to various ractical strategies. Secifically, this ersective (i) rovides a common umbrella for many existing regularization rinciles, including sectral norm and gradient enalties, or adversarial training, (ii) leads to new effective regularization enalties, and (iii) suggests hybrid strategies combining lower and uer bounds to get better aroximations of the RKHS norm. We exerimentally show this aroach to be effective when learning on small datasets, or to obtain adversarially robust models.
"
996,2019,A Persistent Weisfeiler--Lehman Procedure for Graph Classification,Poster,"The Weisfeiler--Lehman grah kernel exhibits cometitive erformance
in many grah classification tasks. However, its subtree features are not able to cature connected comonents and cycles, toological features known for characterising grahs. To extract such features, we leverage roagated node label
information and transform unweighted grahs into metric ones. This ermits us to augment the subtree features with toological information obtained using ersistent homology, a concet from toological data analysis. Our method, which we formalise as a generalisation of Weisfeiler--Lehman subtree features, exhibits favourable classification accuracy and its imrovements in redictive erformance are mainly driven by including cycle information.
"
997,2019,Rehashing Kernel Evaluation in High Dimensions,Poster,"Kernel methods are effective but do not scale well to large scale data, esecially in high dimensions where the geometric data structures used to accelerate kernel evaluation suffer from the curse of dimensionality. 
Recent theoretical advances have  roosed fast kernel evaluation algorithms leveraging hashing techniques with worst-case asymtotic imrovements. However, these advances are largely confined to the theoretical realm due to concerns such as suer-linear rerocessing time and diminishing gains in non-worst case datasets. In this aer, we close the ga between theory and ractice by addressing these challenges via rovable and ractical rocedures for adative samle size selection, rerocessing time reduction, and refined variance bounds that quantify the data-deendent erformance of random samling and hashing-based kernel evaluation methods. Our exeriments show that these new tools offer u to $10\times$ imrovement in evaluation time on a range of synthetic and real-world datasets."
998,2019,Large-Scale Sparse Kernel Canonical Correlation Analysis,Poster,"This aer resents gradKCCA, a large-scale sarse non-linear canonical correlation method. Like Kernel Canonical Correlation Analysis (KCCA), our method finds non-linear relations through kernel functions, but it does not rely on a kernel matrix, a known bottleneck for scaling u kernel methods. gradKCCA corresonds to solving KCCA with the additional constraint that the canonical rojection directions in the kernel-induced feature sace have reimages in the original data sace. Firstly, this modification allows us to very efficiently maximize kernel canonical correlation through an alternating rojected gradient algorithm working in the original data sace. Secondly, we can control the sarsity of the rojection directions by constraining the $\ell_1$ norm of the reimages of the rojection directions, facilitating the interretation of the discovered atterns, which is not available through KCCA. Our emirical exeriments demonstrate that gradKCCA outerforms state-of-the-art CCA methods in terms of seed and robustness to noise both in simulated and real-world datasets."
999,2019,A Kernel Theory of Modern Data Augmentation,Poster,"Data augmentation, a technique in which a training set is exanded with class-reserving transformations, is ubiquitous in modern machine learning ielines. In this aer, we seek to establish a theoretical framework for understanding data augmentation. We aroach this from two directions: First, we rovide a general model of augmentation as a Markov rocess, and show that kernels aear naturally with resect to this model, even when we do not emloy kernel classification. Next, we analyze more directly the effect of augmentation on kernel classifiers, showing that data augmentation can be aroximated by first-order feature averaging and second-order variance regularization comonents. These frameworks both serve to illustrate the ways in which data augmentation affects the downstream learning model, and the resulting analyses rovide novel connections between rior work in invariant kernels, tangent roagation, and robust otimization. Finally, we rovide several roof-of-concet alications showing that our theory can be useful for accelerating machine learning workflows, such as reducing the amount of comutation needed to train using augmented data, and redicting the utility of a transformation rior to training.
"
1000,2019,kernelPSI: a Post-Selection Inference Framework for Nonlinear Variable Selection,Poster,"Model selection is an essential task for many alications in scientific discovery. The most common aroaches rely on univariate linear measures of association between each feature and the outcome. Such classical selection rocedures fail to take into account nonlinear effects and interactions between features. Kernel-based selection rocedures have been roosed as a solution. However, current strategies for kernel selection fail to measure the significance of a joint model constructed through the combination of the basis kernels. In the resent work, we exloit recent advances in ost-selection inference to roose a valid statistical test for the association of a joint model of the selected kernels with the outcome. The kernels are selected via a ste-wise rocedure which we model as a succession of quadratic constraints in the outcome variable. 
"
1001,2019,Scalable Learning in Reproducing Kernel Krein Spaces,Poster,"We rovide the first mathematically comlete derivation of the Nyström method for low-rank aroximation of indefinite kernels and roose an efficient method for finding an aroximate eigendecomosition of such kernel matrices. Building on this result, we devise highly scalable methods for learning in reroducing kernel Krein saces. The devised aroaches rovide a rinciled and theoretically well-founded means to tackle large scale learning roblems with indefinite kernels. The main motivation for our work comes from roblems with structured reresentations (e.g., grahs, strings, time-series), where it is relatively easy to devise a airwise (dis)similarity function based on intuition andor knowledge of domain exerts. Such functions are tyically not ositive definite and it is often well beyond the exertise of ractitioners to verify this condition. The effectiveness of the devised aroaches is evaluated emirically using indefinite kernels defined on structured and vectorial data reresentations.
"
1002,2019,Dirichlet Simplex Nest and Geometric Inference,Poster,"We roose Dirichlet Simlex Nest, a class of robabilistic models suitable for a variety of data tyes, and develo fast and rovably accurate inference algorithms by accounting for the model's convex geometry and low dimensional simlicial structure. By exloiting the connection to Voronoi tessellation and roerties of Dirichlet distribution, the roosed inference algorithm is shown to achieve consistency and strong error bound guarantees on a range of model settings and data distributions. The effectiveness of our model and the learning algorithm is demonstrated by simulations and by analyses of text and financial data.
"
1003,2019,Bayesian leave-one-out cross-validation for large data,Poster,"Model inference, such as model comarison, model checking, and model selection, is an imortant art of model develoment. Leave-one-out cross-validation (LOO) is a general aroach for assessing the generalizability of a model, but unfortunately, LOO does not scale well to large datasets. We roose a combination of using aroximate inference techniques and robability-roortional-to-size-samling (PPS) for fast LOO model evaluation for large datasets. We rovide both theoretical and emirical results showing good roerties for large data.
"
1004,2019,Rao-Blackwellized Stochastic Gradients for Discrete Distributions,Poster,"We wish to comute the gradient of an exectation
over a finite or countably infinite samle
sace having K ≤ ∞ categories. When K is indeed
infinite, or finite but very large, the relevant
summation is intractable. Accordingly, various
stochastic gradient estimators have been roosed.
In this aer, we describe a technique that can be
alied to reduce the variance of any such estimator,
without changing its bias—in articular,
unbiasedness is retained. We show that our technique
is an instance of Rao-Blackwellization, and 
we demonstrate the imrovement it yields on a 
semi-suervised classification roblem and a ixel attention task.
"
1005,2019,Neurally-Guided Structure Inference,Poster,"Most structure inference methods either rely on exhaustive search or are urely data-driven. Exhaustive search robustly infers the structure of arbitrarily comlex data, but it is slow. Data-driven methods allow efficient inference, but do not generalize when test data have more comlex structures than training data. In this aer, we roose a hybrid inference algorithm, the Neurally-Guided Structure Inference (NG-SI), keeing the advantages of both search-based and data-driven methods. The key idea of NG-SI is to use a neural network to guide the hierarchical, layer-wise search over the comositional sace of structures. We evaluate our algorithm on two reresentative structure inference tasks: robabilistic matrix decomosition and symbolic rogram arsing. It outerforms data-driven and search-based alternatives on both tasks.
"
1006,2019,Bayesian Joint Spike-and-Slab Graphical Lasso,Poster,"In this article, we roose a new class of riors for Bayesian inference with multile Gaussian grahical models. We introduce Bayesian treatments of two oular rocedures, the grou grahical lasso and the fused grahical lasso, and extend them to a continuous sike-and-slab framework to allow self-adative shrinkage and model selection simultaneously.  We develo an EM algorithm that erforms fast and dynamic exlorations of osterior modes. Our aroach selects sarse models efficiently and automatically with substantially smaller bias than would be induced by alternative regularization rocedures. The erformance of the roosed methods are demonstrated through simulation and two real data examles.
"
1007,2019,Rotation Invariant Householder Parameterization for Bayesian PCA,Poster,"We consider robabilistic PCA and related factor models from a Bayesian ersective. These models are in general not identifiable as the likelihood has a rotational symmetry. This gives rise to comlicated osterior distributions with continuous subsaces of equal density and thus hinders efficiency of inference as well as interretation of obtained arameters. In articular, osterior averages over factor loadings become meaningless and only model redictions are unambiguous. Here, we roose a arameterization based on Householder transformations, which remove the rotational symmetry of the osterior. Furthermore, by relying on results from random matrix theory, we establish the arameter distribution which leaves the model unchanged comared to the original rotationally symmetric formulation. In articular, we avoid the need to comute the Jacobian determinant of the arameter transformation. This allows us to efficiently imlement robabilistic PCA in a rotation invariant fashion in any state of the art toolbox. Here, we imlemented our model in the robabilistic rogramming language Stan and illustrate it on several examles.
"
1008,2019,A Framework for Bayesian Optimization in Embedded Subspaces,Poster,"We resent a theoretically founded aroach for high-dimensional Bayesian otimization based on low-dimensional subsace embeddings.
We rove that the error in the Gaussian rocess model is bounded tightly when going from the original high-dimensional search domain to the low-dimensional embedding.  This imlies that the otimization rocess in the low-dimensional embedding roceeds essentially as if it were run directly on an unknown active subsace of low dimensionality.  The argument alies to a large class of algorithms and GP models, including non-stationary kernels. Moreover, we rovide an efficient imlementation based on hashing and demonstrate emirically that this subsace embedding achieves considerably better results than the reviously roosed methods for high-dimensional BO based on Gaussian matrix rojections and structure-learning.
"
1009,2019,Convolutional Poisson Gamma Belief Network,Poster,"For text analysis, one often resorts to a lossy reresentation that either comletely ignores word order or embeds each word as a low-dimensional dense feature vector. In this aer, we roose convolutional Poisson factor analysis (CPFA) that directly oerates on a lossless reresentation that rocesses the words in each document as a sequence of high-dimensional one-hot vectors. To boost its erformance, we further roose the convolutional Poisson gamma belief network (CPGBN) that coules CPFA with the gamma belief network via a novel robabilistic ooling layer. CPFA forms words into hrases and catures very secific hrase-level toics, and CPGBN further builds a hierarchy of increasingly more general hrase-level toics. For efficient inference, we develo both a Gibbs samler and a Weibull distribution based convolutional variational auto-encoder. Exerimental results demonstrate that CPGBN can extract high-quality text latent reresentations that cature the word order information, and hence can be leveraged as a building block to enrich a wide variety of existing latent variable models that ignore word order.
"
1010,2019,Automatic Posterior Transformation for Likelihood-Free Inference,Poster,"How can one erform Bayesian inference on stochastic simulators with intractable likelihoods? A recent aroach is to learn the osterior from adatively roosed simulations using neural network-based conditional density estimators. However, existing methods are limited to a narrow range of roosal distributions or require imortance weighting that can limit erformance in ractice. Here we resent automatic osterior transformation (APT), a new sequential neural osterior estimation method for simulation-based inference. APT can modify the osterior estimate using arbitrary, dynamically udated roosals, and is comatible with owerful flow-based density estimators. It is more flexible, scalable and efficient than revious simulation-based inference techniques. APT can oerate directly on high-dimensional time series and image data, oening u new alications for likelihood-free inference.
"
1011,2019,Active Learning for Decision-Making from Imbalanced Observational Data,Poster,"Machine learning can hel ersonalized decision suort by learning models to redict individual treatment effects (ITE). This work studies the reliability of rediction-based decision-making in a task of deciding which action $a$ to take for a target unit after observing its covariates $\tilde{x}$ and redicted outcomes $\hat{}(\tilde{y} \mid \tilde{x}, a)$. An examle case is ersonalized medicine and the decision of which treatment to give to a atient. A common roblem when learning these models from observational data is imbalance, that is, difference in treatedcontrol covariate distributions, which is known to increase the uer bound of the exected ITE estimation error. We roose to assess the decision-making reliability by estimating the ITE model's Tye S error rate, which is the robability of the model inferring the sign of the treatment effect wrong. Furthermore, we use the estimated reliability as a criterion for active learning, in order to collect new (ossibly exensive) observations, instead of making a forced choice based on unreliable redictions. We demonstrate the effectiveness of this decision-making aware active learning in two decision-making tasks: in simulated data with binary outcomes and in a medical dataset with synthetic and continuous treatment outcomes."
1012,2019,Validating Causal Inference Models via Influence Functions,Poster,"The roblem of estimating causal effects of treatments from observational data falls beyond the realm of suervised learning — because counterfactual data is inaccessible, we can never observe the true causal effects. In the absence of ""suervision"", how can we evaluate the erformance of causal inference methods? In this aer, we use influence functions — the functional derivatives of a loss function — to develo a model validation rocedure that estimates the estimation error of causal inference methods. Our rocedure utilizes a Taylor-like exansion to aroximate the loss function of a method on a given dataset in terms of the influence functions of its loss on a ""synthesized"", roximal dataset with known causal effects. Under minimal regularity assumtions, we show that our rocedure is consistent and efficient. Exeriments on 77 benchmark datasets show that using our rocedure, we can accurately redict the comarative erformances of state-of-the-art causal inference methods alied to a given observational study.
"
1013,2019,"Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks",Poster,"Predicting the number of clock cycles a rocessor takes to execute a block of assembly instructions in steady state (the throughut) is imortant for both comiler designers and erformance engineers. Building an analytical model to do so is esecially comlicated in modern x86-64 Comlex Instruction Set Comuter (CISC) machines with sohisticated rocessor microarchitectures in that it is tedious, error rone, and must be erformed from scratch for each rocessor generation. In this aer we resent Ithemal, the first tool which learns to redict the throughut of a set of instructions. Ithemal uses a hierarchical LSTM--based aroach to redict throughut based on the ocodes and oerands of instructions in a basic block. We show that Ithemal is more accurate than state-of-the-art hand-written tools currently used in comiler backends and static machine code analyzers. In articular, our model has less than half the error of state-of-the-art analytical models (LLVM's llvm-mca and Intel's IACA). Ithemal is also able to redict these throughut values just as fast as the aforementioned tools, and is easily orted across a variety of rocessor microarchitectures with minimal develoer effort.
"
1014,2019,Learning to Groove with Inverse Sequence Transformations,Poster,"We exlore models for translating abstract musical ideas (scores, rhythms) into exressive erformances using seq2seq and recurrent variational information bottleneck (VIB) models. Though seq2seq models usually require ainstakingly aligned corora, we show that it is ossible to adat an aroach from the Generative Adversarial Network (GAN) literature (e.g. Pix2Pix, Vid2Vid) to sequences, creating large volumes of aired data by erforming simle transformations and training generative models to lausibly invert these transformations. Music, and drumming in articular, rovides a strong test case for this aroach because many common transformations (quantization, removing voices) have clear semantics, and learning to invert them has real-world alications.  Focusing on the case of drum set layers, we create and release a new dataset for this urose, containing over 13 hours of recordings by rofessional drummers aligned with fine-grained timing and dynamics information.  We also exlore some of the creative otential of these models, demonstrating imrovements on state-of-the-art methods for Humanization (instantiating a erformance from a musical score).
"
1015,2019,Grid-Wise Control for Multi-Agent Reinforcement Learning in Video Game AI,Poster,"We consider the roblem of multi-agent reinforcement learning (MARL) in video game AI, where the agents are located in a satial grid-world environment and the number of agents varies both within and across eisodes. The challenge is to flexibly control an arbitrary number of agents while achieving effective collaboration. Existing MARL methods usually suffer from the trade-off between these two considerations. To address the issue, we roose a novel architecture that learns a satial joint reresentation of all the agents and oututs grid-wise actions. Each agent will be controlled indeendently by taking the action from the grid it occuies. By viewing the state information as a grid feature ma, we emloy a convolutional encoder-decoder as the olicy network. This architecture naturally romotes agent communication because of the large recetive field rovided by the stacked convolutional layers. Moreover, the satially shared convolutional arameters enable fast arallel exloration that the exeriences discovered by one agent can be immediately transferred to others. The roosed method can be conveniently integrated with general reinforcement learning algorithms, e.g., PPO and Q-learning. We demonstrate the effectiveness of the roosed method in extensive challenging multi-agent tasks in StarCraft II.
"
1016,2019,HOList: An Environment for Machine Learning of Higher Order Logic Theorem Proving,Poster,"We resent an environment, benchmark, and dee learning driven automated theorem rover for higher-order logic. Higher-order interactive theorem rovers enable the formalization of arbitrary mathematical theories and thereby resent an interesting challenge for dee learning. We rovide an oen-source framework based on the HOL Light theorem rover that can be used as a reinforcement learning environment. HOL Light comes with a broad coverage of basic mathematical theorems on calculus and the formal roof of the Keler conjecture, from which we derive a challenging benchmark for automated reasoning aroaches. We also resent a dee reinforcement learning driven automated theorem rover, DeeHOL, that gives strong initial results on this benchmark.
"
1017,2019,Molecular Hypergraph Grammar with Its Application to Molecular Optimization,Poster,"Molecular otimization aims to discover novel molecules with desirable roerties, and its two fundamental challenges are: (i) it is not trivial to generate valid molecules in a controllable way due to hard chemical constraints such as the valency conditions, and (ii) it is often costly to evaluate a roerty of a novel molecule, and therefore, the number of roerty evaluations is limited. These challenges are to some extent alleviated by a combination of a variational autoencoder (VAE) and Bayesian otimization (BO), where VAE converts a molecule intofrom its latent continuous vector, and BO otimizes a latent continuous vector (and its corresonding molecule) within a limited number of roerty evaluations. While the most recent work, for the first time, achieved 100% validity, its architecture is rather comlex due to auxiliary neural networks other than VAE, making it difficult to train. This aer resents a molecular hyergrah grammar variational autoencoder (MHG-VAE), which uses a single VAE to achieve 100% validity. Our idea is to develo a grah grammar encoding the hard chemical constraints, called molecular hyergrah grammar (MHG), which guides VAE to always generate valid molecules. We also resent an algorithm to construct MHG from a set of molecules.
"
1018,2019,Graph Neural Network for Music Score Data and Modeling Expressive Piano Performance,Poster,"Music score is often handled as one-dimensional sequential data. Unlike words in a text document, notes in music score can be layed simultaneously by the olyhonic nature and each of them has its own duration. In this aer, we reresent the unique form of musical score using grah neural network and aly it for rendering exressive iano erformance from the music score. Secifically, we design the model using note-level gated grah neural network and measure-level hierarchical attention network with bidirectional long short-term memory with an iterative feedback method. In addition, to model different styles of erformance for a given inut score, we emloy a variational auto-encoder. The result of the listening test shows that our roosed model generated more human-like erformances comared to a baseline model and a hierarchical attention network model that handles music score as a word-like sequence.
"
1019,2019,Learning to Prove Theorems via Interacting with Proof Assistants,Poster,"Humans rove theorems by relying on substantial high-level reasoning and roblem-secific insights. Proof assistants offer a formalism that resembles human mathematical reasoning, reresenting theorems in higher-order logic and roofs as high-level tactics. However, human exerts have to construct roofs manually by entering tactics into the roof assistant. In this aer, we study the roblem of using machine learning to automate the interaction with roof assistants. We construct CoqGym, a large-scale dataset and learning environment containing 71K human-written roofs from 123 rojects develoed with the Coq roof assistant. We develo ASTactic, a dee learning-based model that generates tactics as rograms in the form of abstract syntax trees (ASTs). Exeriments show that ASTactic trained on CoqGym can generate effective tactics and can be used to rove new theorems not reviously rovable by automated methods. Code is available at htts:github.comrinceton-vlCoqGym.
"
1020,2019,Circuit-GNN: Graph Neural Networks for Distributed Circuit Design,Poster,"We resent Circuit-GNN, a grah neural network (GNN) model for designing distributed circuits. Today, designing distributed circuits is a slow rocess that can take months from an exert engineer.  Our model both automates and seeds u the rocess. The model learns to simulate the electromagnetic (EM) roerties of distributed circuits. Hence, it can be used to relace traditional EM simulators, which tyically take tens of minutes for each design iteration. Further, by leveraging neural networks' differentiability, we can use our model to solve the inverse roblem -- i.e., given desirable EM secifications, we roagate the gradient to otimize the circuit arameters and toology to satisfy the secifications. 
We exloit the flexibility of GNN to create one model that works for different circuit toologies. 
We comare our model with a commercial simulator showing that it reduces simulation time by four orders of magnitude.  We also demonstrate the value of our model by using it to design a Terahertz channelizer, a difficult task that requires a secialized exert.  The results show that our model roduces a channelizer whose erformance is as good as a manually otimized design, and can save the exert several weeks of toology and arameter otimization. Most interestingly, our model comes u with new designs that differ from the limited temlates commonly used by engineers in the field, hence significantly exanding the design sace. 
"
1021,2019,Learning to Optimize Multigrid PDE Solvers,Poster,"Constructing fast numerical solvers for artial differential equations (PDEs) is crucial for many scientific discilines.  A leading technique for solving large-scale PDEs is using multigrid methods. At the core of a multigrid solver is the rolongation matrix, which relates between different scales of the roblem. This matrix is strongly roblem-deendent, and its otimal construction is critical to the efficiency of the solver. In ractice, however, devising multigrid algorithms for new roblems often oses formidable challenges. In this aer we roose a framework for learning multigrid solvers. Our method learns a (single) maing from discretized PDEs to rolongation oerators for a broad class of 2D diffusion roblems. We train a neural network once for the entire class of PDEs, using an efficient and unsuervised loss function. Our tests demonstrate imroved convergence rates comared to the widely used Black-Box multigrid scheme, suggesting that our method successfully learned rules for constructing rolongation matrices.
"
1022,2019,A Block Coordinate Descent Proximal Method for Simultaneous Filtering and Parameter Estimation,Poster,"We roose and analyze a block coordinate descent roximal algorithm (BCD-rox) for simultaneous filtering and arameter estimation of ODE models. As we show on ODE systems with u to d=40 dimensions, as comared to state-of-the-art methods, BCD-rox exhibits increased robustness (to noise, arameter initialization, and hyerarameters), decreased training times, and imroved accuracy of both filtered states and estimated arameters. We show how BCD-rox can be used with multiste numerical discretizations, and we establish convergence of BCD-rox under hyotheses that include real systems of interest.
"
1023,2019,Learning Hawkes Processes Under Synchronization Noise,Poster,"Multivariate Hawkes rocesses (MHP) are widely used in a variety of fields to model the occurrence of discrete events. Prior work on learning MHPs has only focused on inference in the resence of erfect traces without noise. We address the roblem of learning the causal structure of MHPs when observations are subject to an unknown delay. In articular, we introduce the so-called synchronization noise, where the stream of events generated by each dimension is subject to a random and unknown time shift. We characterize the robustness of the classic maximum likelihood estimator to synchronization noise, and we introduce a new aroach for learning the causal structure in the resence of noise. Our exerimental results show that our aroach accurately recovers the causal structure of MHPs for a wide range of noise levels, and significantly outerforms classic estimation methods.
"
1024,2019,Generative Adversarial User Model for Reinforcement Learning Based Recommendation System,Poster,"There are great interests as well as many challenges in alying reinforcement learning (RL) to recommendation systems. In this setting, an online user is the environment; neither the reward function nor the environment dynamics are clearly defined, making the alication of RL challenging. In this aer, we roose a novel model-based reinforcement learning framework for recommendation systems, where we develo a generative adversarial network to imitate user behavior dynamics and learn her reward function. Using this user model as the simulation environment, we develo a novel Cascading DQN algorithm to obtain a combinatorial recommendation olicy which can handle a large number of candidate items efficiently. In our exeriments with real data, we show this generative adversarial user model can better exlain user behavior than alternatives, and the RL olicy based on this model can lead to a better long-term reward for the user and higher click rate for the system.
"
1025,2019,A Statistical Investigation of Long Memory in Language and Music,Poster,"Reresentation and learning of long-range deendencies is a central challenge confronted in modern alications of machine learning to sequence data. Yet desite the rominence of this issue, the basic roblem of measuring long-range deendence, either in a given data source or as reresented in a trained dee model, remains largely limited to heuristic tools. We contribute a statistical framework for investigating long-range deendence in current alications of dee sequence modeling, drawing on the well-develoed theory of long memory stochastic rocesses. This framework yields testable imlications concerning the relationshi between long memory in real-world data and its learned reresentation in a dee learning architecture, which are exlored through a semiarametric framework adated to the high-dimensional setting. 
"
1026,2019,Deep Factors for Forecasting,Poster,"Producing robabilistic forecasts for large collections of similar andor deendent time series is a ractically highly relevant, yet challenging task. Classical time series models fail to cature comlex atterns in the data and multivariate techniques struggle to scale to large roblem sizes, but their reliance on strong structural assumtions makes them data-efficient and allows them to rovide estimates of uncertainty. The converse is true for models based on dee neural networks, which can learn comlex atterns and deendencies given enough data. In this aer, we roose a hybrid model that incororates the benefits of both aroaches. Our new method is data-driven and scalable via a latent, global, dee comonent.  It also handles uncertainty through a local classical model. We rovide both theoretical and emirical evidence for the soundness of our aroach through a necessary and sufficient decomosition of exchangeable time series into a global and a local art and extensive exeriments.  Our exeriments demonstrate the advantages of our model both in term of data efficiency and comutational comlexity.
"
1027,2019,Weakly-Supervised Temporal Localization via Occurrence Count Learning,Poster,"We roose a novel model for temoral detection and localization which allows the training of dee neural networks using only counts of event occurrences as training labels. This owerful weakly-suervised framework alleviates the burden of the imrecise and time consuming rocess of annotating event locations in temoral data. Unlike existing methods, in which localization is exlicitly achieved by design, our model learns localization imlicitly as a byroduct of learning to count instances. This unique feature is a direct consequence of the model's theoretical roerties. We validate the effectiveness of our aroach in a number of exeriments (drum hit and iano onset detection in audio, digit detection in images) and demonstrate erformance comarable to that of fully-suervised state-of-the-art methods, desite much weaker training requirements.
"
1028,2019,Switching Linear Dynamics for Variational Bayes Filtering,Poster,"System identification of comlex and nonlinear systems is a central roblem for model redictive control and model-based reinforcement learning. 
Desite their comlexity, such systems can often be aroximated well by a set of linear dynamical systems if broken into aroriate subsequences. 
This mechanism not only hels us find good aroximations of dynamics, but also gives us deeer insight into the underlying system. 
Leveraging Bayesian inference, Variational Autoencoders and Concrete relaxations, we show how to learn a richer and more meaningful state sace, e.g. encoding joint constraints and collisions with walls in a maze, from artial and high-dimensional observations.
This reresentation translates into a gain of accuracy of learned dynamics showcased on various simulated tasks.
"
1029,2019,Imputing Missing Events in Continuous-Time Event Streams,Poster,"Events in the world may be caused by other, unobserved events. We consider sequences of events in continuous time. Given a robability model of comlete sequences, we roose article smoothing---a form of sequential imortance samling---to imute the missing events in an incomlete sequence. We develo a trainable family of roosal distributions based on a tye of bidirectional continuous-time LSTM: Bidirectionality lets the roosals condition on future observations, not just on the ast as in article filtering. Our method can samle an ensemble of ossible comlete sequences (articles), from which we form a single consensus rediction that has low Bayes risk under our chosen loss metric. We exeriment in multile synthetic and real domains, using different missingness mechanisms, and modeling the comlete sequences in each domain with a neural Hawkes rocess (Mei &am; Eisner 2017). On held-out incomlete sequences, our method is effective at inferring the ground-truth unobserved events, with article smoothing consistently imroving uon article filtering.
"
1030,2019,Understanding and Controlling Memory in Recurrent Neural Networks,Poster,"To be effective in sequential data rocessing, Recurrent Neural Networks (RNNs) are required to kee track of ast events by creating memories. While the relation  between memories and the network’s hidden state dynamics was established over the last decade, revious works in this direction were of a redominantly descritive nature focusing mainly on locating the dynamical objects of interest. In articular, it remained unclear how dynamical observables affect the erformance, how they form and whether they can be maniulated. Here, we utilize different training rotocols, datasets and architectures to obtain a range of networks solving a delayed classification task with similar erformance, alongside substantial differences in their ability to extraolate for longer delays. We analyze the dynamics of the network’s hidden state, and uncover the reasons for this difference. Each memory is found to be associated with a nearly steady state of the dynamics which we refer to as a ’slow oint’. Slow oint seeds redict extraolation erformance across all datasets, rotocols and architectures tested. Furthermore, by tracking the formation of the slow oints we are able to understand the origin of differences between training rotocols. Finally, we roose a novel regularization technique that is based on the relation between hidden state seeds and memory longevity. Our technique maniulates these seeds, thereby leading to a dramatic imrovement in memory robustness over time, and could ave the way for a new class of regularization methods.
"
1031,2019,Recurrent Kalman Networks: Factorized Inference in High-Dimensional Deep Feature Spaces,Poster,"In order to integrate uncertainty estimates into dee time-series modelling, Kalman Filters (KFs) (Kalman et al., 1960) have been integrated with dee learning models, however, such aroaches tyically rely on aroximate inference tech-
niques such as variational inference which makes learning more comlex and often less scalable due to aroximation errors.
We roose a new dee aroach to Kalman filtering which can be learned directly in an end-to-end manner using backroagation without additional aroximations.
Our aroach uses a high-dimensional factorized latent state reresentation for which the Kalman udates simlify to scalar oerations and thus avoids hard to backroagate, comutationally heavy and otentially unstable matrix inversions.
Moreover, we use locally linear dynamic models to efficiently roagate the latent state to the next time ste.
The resulting network architecture, which we call Recurrent Kalman Network (RKN), can be used for any time-series data, similar to a LSTM (Hochreiter &am; Schmidhuber, 1997) but uses an exlicit reresentation of uncertainty.
As shown by our exeriments, the RKN obtains much more accurate uncertainty estimates than an LSTM or Gated Recurrent Units (GRUs) (Cho et al., 2014) while also showing a slightly imroved rediction erformance and outerforms various recent generative models on an image imutation task.
"
1032,2019,Subspace Robust Wasserstein Distances,Poster,"Making sense of Wasserstein distances between discrete measures in high-dimensional settings remains a challenge. Recent work has advocated a two-ste aroach to imrove robustness and facilitate the comutation of otimal transort, using for instance rojections on random real lines, or a reliminary quantization of the measures to reduce the size of their suort. We roose in this work a codemax-min'' robust variant of the Wasserstein distance by considering the maximal ossible distance that can be realized between two measures, assuming they can be rojected orthogonally on a lower k-dimensional subsace. Alternatively, we show that the corresondingcodemin-max'' OT roblem has a tight convex relaxation which can be cast as that of finding an otimal transort lan with a low transortation cost, where the cost is alternatively defined as the sum of the k largest eigenvalues of the second order moment matrix of the dislacements (or matchings) corresonding to that lan (the usual OT definition only considers the trace of that matrix). We show that both quantities inherit several favorable roerties from the OT geometry. We roose two algorithms to comute the latter formulation using entroic regularization, and illustrate the interest of this aroach emirically.
"
1033,2019,Decomposing feature-level variation with Covariate Gaussian Process Latent Variable Models,Poster,"The interretation of comlex high-dimensional data tyically requires the use of dimensionality reduction techniques to extract exlanatory low-dimensional reresentations. However, in many real-world roblems these reresentations may not be sufficient to aid interretation on their own, and it would be desirable to interret the model in terms of the original features themselves. Our goal is to characterise how feature-level variation deends on latent low-dimensional reresentations, external covariates, and non-linear interactions between the two. In this aer, we roose to achieve this through a structured kernel decomosition in a hybrid Gaussian Process model which we call the Covariate Gaussian Process Latent Variable Model (c-GPLVM). We demonstrate the utility of our model on simulated examles and alications in disease rogression modelling from high-dimensional gene exression data in the resence of additional henotyes. In each setting we show how the c-GPLVM can extract low-dimensional structures from high-dimensional data sets whilst allowing a breakdown of feature-level variability that is not resent in other commonly used dimensionality reduction aroaches.
"
1034,2019,Active Manifolds: A non-linear analogue to Active Subspaces,Poster,"We resent an aroach to analyze $C^1(\mathbb{R}^m)$ functions that addresses limitations resent in the Active Subsaces (AS) method of Constantine et al. (2014; 2015). Under aroriate hyotheses, our Active Manifolds (AM) method identifies a 1-D curve in the domain (the active manifold) on which nearly all values of the unknown function are attained, which can be exloited for aroximation or analysis, esecially when $m$ is large (high-dimensional inut sace). We rovide theorems justifying our AM technique and an algorithm ermitting functional aroximation and sensitivity analysis. Using accessible, low-dimensional functions as initial examles, we show AM reduces aroximation error by an order of magnitude comared to AS, at the exense of more comutation. Following this, we revisit the sensitivity analysis by Glaws et al. (2017), who aly AS to analyze a magnetohydrodynamic ower generator model, and comare the erformance of AM on the same data. Our analysis rovides detailed information not catured by AS, exhibiting the influence of each arameter individually along an active manifold.  Overall, AM reresents a novel technique for analyzing functional models with benefits including:  reducing $m$-dimensional analysis to a 1-D analogue, ermitting more accurate regression than AS (at more comutational exense), enabling more informative sensitivity analysis, and granting accessible visualizations (2-D lots) of arameter sensitivity along the AM."
1035,2019,Optimal Minimal Margin Maximization with Boosting,Poster,"Boosting algorithms iteratively roduce linear combinations of more and more base hyotheses and it has been observed exerimentally that the generalization error kees imroving even after achieving zero training error. One oular exlanation attributes this to imrovements in margins. A common goal in a long line of research, is to obtain large margins using as few base hyotheses as ossible, culminating with the AdaBoostV algorithm by Rätsch and Warmuth [JMLR’05]. The AdaBoostV algorithm was later conjectured to yield an otimal trade-off between number of hyotheses trained and the minimal margin over all training oints (Nie, Warmuth, Vishwanathan and Zhang [JMLR’13]). Our main contribution is a new algorithm refuting this conjecture. Furthermore, we rove a lower bound which imlies that our new algorithm is otimal.
"
1036,2019,Generalized Linear Rule Models,Poster,"This aer considers generalized linear models using rule-based features, also referred to as rule ensembles, for regression and robabilistic classification. Rules facilitate model interretation while also caturing nonlinear deendences and interactions. Our roblem formulation accordingly trades off rule set comlexity and rediction accuracy. Column generation is used to otimize over an exonentially large sace of rules without re-generating a large subset of candidates or greedily boosting rules one by one. The column generation subroblem is solved using either integer rogramming or a heuristic otimizing the same objective. In exeriments involving logistic and linear regression, the roosed methods obtain better accuracy-comlexity trade-offs than existing rule ensemble algorithms. At one end of the trade-off, the methods are cometitive with less interretable benchmark models.
"
1037,2019,"Fast Incremental von Neumann Graph Entropy Computation: Theory, Algorithm, and Applications",Poster,"The von Neumann grah entroy (VNGE) facilitates measurement of information divergence and distance between grahs in a grah sequence. It has been successfully alied to various learning tasks driven by network-based data. While effective, VNGE is comutationally demanding as it requires the full eigensectrum of the grah Lalacian matrix.  In this aer, we roose a new comutational framework, Fast Incremental von Neumann Grah EntRoy (FINGER), which aroaches VNGE with a erformance guarantee. FINGER reduces the cubic comlexity of VNGE to linear comlexity in the number of nodes and edges, and thus enables online comutation based on incremental grah changes. We also show asymtotic equivalence of FINGER to the exact VNGE, and derive its aroximation error bounds. Based on FINGER, we roose efficient algorithms for comuting Jensen-Shannon distance between grahs. Our exerimental results on different random grah models demonstrate the comutational efficiency and the asymtotic equivalence of FINGER. In addition, we aly FINGER to two real-world alications and one synthesized anomaly detection dataset, and corroborate its suerior erformance over seven baseline grah similarity methods.
"
1038,2019,Variational Inference for sparse network reconstruction from count data,Poster,"Networks rovide a natural yet statistically grounded way to deict and understand how a set of entities interact. However, in many situations interactions are not directly observed and the network needs to be reconstructed based on observations collected for each entity. Our work focuses on the situation where these observations consist of counts. A tyical examle is the reconstruction of an ecological network based on abundance data. In this setting, the abundance of a set of secies is collected in a series of samles andor environments and we aim at inferring direct interactions between the secies. The abundances at hand can be, for examle, direct counts of individuals (ecology of macro-organisms) or read counts resulting from metagenomic sequencing (microbial ecology). 

Whatever the aroach chosen to infer such a network, it has to account for the eculiaraties of the data at hand. The first, obvious one, is that the data are counts, i.e. non continuous. Also, the observed counts often vary over many orders of magnitude and are more disersed than exected under a simle model, such as the Poisson distribution. The observed counts may also result from different samling efforts in each samle andor for each entity, which hamers direct comarison. Furthermore, because the network is suosed to reveal only direct interactions, it is highly desirable to account for covariates describing the environment to avoid surious edges.

Many methods of network reconstruction from count data have been roosed. In the context of microbial ecology, most methods (SarCC, REBACCA, SPIEC-EASI, gCODA, BanOCC) rely on a two-ste strategy: transform the counts to seudo Gaussian observations using simle transforms before moving back to the setting of Gaussian Grahical Models, for which state of the art methods exist to infer the network, but only in a Gaussian world. In this work, we consider instead a full-fledged robabilistic model with a latent layer where the counts follow Poisson distributions, conditional to latent (hidden) Gaussian correlated variables. In this model, known as Poisson log-normal (PLN), the deendency structure is comletely catured by the latent layer and we model counts, rather than transformations thereof. To our knowledge, the PLN framework is quite new and has only been used by two other recent methods (Mint and lnDAG) to reconstruct networks from count data. In this work, we use the same mathematical framework but adot a different otimization strategy which alleviates the whole otimization rocess. We also fully exloit the connection between the PLN framework and generalized linear models to account for the eculiarities of microbiological data sets.

The network inference ste is done as usual by adding sarsity inducing constraints on the inverse covariance matrix of the latent Gaussian vector to select only the most imortant interactions between secies. Unlike the usual Gaussian setting, the enalized likelihood is generally not tractable in this framework. We resort instead to a variational aroximation for arameter inference and solve the corresonding otimization roblem by alternating a gradient descent on the variational arameters and a grahical-Lasso ste on the covariance matrix. We also select the sarsity arameter using the resamling-based StARS rocedure.

We show that the sarse PLN aroach has better erformance than existing methods on simulated datasets and that it extracts relevant signal from microbial ecology datasets. We also show that the inference scales to datasets made u of hundred of secies and samles, in line with other methods in the field.

In short, our contributions to the field are the following: we extend the use of PLN distributions in network inference by (i) accounting for covariates and offset and thus removing some surious edges induced by confounding factors, (ii) accounting for different samling effort to  integrate data sets from different sources and thus infer interactions between different tyes of organisms (e.g. bacteria - fungi), (iii) develoing an inference rocedure based on the iterative otimization of a well defined objective function. Our objective function is a rovable lower bound of the observed likelihood and our rocedure accounts for the uncertainty associated with the estimation of the latent variable, unlike the algorithm resented in Mint and lnDAG.
"
1039,2019,Simplifying Graph Convolutional Networks,Poster,"Grah Convolutional Networks (GCNs) and their variants have exerienced significant attention and have become the de facto methods for learning grah reresentations. 
GCNs derive insiration rimarily from recent dee learning aroaches, and as a result, may inherit unnecessary comlexity and redundant comutation. 
In this aer, we reduce this excess comlexity through successively removing nonlinearities and collasing weight matrices between consecutive layers. 
We theoretically analyze the resulting linear model and show that it corresonds to a fixed low-ass filter followed by a linear classifier. 
Notably, our exerimental evaluation demonstrates that these simlifications do not negatively imact accuracy in many downstream alications.
Moreover, the resulting model scales to larger datasets, is naturally interretable, and yields u to two orders of magnitude seedu over FastGCN. 
"
1040,2019,Robust Influence Maximization for Hyperparametric Models,Poster,"In this aer we study the roblem of robust influence
maximization in the indeendent cascade
model under a hyerarametric assumtion. In
social networks users influence and are influenced
by individuals with similar characteristics and
as such they are associated with some features.
A recent surging research direction in influence
maximization focuses on the case where the edge
robabilities on the grah are not arbitrary but are
generated as a function of the features of the users
and a global hyerarameter. We roose a model
where the objective is to maximize the worst-case
number of influenced users for any ossible value
of that hyerarameter. We rovide theoretical
results showing that roer robust solution in our
model is NP-hard and an algorithm that achieves
imroer robust otimization. We make-use of
samling based techniques and of the renowned
multilicative weight udates algorithm. Additionally
we validate our method emirically and
rove that it outerforms the state-of-the-art robust
influence maximization techniques.
"
1041,2019,"HyperGAN: A Generative Model for Diverse, Performant Neural Networks",Poster,"We introduce HyerGAN, a generative model that learns to generate all the arameters of a dee neural network. HyerGAN first transforms low dimensional noise into a latent sace, which can be samled from to obtain diverse, erformant sets of arameters for a target architecture. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of generated samles with a classification loss. This is equivalent to minimizing the KL-divergence between the distribution of generated arameters, and the unknown true arameter distribution. We aly HyerGAN to classification, showing that HyerGAN can learn to generate arameters which solve the MNIST and CIFAR-10 datasets with cometitive erformance to fully suervised learning, while also generating a rich distribution of effective arameters. We also show that HyerGAN can also rovide better uncertainty estimates than standard ensembles. This is evidenced by the ability of HyerGAN-generated ensembles to detect out of distribution data as well as adversarial examles.
"
1042,2019,Rates of Convergence for Sparse Variational Gaussian Process Regression,Poster,"Excellent variational aroximations to Gaussian rocess osteriors have been develoed which avoid the $\mathcal{O}\left(N^3\right)$ scaling with dataset size $N$. They reduce the comutational cost to $\mathcal{O}\left(NM^2\right)$, with $M\ll N$ the number of \emh{inducing variables}, which summarise the rocess. While the comutational cost seems to be linear in $N$, the true comlexity of the algorithm deends on how $M$ must increase to ensure a certain quality of aroximation. We show that with high robability the KL divergence can be made arbitrarily small by growing $M$ more slowly than $N$. A articular case is that for regression with normally distributed inuts in D-dimensions with the Squared Exonential kernel, $M=\mathcal{O}(\log^D N)$ suffices. Our results show that as datasets grow, Gaussian rocess osteriors can be aroximated chealy, and rovide a concrete rule for how to increase $M$ in continual learning scenarios."
1043,2019,Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations,Poster,"The key idea behind the unsuervised learning of disentangled reresentations is that real-world data is generated by a few exlanatory factors of variation which can be recovered by unsuervised learning algorithms.
In this aer, we rovide a sober look at recent rogress in the field and challenge some common assumtions.
We first theoretically show that the unsuervised learning of disentangled reresentations is fundamentally imossible without inductive biases on both the models and the data.
Then, we train more than $12000$ models covering most rominent methods and evaluation metrics in a reroducible large-scale exerimental study on seven different data sets.
We observe that while the different methods successfully enforce roerties ``encouraged'' by the corresonding losses, well-disentangled models seemingly cannot be identified without suervision.
Furthermore, increased disentanglement does not seem to lead to a decreased samle comlexity of learning for downstream tasks. 
Our results suggest that future work on disentanglement learning should be exlicit about the role of inductive biases and (imlicit) suervision, investigate concrete benefits of enforcing disentanglement of the learned reresentations, and consider a reroducible exerimental setu covering several data sets."
1044,2019,Sum-of-Squares Polynomial Flow,Poster,"Triangular ma is a recent construct in robability theory that allows one to transform any source robability density function to any target density function. 
Based on triangular mas, we roose a general framework for high-dimensional density estimation, by secifying one-dimensional transformations (equivalently conditional densities) and aroriate conditioner networks. This framework (a) reveals the commonalities and differences of existing autoregressive and flow based methods, (b) allows a unified understanding of the limitations and reresentation ower of these recent aroaches and, (c) motivates us to uncover a new Sum-of-Squares (SOS) flow that is interretable, universal, and easy to train. We erform several synthetic exeriments on various density geometries to demonstrate the benefits (and short-comings) of such transformations. SOS flows achieve cometitive results in simulations and several real-world datasets. 
"
1045,2019,FloWaveNet : A Generative Flow for Raw Audio,Poster,"Most modern text-to-seech architectures use a WaveNet vocoder for synthesizing high-fidelity waveform audio, but there have been limitations, such as high inference time, in ractical alications due to its ancestral samling scheme. The recently suggested Parallel WaveNet and ClariNet has achieved real-time audio synthesis caability by incororating inverse autoregressive flow (IAF) for arallel samling. However, these aroaches require a two-stage training ieline with a well-trained teacher network and can only roduce natural sound by using robability distillation along with heavily-engineered auxiliary loss terms. We roose FloWaveNet, a flow-based generative model for raw audio synthesis. FloWaveNet requires only a single-stage training rocedure and a single maximum likelihood loss, without any additional auxiliary terms, and it is inherently arallel due to the characteristics of generative flow. The model can efficiently samle raw audio in real-time, with clarity comarable to revious two-stage arallel models. The code and samles for all models, including our FloWaveNet, are available on GitHub.
"
1046,2019,Are Generative Classifiers More Robust to Adversarial Attacks?,Poster,"There is a rising interest in studying the robustness of dee neural network classifiers against adversaries, with both advanced attack and defence techniques being actively develoed. However, most recent work focuses on discriminative classifiers, which only model the conditional distribution of the labels given the inuts. In this aer, we roose and investigate the dee Bayes classifier, which imroves classical naive Bayes with conditional dee generative models. We further develo detection methods for adversarial examles, which reject inuts with low likelihood under the generative model. Exerimental results suggest that dee Bayes classifiers are more robust than dee discriminative classifiers, and that the roosed detection methods are effective against many recently roosed attacks.
"
1047,2019,"A Gradual, Semi-Discrete Approach to Generative Network Training via Explicit Wasserstein Minimization",Poster,"This aer rovides a simle rocedure to fit generative networks to target distributions, with the goal of a small Wasserstein distance (or other otimal transort costs). The aroach is based on two rinciles: (a) if the source randomness of the network is a continuous distribution (the ""semi-discrete"" setting), then the Wasserstein distance is realized by a deterministic otimal transort maing; (b) given an otimal transort maing between a generator network and a target distribution, the Wasserstein distance may be decreased via a regression between the generated data and the maed target oints. The rocedure here therefore alternates these two stes, forming an otimal transort and regressing against it, gradually adjusting the generator network towards the target distribution. Mathematically, this aroach is shown to minimize the Wasserstein distance to both the emirical target distribution, and also its underlying oulation counterart. Emirically, good erformance is demonstrated on the training and testing sets of the MNIST and Thin-8 data. The aer closes with a discussion of the unsuitability of the Wasserstein distance for certain tasks, as has been identified in rior work (Arora et al., 2017; Huang et al., 2017).
"
1048,2019,Disentangling Disentanglement in Variational Autoencoders,Poster,"We develo a generalisation of disentanglement in variational autoencoders (VAEs)---decomosition of the latent reresentation---characterising it as the fulfilment of two factors: a) the latent encodings of the data having an aroriate level of overla, and b) the aggregate encoding of the data conforming to a desired structure, reresented through the rior. Decomosition ermits disentanglement, i.e. exlicit indeendence between latents, as a secial case, but also allows for a much richer class of roerties to be imosed on the learnt reresentation, such as sarsity, clustering, indeendent subsaces, or even intricate hierarchical deendency relationshis. We show that the $\beta$-VAE varies from the standard VAE redominantly in its control of latent overla and that for the standard choice of an isotroic Gaussian rior, its objective is invariant to rotations of the latent reresentation. Viewed from the decomosition ersective, breaking this invariance with simle maniulations of the rior can yield better disentanglement with little or no detriment to reconstructions. We further demonstrate how other choices of rior can assist in roducing different decomositions and introduce an alternative training objective that allows the control of both decomosition factors in a rinciled manner.
"
1049,2019,EDDI: Efficient Dynamic Discovery of High-Value Information with Partial VAE,Poster,"Many real-life decision making situations allow further relevant information to be acquired at a secific cost, for examle, in assessing the health status of a atient we may decide to take additional measurements such as diagnostic tests or imaging scans before making a final assessment.  Acquiring more relevant information enables better decision making, but may be costly.  How can we trade off the desire to make good decisions by acquiring further information with the cost of erforming that acquisition? To this end, we roose a rinciled framework, named \emh{EDDI} (Efficient Dynamic Discovery of high-value Information), based on the theory of Bayesian exerimental design. In EDDI, we roose a novel \emh{artial variational autoencoder} (Partial VAE) to redict missing data entries roblematically given any subset of the observed ones, and combine it with an acquisition function that maximizes exected information gain on a set of target variables. We show cost reduction at the same decision quality and imroved decision quality at the same cost in multile machine learning benchmarks and two real-world health-care alications. 
"
1050,2019,A Wrapped Normal Distribution on Hyperbolic Space for Gradient-Based Learning,Poster,"Hyerbolic sace is a geometry that is known to be well-suited for reresentation learning of data with an underlying hierarchical structure. In this aer, we resent a novel hyerbolic distribution called hyerbolic wraed distribution, a wraed normal distribution on hyerbolic sace whose density can be evaluated analytically and differentiated with resect to the arameters. Our distribution enables the gradient-based learning of the robabilistic models on hyerbolic sace that could never have been considered before. Also, we can samle from this hyerbolic robability distribution without resorting to auxiliary means like rejection samling. As alications of our distribution, we develo a hyerbolic-analog of variational autoencoder and a method of robabilistic word embedding on hyerbolic sace. We demonstrate the efficacy of our distribution on various datasets including MNIST, Atari 2600 Breakout, and WordNet.
"
1051,2019,Emerging Convolutions for Generative Normalizing Flows,Poster,"Generative flows are attractive because they admit exact likelihood otimization and efficient image synthesis. Recently, Kingma &am; Dhariwal (2018) demonstrated with Glow that generative flows are caable of generating high quality images. We generalize the 1 × 1 convolutions roosed in Glow to invertible d × d convolutions, which are more flexible since they oerate on both channel and satial axes. We roose two methods to roduce invertible convolutions, that have recetive fields identical to standard convolutions: Emerging convolutions are obtained by chaining secific autoregressive convolutions, and eriodic convolutions are decouled in the frequency domain. Our exeriments show that the flexibility of d × d convolutions significantly imroves the erformance of generative flow models on galaxy images, CIFAR10 and ImageNet.
"
1052,2019,A Large-Scale Study on Regularization and Normalization in GANs,Poster,"Generative adversarial networks (GANs) are a class of dee generative models which aim to learn a target distribution in an unsuervised fashion. While they were successfully alied to many roblems, training a GAN is a notoriously challenging task and requires a significant number of hyerarameter tuning, neural architecture engineering, and a non-trivial amount of ``tricks"". The success in many ractical alications couled with the lack of a measure to quantify the failure modes of GANs resulted in a lethora of roosed losses, regularization and normalization schemes, as well as neural architectures. In this work we take a sober view of the current state of GANs from a ractical ersective. We discuss and evaluate common itfalls and reroducibility issues, oen-source our code on Github, and rovide re-trained models on TensorFlow Hub.
"
1053,2019,Variational Annealing of GANs: A Langevin Perspective,Poster,"The generative adversarial network (GAN) has received considerable attention recently as a model for data synthesis, without an exlicit secification of a likelihood function. There has been commensurate interest in leveraging likelihood estimates to imrove GAN training. To enrich the understanding of this fast-growing yet almost exclusively heuristic-driven subject, we elucidate the theoretical roots of some of the emirical attemts to stabilize and imrove GAN training with the introduction of likelihoods. We highlight new insights from variational theory of diffusion rocesses to derive a likelihood-based regularizing scheme for GAN training, and resent a novel aroach to train GANs with an unnormalized distribution instead of emirical samles. To substantiate our claims, we rovide exerimental evidence on how our theoretically-insired new algorithms imrove uon current ractice.
"
1054,2019,Invertible Residual Networks,Poster,"We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Tyically, enforcing invertibility requires artitioning dimensions or restricting network architectures. In contrast, our aroach only requires adding a simle normalization ste during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To comute likelihoods, we introduce a tractable aroximation to the Jacobian log-determinant of a residual block. Our emirical evaluation shows that invertible ResNets erform cometitively with both state-of-the-art image classifiers and flow-based generative models, something that has not been reviously achieved with a single architecture.
"
1055,2019,NAS-Bench-101: Towards Reproducible Neural Architecture Search,Poster,"Recent advances in neural architecture search (NAS) demand tremendous comutational resources, which makes it difficult to reroduce exeriments and imoses a barrier-to-entry to researchers without access to large-scale comutation. We aim to ameliorate these roblems by introducing NAS-Bench-101, the first ublic architecture dataset for NAS research. To build NAS-Bench-101, we carefully constructed a comact, yet exressive, search sace, exloiting grah isomorhisms to identify 423k unique convolutional architectures. We trained and evaluated all of these architectures multile times on CIFAR-10 and comiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the re-comuted dataset. We demonstrate its utility by analyzing the dataset as a whole and by benchmarking a range of architecture otimization algorithms.
"
1056,2019,Approximated Oracle Filter Pruning for Destructive CNN Width Optimization,Poster,"It is not easy to design and run Convolutional Neural Networks (CNNs) due to: 1) finding the otimal number of filters (i.e., the width) at each layer is tricky, given an architecture; and 2) the comutational intensity of CNNs imedes the deloyment on comutationally limited devices. Oracle Pruning is designed to remove the unimortant filters from a well-trained CNN, which estimates the filters' imortance by ablating them in turn and evaluating the model, thus delivers high accuracy but suffers from intolerable time comlexity, and requires a given resulting width but cannot automatically find it. To address these roblems, we roose Aroximated Oracle Filter Pruning (AOFP), which kees searching for the least imortant filters in a binary search manner, makes runing attemts by masking out filters randomly, accumulates the resulting errors, and finetunes the model via a multi-ath framework. As AOFP enables simultaneous runing on multile layers, we can rune an existing very dee CNN with accetable time cost, negligible accuracy dro, and no heuristic knowledge, or re-design a model which exerts higher accuracy and faster inference.
"
1057,2019,LegoNet: Efficient Convolutional Neural Networks with Lego Filters,Poster,"This aer aims to build efficient convolutional neural networks using a set of Lego filters. Many successful building blocks, e.g., incetion and residual modules, have been designed to refresh state-of-the-art records of CNNs on visual recognition tasks. Beyond these high-level modules, we suggest that an ordinary filter in the neural network can be ugraded to a sohisticated module as well. Filter modules are established by assembling a shared set of Lego filters that are often of much lower dimensions. Weights in Lego filters and binary masks to stack Lego filters for these filter modules can be simultaneously otimized in an end-to-end manner as usual. Insired by network engineering, we develo a slit-transform-merge strategy for an efficient convolution by exloiting intermediate Lego feature mas. The comression and acceleration achieved by Lego Networks using the roosed Lego filters have been theoretically discussed. Exerimental results on benchmark datasets and dee models demonstrate the advantages of the roosed Lego filters and their otential real-world alications on mobile devices.
"
1058,2019,Sorting Out Lipschitz Function Approximation,Poster,"Training neural networks under a strict Lischitz constraint is useful for rovable adversarial robustness, generalization bounds, interretable gradients, and Wasserstein distance estimation. By the comosition roerty of Lischitz functions, it suffices to ensure that each individual affine transformation or nonlinear activation  is 1-Lischitz. The challenge is to do this while maintaining the exressive ower. We identify a necessary roerty for such an architecture: each of the layers must reserve the gradient norm during backroagation. Based on this, we roose to combine a gradient norm reserving activation function, GrouSort, with norm-constrained weight matrices. We show that norm-constrained GrouSort architectures are universal Lischitz function aroximators. Emirically, we show that norm-constrained GrouSort networks achieve tighter estimates of Wasserstein distance than their ReLU counterarts and can achieve rovable adversarial robustness guarantees with little cost to accuracy. 
"
1059,2019,"Graph Element Networks: adaptive, structured computation and memory",Poster,"We exlore the use of grah neural networks (GNNs) to model satial rocesses in which there is no a riori grahical structure.  Similar to finite element analysis, we assign nodes of a GNN to satial locations and use a comutational rocess defined on the grah to model the relationshi between an initial function defined over a sace and a resulting function in the same sace.   We use GNNs as a comutational substrate, and show that the locations of the nodes in sace as well as their connectivity can be otimized to focus on the most comlex arts of the sace.  Moreover, this reresentational strategy allows the learned inut-outut relationshi to generalize over the size of the underlying sace and run the same model at different levels of recision, trading comutation for accuracy.  We demonstrate this method on a traditional PDE roblem, a hysical rediction roblem from robotics, and learning to redict scene images from novel viewoints.
"
1060,2019,Training CNNs with Selective Allocation of Channels,Poster,"Recent rogress in dee convolutional neural networks (CNNs) have enabled a simle aradigm of architecture design: larger models tyically achieve better accuracy. Due to this, in modern CNN architectures, it becomes more imortant to design models that generalize well under certain resource constraints, e.g. the number of arameters. In this aer, we roose a simle way to imrove the caacity of any CNN model having large-scale features, without adding more arameters. In articular, we modify a standard convolutional layer to have a new functionality of channel-selectivity, so that the layer is trained to select imortant channels to re-distribute their arameters. Our exerimental results under various CNN architectures and datasets demonstrate that the roosed new convolutional layer allows new otima that generalize better via efficient resource utilization, comared to the baseline.
"
1061,2019,Equivariant Transformer Networks,Poster,"How can rior knowledge on the transformation invariances of a domain be incororated into the architecture of a neural network? We roose Equivariant Transformers (ETs), a family of differentiable image-to-image maings that imrove the robustness of models towards re-defined continuous transformation grous. Through the use of secially-derived canonical coordinate systems, ETs incororate functions that are equivariant by construction with resect to these transformations. We show emirically that ETs can be flexibly comosed to imrove model robustness towards more comlicated transformation grous in several arameters. On a real-world image classification task, ETs imrove the samle efficiency of ResNet classifiers, achieving relative imrovements in error rate of u to 15% in the limited data regime while increasing model arameter count by less than 1%.
"
1062,2019,Overcoming Multi-model Forgetting,Poster,"We identify a henomenon, which we refer to as multi-model forgetting, that occurs when sequentially training multile dee networks with artially-shared arameters; the erformance of reviously-trained models degrades as one otimizes a subsequent one, due to the overwriting of shared arameters. To overcome this, we introduce a statistically-justified weight lasticity loss that regularizes the learning of a model's shared arameters according to their imortance for the revious models, and demonstrate its effectiveness when training two models sequentially and for neural architecture search. Adding weight lasticity in neural architecture search reserves the best models to the end of the search and yields imroved results in both natural language rocessing and comuter vision tasks.
"
1063,2019,Bayesian Nonparametric Federated Learning of Neural Networks,Poster,"In federated learning roblems, data is scattered across different servers and exchanging or ooling it is often imractical or rohibited. We develo a Bayesian nonarametric framework for federated learning with neural networks. Each data server is assumed to rovide local neural network weights, which are modeled through our framework. We then develo an inference aroach that allows us to synthesize a more exressive global network without additional suervision, data ooling and with as few as a single communication round. We then demonstrate the efficacy of our aroach on federated learning roblems simulated from two oular image classification datasets.
"
1064,2019,How does Disagreement Help Generalization against Label Corruption?,Poster,"Learning with noisy labels is one of the hottest roblems in weakly-suervised learning. Based on memorization effects of dee neural networks, training on small-loss instances becomes very romising for handling noisy labels. This fosters the state-of-the-art aroach ""Co-teaching"" that cross-trains two dee neural networks using the small-loss trick. However, with the increase of eochs, two networks converge to a consensus and Co-teaching reduces to the self-training MentorNet. To tackle this issue, we roose a robust learning aradigm called Co-teaching+, which bridges the ""Udate by Disagreement'' strategy with the original Co-teaching. First, two networks feed forward and redict all data, but kee rediction disagreement data only. Then, among such disagreement data, each network selects its small-loss data, but back roagates the small-loss data from its eer network and udates its own arameters. Emirical results on benchmark datasets demonstrate that Co-teaching+ is much suerior to many state-of-the-art methods in the robustness of trained models.
"
1065,2019,EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis,Poster,"Reducing the test time resource requirements of a neural network while reserving test accuracy is crucial for running inference on resource-constrained devices. To achieve this goal, we introduce a novel network rearameterization based on the Kronecker-factored eigenbasis (KFE), and then aly Hessian-based structured runing methods in this basis. As oosed to existing Hessian-based runing algorithms which do runing in arameter coordinates, our method works in the KFE where different weights are aroximately indeendent, enabling accurate runing and fast comutation. We demonstrate emirically the effectiveness of the roosed method through extensive exeriments. In articular, we highlight that the imrovements are esecially significant for more challenging datasets and networks. With negligible loss of accuracy, an iterative-runing version gives a 10x reduction in model size and a 8x reduction in FLOPs on wide ResNet32. 
"
1066,2019,Addressing the Loss-Metric Mismatch with Adaptive Loss Alignment,Poster,"In most machine learning training aradigms a fixed, often handcrafted, loss function is assumed to be a good roxy for an underlying evaluation metric. In this work we assess this assumtion by meta-learning an adative loss function to directly otimize the evaluation metric. We roose a samle efficient reinforcement learning aroach for adating the loss dynamically during training. We emirically show how this formulation imroves erformance by simultaneously otimizing the evaluation metric and smoothing the loss landscae. We verify our method in metric learning and classification scenarios, showing considerable imrovements over the state-of-the-art on a diverse set of tasks. Imortantly, our method is alicable to a wide range of loss functions and evaluation metrics. Furthermore, the learned olicies are transferable across tasks and data, demonstrating the versatility of the method. 
"
1067,2019,Deep Compressed Sensing,Poster,"Comressed sensing (CS) rovides an elegant framework for recovering sarse signals from comressed measurements. For examle, CS can exloit the structure of natural images and recover an image from only a few random measurements. Unlike oular autoencoding models, reconstruction in CS is osed as an otimisation roblem that is searate from sensing. CS is flexible and data efficient, but its alication has been restricted by the strong assumtion of sarsity and costly reconstruction rocess. A recent aroach that combines CS with neural network generators has removed the constraint of sarsity, but reconstruction remains slow. Here we roose a novel framework that significantly imroves both the erformance and seed of signal recovery by jointly training a generator and the otimisation rocess for reconstruction via meta-learning. We exlore training the measurements with different objectives, and derive a family of models based on minimising measurement errors. We show that Generative Adversarial Nets (GANs) can be viewed as a secial case in this family of models. Borrowing insights from the CS ersective, we develo a novel way of imroving GANs using gradient information from the discriminator.
"
1068,2019,Differentiable Dynamic Normalization for Learning Deep Representation,Poster,"This work resents Dynamic Normalization (DN), which is able to learn arbitrary normalization oerations for different convolutional layers in a dee ConvNet. Unlike existing normalization aroaches that redefined comutations of the statistics (mean and variance), DN learns to estimate them. DN has several aealing benefits. First, it adats to various networks, tasks, and batch sizes. Second, it can be easily imlemented and trained in a differentiable end-to-end manner with merely small number of arameters. Third, its matrix formulation reresents a wide range of normalization methods, shedding light on analyzing them theoretically. Extensive studies show that DN outerforms its counterarts in CIFAR10 and ImageNet.
"
1069,2019,Toward Understanding the Importance of Noise in Training Neural Networks,Poster,"Numerous emirical evidence has corroborated that the noise lays a crucial rule in effective and efficient training of dee neural networks. The theory behind, however, is still largely unknown. This aer studies this fundamental roblem through training a simle two-layer convolutional neural network model. Although training such a network requires to solve a non-convex otimization roblem with a surious local otimum and a global otimum, we rove that a erturbed gradient descent algorithm in conjunction with noise annealing is guaranteed to converge to a global otimum in olynomial time with arbitrary initialization. This imlies that the noise enables the algorithm to efficiently escae from the surious local otimum. Numerical exeriments are rovided to suort our theory.
"
1070,2019,Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group,Poster,"We introduce a novel aroach to erform first-order otimization with orthogonal and unitary constraints.
This aroach is based on a arametrization stemming from Lie grou theory through the exonential ma. 
The arametrization transforms the constrained otimization roblem into an unconstrained one over a Euclidean sace, for which common first-order otimization methods can be used.
The theoretical results resented are general enough to cover the secial orthogonal grou, the unitary grou and, in general, any connected comact Lie grou.
We discuss how this and other arametrizations can be comuted efficiently through an imlementation trick, making numerically comlex arametrizations usable at a negligible runtime cost in neural networks.
In articular, we aly our results to RNNs with orthogonal recurrent weights, yielding a new architecture called exRNN.
We demonstrate how our method constitutes a more robust aroach to otimization with orthogonal constraints, showing faster, accurate, and more stable convergence in several tasks designed to test RNNs.
"
1071,2019,Breaking Inter-Layer Co-Adaptation by Classifier Anonymization,Poster,"This study addresses an issue of co-adatation between a feature extractor and a classifier in a neural network. A naive joint otimization of a feature extractor and a classifier often brings situations in which an excessively comlex feature distribution adated to a very secific classifier degrades the test erformance. We introduce a method called Feature-extractor Otimization through Classifier Anonymization (FOCA), which is designed to avoid an exlicit co-adatation between a feature extractor and a articular classifier by using many randomly-generated, weak classifiers during otimization. We ut forth a mathematical roosition that states the FOCA features form a oint-like distribution within the same class in a class-searable fashion under secial conditions. Real-data exeriments under more general conditions rovide suortive evidences.
"
1072,2019,Understanding the Impact of Entropy on Policy Optimization,Poster,"Entroy regularization is commonly used to imrove olicy otimization in reinforcement learning. It is believed to hel with exloration by encouraging the selection of more stochastic olicies. In this work, we analyze this claim using new visualizations of the otimization landscae based on randomly erturbing the loss function. We first show that even with access to the exact gradient, olicy otimization is difficult due to the geometry of the objective function. We then qualitatively show that in some environments, a olicy with higher entroy can make the otimization landscae smoother, thereby connecting local otima and enabling the use of larger learning rates. This aer resents new tools for understanding the otimization landscae, shows that olicy entroy serves as a regularizer, and highlights the challenge of designing general-urose olicy otimization algorithms.
"
1073,2019,"Probability Functional Descent: A Unifying Perspective on GANs, Variational Inference, and Reinforcement Learning",Poster,"The goal of this aer is to rovide a unifying view of a wide range of roblems of interest in machine learning by framing them as the minimization of functionals defined on the sace of robability measures. In articular, we show that generative adversarial networks, variational inference, and actor-critic methods in reinforcement learning can all be seen through the lens of our framework. We then discuss a generic otimization algorithm for our formulation, called robability functional descent (PFD), and show how this algorithm recovers existing methods develoed indeendently in the settings mentioned earlier.
"
1074,2019,Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning,Poster,"We roose a unified mechanism for achieving coordination and communication in Multi-Agent Reinforcement Learning (MARL), through rewarding agents for having causal influence over other agents' actions. Causal influence is assessed using counterfactual reasoning. At each timeste, an agent simulates alternate actions that it could have taken, and comutes their effect on the behavior of other agents. Actions that lead to bigger changes in other agents' behavior are considered influential and are rewarded. We show that this is equivalent to rewarding agents for having high mutual information between their actions. 
Emirical results demonstrate that influence leads to enhanced coordination and communication in challenging social dilemma environments, dramatically increasing the learning curves of the dee RL agents, and leading to more meaningful learned communication rotocols. The influence rewards for all agents can be comuted in a decentralized way by enabling agents to learn a model of other agents using dee neural networks. In contrast, key revious works on emergent communication in the MARL setting were unable to learn diverse olicies in a decentralized manner and had to resort to centralized training. 
Consequently, the influence reward oens u a window of new oortunities for research in this area.
"
1075,2019,Maximum Entropy-Regularized Multi-Goal Reinforcement Learning,Poster,"In Multi-Goal Reinforcement Learning, an agent learns to achieve multile goals with a goal-conditioned olicy. During learning, the agent first collects the trajectories into a relay buffer, and later these trajectories are selected randomly for relay. However, the achieved goals in the relay buffer are often biased towards the behavior olicies. From a Bayesian ersective, when there is no rior knowledge about the target goal distribution, the agent should learn uniformly from diverse achieved goals. Therefore, we first roose a novel multi-goal RL objective based on weighted entroy. This objective encourages the agent to maximize the exected return, as well as to achieve more diverse goals.  Secondly, we develoed a maximum entroy-based rioritization framework to otimize the roosed objective. For evaluation of this framework, we combine it with Dee Deterministic Policy Gradient, both with or without Hindsight Exerience Relay. On a set of multi-goal robotic tasks of OenAI Gym, we comare our method with other baselines and show romising imrovements in both erformance and samle-efficiency. 
"
1076,2019,Imitating Latent Policies from Observation,Poster,"In this aer, we describe a novel aroach to imitation learning that infers latent olicies directly from state observations. We introduce a method that characterizes the causal effects of latent actions on observations while simultaneously redicting their likelihood. We then outline an action alignment rocedure that leverages a small amount of environment interactions to determine a maing between the latent and real-world actions. We show that this corrected labeling can be used for imitating the observed behavior, even though no exert actions are given. We evaluate our aroach within classic control environments and a latform game and demonstrate that it erforms better than standard aroaches. Code for this work is available at htts:github.comashedwardsILPO.
"
1077,2019,SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning,Poster,"Model-based reinforcement learning (RL) has roven to be a data efficient aroach for learning control tasks but is difficult to utilize in domains with comlex observations such as images. In this aer, we resent a method for learning reresentations that are suitable for iterative model-based olicy imrovement, even when the underlying dynamical system has comlex dynamics and image observations, in that these reresentations are otimized for inferring simle dynamics and cost models given data from the current olicy. This enables a model-based RL method based on the linear-quadratic regulator (LQR) to be used for systems with image observations. We evaluate our aroach on a range of robotics tasks, including maniulation with a real-world robotic arm directly from images. We find that our method roduces substantially better final erformance than other model-based RL methods while being significantly more efficient than model-free RL.
"
1078,2019,Dimension-Wise Importance Sampling Weight Clipping for Sample-Efficient Reinforcement Learning,Poster,"In imortance samling (IS)-based reinforcement learning algorithms such as Proximal Policy Otimization (PPO), IS weights are tyically clied to avoid large variance in learning. However, olicy udate from clied statistics induces large bias in tasks with high action dimensions, and bias from cliing makes it difficult to reuse old samles with large IS weights. In this aer, we consider PPO, a reresentative on-olicy algorithm, and roose its imrovement by dimension-wise IS weight cliing which searately clis the IS weight of each action dimension to avoid large bias and adatively controls the IS weight to bound olicy udate from the current olicy. This new technique enables efficient learning for high action-dimensional tasks and reusing of old samles like in off-olicy learning to increase the samle efficiency. Numerical results show that the roosed new algorithm outerforms PPO and other RL algorithms in various Oen AI Gym tasks.
"
1079,2019,Structured agents for physical construction,Poster,"Physical construction---the ability to comose objects, subject to hysical dynamics, to serve some function---is fundamental to human intelligence. We introduce a suite of challenging hysical construction tasks insired by how children lay with blocks, such as matching a target configuration, stacking blocks to connect objects together, and creating shelter-like structures over target objects. We examine how a range of dee reinforcement learning agents fare on these challenges, and introduce several new aroaches which rovide suerior erformance. Our results show that agents which use structured reresentations (e.g., objects and scene grahs) and structured olicies (e.g., object-centric actions) outerform those which use less structured reresentations, and generalize better beyond their training when asked to reason about larger scenes. Model-based agents which use Monte-Carlo Tree Search also outerform strictly model-free agents in our most challenging construction roblems. We conclude that aroaches which combine structured reresentations and reasoning with owerful learning are a key ath toward agents that ossess rich intuitive hysics, scene understanding, and lanning.
"
1080,2019,Learning Novel Policies For Tasks,Poster,"In this work, we resent a reinforcement learning algorithm that can find a variety of olicies (novel olicies) for a task that is given by a task reward function. Our method does this by creating a second reward function that recognizes reviously seen state sequences and rewards those by novelty, which is measured using autoencoders that have been trained on state sequences from reviously discovered olicies. We resent a two-objective udate technique for olicy gradient algorithms in which each udate of the olicy is a comromise between imroving the task reward and imroving the novelty reward. Using this method, we end u with a collection of olicies that solves a given task as well as carrying out action sequences that are distinct from one another. We demonstrate this method on maze navigation tasks, a reaching task for a simulated robot arm, and a locomotion task for a hoer. We also demonstrate the effectiveness of our aroach on decetive tasks in which olicy gradient methods often get stuck.
"
1081,2019,Taming MAML: Efficient unbiased meta-reinforcement learning,Poster,"While meta reinforcement learning (Meta-RL) methods have achieved remarkable success, obtaining correct and low variance estimates for olicy gradients remains a significant challenge. In articular, estimating a large Hessian, oor samle efficiency and unstable training continue to make Meta-RL difficult. We roose a surrogate objective function named, Taming MAML (TMAML), that adds control variates into gradient estimation via automatic differentiation. TMAML imroves the quality of gradient estimation by reducing variance without introducing bias. We further roose a version of our method that extends the meta-learning framework to learning the control variates themselves, enabling efficient and scalable learning from a distribution of MDPs. We emirically comare our aroach with MAML and other variance-bias trade-off methods including DICE, LVC, and action-deendent control variates. Our aroach is easy to imlement and outerforms existing methods in terms of the variance and accuracy of gradient estimation, ultimately yielding higher erformance across a variety of challenging Meta-RL environments.
"
1082,2019,Self-Supervised Exploration via Disagreement,Poster,"Efficient exloration is a long-standing roblem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setus. In this aer, we roose a formulation for exloration insired by the work in active learning literature. Secifically, we train an ensemble of dynamics models and incentivize the agent to exlore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exloring in a self-suervised manner without any external reward. Notably, we further leverage the disagreement objective to otimize the agent's olicy in a differentiable manner, without using reinforcement learning, which results in a samle-efficient exloration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we imlement our differentiable exloration on a real robot which learns to interact with objects comletely from scratch. Project videos and code are at htts:athak22.github.ioexloration-by-disagreement
"
1083,2019,Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables,Poster,"Dee reinforcement learning algorithms require large amounts of exerience to learn an individual task. While meta-reinforcement learning (meta-RL) algorithms can enable agents to learn new skills from small amounts of exerience, several major challenges reclude their racticality. Current methods rely heavily on on-olicy exerience, limiting their samle efficiency. They also lack mechanisms to reason about task uncertainty when adating to new tasks, limiting their effectiveness on sarse reward roblems. In this aer, we address these challenges by develoing an off-olicy meta-RL algorithm that disentangles task inference and control. In our aroach, we erform online robabilistic filtering of latent task variables to infer how to solve a new task from small amounts of exerience. This robabilistic interretation enables osterior samling for structured and efficient exloration. We demonstrate how to integrate these task variables with off-olicy RL algorithms to achieve both meta-training and adatation efficiency. Our method outerforms rior algorithms in samle efficiency by 20-100X as well as in asymtotic erformance on several meta-RL benchmarks.
"
1084,2019,The Natural Language of Actions,Poster,"We introduce Act2Vec, a general framework for learning context-based action reresentation for Reinforcement Learning. Reresenting actions in a vector sace hel reinforcement learning algorithms achieve better erformance by grouing similar actions and utilizing relations between different actions. We show how rior knowledge of an environment can be extracted from demonstrations and injected into action vector reresentations that encode natural comatible behavior. We then use these for augmenting state reresentations as well as imroving function aroximation of Q-values. We visualize and test action embeddings in three domains including a drawing task, a high dimensional navigation task, and the large action sace domain of StarCraft II.
"
1085,2019,Control Regularization for Reduced Variance Reinforcement Learning,Poster,"Dealing with high variance is a significant challenge in model-free reinforcement learning (RL). Existing methods are unreliable, exhibiting high variance in erformance from run to run using different initializationsseeds. Focusing on roblems arising in continuous control, we roose a functional regularization aroach to augmenting model-free RL. In articular, we regularize the behavior of the dee olicy to be similar to a olicy rior, i.e., we regularize in function sace. We show that functional regularization yields a bias-variance trade-off, and roose an adative tuning strategy to otimize this trade-off. When the olicy rior has control-theoretic stability guarantees, we further show that this regularization aroximately reserves those stability guarantees throughout learning. We validate our aroach emirically on a range of settings, and demonstrate significantly reduced variance, guaranteed dynamic stability, and more efficient learning than dee RL alone.
"
1086,2019,On the Generalization Gap in Reparameterizable Reinforcement Learning,Poster,"Understanding generalization in reinforcement learning (RL) is a significant challenge, as many common assumtions of traditional suervised learning theory do not aly. We focus on the secial class of rearameterizable RL roblems, where the trajectory distribution can be decomosed using the rearametrization trick. For this roblem class, estimating the exected return is efficient and the trajectory can be comuted deterministically given eriheral random variables, which enables us to study rearametrizable RL using suervised learning and transfer learning theory. Through these relationshis, we derive guarantees on the ga between the exected and emirical return for both intrinsic and external errors, based on Rademacher comlexity as well as the PAC-Bayes bound. Our bound suggests the generalization caability of rearameterizable RL is related to multile factors including ``smoothness'' of the environment transition, reward and agent olicy function class. We also emirically verify the relationshi between the generalization ga and these factors through simulations.
"
1087,2019,Trajectory-Based Off-Policy Deep Reinforcement Learning,Poster,"Policy gradient methods are owerful reinforcement learning algorithms and have been demonstrated to solve many comlex tasks. However, these methods are also data-inefficient, afflicted with high variance gradient estimates, and frequently get stuck in local otima. This work addresses these weaknesses by combining recent imrovements in the reuse of off-olicy data and exloration in arameter sace with deterministic behavioral olicies. The resulting objective
is amenable to standard neural network otimization strategies like stochastic gradient descent or stochastic gradient Hamiltonian Monte Carlo. Incororation of revious rollouts via imortance samling greatly imroves data-efficiency, whilst
stochastic otimization schemes facilitate the escae from local otima. We evaluate the roosed aroach on a series of continuous control benchmark tasks. The results show that the roosed algorithm is able to successfully and reliably learn solutions using fewer system interactions than standard olicy gradient methods.
"
1088,2019,A Deep Reinforcement Learning Perspective on Internet Congestion Control,Poster,"We resent and investigate a novel and timely alication domain for dee reinforcement learning (RL): Internet congestion control. Congestion control is the core networking task of modulating traffic sources' data-transmission rates to efficiently utilize network caacity, and is the subject of extensive attention in light of the advent of Internet services such as live video, virtual reality, Internet-of-Things, and more. We show that casting congestion control as RL enables training dee network olicies that cature intricate atterns in data traffic and network conditions, and leverage this to outerform the state-of-the-art. We also highlight significant challenges facing real-world adotion of RL-based congestion control, including fairness, safety, and generalization, which are not trivial to address within conventional RL formalism. To facilitate further research and reroducibility of our results, we resent a test suite for RL-guided congestion control based on the OenAI Gym interface.
"
1089,2019,Model-Based Active Exploration,Poster,"Efficient exloration is an unsolved roblem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This aer introduces an efficient {\em active} exloration algorithm, Model-Based Active eXloration (MAX), which uses an ensemble of forward models to lan to observe novel events. This is carried out by otimizing agent behaviour with resect to a measure of novelty derived from the Bayesian ersective of exloration, which is estimated using the disagreement between the futures redicted by the ensemble members. We show emirically that in semi-random discrete environments where directed exloration is critical to make rogress, MAX is at least an order of magnitude more efficient than strong baselines. MAX scales to high-dimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.
"
1090,2019,Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations,Poster,"A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outerform the demonstrator. This is because IRL tyically seeks a reward function that makes the demonstrator aear near-otimal, rather than inferring the underlying intentions of the demonstrator that may have been oorly executed in ractice. In this aer, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtraolation (T-REX), that extraolates beyond a set of (aroximately) ranked demonstrations in order to infer high-quality reward functions from a set of otentially oor demonstrations. When combined with dee reinforcement learning, T-REX outerforms state-of-the-art imitation learning and IRL methods on multile Atari and MuJoCo benchmark tasks and achieves erformance that is often more than twice the erformance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extraolate intention by simly watching a learner noisily imrove at a task over time.
"
1091,2019,Distributional Multivariate Policy Evaluation and Exploration with the Bellman GAN,Poster,"The recently roosed distributional aroach to reinforcement learning (DiRL) is centered on learning the distribution of the reward-to-go, often referred to as the value distribution. In this work, we show that the distributional Bellman equation, which drives DiRL methods, is equivalent to a generative adversarial network (GAN) model. In this formulation, DiRL can be seen as learning a dee generative model of the value distribution, driven by the discreancy between the distribution of the current value, and the distribution of the sum of current reward and next value. We use this insight to roose a GAN-based aroach to DiRL, which leverages the strengths of GANs in learning distributions of high dimensional data. In articular, we show that our GAN aroach can be used for DiRL with multivariate rewards, an imortant setting which cannot be tackled with rior methods. The multivariate setting also allows us to unify learning the distribution of values and state transitions, and we exloit this idea to devise a novel exloration method that is driven by the discreancy in estimating both values and states.
"
1092,2019,A Baseline for Any Order Gradient Estimation in Stochastic Computation Graphs,Poster,"By enabling correct differentiation in Stochastic Comutation Grahs (SCGs), the infinitely differentiable Monte-Carlo estimator (DiCE) can generate correct estimates for the higher order gradients that arise in, e.g., multi-agent reinforcement learning and meta-learning. However, the baseline term in DiCE that serves as a control variate for reducing variance alies only to first order gradient estimation, limiting the utility of higher-order gradient estimates. To imrove the samle efficiency of DiCE, we roose a new baseline term for higher order gradient estimation. This term may be easily included in the objective, and roduces unbiased variance-reduced estimators under (automatic) differentiation, without affecting the estimate of the objective itself or of the first order gradient estimate. It reuses the same baseline function (e.g., the state-value function in reinforcement learning) already used for the first order baseline. We rovide theoretical analysis and numerical evaluations of this new baseline, which demonstrate that it can dramatically reduce the variance of DiCE's second order gradient estimators and also show emirically that it reduces the variance of third and fourth order gradients. This comutational tool can be easily used to estimate higher order gradients with unrecedented efficiency and simlicity wherever automatic differentiation is utilised, and it has the otential to unlock alications of higher order gradients in reinforcement learning and meta-learning.
"
1093,2019,Remember and Forget for Experience Replay,Poster,"Exerience relay (ER) is a fundamental comonent of off-olicy dee reinforcement learning (RL). ER recalls exeriences from ast iterations to comute gradient estimates for the current olicy, increasing data-efficiency. However, the accuracy of such udates may deteriorate when the olicy diverges from ast behaviors and can undermine the erformance of ER. Many algorithms mitigate this issue by tuning hyer-arameters to slow down olicy changes. An alternative is to actively enforce the similarity between olicy and the  exeriences in the relay memory. We introduce Remember and Forget Exerience Relay (ReF-ER), a novel method that can enhance RL algorithms with arameterized olicies. ReF-ER (1) skis gradients comuted from exeriences that are too unlikely with the current olicy and (2) regulates olicy changes within a trust region of the relayed behaviors. We coule ReF-ER with Q-learning, deterministic olicy gradient and off-olicy gradient methods. We find that ReF-ER consistently imroves the erformance of continuous-action, off-olicy RL on fully observable benchmarks and artially observable flow control roblems.
"
1094,2019,Tensor Variable Elimination for Plated Factor Graphs,Poster,"A wide class of machine learning algorithms can be reduced to variable elimination on factor grahs. While factor grahs rovide a unifying notation for these algorithms, they do not rovide a comact way to exress reeated structure when comared to late diagrams for directed grahical models. To exloit efficient tensor algebra in grahs with lates of variables, we generalize undirected factor grahs to lated factor grahs and variable elimination to a tensor variable elimination algorithm that oerates directly on lated factor grahs. Moreover, we generalize comlexity bounds based on treewidth and characterize the class of lated factor grahs for which inference is tractable. As an alication, we integrate tensor variable elimination into the Pyro robabilistic rogramming language to enable exact inference in discrete latent variable models with reeated structure. We validate our methods with exeriments on both directed and undirected grahical models, including alications to olyhonic music modeling, animal movement modeling, and latent sentiment analysis.
"
1095,2019,Predicate Exchange: Inference with Declarative Knowledge,Poster,"Programming languages allow us to exress comlex redicates, but existing inference methods are unable to condition robabilistic models on most of them. To suort a broader class of redicates, we develo an inference rocedure called redicate exchange, which softens redicates. A soft redicate quantifies the extent to which values of model variables are consistent with its hard counterart. We substitute the likelihood term in the Bayesian osterior with a soft redicate, and develo a variant of relica exchange MCMC to draw osterior samles. We imlement redicate exchange as a language agnostic tool which erforms a nonstandard execution of a robabilistic rogram.  We demonstrate the aroach on sequence models of health and inverse rendering. 
"
1096,2019,Discriminative Regularization for Latent Variable Models with Applications to Electrocardiography,Poster,"Generative models often use latent variables to reresent structured variation in high-dimensional data, such as images and medical waveforms. However, these latent variables may ignore subtle, yet meaningful features in the data. Some features may redict an outcome of interest (e.g. heart attack) but account for only a small fraction of variation in the data. We roose a generative model training objective that uses a black-box discriminative model as a regularizer to learn reresentations that reserve this redictive variation. With these discriminatively regularized latent variable models, we visualize and measure variation in the data that influence a black-box redictive model, enabling an exert to better understand each rediction. With this technique, we study models that use electrocardiograms to redict outcomes of clinical interest. We measure our aroach on synthetic and real data with statistical summaries and an exeriment carried out by a hysician.
"
1097,2019,Hierarchical Decompositional Mixtures of Variational Autoencoders,Poster,"Variational autoencoders (VAEs) have received considerable attention, since they allow us to learn exressive neural density estimators effectively and efficiently. However, learning and inference in VAEs is still roblematic due to the sensitive interlay between the generative model and the inference network. Since these roblems become generally more severe in high dimensions, we roose a novel hierarchical mixture model over low-dimensional VAE exerts. Our model decomoses the overall learning roblem into many smaller roblems, which are coordinated by the hierarchical mixture, reresented by a sum-roduct network. In exeriments we show that our models outerform classical VAEs on almost all of our exerimental benchmarks. Moreover, we show that our model is highly data efficient and degrades very gracefully in extremely low data regimes.ow data regimes.
"
1098,2019,Finding Mixed Nash Equilibria of Generative Adversarial Networks,Poster,"Generative adversarial networks (GANs) are known to achieve the state-of-the-art erformance on various generative tasks, but these results come at the exense of a notoriously difficult training hase. Current training strategies tyically draw a connection to otimization theory, whose scoe is restricted to local convergence due to the resence of non-convexity. In this work, we tackle the training of GANs by rethinking the roblem formulation from the mixed Nash Equilibria (NE) ersective. Via a classical lifting trick, we show that essentially all existing GAN objectives can be relaxed into their mixed strategy forms, whose global otima can be solved via samling, in contrast to the exclusive use of otimization framework in revious work. We further roose a mean-aroximation samling scheme, which allows to systematically exloit methods for bi-affine games to delineate novel, ractical training algorithms of GANs. Finally, we rovide exerimental evidence that our aroach yields comarable or suerior results to contemorary training algorithms, and outerforms classical methods such as SGD, Adam, and RMSPro. 
"
1099,2019,CompILE: Compositional Imitation Learning and Execution,Poster,"We introduce Comositional Imitation Learning and Execution (ComILE): a framework for learning reusable, variable-length segments of hierarchically-structured behavior from demonstration data. ComILE uses a novel unsuervised, fully-differentiable sequence segmentation module to learn latent encodings of sequential data that can be re-comosed and executed to erform new tasks. Once trained, our model generalizes to sequences of longer length and from environment instances not seen during training. We evaluate ComILE in a challenging 2D multi-task environment and a continuous control task, and show that it can find correct task boundaries and event encodings in an unsuervised manner. Latent codes and associated behavior olicies discovered by ComILE can be used by a hierarchical agent, where the high-level olicy selects actions in the latent code sace, and the low-level, task-secific olicies are simly the learned decoders. We found that our ComILE-based agent could learn given only sarse rewards, where agents without task-secific olicies struggle.
"
1100,2019,Sparse Multi-Channel Variational Autoencoder for the Joint Analysis of Heterogeneous Data,Poster,"Interretable modeling of heterogeneous data channels is essential in medical alications, for examle when jointly analyzing clinical scores and medical images.
Variational Autoencoders (VAE) are owerful generative models that learn reresentations of comlex data.
The flexibility of VAE may come at the exense of lack of interretability in describing the joint relationshi between heterogeneous data.
To tackle this roblem, in this work we extend the variational framework of VAE to bring arsimony and interretability when jointly account for latent relationshis across multile channels.
In the latent sace, this is achieved by constraining the variational distribution of each channel to a common target rior.
Parsimonious latent reresentations are enforced by variational droout.
Exeriments on synthetic data show that our model correctly identifies the rescribed latent dimensions and data relationshis across multile testing scenarios.
When alied to imaging and clinical data, our method allows to identify the joint effect of age and athology in describing clinical condition in a large scale clinical cohort.
"
1101,2019,Deep Generative Learning via Variational Gradient Flow,Poster,"We roose a framework to learn dee generative models via \textbf{V}ariational \textbf{Gr}adient Fl\textbf{ow} (VGrow) on robability saces. The evolving distribution that asymtotically converges to the target distribution is governed by a vector field, which is the negative gradient of the first variation of the $f$-divergence between them. We rove that the evolving distribution coincides with the ushforward distribution through the infinitesimal time comosition of residual mas that are erturbations of the identity ma along the vector field. The vector field deends on the density ratio of the ushforward distribution and the target distribution, which can be consistently learned from a binary classification roblem. Connections of our roosed VGrow method with other oular methods, such as VAE, GAN and flow-based methods, have been established in this framework, gaining new insights of dee generative learning. We also evaluated several commonly used divergences, including Kullback-Leibler, Jensen-Shannon, Jeffreys divergences as well as our newly discovered ``logD'' divergence which serves as the objective function of the logD-trick GAN. Exerimental results on benchmark datasets demonstrate that VGrow can generate high-fidelity images in a stable and efficient manner, achieving cometitive erformance with state-of-the-art GANs."
1102,2019,Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design,Poster,"Flow-based generative models are owerful exact likelihood models with efficient samling and inference. Desite their comutational efficiency, flow-based models generally have much worse density modeling erformance comared to state-of-the-art autoregressive models. In this aer, we investigate and imrove uon three limiting design choices emloyed by flow-based models in rior work: the use of uniform noise for dequantization, the use of inexressive affine flows, and the use of urely convolutional conditioning networks in couling layers. Based on our findings, we roose Flow++, a new flow-based model that is now the state-of-the-art non-autoregressive model for unconditional density estimation on standard image benchmarks. Our work has begun to close the significant erformance ga that has so far existed between autoregressive models and flow-based models.
"
1103,2019,Learning Neurosymbolic Generative Models via Program Synthesis,Poster,"Generative models have become significantly more owerful in recent years. However, these models continue to have difficulty caturing global structure in data. For examle, images of buildings tyically contain satial atterns such as windows reeating at regular intervals, but state-of-the-art models have difficulty generating these atterns. We roose to address this roblem by incororating rograms reresenting global structure into generative models—e.g., a 2D for-loo may reresent a reeating attern of windows—along with a framework for learning these models by leveraging rogram synthesis to obtain training data. On both synthetic and real-world data, we demonstrate that our aroach substantially outerforms state-of-the-art at both generating and comleting images with global structure.
"
1104,2019,Theoretically Principled Trade-off between Robustness and Accuracy,Poster,"We identify a trade-off between robustness and accuracy that serves as a guiding rincile in the design of defenses against adversarial examles. Although this roblem has been widely studied emirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decomose the rediction error for adversarial examles (robust error) as the sum of the natural (classification) error and boundary error, and rovide a differentiable uer bound using the theory of classification-calibrated loss, which is shown to be the tightest ossible uer bound uniform over all robability distributions and measurable redictors. Insired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our roosed algorithm erforms well exerimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st lace out of ~2,000 submissions, surassing the runner-u aroach by 11.41% in terms of mean L_2 erturbation distance.
"
1105,2019,The Odds are Odd: A Statistical Test for Detecting Adversarial Examples,Poster,"We investigate conditions under which test statistics exist that can reliably detect examles, which have been adversarially maniulated in a white-box attack. These statistics can be easily comuted and calibrated by randomly corruting inuts. They exloit certain anomalies that adversarial attacks introduce, in articular if they follow the aradigm of choosing erturbations otimally under -norm constraints. Access to the log-odds is the only requirement to defend models. We justify our aroach emirically, but also rovide conditions under which detectability via the suggested test statistics is guaranteed to be effective. In our exeriments, we show that it is even ossible to correct test time redictions for adversarial attacks with high accuracy.
"
1106,2019,ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation,Poster,"Dee neural networks are vulnerable to adversarial attacks. The literature is rich with algorithms that can easily craft successful adversarial examles. In contrast, the erformance of defense techniques still lags behind. This aer rooses ME-Net, a defense method that leverages matrix estimation (ME). In ME-Net, images are rerocessed using two stes: first ixels are randomly droed from the image; then, the image is reconstructed using ME. We show that this rocess destroys the adversarial structure of the noise, while re-enforcing the global structure in the original image. Since humans tyically rely on such global structures in classifying images, the rocess makes the network mode comatible with human ercetion. We conduct comrehensive exeriments on revailing benchmarks such as MNIST, CIFAR-10, SVHN, and Tiny-ImageNet. Comaring ME-Net with state-of-the-art defense mechanisms shows that ME-Net consistently outerforms rior techniques, imroving robustness against both black-box and white-box attacks.
"
1107,2019,Certified Adversarial Robustness via Randomized Smoothing,Poster,"We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial erturbations under the L2 norm.  While this ""randomized smoothing"" technique has been roosed before in the literature, we are the first to rovide a tight analysis, which establishes a close connection between L2 robustness and Gaussian noise.  We use the technique to train an ImageNet classifier with e.g. a certified to-1 accuracy of 49% under adversarial erturbations with L2 norm less than 0.5 (=127255).  Smoothing is the only aroach to certifiably robust classification which has been shown feasible on full-resolution ImageNet. On smaller-scale datasets where cometing aroaches to certified L2 robustness are viable, smoothing delivers higher certified accuracies.
The emirical success of the aroach suggests that rovable methods based on randomization at rediction time are a romising direction for future research into adversarially robust classification.
"
1108,2019,"Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition",Poster,"Adversarial examles are inuts to machine learning models designed by an adversary to cause an incorrect outut. So far, adversarial examles have been studied most extensively in the image domain. In this domain, adversarial examles can be constructed by imercetibly modifying images to cause misclassification, and are ractical in the hysical world. In contrast, current targeted adversarial examles on seech recognition systems have neither of these roerties: humans can easily identify the adversarial erturbations, and they are not effective when layed over-the-air. This aer makes rogress on both of these fronts. First, we develo effectively imercetible audio adversarial examles (verified through a human study) by leveraging the sychoacoustic rincile of auditory masking, while retaining 100% targeted success rate on arbitrary full-sentence targets. Then, we make rogress towards hysical-world audio adversarial examles by constructing erturbations which remain effective even after alying highly-realistic simulated environmental distortions.
"
1109,2019,Parsimonious Black-Box Adversarial Attacks via Efficient Combinatorial Optimization,Poster,"Solving for adversarial examles with rojected gradient descent has been demonstrated to be highly effective in fooling the neural network based classifiers. However, in the black-box setting, the attacker is limited only to the query access to the network and solving for a successful adversarial examle becomes much more difficult. To this end, recent methods aim at estimating the true gradient signal based on the inut queries but at the cost of excessive queries. We roose an efficient discrete surrogate to the otimization roblem which does not require estimating the gradient and consequently becomes free of the first order udate hyerarameters to tune. Our exeriments on Cifar-10 and ImageNet show the state of the art black-box attack erformance with significant reduction in the required queries comared to a number of recently roosed methods. The source code is available at htts:github.comsnu-mllabarsimonious-blackbox-attack.
"
1110,2019,Wasserstein Adversarial Examples via Projected Sinkhorn Iterations,Poster,"A raidly growing area of work has studied the existence of adversarial examles, dataoints which have been erturbed to fool a classifier, but the vast majority of these works have focused rimarily on threat models defined by $\ell_$ norm-bounded erturbations. In this aer, we roose a new threat model for adversarial attacks based on the Wasserstein distance. In the image classification setting, such distances measure the cost of moving ixel mass, which can naturally reresent ``standard'' image maniulations such as scaling, rotation, translation, and distortion (and can otentially be alied to other settings as well). To generate Wasserstein adversarial examles, we develo a rocedure for aroximate rojection onto the Wasserstein ball, based uon a modified version of the Sinkhorn iteration. The resulting algorithm can successfully attack image classification models, bringing traditional CIFAR10 models down to 3% accuracy within a Wasserstein ball with radius 0.1 (i.e., moving 10% of the image mass 1 ixel), and we demonstrate that PGD-based adversarial training can imrove this adversarial accuracy to 76%. In total, this work oens u a new direction of study in adversarial robustness, more formally considering convex metrics that accurately cature the invariances that we tyically believe should exist in classifiers, and code for all exeriments in the aer is available at htts:github.comlocuslabrojected_sinkhorn. "
1111,2019,Transferable Clean-Label Poisoning Attacks on Deep Neural Nets,Poster,"In this aer, we exlore clean-label oisoning attacks on dee convolutional networks with access to neither the network's outut nor its architecture or arameters. Our goal is to ensure that after injecting the oisons into the training data, a model with unknown architecture and arameters trained on that data will misclassify the target image into a secific class. To achieve this goal, we generate multile oison images from the base class by adding small erturbations which cause the oison images to tra the target image within their convex olytoe in feature sace. We also demonstrate that using Droout during crafting of the oisons and enforcing this objective in multile layers enhances transferability, enabling attacks against both the transfer learning and end-to-end training settings. We demonstrate transferable attack success rates of over 50% by oisoning only 1% of the training set.
"
1112,2019,NATTACK: Learning the Distributions of Adversarial Examples for an Improved  Black-Box  Attack on Deep Neural Networks,Poster,"Powerful adversarial attack methods are vital for understanding how to construct robust dee neural networks (DNNs) and for thoroughly testing defense techniques. In this aer, we roose a black-box adversarial attack algorithm that can defeat both vanilla DNNs and those generated by various defense techniques develoed recently. Instead of searching for an ""otimal"" adversarial examle for a benign inut to a targeted DNN, our algorithm finds a robability density distribution over a small region centered around the inut, such that a samle drawn from this  distribution is likely an adversarial examle, without the need of accessing the DNN's internal layers or weights. Our aroach is universal as it can successfully attack different neural networks by a single algorithm. It is also strong; according to the testing against 2 vanilla DNNs and 13 defended ones, it outerforms state-of-the-art black-box or white-box attack methods for most test cases. Additionally, our results reveal that adversarial training remains one of the best defense techniques, and the adversarial examles are not as transferable across defended DNNs as them across vanilla DNNs.
"
1113,2019,Simple Black-box Adversarial Attacks,Poster,"We roose an intriguingly simle method for the construction of adversarial images in the black-box setting. In constrast to the white-box scenario, constructing black-box adversarial images has the additional constraint on query budget, and efficient attacks remain an oen roblem to date. With only the mild assumtion of requiring continuous-valued confidence scores, our highly query-efficient algorithm utilizes the following simle iterative rincile: we randomly samle a vector from a redefined orthonormal basis and either add or subtract it to the target image. Desite its simlicity, the roosed method can be used for both untargeted and targeted attacks -- resulting in reviously unrecedented query efficiency in both settings. We demonstrate the efficacy and efficiency of our algorithm on several real world settings including the Google Cloud Vision API. We argue that our roosed algorithm should serve as a strong baseline for future black-box attacks, in articular because it is  extremely fast and its imlementation requires less than 20 lines of PyTorch code. 
"
1114,2019,Causal Identification under Markov Equivalence: Completeness Results,Poster,"Causal effect identification is the task of determining whether a causal distribution is comutable from the combination of an observational distribution and substantive knowledge about the domain under investigation. One of the most studied versions of this roblem assumes that knowledge is articulated in the form of a fully known causal diagram, which is arguably a strong assumtion in many settings. In this aer, we relax this requirement and consider that the knowledge is articulated in the form of an equivalence class of causal diagrams, in articular, a artial ancestral grah (PAG). This is attractive because a PAG can be learned directly from data, and the scientist does not need to commit to a articular, unique diagram. There are different sufficient conditions for identification in PAGs, but none is comlete. We derive a comlete algorithm for identification given a PAG. This imlies that whenever the causal effect is identifiable, the algorithm returns a valid identification exression; alternatively, it will throw a failure condition, which means that the effect is rovably not identifiable. We further rovide a grahical characterization of non-identifiability of causal effects in PAGs.
"
1115,2019,Counterfactual Off-Policy Evaluation with Gumbel-Max Structural Causal Models,Poster,"We introduce an off-olicy evaluation rocedure for highlighting eisodes where alying a reinforcement learned (RL) olicy is likely to have roduced a substantially different outcome than the observed olicy.  In articular, we introduce a class of structural causal models (SCMs) for generating counterfactual trajectories in finite artially observable Markov Decision Processes (POMDPs).  We see this as a useful rocedure for off-olicy ``debugging'' in high-risk settings (e.g., healthcare); by decomosing the exected difference in reward between the RL and observed olicy into secific eisodes, we can identify eisodes where the counterfactual difference in reward is most dramatic.  This in turn can be used to facilitate review of secific eisodes by domain exerts. We demonstrate the utility of this rocedure with a synthetic environment of sesis management.
"
1116,2019,Causal Discovery and Forecasting in Nonstationary Environments with State-Space Models,Poster,"recode  In many scientific fields, such as economics and neuroscience, we are often faced with nonstationary time series, and concerned with both finding causal relations and forecasting the values of variables of interest, both of which are articularly challenging in such nonstationary environments. In this aer, we study causal discovery and forecasting for nonstationary time series. By exloiting a articular tye of state-sace model to reresent the rocesses, we show that nonstationarity hels to identify the causal structure, and that forecasting naturally benefits from learned causal knowledge. Secifically, we allow changes in both causal strengths and noise variances in the nonlinear state-sace models, which, interestingly, renders both the causal structure and model arameters identifiable. Given the causal model, we treat forecasting as a roblem in Bayesian inference in the causal model, which exloits the time-varying roerty of the data and adats to new observations in a rinciled manner.  Exerimental results on synthetic and real-world data sets demonstrate the efficacy of the roosed methods.
codere
"
1117,2019,Classifying Treatment Responders Under Causal Effect Monotonicity,Poster,"In the context of individual-level causal inference, we study the roblem of redicting whether someone will resond or not to a treatment based on their features and ast examles of features, treatment indicator (e.g., drugno drug), and a binary outcome (e.g., recovery from disease). As a classification task, the roblem is made difficult by not knowing the examle outcomes under the oosite treatment indicators. We assume the effect is monotonic, as in advertising's effect on a urchase or bail-setting's effect on reaearance in court: either it would have haened regardless of treatment, not haened regardless, or haened only deending on exosure to treatment. Predicting whether the latter is latently the case is our focus. While revious work focuses on conditional average treatment effect estimation, formulating the roblem as a classification task allows us to develo new tools more suited to this roblem. By leveraging monotonicity, we develo new discriminative and generative algorithms for the resonder-classification roblem. We exlore and discuss connections to corruted data and olicy learning. We rovide an emirical study with both synthetic and real datasets to comare these secialized algorithms to standard benchmarks.
"
1118,2019,Learning Models from Data with Measurement Error: Tackling Underreporting,Poster,"Measurement error in observational datasets can lead to systematic bias in inferences based on these datasets. As studies based on observational data are increasingly used to inform decisions with real-world imact, it is critical that we develo a robust set of techniques for analyzing and adjusting for these biases. In this aer we resent a method for estimating the distribution of an outcome given a binary exosure that is subject to underreorting. Our method is based on a missing data view of the measurement error roblem, where the true exosure is treated as a latent variable that is marginalized out of a joint model. We rove three different conditions under which the outcome distribution can still be identified from data containing only error-rone observations of the exosure. We demonstrate this method on synthetic data and analyze its sensitivity to near violations of the identifiability conditions. Finally, we use this method to estimate the effects of maternal smoking and heroin use during regnancy on childhood obesity, two imort roblems from ublic health. Using the roosed method, we estimate these effects using only subject-reorted drug use data and refine the range of estimates generated by a sensitivity analysis-based aroach. Further, the estimates roduced by our method are consistent with existing literature on both the effects of maternal smoking and the rate at which subjects underreort smoking.
"
1119,2019,Adjustment Criteria for Generalizing Experimental Findings,Poster,"Generalizing causal effects from a controlled exeriment to settings beyond the articular study oulation is arguably one of the central tasks found in emirical circles. 
While a roer design and careful execution of the exeriment would  suort, under mild conditions, the validity of inferences about the oulation in which the exeriment was conducted, two challenges make the extraolation ste to different oulations somewhat involved, namely, transortability and samling selection bias. 
The former is concerned with disarities in the distributions and causal mechanisms between the domain (i.e., settings, oulation, environment) where the exeriment is conducted and where the inferences are intended; the latter with distortions in the samle's roortions due to referential selection of units into the study. 
In this aer, we investigate the assumtions and machinery necessary for using \textit{covariate adjustment} to correct for the biases generated by both of these roblems, and generalize exerimental data to infer causal effects in a new domain. We derive comlete grahical conditions to determine if a set of covariates is admissible for adjustment in this new setting. Building on the grahical characterization, we develo an efficient algorithm that enumerates all ossible admissible sets with oly-time delay guarantee; this can be useful for when some variables are referred over the others due to different costs or amenability to measurement.
"
1120,2019,Conditional Independence in Testing Bayesian Networks,Poster,"Testing Bayesian Networks (TBNs) were introduced recently to reresent a set of distributions, one of which is selected based on the given evidence and used for reasoning. TBNs are more exressive than classical Bayesian Networks (BNs): Marginal queries corresond to multi-linear functions in BNs and to iecewise multi-linear functions in TBNs. Moreover, TBN queries are universal aroximators, like neural networks. In this aer, we study conditional indeendence in TBNs, showing that it can be inferred from d-searation as in BNs. We also study the role of TBN exressiveness and indeendence in dealing with the roblem of learning with incomlete models (i.e., ones that miss nodes or edges from the data-generating model). Finally, we illustrate our results on a number of concrete examles, including a case study on Hidden Markov Models.
"
1121,2019,Sensitivity Analysis of Linear Structural Causal Models,Poster,"Causal inference requires assumtions about the data generating rocess, many of which are unverifiable from the data. Given that some causal assumtions might be uncertain or disuted, formal methods are needed to quantify how sensitive research conclusions are to violations of those assumtions. Although an extensive literature exists on the toic, most results are limited to secific model structures, while a general-urose algorithmic framework for sensitivity analysis is still lacking. In this aer, we develo a formal, systematic aroach to sensitivity analysis for arbitrary linear Structural Causal Models (SCMs).  We start by formalizing sensitivity analysis as a constrained identification roblem. We then develo an efficient, grah-based identification algorithm that exloits non-zero constraints on both directed and bidirected edges. This allows researchers to systematically derive sensitivity curves for a target causal quantity with an arbitrary set of ath coefficients and error covariances as sensitivity arameters. These results can be used to dislay the degree to which violations of causal assumtions affect the target quantity of interest, and to judge, on scientific grounds, whether roblematic degrees of violations are lausible.
"
1122,2019,More Efficient Off-Policy Evaluation through Regularized Targeted Learning,Poster,"We study the roblem of off-olicy evaluation (OPE) in Reinforcement Learning (RL), where the aim is to estimate the erformance of a new olicy given historical data that may have been generated by a different olicy, or olicies. In articular, we introduce a novel doubly-robust estimator for the OPE roblem in RL, based on the Targeted Maximum Likelihood Estimation rincile from the statistical causal inference literature. We also introduce several variance reduction techniques that lead to imressive erformance gains in off-olicy evaluation. We show emirically that our estimator uniformly wins over existing off-olicy evaluation methods across multile RL environments and various levels of model missecification. Finally, we further the existing theoretical analysis of estimators for the RL off-olicy estimation roblem by showing their $O_P(1\sqrt{n})$ rate of convergence and characterizing their asymtotic distribution."
1123,2019,Inferring Heterogeneous Causal Effects in Presence of Spatial Confounding,Poster,"We address the roblem of inferring the causal effect of an exosure on an outcome across sace, using observational data. The data is ossibly subject to unmeasured confounding variables which, in a standard aroach, must be adjusted for by estimating a nuisance function. Here we develo a method that eliminates the nuisance function, while mitigating the resulting errors-in-variables. The result is a robust and accurate inference method for satially varying heterogeneous causal effects. The roerties of the method are demonstrated on synthetic as well as real data from Germany and the US.
"
1124,2019,Adversarially Learned Representations for Information Obfuscation and Inference,Poster,"Data collection and sharing are ervasive asects of modern society. This rocess can either be voluntary, as in the case of a erson taking a facial image to unlock hisher hone, or incidental, such as traffic cameras collecting videos on edestrians. An undesirable side effect of these rocesses is that shared data can carry information about attributes that users might consider as sensitive, even when such information is of limited use for the task. It is therefore desirable for both data collectors and users to design rocedures that minimize sensitive information leakage. Balancing the cometing objectives of roviding meaningful individualized service levels and inference while obfuscating sensitive information is still an oen roblem. In this work, we take an information theoretic aroach that is imlemented as an unconstrained adversarial game between Dee Neural Networks in a rinciled, data-driven manner. This aroach enables us to learn domain-reserving stochastic transformations that maintain erformance on existing algorithms while minimizing sensitive information leakage.
"
1125,2019,Adaptive Neural Trees,Poster,"Dee neural networks and decision trees oerate on largely searate aradigms; tyically, the former erforms reresentation learning with re-secified architectures, while the latter is characterised by learning hierarchies over re-secified features with data-driven architectures. We unite the two via adative neural trees (ANTs), a model that incororates reresentation learning into edges, routing functions and leaf nodes of a decision tree, along with a backroagation-based training algorithm that adatively grows the architecture from rimitive modules (e.g., convolutional layers). We demonstrate that, whilst achieving cometitive erformance on classification and regression datasets, ANTs benefit from (i) lightweight inference via conditional comutation, (ii) hierarchical searation of features useful to the redictive task e.g. learning meaningful class associations, such as searating natural vs. man-made objects, and (iii) a mechanism to adat the architecture to the size and comlexity of the training dataset.
"
1126,2019,Connectivity-Optimized Representation Learning via Persistent Homology,Poster,"We study the roblem of learning reresentations with controllable connectivity roerties. This is beneficial in situations when the imosed structure can be leveraged ustream. In articular, we control the connectivity of an autoencoder's latent sace via a novel tye of loss, oerating on information from ersistent homology. Under mild conditions, this loss is differentiable and we resent a theoretical analysis of the roerties induced by the loss. We choose one-class learning as our ustream task and demonstrate that the imosed structure enables informed arameter selection for modeling the in-class distribution via kernel density estimators. Evaluated on comuter vision data, these one-class models exhibit cometitive erformance and, in a low samle size regime, outerform other methods by a large margin. Notably, our results indicate that a single 
autoencoder, trained on auxiliary (unlabeled) data, yields a maing into latent sace that can be reused across datasets for one-class learning.
"
1127,2019,Minimal Achievable Sufficient Statistic Learning,Poster,"We introduce Minimal Achievable Sufficient Statistic (MASS) Learning, a machine learning training objective for which the minima are minimal sufficient statistics with resect to a class of functions being otimized over (e.g., dee networks). In deriving MASS Learning, we also introduce Conserved Differential Information (CDI), an information-theoretic quantity that — unlike standard mutual information — can be usefully alied to deterministically-deendent continuous random variables like the inut and outut of a dee network. In a series of exeriments, we show that dee networks trained with MASS Learning achieve cometitive erformance on suervised learning, regularization, and uncertainty quantification benchmarks.
"
1128,2019,Learning to Route in Similarity Graphs,Poster,"Recently similarity grahs became the leading aradigm for efficient nearest neighbor search, outerforming traditional tree-based and LSH-based methods. Similarity grahs erform the search via greedy routing: a query traverses the grah and in each vertex moves to the adjacent vertex that is the closest to this query. In ractice, similarity grahs are often suscetible to local minima, when queries do not reach its nearest neighbors, getting stuck in subotimal vertices. In this aer we roose to learn the routing function that overcomes local minima via incororating information about the grah global structure. In articular, we augment the vertices of a given grah with additional reresentations that are learned to rovide the otimal routing from the start vertex to the query nearest neighbor. By thorough exeriments, we demonstrate that the roosed learnable routing successfully diminishes the local minima roblem and significantly imroves the overall search erformance.
"
1129,2019,Invariant-Equivariant Representation Learning for Multi-Class Data,Poster,"Reresentations learnt through dee neural networks tend to be highly informative, but oaque in terms of what information they learn to encode. We introduce an aroach to robabilistic modelling that learns to reresent data with two searate dee reresentations: an invariant reresentation that encodes the information of the class from which the data belongs, and an equivariant reresentation that encodes the symmetry transformation defining the articular data oint within the class manifold (equivariant in the sense that the reresentation varies naturally with symmetry transformations). This aroach is based rimarily on the strategic routing of data through the two latent variables, and thus is concetually transarent, easy to imlement, and in-rincile generally alicable to any data comrised of discrete classes of continuous distributions (e.g. objects in images, toics in language, individuals in behavioural data). We demonstrate qualitatively comelling reresentation learning and cometitive quantitative erformance, in both suervised and semi-suervised settings, versus comarable modelling aroaches in the literature with little fine tuning.
"
1130,2019,Infinite Mixture Prototypes for Few-shot Learning,Poster,"We roose infinite mixture rototyes to adatively reresent both simle and comlex data distributions for few-shot learning. Infinite mixture rototyes combine dee reresentation learning with Bayesian nonarametrics, reresenting each class by a set of clusters, unlike existing rototyical methods that reresent each class by a single cluster. By inferring the number of clusters, infinite mixture rototyes interolate between nearest neighbor and rototyical reresentations in a learned feature sace, which imroves accuracy and robustness in the few-shot regime. We show the imortance of adative caacity for caturing comlex data distributions such as suer-classes (like alhabets in character recognition), with 10-25% absolute accuracy imrovements over rototyical networks, while still maintaining or imroving accuracy on  standard few-shot learning benchmarks. By clustering labeled and unlabeled data with the same rule, infinite mixture rototyes achieve state-of-the-art semi-suervised accuracy, and can erform urely unsuervised clustering, unlike existing fully- and semi-suervised rototyical methods.
"
1131,2019,MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing,Poster,"Existing oular methods for semi-suervised learning with Grah Neural Networks (such as the Grah Convolutional Network) rovably cannot learn a general class of neighborhood mixing relationshis. To address this weakness, we roose a new model, MixHo, that can learn these relationshis, including difference oerators, by reeatedly mixing feature reresentations of neighbors at various distances. MixHo requires no additional memory or comutational comlexity, and outerforms on challenging baselines. In addition, we roose sarsity regularization that allows us to visualize how the network rioritizes neighborhood information across different grah datasets. Our analysis of the learned architectures reveals that neighborhood mixing varies er datasets.
"
1132,2019,Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting,Poster,"Addressing catastrohic forgetting is one of the key challenges in continual learning where machine learning systems are trained with sequential or streaming tasks. 
Desite recent remarkable rogress in state-of-the-art dee learning, dee neural networks (DNNs) are still lagued with the catastrohic forgetting roblem. This aer resents a concetually simle yet general and effective framework for handling catastrohic forgetting in continual learning with DNNs. The roosed method consists of two comonents: a neural structure otimization comonent and a arameter learning andor fine-tuning comonent. By searating the exlicit neural structure learning and the arameter estimation, not only is the roosed method caable of evolving neural structures in an intuitively meaningful way, but also shows strong caabilities of alleviating catastrohic forgetting in exeriments. Furthermore, the roosed method outerforms all other baselines on the ermuted MNIST dataset, the slit CIFAR100 dataset and the Visual Domain Decathlon dataset in continual learning setting.  
"
1133,2019,Exploration Conscious Reinforcement Learning Revisited,Poster,"The Exloration-Exloitation tradeoff arises in Reinforcement Learning when one cannot tell if a olicy is otimal. Then, there is a constant need to exlore new actions instead of exloiting ast exerience. In ractice, it is common to resolve the tradeoff by using a fixed exloration mechanism, such as  $\esilon$-greedy exloration or by adding Gaussian noise, while still trying to learn an otimal olicy. In this work, we take a different aroach and study exloration-conscious criteria, that result in otimal olicies with resect to the exloration mechanism. Solving these criteria, as we establish, amounts to solving a surrogate Markov Decision Process. We continue and analyze roerties of exloration-conscious otimal olicies and characterize two general aroaches to solve such criteria. Building on the aroaches, we aly simle changes in existing tabular and dee Reinforcement Learning algorithms and emirically demonstrate suerior erformance relatively to their non-exloration-conscious counterarts, both for discrete and continuous action saces."
1134,2019,Complexity of Linear Regions in Deep Networks,Poster,"It is well-known that the exressivity of a neural network deends on its architecture, with deeer networks exressing more comlex functions. In the case of networks that comute iecewise linear functions, such as those with ReLU activation, the number of distinct linear regions is a natural measure of exressivity. It is ossible to construct networks with merely a single region, or for which the number of linear regions grows exonentially with deth; it is not clear where within this range most networks fall in ractice, either before or after training. In this aer, we rovide a mathematical framework to count the number of linear regions of a iecewise linear network and measure the volume of the boundaries between these regions. In articular, we rove that for networks at initialization, the average number of regions along any one-dimensional subsace grows linearly in the total number of neurons, far below the exonential uer bound. We also find that the average distance to the nearest region boundary at initialization scales like the inverse of the number of neurons. Our theory suggests that, even after training, the number of linear regions is far below exonential, an intuition that matches our emirical observations. We conclude that the ractical exressivity of neural networks is likely far below that of the theoretical maximum, and that this ga can be quantified.
"
1135,2019,On Connected Sublevel Sets in Deep Learning,Poster,"This aer shows that every sublevel set of the loss function of a class of dee over-arameterized neural nets with iecewise linear activation functions is connected and unbounded. This imlies that the loss has no bad local valleys and all of its global minima are connected within a unique and otentially very large global valley.
"
1136,2019,Adversarial Examples Are a Natural Consequence of Test Error in Noise,Poster,"recodeOver the last few years, the henomenon of \emh{adversarial examles} --- maliciously constructed inuts that fool trained machine learning models --- has catured the attention of the research community, esecially when restricted to small modifications of a correctly handled inut. Less surrisingly, image classifiers also lack human-level erformance on randomly corruted images, such as images with additive Gaussian noise. In this aer we rovide both emirical and theoretical evidence that these are two manifestations of the same underlying henomenon, and therefore the adversarial robustness and corrution robustness research rograms are closely related. This suggests that imroving adversarial robustness should go hand in hand with imroving erformance in the resence of more general and realistic image corrutions. This yields a comutationally tractable evaluation metric for defenses to consider: test error in noisy image distributions.
codere
"
1137,2019,Greedy Layerwise Learning Can Scale To ImageNet,Poster,"Shallow suervised 1-hidden layer neural networks have a number of favorable roerties that make them easier to interret, analyze, and otimize than their dee counterarts, but lack their reresentational ower. Here we use 1-hidden layer learning roblems to sequentially build dee networks layer by layer, which can inherit roerties from shallow networks. Contrary to revious aroaches using shallow networks, we focus on roblems where dee learning is reorted as critical for success. We thus study CNNs on image classification tasks using the large-scale ImageNet dataset and the CIFAR-10 dataset. Using a simle set of ideas for architecture and training we find that solving sequential 1-hidden-layer auxiliary roblems lead to a CNN that exceeds AlexNet erformance on ImageNet. Extending this training methodology to construct individual layers by solving 2-and-3-hidden layer auxiliary roblems, we obtain an 11-layer network that exceeds several members of the VGG model family on ImageNet, and can train a VGG-11 model to the same accuracy as end-to-end learning. To our knowledge, this is the first cometitive alternative to end-to-end training of CNNs that can scale to ImageNet. We illustrate several interesting roerties of these models and conduct a range of exeriments to study the roerties this training induces on the intermediate layers.
"
1138,2019,On the Impact of the Activation function on Deep Neural Networks Training,Poster,"The weight initialization and the activation function of dee neural networks have a crucial imact on the erformance of the training rocedure. An inaroriate selection can lead to the loss of information of the inut during forward roagation and the exonential vanishingexloding of gradients during back-roagation. Understanding the theoretical roerties of untrained random networks is key to identifying which dee networks may be trained successfully as recently demonstrated by Samuel et al. (2017) who showed that for dee feedforward neural networks only a secific choice of hyerarameters known as the `Edge of Chaos' can lead to good erformance. While the work by Samuel et al. (2017) discuss trainability issues, we focus here on training acceleration and overall erformance. We give a comrehensive theoretical analysis of the Edge of Chaos and show that we can indeed tune the initialization arameters and the activation function in order to accelerate the training and imrove the erformance.
"
1139,2019,Estimating Information Flow in Deep Neural Networks,Poster,"We study the estimation of the mutual information I(X;Temℓ) between the inut X to a dee neural network (DNN) and the outut vector Temℓ of its ℓ-th hidden layer (an “internal reresentation”). Focusing on feedforward networks with fixed weights and noisy internal reresentations, we develo a rigorous framework for accurate estimation of I(X;Temℓ). By relating I(X;Temℓ) to information transmission over additive white Gaussian noise channels, we reveal that comression, i.e. reduction in I(X;Temℓ) over the course of training, is driven by rogressive geometric clustering of the reresentations of samles from the same class. Exerimental results verify this connection. Finally, we shift focus to urely deterministic DNNs, where I(X;Temℓ) is rovably vacuous, and show that nevertheless, these models also cluster inuts belonging to the same class. The binning-based aroximation of I(X;T_ℓ) emloyed in ast works to measure comression is identified as a measure of clustering, thus clarifying that these exeriments were in fact tracking the same clustering henomenon. Leveraging the clustering ersective, we rovide new evidence that comression and generalization may not be causally related and discuss otential future research ideas.
"
1140,2019,The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects,Poster,"Understanding the behavior of stochastic gradient descent (SGD) in the context of dee neural networks has raised lots of concerns recently. 
Along this line, we  study a general form of gradient based otimization dynamics with unbiased noise, which unifies SGD and standard Langevin dynamics.
Through investigating this general otimization dynamics, we analyze the behavior of SGD on escaing from minima and its regularization effects. A novel indicator is derived to characterize the efficiency of escaing from minima through measuring the alignment of noise covariance and the curvature of loss function.
Based on this indicator, two conditions are established to show which tye of noise structure is suerior to isotroic noise in term of escaing efficiency.
We further show that the anisotroic noise in SGD satisfies the two conditions, and thus hels to  escae from shar and oor minima effectively, towards more stable and flat minima that tyically generalize well.
We systematically design various exeriments to verify the benefits of the anisotroic noise, comared with full gradient descent lus isotroic diffusion (i.e. Langevin dynamics).
"
1141,2019,Characterizing Well-Behaved vs. Pathological Deep Neural Networks,Poster,"We introduce a novel aroach, requiring only mild assumtions, for the characterization of dee neural networks at initialization. Our aroach alies both to fully-connected and convolutional networks and easily incororates batch normalization and ski-connections. Our key insight is to consider the evolution with deth of statistical moments of signal and noise, thereby characterizing the resence or absence of athologies in the hyothesis sace encoded by the choice of hyerarameters. We establish: (i) for feedforward networks, with and without batch normalization, the multilicativity of layer comosition inevitably leads to ill-behaved moments and athologies; (ii) for residual networks with batch normalization, on the other hand, ski-connections induce ower-law rather than exonential behaviour, leading to well-behaved moments and no athology.
"
1142,2019,Understanding Geometry of Encoder-Decoder CNNs,Poster,"Encoder-decoder networks using convolutional neural network (CNN) architecture have been extensively used in dee learning literatures thanks to its excellent erformance for various inverse roblems in comuter vision, medical imaging, etc.
However,  it is still difficult to obtain coherent geometric view why such an architecture gives the desired erformance. Insired by recent theoretical understanding on  generalizability, exressivity and otimization landscae of neural networks, as well as the theory of   convolutional framelets, here we rovide a unified theoretical framework  that leads to a better understanding of geometry of encoder-decoder CNNs. Our unified mathematical framework shows that encoder-decoder CNN architecture is closely related to nonlinear basis reresentation
using  combinatorial convolution frames, whose exressibility increases exonentially with the network deth. We also  demonstrate  the imortance of skied connection  in terms of exressibility,  and  otimization landscae. 
"
1143,2019,Traditional and Heavy Tailed Self Regularization in Neural Network Models,Poster,"Random Matrix Theory (RMT) is alied to analyze the weight matrices of Dee Neural Networks (DNNs), including both roduction quality, re-trained models such as AlexNet and Incetion, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Emirical and theoretical results clearly indicate that the emirical sectral density (ESD) of DNN layer matrices dislays signatures of traditionally-regularized statistical models, even in the absence of exogenously secifying traditional forms of regularization, such as Droout or Weight Norm constraints.  Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develo a theory to identify \emh{5+1 Phases of Training}, corresonding to increasing amounts of \emh{Imlicit Self-Regularization}.  For smaller andor older DNNs, this Imlicit Self-Regularization is like traditional Tikhonov regularization, in that there is a ``size scale'' searating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of \emh{Heavy-Tailed Self-Regularization}, similar to the self-organization seen in the statistical hysics of disordered systems.  This imlicit Self-Regularization can deend strongly on the many knobs of the training rocess.  By exloiting the generalization ga henomena, we demonstrate that we can cause a small model to exhibit all 5+1 hases of training simly by changing the batch size.
"
1144,2019,Almost surely constrained convex optimization,Poster,"We roose a stochastic gradient framework for solving stochastic comosite convex otimization roblems with (ossibly) infinite number of linear inclusion constraints that need to be satisfied almost surely. We use smoothing and homotoy techniques to handle constraints without the need for matrix-valued rojections. We show for our stochastic gradient algorithm $\mathcal{O}(\log(k)\sqrt{k})$ convergence rate for general convex objectives and $\mathcal{O}(\log(k)k)$ convergence rate for restricted strongly convex objectives. 
These rates are known to be otimal u to logarithmic factor, even without constraints. 
We conduct numerical exeriments on basis ursuit, hard margin suort vector machines and ortfolio otimization roblems and show that our algorithm achieves state-of-the-art ractical erformance."
1145,2019,Generalized Majorization-Minimization,Poster,"Non-convex otimization is ubiquitous in machine learning. Majorization-Minimization (MM) is a owerful iterative rocedure for otimizing non-convex functions that works by otimizing a sequence of bounds on the function. In MM, the bound at each iteration is required to touch the objective function at the otimizer of the revious bound. We show that this touching constraint is unnecessary and overly restrictive. We generalize MM by relaxing this constraint, and roose a new otimization framework, named Generalized Majorization-Minimization (G-MM), that is more flexible. For instance, G-MM can incororate alication-secific biases into the otimization rocedure without changing the objective function. We derive G-MM algorithms for several latent variable models and show emirically that they consistently outerform their MM counterarts in otimizing non-convex objectives. In articular, G-MM algorithms aear to be less sensitive to initialization.
"
1146,2019,On the Computation and Communication Complexity of Parallel SGD with Dynamic Batch Sizes for Stochastic Non-Convex Optimization,Poster,"For SGD based distributed stochastic otimization, comutation comlexity, measured by the convergence rate in terms of the number of stochastic gradient calls, and communication comlexity, measured by the number of inter-node communication rounds, are two most imortant erformance metrics.  The classical data-arallel imlementation of SGD over N workers can achieve linear seedu of its convergence rate but incurs an inter-node communication round at each batch. We study the benefit of using dynamically increasing batch sizes in arallel SGD for stochastic non-convex otimization by charactering the attained convergence rate and the required number of communication rounds.  We show that for stochastic non-convex otimization under the P-L condition, the classical data-arallel SGD with exonentially increasing batch sizes can achieve the fastest known $O(1(NT))$ convergence with linear seedu using only $\log(T)$ communication rounds. For general stochastic non-convex otimization, we roose a Catalyst-like algorithm to achieve the fastest known $O(1\sqrt{NT})$ convergence with only $O(\sqrt{NT}\log(\frac{T}{N}))$ communication rounds.  "
1147,2019,Simple Stochastic Gradient Methods for Non-Smooth Non-Convex Regularized Optimization,Poster,"Our work focuses on stochastic gradient methods for otimizing a smooth non-convex loss function with a non-smooth non-convex regularizer. Research on this class of roblem is quite limited, and until recently no non-asymtotic convergence results have been reorted. We resent two simle stochastic gradient algorithms, for finite-sum and general stochastic otimization roblems, which have suerior convergence comlexities comared to the current state-of-the-art. We also comare our algorithms' erformance in ractice for emirical risk minimization. 
"
1148,2019,Surrogate Losses for Online Learning of Stepsizes in Stochastic Non-Convex Optimization,Poster,"Stochastic Gradient Descent (SGD) has layed a central role in machine learning. However, it requires a carefully hand-icked stesize for fast convergence, which is notoriously tedious and time-consuming to tune. Over the last several years, a lethora of adative gradient-based algorithms have emerged to ameliorate this roblem. In this aer, we roose new surrogate losses to cast the roblem of learning the otimal stesizes for the stochastic otimization of a non-convex smooth objective function onto an online convex otimization roblem. This allows the use of no-regret online algorithms to comute otimal stesizes on the fly. In turn, this results in a SGD algorithm with self-tuned stesizes that guarantees convergence rates that are automatically adative to the level of noise.
"
1149,2019,Efficient Dictionary Learning with Gradient Descent,Poster,"Randomly initialized first-order otimization algorithms are the method of choice for solving many high-dimensional nonconvex roblems in machine learning, yet general theoretical guarantees cannot rule out convergence to critical oints of oor objective value. For some highly structured nonconvex roblems however, the success of gradient descent can be understood by studying the geometry of the objective. We study one such roblem -- comlete orthogonal dictionary learning, and rovide converge guarantees for randomly initialized gradient descent to the neighborhood of a global otimum. The resulting rates scale as low order olynomials in the dimension even though the objective ossesses an exonential number of saddle oints. This efficient convergence can be viewed as a consequence of negative curvature normal to the stable manifolds associated with saddle oints, and we rovide evidence that this feature is shared by other nonconvex roblems of imortance as well. 
"
1150,2019,Plug-and-Play Methods Provably Converge with Properly Trained Denoisers,Poster,"Plug-and-lay (PnP) is a non-convex framework that integrates modern denoising riors, such as BM3D or dee learning-based denoisers, into ADMM or other roximal algorithms. An advantage of PnP is that one  can use re-trained denoisers when there is not sufficient data for end-to-end training. Although PnP has been recently studied extensively with great emirical success, theoretical analysis addressing even the most basic question of convergence has been insufficient. In this aer, we theoretically establish convergence of PnP-FBS and PnP-ADMM, without using diminishing stesizes, under a certain Lischitz condition on the denoisers. We then roose real sectral normalization, a technique for training dee learning-based denoisers to satisfy the roosed Lischitz condition. Finally, we resent exerimental results validating the theory.
"
1151,2019,Riemannian adaptive stochastic gradient algorithms on matrix manifolds,Poster,"Adative stochastic gradient algorithms in the Euclidean sace have attracted much attention lately. Such exlorations on Riemannian manifolds, on the other hand, are relatively new, limited, and challenging. This is because of the intrinsic non-linear structure of the underlying manifold and the absence of a canonical coordinate system. In machine learning alications, however, most manifolds of interest are reresented as matrices with notions of row and column subsaces. In addition, the imlicit manifold-related constraints may also lie on such subsaces. For examle, the Grassmann manifold is the set of column subsaces. To this end, such a rich structure should not be lost by transforming matrices to just a stack of vectors while develoing otimization algorithms on manifolds. We roose novel stochastic gradient algorithms for roblems on Riemannian matrix manifolds by adating the row and column subsaces of gradients. Our algorithms are rovably convergent and they achieve the convergence rate of order $O(log(T)sqrt(T))$, where $T$ is the number of iterations. Our exeriments illustrate that the roosed algorithms outerform existing Riemannian adative stochastic algorithms. "
1152,2019,Stochastic Optimization for DC Functions and Non-smooth Non-convex Regularizers with Non-asymptotic Convergence,Poster,"Difference of convex (DC) functions cover a broad family of non-convex and ossibly non-smooth and non-differentiable functions, and have wide alications in machine learning and statistics. Although deterministic algorithms for DC functions have been extensively studied, stochastic otimization that is more suitable for learning with big data remains under-exlored. In this aer, we roose new stochastic otimization algorithms and study their first-order convergence theories for solving a broad family of DC functions. We imrove the existing algorithms and theories of stochastic otimization for DC functions from both ractical and theoretical ersectives. Moreover, we extend the roosed stochastic algorithms for DC functions to solve roblems with a general non-convex non-differentiable regularizer, which does not necessarily have a DC decomosition but enjoys an efficient roximal maing.  To the best of our knowledge, this is the first work that gives the first non-asymtotic convergence for solving non-convex otimization whose objective has a general non-convex non-differentiable regularizer.
"
1153,2019,Alternating Minimizations Converge to Second-Order Optimal Solutions,Poster,"This work studies the second-order convergence for both standard alternating minimization and roximal alternating minimization. We show that under mild assumtions on the (nonconvex) objective function, both algorithms avoid strict saddles almost surely from random initialization. Together with known first-order convergence results, this imlies both algorithms converge to a second-order stationary oint. This solves an oen roblem for the second-order convergence of alternating minimization algorithms that have been widely used in ractice to solve large-scale nonconvex roblems due to their simle imlementation, fast convergence, and suerb emirical erformance.
"
1154,2019,Provably Efficient Imitation Learning from Observation Alone,Poster,"We study Imitation Learning (IL) from Observations alone (ILFO) in large-scale MDPs. While most IL algorithms rely on an exert to directly rovide actions to the learner, in this setting the exert only sulies sequences of observations. We design a new model-free algorithm for ILFO, Forward Adversarial Imitation Learning (FAIL), which learns a sequence of time-deendent olicies by minimizing an Integral Probability Metric between the observation distributions of the exert olicy and the learner. FAIL rovably learns a near-otimal olicy with a number of samles that is olynomial in all relevant arameters but indeendent of the number of unique observations. The resulting theory extends the domain of rovably samle efficient learning algorithms beyond existing results that tyically only consider tabular RL settings or settings that require access to a near-otimal reset distribution.  We also demonstrate the efficacy ofFAIL on multile OenAI Gym control tasks.
"
1155,2019,Dead-ends and Secure Exploration in Reinforcement Learning,Poster,"Many interesting alications of reinforcement learning (RL) involve MDPs that include numerous codedead-end"" states. Uon reaching a dead-end state, the agent continues to interact with the environment in a dead-end trajectory before reaching an undesired terminal state, regardless of whatever actions are chosen. The situation is even worse when existence of many dead-end states is couled with distant ositive rewards from any initial state (we term this as Bridge Effect). Hence, conventional exloration techniques often incur rohibitively many training stes before convergence. To deal with the bridge effect, we roose a condition for exloration, called security. We next establish formal results that translate the security condition into the learning roblem of an auxiliary value function. This new value function is used to cacodeany"" given exloration olicy and is guaranteed to make it secure. As a secial case, we use this theory and introduce secure random-walk. We next extend our results to the dee RL settings by identifying and addressing two main challenges that arise. Finally, we emirically comare secure random-walk with standard benchmarks in two sets of exeriments including the Atari game of Montezuma's Revenge. 
"
1156,2019,Statistics and Samples in Distributional Reinforcement Learning,Poster,"We resent a unifying framework for designing and analysing distributional reinforcement learning (DRL) algorithms in terms of recursively estimating statistics of the return distribution. Our key insight is that DRL algorithms can be decomosed as the combination of some statistical estimator and a method for imuting a return distribution consistent with that set of statistics. With this new understanding, we are able to rovide imroved analyses of existing DRL algorithms as well as construct a new algorithm (EDRL) based uon estimation of the exectiles of the return distribution. We comare EDRL with existing methods on a variety of MDPs to illustrate concrete asects of our analysis, and develo a dee RL variant of the algorithm, ER-DQN, which we evaluate on the Atari-57 suite of games.
"
1157,2019,Hessian Aided Policy Gradient,Poster,"	Reducing the variance of estimators for olicy gradient has long been the focus of reinforcement learning research.
	While classic algorithms like REINFORCE find an $\esilon$-aroximate first-order stationary oint in $\OM({1}{\esilon^4})$ random trajectory simulations, no rovable  imrovement on the comlexity has been made so far.
	This aer resents a Hessian aided olicy gradient method with the first imroved samle comlexity of $\OM({1}{\esilon^3})$.
	While our method exloits information from the olicy Hessian, it can be imlemented in linear time with resect to the arameter dimension and is hence alicable to sohisticated DNN arameterization.
	Simulations on standard tasks validate the efficiency of our method."
1158,2019,Provably Efficient Maximum Entropy Exploration,Poster,"Suose an agent is in a (ossibly unknown) Markov Decision Process in the absence of a reward signal, what might we hoe that an agent can efficiently learn to do? This work studies a broad class of objectives that are defined solely as functions of the state-visitation frequencies that are induced by how the agent behaves. For examle, one natural, intrinsically defined, objective roblem is for the agent to learn a olicy which induces a distribution over state sace that is as uniform as ossible, which can be measured in an entroic sense.  We rovide an efficient algorithm to otimize such such intrinsically defined objectives, when given access to a black box lanning oracle (which is robust to function aroximation). Furthermore, when restricted to the tabular setting where we have samle based access to the MDP, our roosed algorithm is rovably efficient, both in terms of its samle and comutational comlexities. Key to our algorithmic methodology is utilizing the conditional gradient method (a.k.a. the Frank-Wolfe algorithm) which utilizes an aroximate MDP solver.
"
1159,2019,Combining parametric and nonparametric models for off-policy evaluation,Poster,"We consider a model-based aroach to erform batch off-olicy evaluation in reinforcement learning. Our method takes a mixture-of-exerts aroach to combine arametric and non-arametric models of the environment such that the final value estimate has the least exected error. We do so by first estimating the local accuracy of each model and then using a lanner to select which model to use at every time ste as to minimize the return error estimate along entire trajectories. Across a variety of domains, our mixture-based aroach outerforms the individual models alone as well as state-of-the-art imortance samling-based estimators.
"
1160,2019,Sample-Optimal Parametric Q-Learning Using Linearly Additive Features,Poster,"
Consider a Markov decision rocess (MDP) that admits a set of state-action features, which can linearly exress the rocess's robabilistic transition model. We roose a arametric Q-learning algorithm that finds an aroximate-otimal olicy using a samle size roortional to the feature dimension $K$ and invariant with resect to the size of the state sace. To further imrove its samle efficiency, we exloit the monotonicity roerty and intrinsic noise structure of the Bellman oerator, rovided the existence of anchor state-actions that imly imlicit non-negativity in the feature sace. We augment the algorithm using techniques of variance reduction, monotonicity reservation, and confidence bounds. It is roved to find a olicy which is $\esilon$-otimal from any initial state with high robability using $\widetilde{O}(K\esilon^2(1-\gamma)^3)$ samle transitions for arbitrarily large-scale MDP with a discount factor $\gamma\in(0,1)$. A matching information-theoretical lower bound is roved, confirming the samle otimality of the roosed method with resect to all arameters (u to olylog factors)."
1161,2019,Transfer of Samples in Policy Search via Multiple Importance Sampling,Poster,"We consider the transfer of exerience samles in reinforcement learning. Most of the revious works in this context focused on value-based settings, where transferring instances conveniently reduces to the transfer of (s,a,s',r) tules. In this aer, we consider the more comlex case of reusing samles in olicy search methods, in which the agent is required to transfer entire trajectories between environments with different transition models. By leveraging ideas from multile imortance samling, we roose robust gradient estimators that effectively achieve this goal, along with several techniques to reduce their variance. In the case where the transition models are known, we theoretically establish the robustness to the negative transfer for our estimators. In the case of unknown models, we roose a method to efficiently estimate them when the target task belongs to a finite set of ossible tasks and when it belongs to some reroducing kernel Hilbert sace. We rovide emirical results to show the effectiveness of our estimators.
"
1162,2019,Action Robust Reinforcement Learning and Applications in Continuous Control,Poster,"A olicy is said to be robust if it maximizes the reward while considering a bad, or even adversarial, model. In this work we formalize two new criteria of robustness to action uncertainty. 
Secifically, we consider two scenarios in which the agent attemts to erform an action $\action$, and (i) with robability $\alha$, an alternative adversarial action $\bar \action$ is taken, or (ii) an adversary adds a erturbation to the selected action in the case of continuous action sace. We show that our criteria are related to common forms of uncertainty in robotics domains, such as the occurrence of abrut forces, and suggest algorithms in the tabular case. Building on the suggested algorithms, we generalize our aroach to dee reinforcement learning (DRL) and rovide extensive exeriments in the various MuJoCo domains. 
Our exeriments show that not only does our aroach roduce robust olicies, but it also imroves the erformance in the absence of erturbations. 
This generalization indicates that action-robustness can be thought of as imlicit regularization in RL roblems."
1163,2019,Kernel-Based Reinforcement Learning in Robust Markov Decision Processes,Poster,"The robust Markov decision rocesses (MDP) framework aims to address the roblem of arameter uncertainty due to model mismatch, aroximation errors or even adversarial behaviors. It is esecially relevant when deloying the learned olicies in real-world alications. Scaling u the robust MDP framework to large or continuous state sace remains a challenging roblem. The use of function aroximation in this case is usually inevitable and this can only amlify the roblem of model mismatch and arameter uncertainties. It has been reviously shown that, in the case of MDPs with state aggregation, the robust olicies enjoy a tighter erformance bound comared to standard solutions due to its reduced sensitivity to aroximation errors. We extend these results to the much larger class of kernel-based aroximators and show, both analytically and emirically that the robust olicies can significantly outerform the non-robust counterart.
"
1164,2019,Optimal Algorithms for Lipschitz Bandits with Heavy-tailed Rewards,Poster,"We study Lischitz bandits, where a learner reeatedly lays one arm from an infinite arm set and then receives a stochastic reward whose exectation is a Lischitz function of the chosen arm. Most of existing work assume the reward distributions are bounded or at least sub-Gaussian, and thus do not aly to heavy-tailed rewards arising in many real-world scenarios such as web advertising and financial markets. To address this limitation, in this aer we relax the assumtion on rewards to allow arbitrary distributions that have finite $(1+\esilon)$-th moments for some $\esilon \in (0, 1]$, and roose algorithms that enjoy a sublinear regret of $\widetilde{O}(T^{(d_z\esilon + 1)(d_z \esilon + \esilon + 1)})$ where $T$ is the time horizon and $d_z$ is the zooming dimension. The key idea is to exloit the Lischitz roerty of the exected reward function by adatively discretizing the arm set, and emloy uer confidence bound olicies with robust mean estimators designed for heavy-tailed distributions. Furthermore, we rovide a lower bound for Lischitz bandits with heavy-tailed rewards, and show that our algorithms are otimal in terms of $T$. Finally, we conduct numerical exeriments to demonstrate the effectiveness of our algorithms."
1165,2019,Target Tracking for Contextual Bandits: Application to Demand Side Management,Poster,"We roose a contextual-bandit aroach for demand side management by offering rice incentives. More recisely, a target mean consumtion is set at each round and the mean consumtion is modeled as a comlex function of the distribution of rices sent and of some contextual variables such as the temerature, weather, and so on. The erformance of our strategies is measured in quadratic losses through a regret criterion. We offer $T^{23}$ uer bounds on this regret (u to oly-logarithmic terms)---and even faster rates under stronger assumtions---for strategies insired by standard strategies for contextual bandits (like LinUCB, see Li et al., 2010). Simulations on a real data set gathered by UK Power Networks, in which rice incentives were offered, show that our strategies are effective and may indeed manage demand resonse by suitably icking the rice levels."
1166,2019,Correlated bandits or: How to minimize mean-squared error online,Poster,"While the objective in traditional multi-armed bandit roblems is to  find the arm with the highest mean, in many settings, finding an arm that best catures information about other arms is of interest.  This objective, however, requires learning the underlying correlation structure and not just the means.  Sensors lacement for industrial surveillance and cellular network monitoring are a few alications, where the underlying correlation structure lays an imortant role. Motivated by such alications, we formulate the correlated bandit roblem, where the objective is to find the arm with the lowest mean-squared error (MSE) in estimating all the arms. To this end, we derive first an MSE estimator based on samle variancescovariances and show that our estimator exonentially concentrates around the true MSE. Under a best-arm identification framework, we roose a successive rejects tye algorithm and rovide bounds on the robability of error in identifying the best arm. Using minimax theory, we also derive fundamental erformance limits for the correlated bandit roblem. 
"
1167,2019,Stay With Me: Lifetime Maximization Through Heteroscedastic Linear Bandits With Reneging,Poster,"Sequential decision making for lifetime maximization is a critical roblem in many real-world alications, such as medical treatment and ortfolio selection. In these alications, a ``reneging'' henomenon, where articiants may disengage from future interactions after observing an unsatisfiable outcome, is rather revalent. To address the above issue, this aer rooses a model of heteroscedastic linear bandits with reneging, which allows each articiant to have a distinct ``satisfaction level,"" with any interaction outcome falling short of that level resulting in that articiant reneging. Moreover, it allows the variance of the outcome to be context-deendent. Based on this model, we develo a UCB-tye olicy, namely HR-UCB, and rove that it achieves $\mathcal{O}\big(\sqrt{{T}(\log({T}))^{3}}\big)$ regret. Finally, we validate the erformance of HR-UCB via simulations. "
1168,2019,"Garbage In, Reward Out: Bootstrapping Exploration in Multi-Armed Bandits",Poster,"We roose a bandit algorithm that exlores by randomizing its history of rewards. Secifically, it ulls the arm with the highest mean reward in a non-arametric bootstra samle of its history with seudo rewards. We design the seudo rewards such that the bootstra mean is otimistic with a sufficiently high robability. We call our algorithm Giro, which stands for garbage in, reward out. We analyze Giro in a Bernoulli bandit and derive a $O(K \Delta^{-1} \log n)$ bound on its $n$-round regret, where $\Delta$ is the difference in the exected rewards of the otimal and the best subotimal arms, and $K$ is the number of arms. The main advantage of our exloration design is that it easily generalizes to structured roblems. To show this, we roose contextual Giro with an arbitrary reward generalization model. We evaluate Giro and its contextual variant on multile synthetic and real-world roblems, and observe that it erforms well."
1169,2019,Beating Stochastic and Adversarial Semi-bandits Optimally and Simultaneously,Poster,"We develo the first general semi-bandit algorithm that simultaneously achieves $\mathcal{O}(\log T)$ regret for stochastic environments
and $\mathcal{O}(\sqrt{T})$ regret for adversarial environments
without knowledge of the regime or the number of rounds $T$.
The leading roblem-deendent constants of our bounds are not only otimal in some worst-case sense studied reviously,
but also otimal for two concrete instances of semi-bandit roblems.
Our algorithm and analysis extend the recent work of (Zimmert & Seldin, 2019) for the secial case of multi-armed bandits,
but imortantly requires a novel hybrid regularizer designed secifically for semi-bandit.
Exerimental results on synthetic data show that our algorithm indeed erforms well uniformly over different environments.
We finally rovide a reliminary extension of our results to the full bandit feedback."
1170,2019,Bilinear Bandits with Low-rank Structure ,Poster,"  We introduce the bilinear bandit roblem with low-rank structure in which an action takes the form of a air of arms from two different entity tyes, and the reward is a bilinear function of the known feature vectors of the arms.  The unknown in the roblem is a $d_1$ by $d_2$ matrix $\mathbf{\Theta}^*$ that defines the reward, and has low rank $r \ll \min\{d_1,d_2\}$.  Determination of $\mathbf{\Theta}^*$ with this low-rank structure oses a significant challenge in finding the right exloration-exloitation tradeoff.  In this work, we roose a new two-stage algorithm called ``Exlore-Subsace-Then-Refine'' (ESTR). The first stage is an exlicit subsace exloration, while the second stage is a linear bandit algorithm called ``almost-low-dimensional OFUL'' (LowOFUL) that exloits and further refines the estimated subsace via a regularization technique.  We show that the regret of ESTR is $\widetilde{\mathcal{O}}((d_1+d_2)^{32} \sqrt{r T})$ where $\widetilde{\mathcal{O}}$ hides logarithmic factors and $T$ is the time horizon, which imroves uon the regret of $\widetilde{\mathcal{O}}(d_1d_2\sqrt{T})$ attained for a na\""ive linear bandit reduction.  We conjecture that the regret bound of ESTR is unimrovable u to olylogarithmic factors, and our reliminary exeriment shows that ESTR outerforms a na\""ive linear bandit reduction."
1171,2019,Online Learning to Rank with Features,Poster,"We introduce a new model for online ranking in which the click robability factors into an examination and attractiveness function and the attractiveness function is a linear function of a feature vector and an unknown arameter. Only relatively mild assumtions are made on the examination function. A novel algorithm for this setu is analysed, showing that the deendence on the number of items is relaced by a deendence on the dimension, allowing the new algorithm to handle a large number of items. When reduced to the orthogonal case, the regret of the algorithm imroves on the state-of-the-art.
"
1172,2019,On the Design of Estimators for Bandit Off-Policy Evaluation,Poster,"Off-olicy evaluation is the roblem of estimating the value of a target olicy using data collected under a different olicy. Given a base estimator for bandit off-olicy evaluation and a arametrized class of control variates, we address the roblem of comuting a control variate in that class that reduces the risk of the base estimator. We derive the oulation risk as a function of the class arameters and we establish conditions that guarantee risk imrovement. We resent our main results in the context of multi-armed bandits, and we roose a simle design for contextual bandits that gives rise to an estimator that is shown to erform well in multi-class cost-sensitive classification datasets.
"
1173,2019,Dynamic Learning with Frequent New Product Launches: A Sequential Multinomial Logit Bandit Problem,Poster,"Motivated by the henomenon that comanies introduce new roducts to kee abreast with customers' raidly changing tastes, we consider a novel online learning setting where a rofit-maximizing seller needs to learn customers' references through offering recommendations, which may contain existing roducts and new roducts that are launched in the middle of a selling eriod. We roose a sequential multinomial logit (SMNL) model to characterize customers' behavior when roduct recommendations are resented in tiers. For the offline version with known customers' references, we roose a olynomial-time algorithm and characterize the roerties of the otimal tiered roduct recommendation. For the online roblem, we roose a learning algorithm and quantify its regret bound. Moreover, we extend the setting to incororate a constraint which ensures every new roduct is learned to a given accuracy. Our results demonstrate the tier structure can be used to mitigate the risks associated with learning new roducts.
"
1174,2019,Context-Aware Zero-Shot Learning for Object Recognition,Poster,"Zero-Shot  Learning  (ZSL)  aims  at  classifying unlabeled objects by leveraging auxiliary knowledge, such as semantic reresentations. A limitation of revious aroaches is that only intrinsic roerties of objects, e.g. their visual aearance, are taken into account while their context, e.g. the surrounding objects in the image, is ignored. Following the intuitive rincile that objects tend to be found in certain contexts but not others, we roose a new and challenging aroach, context-aware ZSL, that leverages semantic reresentations in a new way to model the conditional likelihood of an object to aear in a given context. Finally, through extensive exeriments conducted on Visual Genome, we show that contextual information can substantially imrove the standard ZSL aroach and is robust to unbalanced classes.
"
1175,2019,Band-limited Training and Inference for Convolutional Neural Networks,Poster,"The convolutional layers are core building blocks of neural network architectures. In general, a convolutional filter alies to the entire frequency sectrum of the inut data. We exlore artificially constraining the frequency sectra of these filters and data, called band-limiting, during training. The frequency domain constraints aly to both the feed-forward and back-roagation stes. Exerimentally, we observe that Convolutional Neural Networks (CNNs) are resilient to this comression scheme and results suggest that CNNs learn to leverage lower-frequency comonents. In articular, we found: (1) band-limited training can effectively control the resource usage (GPU and memory); (2) models trained with band-limited layers retain high rediction accuracy; and (3) requires no modification to existing training algorithms or neural network architectures to use unlike other comression schemes. 
"
1176,2019,Learning Classifiers for Target Domain with Limited or No Labels,Poster,"In comuter vision alications, such as domain adatation (DA), few shot learning (FSL) and zero-shot learning (ZSL), we encounter new objects and environments, for which insufficient examles exist to allow for training “models from scratch,” and methods that adat existing models, trained on the resented training environment, to the new scenario are required. We roose a novel visual attribute encoding method that encodes each image as a low-dimensional robability vector
comosed of rototyical art-tye robabilities. The rototyes are learnt to be reresentative of all training data. At test-time we utilize this encoding as an inut to a classifier. At test-time we freeze the encoder and only learnadat the classifier comonent to limited annotated labels in FSL; new semantic attributes in ZSL. We conduct extensive exeriments on benchmark datasets. Our method outerforms state-of-art methods trained for the secific contexts (ZSL, FSL, DA).
"
1177,2019,Population Based Augmentation: Efficient Learning of Augmentation Policy Schedules,Poster,"A key challenge in leveraging data augmentation for neural network training is choosing an effective augmentation olicy from a large search sace of candidate oerations. Proerly chosen augmentation olicies can lead to significant generalization imrovements; however, state-of-the-art aroaches such as AutoAugment are comutationally infeasible to run for the ordinary user. In this aer, we introduce a new data augmentation algorithm, Poulation Based Augmentation (PBA), which generates nonstationary augmentation olicy schedules instead of a fixed augmentation olicy. We show that PBA can match the erformance of AutoAugment on CIFAR-10, CIFAR-100, and SVHN, with three orders of magnitude less overall comute. On CIFAR-10 we achieve a mean test error of 1.46%, which is a slight imrovement uon the current state-of-the-art. The code for PBA is oen source and is available at htts:github.comarcelienba.
"
1178,2019,Anomaly Detection With Multiple-Hypotheses Predictions,Poster,"In one-class-learning tasks, only the normal case (foreground) can be modeled with data, whereas the variation of all ossible anomalies is too erratic to be described by samles. Thus, due to the lack of reresentative data, the wide-sread discriminative aroaches cannot cover such learning tasks, and rather generative models,which attemt to learn the inut density of the foreground, are used. However, generative models suffer from a large inut dimensionality (as in images) and are tyically inefficient learners.We roose to learn the data distribution of the foreground more efficiently with a multi-hyotheses autoencoder. Moreover, the model is criticized by a discriminator, which revents artificial data modes not suorted by data, and which enforces diversity across hyotheses. Our multile-hyotheses-based anomaly detection framework allows the reliable identification of out-of-distribution samles. For anomaly detection on CIFAR-10, it yields u to 3.9% oints imrovement over reviously reorted results. On a real anomaly detection task, the aroach reduces the error of the baseline models from 6.8% to 1.5%.
"
1179,2019,Kernel Mean Matching for Content Addressability of GANs,Poster,"We roose a novel rocedure which adds ""content-addressability"" to any given unconditional imlicit model e.g., a generative adversarial network (GAN). The rocedure allows users to control the generative rocess by secifying a set (arbitrary size) of desired examles based on which similar samles are generated from the model. The roosed aroach, based on kernel mean matching, is alicable to any generative models which transform latent vectors to samles, and does not require retraining of the model. Exeriments on various high-dimensional image generation roblems (CelebA-HQ, LSUN bedroom, bridge, tower) show that our aroach is able to generate images which are consistent with the inut set, while retaining the image quality of the original model. To our knowledge, this is the first work that attemts to construct, at test time, a content-addressable generative model from a trained marginal model. 
"
1180,2019,Neural Inverse Knitting: From Images to Manufacturing Instructions,Poster,"Motivated by the recent otential of mass customization brought by whole-garment knitting machines, we introduce the new roblem of automatic machine instruction generation using a single image of the desired hysical roduct, which we aly to machine knitting. We roose to tackle this roblem by directly learning to synthesize regular machine instructions from real images. We create a cured dataset of real samles with their instruction counterart and roose to use synthetic images to augment it in a novel way. We theoretically motivate our data mixing framework and show emirical results suggesting that making real images look more synthetic is beneficial in our roblem setu.
"
1181,2019,Making Convolutional Networks Shift-Invariant Again,Poster,"Modern convolutional networks are not shift-invariant, as small inut shifts or translations can cause drastic changes in the outut. Commonly used downsamling methods, such as max-ooling, strided-convolution, and average-ooling, ignore the samling theorem. The well-known signal rocessing fix is anti-aliasing by low-ass filtering before downsamling. However, simly inserting this module into dee networks leads to erformance degradation; as a result, it is seldomly used today. We show that when integrated correctly, it is comatible with existing architectural comonents, such as max-ooling. The technique is general and can be incororated across layer tyes and alications, such as image classification and conditional image generation. In addition to increased shift-invariance, we also observe, surrisingly, that anti-aliasing boosts accuracy in ImageNet classification, across several commonly-used architectures. This indicates that anti-aliasing serves as effective regularization. Our results demonstrate that this classical signal rocessing technique has been undeservingly overlooked in modern dee networks.
"
1182,2019,Generative Modeling of Infinite Occluded Objects for Compositional Scene Representation,Poster,"We resent a dee generative model which exlicitly models object occlusions for comositional scene reresentation. Latent reresentations of objects are disentangled into location, size, shae, and aearance, and the visual scene can be generated comositionally by integrating these reresentations and an infinite-dimensional binary vector indicating resences of objects in the scene. By training the model to learn satial deendences of ixels in the unsuervised setting, the number of objects, ixel-level segregation of objects, and resences of objects in overlaing regions can be estimated through inference of latent variables. Extensive exeriments conducted on a series of secially designed datasets demonstrate that the roosed method outerforms two state-of-the-art methods when object occlusions exist.
"
1183,2019,IMEXnet - A Forward Stable Deep Neural Network,Poster,"Dee convolutional neural networks have revolutionized many machine learning and comuter vision tasks, however, some remaining key challenges limit their wider use. These challenges include imroving the network's robustness to erturbations of the inut image and the limited ``field of view'' of convolution oerators. We introduce the IMEXnet that addresses these challenges by adating semi-imlicit methods for artial differential equations. Comared to similar exlicit networks, such as residual networks, our network is more stable, which has recently shown to reduce the sensitivity to small changes in the inut features and imrove generalization. The addition of an imlicit ste connects all ixels in each channel of the image and therefore addresses the field of view roblem while still being comarable to standard convolutions in terms of the number of arameters and comutational comlexity. We also resent a new dataset for semantic segmentation and demonstrate the effectiveness of our architecture using the NYU Deth dataset.
"
1184,2019,Do ImageNet Classifiers Generalize to ImageNet?,Poster,"We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation rocesses, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy dros of 3% - 15% on CIFAR-10 and 11% - 14% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy dros are not caused by adativity, but by the models' inability to generalize to slightly ""harder"" images than those found in the original test sets.
"
1185,2019,Exploring the Landscape of Spatial Robustness,Poster,"The study of adversarial robustness has so far largely focused on erturbations
bound in $\ell_$-norms. However, state-of-the-art models turn out to be also vulnerable to other, more natural classes of erturbations such as translations and rotations. In this work, we thoroughly investigate the vulnerability of
neural network--based classifiers to rotations and translations. While data
augmentation offers relatively small robustness, we use ideas from robust
otimization and test-time inut aggregation to significantly imrove robustness.
Finally we find that, in contrast to the $\ell_$-norm case, first-order
methods cannot reliably find worst-case erturbations. This highlights
satial robustness as a fundamentally different setting requiring additional
study.
"
1186,2019,Sever: A Robust Meta-Algorithm for Stochastic Optimization,Poster,"In high dimensions, most machine learning methods are brittle to even a small fraction of structured outliers. To address this, we introduce a new meta-algorithm that can take in a base learner such as least squares or stochastic gradient descent, and harden the learner to be resistant to outliers. Our method, Sever, ossesses strong theoretical guarantees yet is also highly scalable -- beyond running the base learner itself, it only requires comuting the to singular vector of a certain n×d matrix. We aly Sever on a drug design dataset and a sam classification dataset, and find that in both cases it has substantially greater robustness than several baselines. On the sam dataset, with 1% corrutions, we achieved 7.4% test error, comared to 13.4%−20.5% for the baselines, and 3% error on the uncorruted dataset. Similarly, on the drug design dataset, with 10% corrutions, we achieved 1.42 mean-squared error test error, comared to 1.51-2.33 for the baselines, and 1.23 error on the uncorruted dataset.
"
1187,2019,Analyzing Federated Learning through an Adversarial Lens,Poster,"Federated learning distributes model training among a multitude of agents, who, guided by rivacy concerns, erform training using their local data but share only model arameter udates, for iterative aggregation at the server to train an overall global model. In this work, we exlore how the federated learning setting gives rise to a new threat, namely model oisoning, which differs from traditional data oisoning. Model oisoning is carried out by an adversary controlling a small number of malicious agents (usually 1) with the aim of causing the global model to misclassify a set of chosen inuts with high conﬁdence. We exlore a number of strategies to carry out this attack on dee neural networks, starting with targeted model oisoning using a simle boosting of the malicious agent’s udate to overcome the effects of other agents. We also roose two critical notions of stealth to detect malicious udates. We byass these by including them in the adversarial objective to carry out stealthy model oisoning. We imrove its stealth with the use of an alternating minimization strategy which alternately otimizes for stealth and the adversarial objective. We also emirically demonstrate that Byzantine-resilient aggregation strategies are not robust to our attacks. Our results indicate that highly constrained adversaries can carry out model oisoning attacks while maintaining stealth, thus highlighting the vulnerability of the federated learning setting and the need to develo effective defense strategies.
"
1188,2019,Fairwashing: the risk of rationalization,Poster,"Black-box exlanation is the roblem of exlaining how a machine learning model -- whose internal logic is hidden to the auditor and generally comlex -- roduces its outcomes. 
Current aroaches for solving this roblem include model exlanation, outcome exlanation as well as model insection. 
While these techniques can be beneficial by roviding interretability, they can be used in a negative manner to erform fairwashing, which we define as romoting the false ercetion that a machine learning model resects some ethical values. 
In articular, we demonstrate that it is ossible to systematically rationalize decisions taken by an unfair black-box model using the model exlanation as well as the outcome exlanation aroaches with a given fairness metric. 
Our solution, LaundryML, is based on a regularized rule list enumeration algorithm whose objective is to search for fair rule lists aroximating an unfair black-box model. 
We emirically evaluate our rationalization technique on black-box models trained on real-world datasets and show that one can obtain rule lists with high fidelity to the black-box model while being considerably less unfair at the same time.
"
1189,2019,Understanding the Origins of Bias in Word Embeddings,Poster,"Poular word embedding algorithms exhibit stereotyical biases, such as gender bias. The widesread use of these algorithms in machine learning 
systems can amlify stereotyes in imortant contexts. Although some methods have been develoed to mitigate this roblem, how word embedding biases arise during training is oorly understood. In this work we develo a technique to address this question. Given a word embedding, our method reveals how erturbing the training corus would affect the resulting embedding bias. By tracing the origins of word embedding bias back to the original training documents, one can identify subsets of documents whose removal would most reduce bias. We demonstrate our methodology on Wikiedia and New York Times corora, and find it to be very accurate.
"
1190,2019,Bias Also Matters: Bias Attribution for Deep Neural Network Explanation,Poster,"The gradient of a dee neural network (DNN) w.r.t. the inut rovides information that can be used to exlain the outut rediction in terms of the inut features and has been widely studied to assist in interreting DNNs. In a linear model (i.e., g(x) = wx + b), the gradient corresonds to the weights w. Such a model can reasonably locally-linearly aroximate a smooth nonlinear DNN, and hence the weights of this local model are the gradient. The bias b, however, is usually overlooked in attribution methods. In this aer, we observe that since the bias in a DNN also has a non-negligible contribution to the correctness of redictions, it can also lay a significant role in understanding DNN behavior. We roose a backroagation-tye algorithm “bias back-roagation (BB)” that starts at the outut layer and iteratively attributes the bias of each layer to its inut nodes as well as combining the resulting bias term of the revious layer. Together with the backroagation of the gradient generating w, we can fully recover the locally linear model g(x) = wx + b. In exeriments, we show that BB can generate comlementary and highly interretable exlanations.
"
1191,2019,Interpreting Adversarially Trained Convolutional Neural Networks,Poster,"We attemt to interret how adversarially trained convolutional neural networks (AT-CNNs) recognize objects. We  design systematic aroaches to interret AT-CNNs in both qualitative and quantitative ways and comare them with normally trained models. Surrisingly, we find that adversarial training alleviates the texture bias of standard CNNs when trained on object recognition tasks, and hels CNNs learn a more shae-biased reresentation. 
We validate our hyothesis from two asects.  First, we comare the salience mas of AT-CNNs and standard CNNs on clean images and images under different transformations. The comarison could visually show that the rediction of the two tyes of CNNs is sensitive to dramatically different tyes of features. Second, to achieve quantitative verification, we construct additional test datasets that destroy either textures or shaes, such as style-transferred version of clean data,  saturated images and atch-shuffled ones, and then evaluate the classification accuracy of AT-CNNs and normal CNNs on these datasets. br 
Our findings shed some light on why AT-CNNs are more robust than those normally trained ones and contribute to a better understanding of adversarial training over CNNs from an interretation ersective. 
"
1192,2019,Counterfactual Visual Explanations,Poster,"In this work, we develo a technique to roduce counterfactual visual exlanations. Given a `query' image $I$ for which a vision system redicts class $c$, a counterfactual visual exlanation identifies how $I$ could change such that the system would outut a different secified class $c'$. To do this, we select a `distractor' image $I'$ that the system redicts as class $c'$ and identify satial regions in $I$ and $I'$ such that relacing the identified region in $I$ with the identified region in $I'$ would ush the system towards classifying $I$ as $c'$. We aly our aroach to multile image classification datasets generating qualitative results showcasing the interretability and discriminativeness of our counterfactual exlanations. To exlore the effectiveness of our exlanations in teaching humans, we resent machine teaching exeriments for the task of fine-grained bird classification. We find that users trained to distinguish bird secies fare better when given access to counterfactual exlanations in addition to training examles. "
1193,2019,Data Poisoning Attacks on Stochastic Bandits,Poster,"Stochastic multi-armed bandits form a class of online learning roblems that have imortant alications in online recommendation systems, adative medical treatment, and many others. Even though otential attacks against these learning algorithms may hijack their behavior, causing catastrohic loss in real-world alications, little is known about adversarial attacks on bandit algorithms. In this aer, we roose a framework of offline attacks on bandit algorithms and study convex otimization based attacks on several oular bandit algorithms. We show that the attacker can force the bandit algorithm to ull a target arm with high robability by a slight maniulation of the rewards in the data. Then we study a form of online attacks on bandit algorithms and roose an adative attack strategy against any bandit algorithm without the knowledge of the bandit algorithm. Our adative attack strategy can hijack the behavior of the bandit algorithm to suffer a linear regret with only a logarithmic cost to the attacker. Our results demonstrate a significant security threat to stochastic bandits. 
"
1194,2019,On the Convergence and Robustness of Adversarial Training,Poster,"Imroving the robustness of dee neural networks (DNNs) to adversarial examles is an imortant yet challenging roblem for secure dee learning. Across existing defense techniques, adversarial training with Projected Gradient Decent (PGD) is amongst the most effective. Adversarial training solves a min-max otimization roblem, with the \textit{inner maximization} generating adversarial examles by maximizing the classification loss, and the \textit{outer minimization} finding model arameters by minimizing the loss on adversarial examles generated from the inner maximization. A criterion that measures how well the inner maximization is solved is therefore crucial for adversarial training. In this aer, we roose such a criterion, namely First-Order Stationary Condition for constrained otimization (FOSC), to quantitatively evaluate the convergence quality of adversarial examles found in the inner maximization. With FOSC, we find that to ensure better robustness, it is essential to use adversarial examles with better convergence quality at the \textit{later stages} of training. Yet at the early stages, high convergence quality adversarial examles are not necessary and may even lead to oor robustness. Based on these observations, we roose a \textit{dynamic} training strategy to gradually increase the convergence quality of the generated adversarial examles, which significantly imroves the robustness of adversarial training. Our theoretical and emirical results show the effectiveness of the roosed method. 
"
1195,2019,Learning with Bad Training Data via Iterative Trimmed Loss Minimization,Poster,"In this aer, we study a simle and generic framework to tackle the roblem of learning model arameters when a fraction of the training samles are corruted. Our aroach is motivated by a simle observation: in a variety of such settings, the evolution of training accuracy (as a function of training eochs) is different for clean samles and bad samles. We roose to iteratively minimize the trimmed loss, by alternating between (a) selecting  samles with lowest current loss, and (b)  retraining a model on only these samles. Analytically, we characterize the statistical erformance and convergence rate of the algorithm for simle and natural linear and non-linear models. Exerimentally, we demonstrate its effectiveness in three settings: (a) dee image classifiers with errors only in labels, (b) generative adversarial networks with bad training images, and (c) dee image classifiers with adversarial (image, label) airs (i.e., backdoor attacks). For the well-studied setting of random label noise, our algorithm achieves  state-of-the-art erformance without having access to any a-riori guaranteed clean samles. 
"
1196,2019,On discriminative learning of prediction uncertainty,Poster,"In classification with a reject otion, the classifier is allowed in uncertain cases to abstain from rediction. The classical cost based model of an otimal classifier with a reject otion requires the cost of rejection to be defined exlicitly. An alternative bounded-imrovement model, avoiding the notion of the reject cost, seeks for a classifier with a guaranteed selective risk and maximal cover. We rove that both models share the same class of otimal strategies, and we rovide an exlicit relation between the reject cost and the target risk being the arameters of the two models. An otimal rejection strategy for both models is based on thresholding the conditional risk defined by osterior robabilities which are usually unavailable. We roose a discriminative algorithm learning an uncertainty function which reserves ordering of the inut sace induced by the conditional risk, and hence can be used to construct otimal rejection strategies.
"
1197,2019,Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels,Poster,"Noisy labels are ubiquitous in real-world datasets, which oses a challenge for robustly training dee neural networks (DNNs) as DNNs usually have the high caacity to memorize the noisy labels. In this aer, we find that the test accuracy can be quantitatively characterized in terms of the noise ratio in datasets. In articular, the test accuracy is a quadratic function of the noise ratio in the case of symmetric noise, which exlains the exerimental findings reviously ublished. Based on our analysis, we aly cross-validation to randomly slit noisy datasets, which identifies most samles that have correct labels. Then we adot the Co-teaching strategy which takes full advantage of the identified samles to train DNNs robustly against noisy labels. Comared with extensive state-of-the-art methods, our strategy consistently imroves the generalization erformance of DNNs under both synthetic and real-world training noise.
"
1198,2019,Does Data Augmentation Lead to Positive Margin?,Poster,"Data augmentation (DA) is commonly used during model training, as it significantly imroves test error and model robustness. DA artificially exands the training set by alying random noise, rotations, cros, or even adversarial erturbations to the inut data. Although DA is widely used, its caacity to rovably imrove robustness is not fully understood. In this work, we analyze the robustness that DA begets by quantifying the margin that DA enforces on emirical risk minimizers. We first focus on linear searators, and then a class of nonlinear models whose labeling is constant within small convex hulls of data oints. We resent lower bounds on the number of augmented data oints required for non-zero margin, and show that commonly used DA techniques may only introduce significant margin after adding exonentially many oints to the data set.
"
1199,2019,Robust Learning from Untrusted Sources,Poster,"Modern machine learning methods often require more data for training than a single exert can rovide. Therefore, it has become a standard rocedure to collect data from multile external sources, \eg via crowdsourcing. Unfortunately, the quality of these sources is not always guaranteed. As further comlications, the data might be stored in a distributed way, or might even have to remain rivate. In this work, we address the question of how to learn robustly in such scenarios. Studying the roblem through the lens of statistical learning theory, we derive a rocedure that allows for learning from all available sources, yet automatically suresses irrelevant or corruted data. We show by extensive exeriments that our method rovides significant imrovements over alternative aroaches from robust statistics and distributed otimization.
"
1200,2019,SELFIE: Refurbishing Unclean Samples for Robust Deep Learning,Poster,"Owing to the extremely high exressive ower of dee neural networks, their side effect is to totally memorize training data even when the labels are extremely noisy. To overcome overfitting on the noisy labels, we roose a novel robust training method called SELFIE. Our key idea is to selectively refurbish and exloit unclean samles that can be corrected with high recision, thereby gradually increasing the number of available training samles. Taking advantage of this design, SELFIE effectively revents the risk of noise accumulation from the false correction and fully exloits the training data. To validate the sueriority of SELFIE, we conducted extensive exerimentation using four real-world or synthetic data sets. The result showed that SELFIE remarkably imroved absolute test error comared with two state-of-the-art methods.
"
1201,2019,Zeno: Distributed Stochastic Gradient Descent with Suspicion-based Fault-tolerance,Poster,"We resent Zeno, a technique to make distributed machine learning, articularly Stochastic Gradient Descent (SGD), tolerant to an arbitrary number of faulty workers. Zeno generalizes revious results that assumed a majority of non-faulty nodes; we need assume only one non-faulty worker. Our key idea is to susect workers that are otentially defective. Since this is likely to lead to false ositives, we use a ranking-based reference mechanism. We rove the convergence of SGD for non-convex roblems under these scenarios. Exerimental results show that Zeno outerforms existing aroaches.
"
1202,2019,Concentration Inequalities for Conditional Value at Risk,Poster,"In this aer we derive new concentration inequalities for the conditional value at risk (CVaR) of a random variable, and comare them to the revious state of the art (Brown, 2007).  We show analytically that our lower bound is strictly tighter than Brown's, and emirically that this difference is significant.  While our uer bound may be looser than Brown's in some cases, we show emirically that in most cases our bound is significantly tighter.  After discussing when each uer bound is suerior, we conclude with emirical results which suggest that both of our bounds will often be significantly tighter than Brown's.
"
1203,2019,Data Poisoning Attacks in Multi-Party Learning,Poster,"In this work, we demonstrate universal multi-arty oisoning attacks that adat and aly to any multi-arty learning rocess with arbitrary interaction attern between the arties. More generally, we introduce and study $(k,)$-oisoning attacks in which an adversary controls $k\in[m]$ of the arties, and for each corruted arty $P_i$, the adversary submits some oisoned data $T'_i$ on behalf of $P_i$ that is still ""$(1-)$-close"" to the correct data $T_i$ (e.g., $1-$ fraction of $T'_i$ is still honestly generated).We rove that for any ""bad"" roerty $B$ of the final trained hyothesis $h$ (e.g., $h$ failing on a articular test examle or having ""large"" risk) that has an arbitrarily small constant robability of haening without the attack, there always is a $(k,)$-oisoning attack that increases the robability of $B$ from $\mu$ to by $\mu^{1- \cdot km} = \mu + \Omega( \cdot km)$. Our attack only uses clean labels, and it is online, as it only knows the the data shared so far."
1204,2019,Distributed Weighted Matching via Randomized Composable Coresets,Poster,"Maximum weight matching is one of the most fundamental combinatorial otimization roblems with a wide range of alications in data mining and bioinformatics. Develoing distributed weighted matching algorithms has been challenging due to the sequential nature of efficient algorithms for this roblem. In this aer, we develo a simle distributed algorithm for the roblem on general grahs with aroximation guarantee of 2 + es that (nearly) matches that of the sequential greedy algorithm. A key advantage of this algorithm is that it can be easily imlemented in only two rounds of comutation in modern arallel comutation frameworks such as MaReduce. We also demonstrate the efficiency of our algorithm in ractice on various grahs (some with half a trillion edges) by achieving objective values always close to what is achievable in the centralized setting.
"
1205,2019,Multivariate Submodular Optimization,Poster,"Submodular functions have found a wealth of new alications in data science and machine learning models in recent years. This has been couled with many algorithmic advances in the area of submodular otimization: (SO) $\min\max~f(S): S \in \mathcal{F}$, where $\mathcal{F}$ is a given family of feasible sets over a ground set $V$ and $f:2^V \rightarrow \mathbb{R}$ is submodular. In this work we focus on a more general class of \emh{multivariate submodular otimization} (MVSO) roblems:  $\min\max~f (S_1,S_2,\ldots,S_k):  S_1 \ulus S_2 \ulus \cdots \ulus S_k \in \mathcal{F}$. Here we use $\ulus$ to denote union of disjoint sets and hence this model is attractive where resources are being allocated across  $k$ agents, who share a ``joint'' multivariate nonnegative objective $f(S_1,S_2,\ldots,S_k)$ that catures some tye of submodularity (i.e. diminishing returns) roerty. We rovide some exlicit examles and otential alications for this new framework. For maximization, we show that ractical algorithms such as accelerated greedy variants and distributed algorithms achieve good aroximation guarantees for very general families (such as matroids and $$-systems). For arbitrary families, we show that monotone (res. nonmonotone) MVSO admits an $\alha (1-1e)$ (res. $\alha \cdot 0.385$) aroximation whenever monotone (res. nonmonotone) SO admits an $\alha$-aroximation over the multilinear formulation. This substantially exands the family of tractable models. On the minimization side we give essentially otimal aroximations in terms of the curvature of $f$."
1206,2019,Beyond Adaptive Submodularity: Approximation Guarantees of Greedy Policy with Adaptive Submodularity Ratio,Poster,"We roose a new concet named adative submodularity ratio to study the greedy olicy for sequential decision making. While the greedy olicy is known to erform well for a wide variety of adative stochastic otimization roblems in ractice, its theoretical roerties have been analyzed only for a limited class of roblems. We narrow the ga between theory and ractice by using adative submodularity ratio, which enables us to rove aroximation guarantees of the greedy olicy for a substantially wider class of roblems. Examles of newly analyzed roblems include imortant alications such as adative influence maximization and adative feature selection. Our adative submodularity ratio also rovides bounds of adativity gas. Exeriments confirm that the greedy olicy erforms well with the alications being considered comared to standard heuristics.
"
1207,2019,Approximating Orthogonal Matrices with Effective Givens Factorization,Poster,"We analyze effective aroximation of unitary matrices. In our formulation, a unitary matrix is reresented as a roduct of rotations in two-dimensional subsaces, so-called Givens rotations. Instead of the quadratic dimension deendence when alying a dense matrix, alying such an aroximation scales with the number factors, each of which can be imlemented efficiently. Consequently, in settings where an aroximation is once comuted and then alied many times, such a reresentation becomes advantageous. Although effective Givens factorization is not ossible for generic unitary oerators, we show that minimizing a sarsity-inducing objective with a coordinate descent algorithm on the unitary grou yields good factorizations for structured matrices. Canonical alications of such a setu are orthogonal basis transforms. We demonstrate numerical results of aroximating the grah Fourier transform, which is the matrix obtained when diagonalizing a grah Lalacian.
"
1208,2019,New results on information theoretic clustering,Poster,"We study the roblem of otimizing the clustering of a set of vectors when the quality of the clustering is measured by the Entroy or the Gini imurity measure.
Our results contribute to  the state of the art both in terms of best known aroximation guarantees and inaroximability bounds: (i) we give the first olynomial time algorithm for Entroy imurity based clustering with aroximation guarantee indeendent of the number of vectors and  (ii) we show that 
the roblem of clustering based on entroy imurity does not admit a PTAS. 
This also imlies  an inaroximability result in information theoretic clustering 
for robability distributions closing a  roblem left oen in [Chaudhury and McGregor, COLT08] and [Ackermann et al., ECCC11]. We also reort 
exeriments with  a new clustering method that was designed on  to of the 
theoretical tools leading to the above results. These exeriments suggest a ractical alicability for our method, in articular, when the number of clusters 
is large.
"
1209,2019,Improved Parallel Algorithms for Density-Based Network Clustering,Poster,"Clustering large-scale networks is  a central toic in unsuervised learning with many alications in machine learning and data mining. A classic aroach to cluster a network is to identify regions of high edge density, which in the literature is catured by two fundamental roblems: the densest subgrah and the $k$-core decomosition roblems. We design massively arallel comutation (MPC) algorithms for these roblems that are considerably faster than rior work. In the case of $k$-core decomosition, our work imroves exonentially on the algorithm rovided by Esfandiari et al.~(ICML'18). Comared to the rior work on densest subgrah resented by Bahmani et al.~(VLDB'12, '14), our result requires quadratically fewer MPC rounds. We comlement our analysis with an exerimental scalability analysis of our techniques.
"
1210,2019,Submodular Observation Selection and Information Gathering for Quadratic Models,Poster,"We study the roblem of selecting most informative subset of a large observation set to enable accurate estimation of unknown arameters. This roblem arises in a variety of settings in machine learning and signal rocessing including feature selection, hase retrieval, and target localization. Since for quadratic measurement models the moment matrix of the otimal estimator is generally unknown, majority of rior work resorts to aroximation techniques such as linearization of the observation model to otimize the alhabetical otimality criteria of an aroximate moment matrix. Conversely, by exloiting a connection to the classical Van Trees' inequality, we derive new alhabetical otimality criteria without distorting the relational structure of the observation model. We further show that under certain conditions on arameters of the roblem these otimality criteria are monotone and (weak) submodular set functions. These results enable us to develo an efficient greedy observation selection algorithm uniquely tailored for quadratic models, and rovide theoretical bounds on its achievable utility.
"
1211,2019,Submodular Cost Submodular Cover with an Approximate Oracle,Poster,"In this work, we study the Submodular Cost Submodular Cover roblem, which is to minimize the submodular cost required to ensure that the submodular benefit function exceeds a given threshold. Existing aroximation ratios for the greedy algorithm assume a value oracle to the benefit function. However, access to a value oracle is not a realistic assumtion for many alications of this roblem, where the benefit function is difficult to comute. We resent two incomarable aroximation ratios for this roblem with an aroximate value oracle and demonstrate that the ratios take on emirically relevant values through a case study with the Influence Threshold roblem in online social networks.
"
1212,2019,"Submodular Streaming in All Its Glory: Tight Approximation, Minimum Memory and Low Adaptive Complexity",Poster,"Streaming algorithms are generally judged by the quality of their solution, memory footrint, and comutational comlexity. In this aer, we study the roblem of maximizing a  monotone submodular function in the streaming setting with a cardinality constraint $k$. We first roose SIEVE-STREAMING++, which requires just one ass over the data, kees only $O(k)$ elements and achieves the tight $\frac{1}{2}$-aroximation guarantee. The best reviously known streaming algorithms either achieve a subotimal $\frac{1}{4}$-aroximation with $\Theta(k)$ memory or the otimal $\frac{1}{2}$-aroximation with $O(k\log k)$ memory. 

Next, we show that by buffering a small fraction of the stream and alying a careful filtering rocedure, one can heavily reduce the number of adative comutational rounds, thus substantially lowering the comutational comlexity of SIEVE-STREAMING++. We then generalize our results to the more challenging multi-source streaming setting. We show how one can achieve the tight $\frac{1}{2}$-aroximation guarantee with $O(k)$ shared memory, while minimizing not only the rounds of comutations but also the total number of communicated bits. Finally, we demonstrate the efficiency of our algorithms on real-world data summarization tasks for multi-source streams of tweets and of YouTube videos."
1213,2019,Hiring Under Uncertainty,Poster,"In this aer we introduce the hiring under uncertainty roblem to model the questions faced by hiring committees in large enterrises and universities alike. Given a set of $n$ eligible candidates, the decision maker needs to choose the sequence of candidates to make offers so as to hire the $k$ best candidates. However, candidates may choose to reject an offer (for instance, due to a cometing offer) and the decision maker has a time limit by which all ositions must be filled. Given an estimate of the robabilities of accetance for each candidate, the hiring under uncertainty roblem is to design a strategy of making offers so that the total exected value of all candidates hired by the time limit is maximized. We rovide a 2-aroximation algorithm for the setting where offers must be made in sequence, an 8-aroximation when offers may be made in arallel, and a 10-aroximation for the more general stochastic knasack setting with finite robes."
1214,2019,Position-aware Graph Neural Networks,Poster,"Learning node embeddings that cature a node's osition within the broader grah structure is crucial for many rediction tasks on grahs. However, existing Grah Neural Network (GNN) architectures have limited ower in caturing the ositionlocation of a given node with resect to all other nodes of the grah. Here we roose Position-aware Grah Neural Networks (P-GNNs), a new class of GNNs for comuting osition-aware node embeddings. P-GNN first samles sets of anchor nodes, comutes the distance of a given target node to each anchor-set, and then learns a non-linear distance-weighted aggregation scheme over the anchor-sets. This way P-GNNs can cature ositionslocations of nodes with resect to the anchor nodes. P-GNNs have several advantages: they are inductive, scalable, and can incororate node feature information. We aly P-GNNs to multile rediction tasks including link rediction and community detection. We show that P-GNNs consistently outerform state of the art GNNs, with u to 66% imrovement in terms of the ROC AUC score.
"
1215,2019,Detecting Overlapping and Correlated Communities without Pure Nodes: Identifiability and Algorithm,Poster,"Many machine learning roblems come in the form of networks with relational data between entities, and one of the key unsuervised learning tasks is to detect communities in such a network. We adot the mixed-membershi stochastic blockmodel as the underlying robabilistic model, and give conditions under which the membershis of a subset of nodes can be uniquely identified. Our method starts by constructing a second-order grah moment, which can be shown to converge to a secific roduct of the true arameters as the size of the network increases. To correctly recover the true membershi arameters, we formulate an otimization roblem using insights from convex geometry. We show that if the true membershis satisfy a so-called sufficiently scattered condition, then solving the roosed roblem correctly identifies the ground truth. We also roose an efficient algorithm for detecting communities, which is significantly faster than rior work and with better convergence roerties. Exeriments on synthetic and real data justify the validity of the roosed learning framework for network data.
"
1216,2019,Learning Generative Models across Incomparable Spaces,Poster,"Generative Adversarial Networks have shown remarkable success in learning a distribution that faithfully recovers a reference distribution in its entirety. However, in some cases, we may want to only learn some asects (e.g., cluster or manifold structure), while modifying others (e.g., style, orientation or dimension). In this work, we roose an aroach to learn generative models across such incomarable saces, and demonstrate how to steer the learned distribution towards target roerties. A key comonent of our model is the Gromov-Wasserstein distance, a notion of discreancy that comares distributions relationally rather than absolutely. While this framework subsumes current generative models in identically reroducing distributions, its inherent flexibility allows alication to tasks in manifold learning, relational learning and cross-domain learning.
"
1217,2019,Relational Pooling for Graph Representations,Poster,"This work generalizes grah neural networks (GNNs) beyond those based on the Weisfeiler-Lehman (WL) algorithm, grah Lalacians, and diffusions.  Our aroach, denoted Relational Pooling (RP), draws from the theory of finite artial exchangeability to rovide a framework with maximal reresentation ower for grahs. RP can work with existing grah reresentation models and, somewhat counterintuitively, can make them even more owerful than the original WL isomorhism test. Additionally, RP allows architectures like Recurrent Neural Networks and Convolutional Neural Networks to be used in a theoretically sound aroach for grah classification. We demonstrate imroved erformance of RP-based grah reresentations over state-of-the-art methods on a number of tasks.
"
1218,2019,Disentangled Graph Convolutional Networks,Poster,"recode    The formation of a real-world grah tyically arises from the highly comlex interaction of many latent factors.
    The existing dee learning methods for grah-structured data neglect the entanglement of the latent factors, rendering the learned reresentations non-robust and hardly exlainable.
    However, learning reresentations that disentangle the latent factors oses great challenges and remains largely unexlored in the literature of grah neural networks.
    In this aer, we introduce the disentangled grah convolutional network (DisenGCN) to learn disentangled node reresentations.
    In articular, we roose a novel neighborhood routing mechanism, which is caable of dynamically identifying the latent factor that may have caused the edge between a node and one of its neighbors,  and accordingly assigning the neighbor to a channel that extracts and convolutes features secific to that factor.
    We theoretically rove the convergence roerties of the routing mechanism.
    Emirical results show that our roosed model can achieve significant erformance gains, esecially when the data demonstrate the existence of many entangled factors.
codere
"
1219,2019,Open Vocabulary Learning on Source Code with a Graph-Structured Cache,Poster,"Machine learning models that take comuter rogram source code as inut tyically use Natural Language Processing (NLP) techniques. However, a major challenge is that code is written using an oen, raidly changing vocabulary due to, e.g., the coinage of new variable and method names.  Reasoning over such a vocabulary is not something for which most NLP methods are designed.  We introduce a Grah-Structured Cache to address this roblem; this cache contains a node for each new word the model encounters with edges connecting each word to its occurrences in the code.  We find that combining this grah-structured cache strategy with recent Grah-Neural-Network-based models for suervised learning on code imroves the models' erformance on a code comletion task and a variable naming task --- with over 100% relative imrovement on the latter --- at the cost of a moderate increase in comutation time.
"
1220,2019,Learning Discrete Structures for Graph Neural Networks,Poster,"Grah neural networks (GNNs) are a oular class of machine learning models that have been successfully alied to a range of roblems. Their major advantage lies in their ability to exlicitly incororate a sarse and discrete deendency structure between data oints. Unfortunately, GNNs can only be used when such a grah-structure is available. In ractice, however, real-world grahs are often noisy and incomlete or might not be available at all. 
With this work, we roose to jointly learn the grah structure and the arameters of grah convolutional networks (GCNs) by aroximately solving a bilevel rogram that learns a discrete robability distribution on the edges of the grah. br 
This allows one to aly GCNs not only in scenarios where the given grah is incomlete or corruted but also in those where a grah is not available.
We conduct a series of exeriments that analyze the behavior of the roosed method and demonstrate that it outerforms related methods by a significant margin. 
"
1221,2019,Compositional Fairness Constraints for Graph Embeddings,Poster,"Learning high-quality node embeddings is a key building block for machine learning models that oerate on grah data, such as social networks and recommender systems. However, existing grah embedding techniques are unable to coe with fairness constraints, e.g., ensuring that the learned reresentations do not correlate with certain attributes, such as age or gender. Here, we introduce an adversarial framework to enforce fairness constraints on grah embeddings. Our aroach is {\em comositional}---meaning that it can flexibly accommodate different combinations of fairness constraints during inference. 
 For instance, in the context of social recommendations, our framework would allow one user to request that their recommendations are invariant to both their age and gender, while also allowing another user to request invariance to just their age. 
 Exeriments on standard knowledge grah and recommender system benchmarks highlight the utility of our roosed framework.
"
1222,2019,A Recurrent Neural Cascade-based Model for Continuous-Time Diffusion,Poster,"Many works have been roosed in the literature to cature the dynamics of diffusion in networks. While some of them define grahical Markovian models to extract temoral relationshis between node infections in networks, others consider diffusion eisodes as sequences of infections via recurrent neural models. In this aer we roose a model at the crossroads of these two extremes, which embeds the history of diffusion in infected nodes as hidden continuous states. Deending on the trajectory followed by the content before reaching a given node, the distribution of influence robabilities may vary. However, content trajectories  are usually hidden in the data, which induces challenging learning roblems. We roose a toological recurrent neural model which exhibits good exerimental erformances for diffusion modeling and rediction.
"
1223,2019,Stochastic Blockmodels meet Graph Neural Networks,Poster,"Stochastic blockmodels (SBM) and their variants, $e.g.$, mixed-membershi and overlaing stochastic blockmodels, are latent variable based generative models for grahs. They have roven to be successful for various tasks, such as discovering the community structure and link rediction on grah-structured data. Recently, grah neural networks, $e.g.$, grah convolutional networks, have also emerged as a romising aroach to learn owerful reresentations (embeddings) for the nodes in the grah, by exloiting grah roerties such as locality and invariance. In this work, we unify these two directions by develoing a \emh{sarse} variational autoencoder for grahs, that retains the interretability of SBMs, while also enjoying the excellent redictive erformance of grah neural nets. Moreover, our framework is accomanied by a fast recognition model that enables fast inference of the node embeddings (which are of indeendent interest for inference in SBM and its variants). Although we develo this framework for a articular tye of SBM, namely the \emh{overlaing} stochastic blockmodel, the roosed framework can be adated readily for other tyes of SBMs. Exerimental results on several benchmarks demonstrate encouraging results on link rediction while learning an interretable latent structure that can be used for community discovery."
1224,2019,Distributed Learning with Sublinear Communication,Poster,"In distributed statistical learning, $N$ samles are slit across $m$ machines and a learner wishes to use minimal communication to learn as well as if the examles were on a single machine. This model has received substantial interest in machine learning due to its scalability and otential for arallel seedu. However, in high-dimensional settings, where the number examles is smaller than the number of features (`""dimension""), the seedu afforded by distributed learning may be overshadowed by the cost of communicating a single examle. This aer investigates the following question: When is it ossible to learn a $d$-dimensional model in the distributed setting with total communication sublinear in $d$? Starting with a negative result, we observe that for learning $\ell_1$-bounded or sarse linear models, no algorithm can obtain otimal error until communication is linear in dimension. Our main result is that by slightly relaxing the standard boundedness assumtions for linear models, we can obtain distributed algorithms that enjoy otimal error with communication \emh{logarithmic} in dimension. This result is based on a family of algorithms that combine mirror descent with randomized sarsificationquantization of iterates, and extends to the general stochastic convex otimization model."
1225,2019,On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization,Poster,"Recent develoments on large-scale distributed machine learning alications, e.g., dee neural networks, benefit enormously from the advances in distributed non-convex otimization techniques, e.g., distributed Stochastic Gradient Descent (SGD).  A series of recent works study the linear seedu roerty of distributed SGD variants with reduced communication. The linear seedu roerty enables us to scale out the comuting caability by adding more comuting nodes into our system. The reduced communication comlexity is desirable since communication overhead is often the erformance bottleneck in distributed systems. Recently, momentum methods are more and more widely adoted by ractitioners to train machine learning models since they can often converge faster and generalize better.  However,  it remains unclear whether any distributed momentum SGD ossesses the same linear seedu roerty as distributed SGD and has reduced communication comlexity.  This aer fills the ga by considering a distributed communication efficient momentum SGD method and roving its linear seedu roerty. 
"
1226,2019,Stochastic Gradient Push for Distributed Deep Learning,Poster,"Distributed data-arallel algorithms aim to accelerate the training of dee neural networks by arallelizing the comutation of large mini-batch gradient udates across multile nodes. Aroaches that synchronize nodes using exact distributed averaging (e.g., via AllReduce) are sensitive to stragglers and communication delays. The PushSum gossi algorithm is robust to these issues, but only erforms aroximate distributed averaging. This aer studies Stochastic Gradient Push (SGP), which combines PushSum with stochastic gradient udates. We rove that SGP converges to a stationary oint of smooth, non-convex objectives at the same sub-linear rate as SGD, and that all nodes achieve consensus. We emirically validate the erformance of SGP on image classification (ResNet-50, ImageNet) and machine translation (Transformer, WMT'16 En-De) workloads.
"
1227,2019,Collective Model Fusion for Multiple Black-Box Experts,Poster,"Model fusion is a fundamental roblem in collec-tive machine learning (ML) where indeendentexerts with heterogeneous learning architecturesare required to combine exertise to imrove re-dictive  erformance.   This  is  articularly  chal-lenging in information-sensitive domains whereexerts do not have access to each other’s internalarchitecture and local data.  This aer resentsthe first collective model fusion framework formultile exerts with heterogeneous black-box ar-chitectures. The roosed method will enable thisby addressing the key issues of how black-boxexerts interact to understand the redictive be-haviors of one another; how these understandingscan be reresented and shared efficiently amongthemselves; and how the shared understandingscan be combined to generate high-quality consen-sus rediction. The erformance of the resultingframework is analyzed theoretically and demon-strated emirically on several datasets.
"
1228,2019,Trading Redundancy for Communication: Speeding up Distributed SGD for Non-convex Optimization,Poster,"Communication overhead is one of the key challenges that hinders the scalability of distributed otimization algorithms to train large neural networks. In recent years, there has been a great deal of research to alleviate communication cost by comressing the gradient vector or using local udates and eriodic model averaging. In this aer, we advocate the use of redundancy towards communication-efficient distributed stochastic algorithms for non-convex otimization. In articular, we, both theoretically and ractically, show that by roerly infusing redundancy to the training data with model averaging, it is ossible to significantly reduce the number of communication rounds. To be more recise, we show that redundancy reduces residual error in local averaging, thereby reaching the same level of accuracy with fewer rounds of communication as comared with revious algorithms. Emirical studies on CIFAR10, CIFAR100 and ImageNet datasets in a distributed environment comlement our theoretical results; they show that our algorithms have additional beneficial asects including tolerance to failures, as well as greater gradient diversity.
"
1229,2019,"Trimming the $\ell_1$ Regularizer: Statistical Analysis, Optimization, and Applications to Deep Learning",Poster,"We study high-dimensional estimators with the trimmed $\ell_1$ enalty, which leaves the h largest arameter entries enalty-free. While otimization techniques for this nonconvex enalty have been studied, the statistical roerties have not yet been analyzed. We resent the first statistical analyses for M-estimation, and characterize suort recovery, $\ell_\infty$ and $\ell_2$ error of the trimmed $\ell_1$ estimates as a function of the trimming arameter h. Our results show different regimes based on how h comares to the true suort size. Our second contribution is a new algorithm for the trimmed regularization roblem, which has the same theoretical convergence rate as difference of convex (DC) algorithms, but in ractice is faster and finds lower objective values. Emirical evaluation of $\ell_1$ trimming for sarse linear regression and grahical model estimation indicate that trimmed $\ell_1$ can outerform vanilla $\ell_1$ and non-convex alternatives. Our last contribution is to show that the trimmed enalty is beneficial beyond M-estimation, and yields romising results for two dee learning tasks: inut structures recovery and network sarsification."
1230,2019,Compressed Factorization: Fast and Accurate Low-Rank Factorization of Compressively-Sensed Data,Poster,"What learning algorithms can be run directly on comressively-sensed data? In this work, we consider the question of accurately and efficiently comuting low-rank matrix or tensor factorizations given data comressed via random rojections. We examine the aroach of first erforming factorization in the comressed domain, and then reconstructing the original high-dimensional factors from the recovered (comressed) factors. In both the matrix and tensor settings, we establish conditions under which this natural aroach will rovably recover the original factors. While it is well-known that random rojections reserve a number of geometric roerties of a dataset, our work can be viewed as showing that they can also reserve certain solutions of non-convex, NP-Hard roblems like non-negative matrix factorization. We suort these theoretical results with exeriments on synthetic data and demonstrate the ractical alicability of comressed factorization on real-world gene exression and EEG time series datasets.
"
1231,2019,Noisy Dual Principal Component Pursuit,Poster,"Dual Princial Comonent Pursuit (DPCP) is a recently roosed non-convex otimization based method for learning subsaces of high relative dimension from noiseless datasets contaminated by as many outliers as the square of the number of inliers. Exerimentally, DPCP has roved to be robust to noise and outerform the oular RANSAC on 3D vision tasks such as road lane detection and relative oses estimation from three views. This aer extends the global otimality and convergence theory of DPCP to the case of data corruted by noise, and further demonstrates its robustness using synthetic and real data. 
"
1232,2019,Learning a Compressed Sensing Measurement Matrix via Gradient Unrolling,Poster,"Linear encoding of sarse vectors is widely oular, but is commonly data-indeendent -- missing any ossible extra (but a riori unknown) structure beyond sarsity. In this aer we resent a new method to learn linear encoders that adat to data, while still erforming well with the widely used $\ell_1$ decoder. The convex $\ell_1$ decoder revents gradient roagation as needed in standard gradient-based training. Our method is based on the insight that unrolling the convex decoder into $T$ rojected subgradient stes can address this issue. Our method can be seen as a data-driven way to learn a comressed sensing measurement matrix. We comare the emirical erformance of 10 algorithms over 6 sarse datasets (3 synthetic and 3 real). Our exeriments show that there is indeed additional structure beyond sarsity in the real datasets; our method is able to discover it and exloit it to create excellent reconstructions with fewer measurements (by a factor of 1.1-3x) comared to the revious state-of-the-art methods. We illustrate an alication of our method in learning label embeddings for extreme multi-label classification, and emirically show that our method is able to match or outerform the recision scores of SLEEC, which is one of the state-of-the-art embedding-based aroaches."
1233,2019,Screening rules for Lasso with  non-convex Sparse Regularizers,Poster,"Leveraging on the convexity of the Lasso roblem, screening rules
hel in accelerating solvers by discarding irrelevant variables, during the otimization rocess. However, because they rovide better theoretical guarantees in identifying relevant   variables, several non-convex regularizers for the Lasso have been roosed in the literature.  This work is the first that introduces
a screening rule strategy into a non-convex Lasso solver. The aroach we roose is based on a iterative majorization-minimization (MM) strategy that includes a screening rule in the  inner solver and a condition for roagating screened variables between iterations of MM. In addition to imrove efficiency of solvers, we also rovide guarantees that the inner solver is able to identify the zeros comonents of its critical oint in finite time.  Our exerimental analysis illustrates the significant comutational gain  brought by the new screening rule comared to classical coordinate-descent  or roximal gradient descent methods.
"
1234,2019,Monge blunts Bayes: Hardness Results for Adversarial Training,Poster,"The last few years have seen a staggering number of 
emirical studies of the
robustness of neural networks in a model of adversarial
erturbations of their inuts. Most
rely on an adversary which carries out local
modifications within rescribed balls. None however has so far
questioned the broader icture: how to frame a \textit{resource-bounded} adversary so
that it can be \textit{severely detrimental} to learning,
a non-trivial roblem which entails at a minimum the choice of loss and classifiers.

We suggest a formal answer for losses that satisfy the minimal statistical
requirement of being \textit{roer}. We in down a simle sufficient roerty for any given class of adversaries to be detrimental to learning,
involving a central measure of ``harmfulness''
which generalizes
the well-known class of integral robability metrics.
A key feature of our result is that it holds for \textit{all} roer losses,
and for a oular subset of these, the otimisation of this central measure aears to be
\textit{indeendent of the loss}. When classifiers
are Lischitz -- a now oular aroach in adversarial training --, this
otimisation resorts to \textit{otimal transort} to make a
low-budget comression of class marginals. Toy exeriments reveal a
finding recently searately observed: training against a sufficiently
budgeted adversary of this kind \textit{imroves} generalization.
"
1235,2019,Better generalization with less data using robust gradient descent,Poster,"For learning tasks where the data (or losses) may be heavy-tailed, algorithms based on emirical risk minimization may require a substantial number of observations in order to erform well off-samle. In ursuit of stronger erformance under weaker assumtions, we roose a technique which uses a chea and robust iterative estimate of the risk gradient, which can be easily fed into any steeest descent rocedure. Finite-samle risk bounds are rovided under weak moment assumtions on the loss gradient. The algorithm is simle to imlement, and emirical tests using simulations and real-world data illustrate that more efficient and reliable learning is ossible without rior knowledge of the loss tails.
"
1236,2019,Near optimal finite time identification of arbitrary linear dynamical systems,Poster,"We derive finite time error bounds for estimating general linear time-invariant (LTI) systems from a single observed trajectory using the method of least squares. We rovide the first analysis of the general case when eigenvalues of the LTI system are arbitrarily distributed in three regimes:  stable, marginally stable, and exlosive. Our analysis yields shar uer bounds for each of these cases searately. We observe that although the underlying rocess behaves quite differently in each of these three regimes, the systematic analysis of a self--normalized martingale difference term hels bound identification error u to logarithmic factors of the lower bound. On the other hand, we demonstrate that the least squares solution may be statistically inconsistent under certain conditions even when the signal-to-noise ratio is high. 
"
1237,2019,Lossless or Quantized Boosting with Integer Arithmetic,Poster,"In suervised learning, efficiency often starts with the choice of a good loss: suort vector machines oularised Hinge loss, Adaboost oularised
the exonential loss, etc. Recent trends in machine learning have
highlighted the necessity for training routines to meet
tight requirements on communication, bandwidth, energy, oerations,
encoding, among others. Fitting the often decades-old state of the art
training routines into these new constraints does not go without ain and uncertainty or
reduction in the original guarantees. 
Our
aer starts with the design of a new strictly roer canonical, twice differentiable loss called the
Q-loss. Imortantly, its mirror udate over
(arbitrary) rational inuts uses only integer arithmetics --
more recisely, the sole use of $+, -, , \times, |.|$. We build a
learning algorithm which is able, under mild assumtions, to achieve a
lossless boosting-comliant training. We
give conditions for a quantization of its main memory footrint,
weights, to be done while keeing the whole algorithm boosting-comliant. Exeriments
dislay that the algorithm can achieve a fast convergence
during the early boosting rounds comared to AdaBoost, even with a weight storage
that can be 30+ times smaller. Lastly, we show that the Bayes risk of the
Q-loss can be used as node slitting criterion for decision trees and
guarantees otimal boosting convergence."
1238,2019,Orthogonal Random Forest for Causal Inference,Poster,"We roose the orthogonal random forest, an algorithm that combines Neyman-orthogonality to reduce sensitivity with resect to estimation error of nuisance arameters with generalized random forests (Athey et al., 2017)---a flexible non-arametric method for statistical estimation of conditional moment models using random forests. We rovide a consistency rate and establish asymtotic normality for our estimator. We show that under mild assumtions on the consistency rate of the nuisance estimator, we can achieve the same error rate as an oracle with a riori knowledge of these nuisance arameters. We show that when the nuisance functions have a locally sarse arametrization, then a local ell_1-enalized regression achieves the required rate. We aly our method to estimate heterogeneous treatment effects from observational data with discrete treatments or continuous treatments, and we show that, unlike rior work, our method rovably allows to control for a high-dimensional set of variables under standard sarsity conditions. We also rovide a comrehensive emirical evaluation of our algorithm on both synthetic and real data.
"
1239,2019,MONK --  Outlier-Robust Mean Embedding Estimation by Median-of-Means,Poster,"Mean embeddings rovide an extremely flexible and owerful tool in machine learning and statistics to reresent robability distributions and define a semi-metric (MMD, maximum mean discreancy; also called N-distance or energy distance), with numerous successful alications. The reresentation is constructed as the exectation of the feature ma defined by a kernel. As a mean, its classical emirical estimator, however, can be arbitrary severely affected even by a single outlier in case of unbounded features. To the best of our knowledge, unfortunately even the consistency of the existing few techniques trying to alleviate this serious sensitivity bottleneck is unknown. In this aer, we show how the recently emerged rincile of median-of-means can be used to design estimators for kernel mean embedding and MMD with excessive resistance roerties to outliers, and otimal sub-Gaussian deviation bounds under mild assumtions.
"
1240,2019,The advantages of multiple classes for reducing overfitting from test set reuse,Poster,"Excessive reuse of holdout data can lead to overfitting. However, there is little concrete evidence of significant overfitting due to holdout reuse in oular multiclass benchmarks today.

Known results show that, in the worst-case, revealing the accuracy of $k$ adatively chosen classifiers on a data set of size $n$ allows to create a classifier with bias of $\Theta(\sqrt{kn})$ for any binary rediction roblem. We show a new uer bound of $\tilde O(\max\{\sqrt{k\log(n)(mn)}, kn\})$ on the worst-case bias that any attack can achieve in a rediction roblem with $m$ classes. Moreover, we resent an efficient attack that achieve a bias of $\Omega(\sqrt{k(m^2 n)})$ and imroves on revious work for the binary setting ($m=2$). We also resent an inefficient attack that achieves a bias of $\tilde\Omega(kn)$. Comlementing our theoretical work, we give new ractical attacks to stress-test multiclass benchmarks by aiming to create as large a bias as ossible with a given number of queries. Our exeriments show that the additional uncertainty of rediction with a large number of classes indeed mitigates the effect of our best attacks."
1241,2019,On the statistical rate of nonlinear recovery in generative models with heavy-tailed data,Poster,"We consider estimating a high-dimensional vector from non-linear measurements where the unknown vector is reresented by a generative model $G:\mathbb{R}^k\rightarrow\mathbb{R}^d$ with $k\ll d$. Such a model oses structural riors on the unknown vector without having a dedicated basis, and in articular allows new and efficient aroaches solving recovery roblems with number of measurements far less than the ambient dimension of the vector. While rogresses have been made recently regarding theoretical understandings on the linear Gaussian measurements, much less is known when the model is ossibly missecified and the measurements are non-Gaussian. 
In this aer, we make a ste towards such a direction by considering the scenario where the measurements are non-Gaussian, subject to ossibly unknown nonlinear transformations and the resonses are heavy-tailed. We then roose new estimators via score functions based on the first and second order Stein's identity, and rove the samle size bound of 
$m=\mathcal{O}(k\varesilon^{-2}\log(L\varesilon))$ achieving an $\varesilon$ error in the form of exonential concentration inequalities. Furthermore, for the secial case of multi-layer ReLU generative model, we imrove the samle bound by a logarithm factor to $m=\mathcal{O}(k\varesilon^{-2}\log(d))$, matching the state-of-art statistical rate in comressed sensing for estimating $k$-sarse vectors. 
On the technical side, we develo new chaining methods bounding heavy-tailed rocesses, which could be of indeendent interest.
"
1242,2019,"Phase transition in PCA with missing data: Reduced signal-to-noise ratio, not sample size!",Poster,"How does missing data affect our ability to learn signal structures? It has been shown that learning signal structure in terms of rincial comonents is deendent on the ratio of samle size and dimensionality and that a critical number of observations is needed before learning starts (Biehl and Mietzner, 1993). Here we generalize this analysis to include missing data. Probabilistic rincial comonent analysis is regularly used for estimating signal structures in datasets with missing data. Our analytic result suggest that the effect of  missing data  is to effectively reduce signal-to-noise ratio rather than - as generally believed - to reduce samle size. The theory redicts a hase transition in the learning curves and this is indeed found both in simulation data and in real datasets.
"
1243,2019,On Medians of (Randomized) Pairwise Means,Poster,"Tournament rocedures, recently introduced in the literature, offer an aealing alternative, from a theoretical ersective at least, to the rincile of Emirical Risk Minimization in machine learning. Statistical learning by Median-of-Means (MoM) basically consists in segmenting the training data into blocks of equal size and comaring the statistical erformance of every air of candidate decision rules on each data block: that with highest erformance on the majority of the blocks is declared as the winner. In the context of nonarametric regression, functions having won all their duels have been shown to outerform emirical risk minimizers w.r.t. the mean squared error under minimal assumtions, while exhibiting robustness roerties. It is the urose of this aer to extend this aroach, in order to address other learning roblems in articular, for which the erformance criterion takes the form of an exectation over airs of observations rather than over one single observation, as may be the case in airwise ranking, clustering or metric learning. Precisely, it is roved here that the bounds achieved by MoM are essentially conserved when the blocks are built by means of indeendent samling without relacement schemes instead of a simle segmentation. These results are next extended to situations where the risk is related to a airwise loss function and its emirical counterart is of the form of a $U$-statistic. Beyond theoretical results guaranteeing the erformance of the learningestimation methods roosed, some numerical exeriments rovide emirical evidence of their relevance in ractice."
1244,2019,Accelerated Linear Convergence of Stochastic Momentum Methods in Wasserstein Distances,Poster,"Momentum methods such as Polyak's heavy ball (HB) method, Nesterov's accelerated gradient (AG) as well as accelerated rojected gradient (APG) method have been commonly used in machine learning ractice, but their erformance is quite sensitive to noise in the gradients. We study these methods under a first-order stochastic oracle model where noisy estimates of the gradients are available. For strongly convex roblems, we show that the distribution of the iterates of AG converges with the accelerated $O(\sqrt{\kaa}\log(1\varesilon))$ linear rate to a ball of radius $\varesilon$ centered at a unique invariant distribution in the 1-Wasserstein metric where $\kaa$ is the condition number as long as the noise variance is smaller than an exlicit uer bound we can rovide. Our analysis also certifies linear convergence rates as a function of the stesize, momentum arameter and the noise variance; recovering the accelerated rates in the noiseless case and quantifying the level of noise that can be tolerated to achieve a given erformance. To the best of our knowledge, these are the first linear convergence results for stochastic momentum methods under the stochastic oracle model. We also develo finer results for the secial case of quadratic objectives, extend our results to the APG method and weakly convex functions showing accelerated rates when the noise magnitude is sufficiently small."
1245,2019,SGD without Replacement: Sharper Rates for General Smooth Convex Functions,Poster,"	We study stochastic gradient descent {\em without relacement} (SGDo) for smooth convex functions. SGDo is widely observed to converge faster than true SGD where each samle is drawn indeendently {\em with relacement} (Bottou,2009) and hence, is more oular in ractice. But it's convergence roerties are not well understood as samling without relacement leads to couling between iterates and gradients. By using method of exchangeable airs to bound Wasserstein distance, we rovide the first non-asymtotic results for SGDo when alied to {\em general smooth, strongly-convex} functions. In articular, we show that SGDo converges at a rate of $O(1K^2)$ while SGD is known to converge at $O(1K)$ rate, where $K$ denotes the number of asses over data and is required to be {\em large enough}. Existing results for SGDo in this setting require additional {\em Hessian Lischitz assumtion} (Gurbuzbalaban et al, 2015; HaoChen and Sra 2018). For {\em small} $K$, we show SGDo can achieve same convergence rate as SGD for {\em general smooth strongly-convex} functions. Existing results in this setting require $K=1$ and hold only for generalized linear models (Shamir,2016). In addition, by careful analysis of the couling, for both large and small $K$, we obtain better deendence on roblem deendent arameters like condition number. "
1246,2019,On the Complexity of Approximating Wasserstein Barycenters,Poster,"We study the comlexity of aroximating the Wasserstein barycenter of $m$ discrete measures, or histograms of size $n$, by contrasting two alternative aroaches that use entroic regularization. The first aroach is based on the Iterative Bregman Projections (IBP) algorithm for which our novel analysis gives a comlexity bound roortional to ${mn^2}{\varesilon^2}$ to aroximate the original non-regularized barycenter. 
On the other hand, using an aroach based on accelerated gradient descent, we obtain a comlexity roortional to~${mn^{2}}{\varesilon}$.
As a byroduct, we show that the regularization arameter in both aroaches has to be roortional to $\varesilon$, which causes instability of both algorithms when the desired accuracy is high. To overcome this issue, we roose a novel roximal-IBP algorithm, which can be seen as a roximal gradient method, which uses IBP on each iteration to make a roximal ste.
We also consider the question of scalability of these algorithms using aroaches from distributed otimization and show that the first algorithm can be imlemented in a centralized distributed setting (masterslave), while the second one is amenable to a more general decentralized distributed setting with an arbitrary network toology."
1247,2019,Estimate Sequences for Variance-Reduced Stochastic Composite Optimization,Poster,"In this aer, we roose a unified view of gradient-based algorithms for stochastic convex comosite otimization by extending the concet of estimate sequence introduced by Nesterov. This oint of view covers the stochastic gradient descent method, variants of the aroaches SAGA, SVRG, and has several advantages: (i) we rovide a generic roof of convergence for the aforementioned methods; (ii) we show that this SVRG variant is adative to strong convexity; (iii) we naturally obtain new algorithms with the same guarantees; (iv) we derive generic strategies to make these algorithms robust to stochastic noise, which is useful when data is corruted by small random erturbations. Finally, we show that this viewoint is useful to obtain new accelerated algorithms in the sense of Nesterov.
"
1248,2019,A Dynamical Systems Perspective on Nesterov Acceleration,Poster,"We resent a dynamical system framework for understanding Nesterov's accelerated gradient method. In contrast to earlier work, our derivation does not rely on a vanishing ste size argument. We show that Nesterov acceleration arises from discretizing an ordinary differential equation with a semi-imlicit Euler integration scheme. We analyze both the underlying differential equation as well as the discretization to obtain insights into the henomenon of acceleration. The analysis suggests that a curvature-deendent daming term lies at the heart of the henomenon. We further establish connections between the discretized and the continuous-time dynamics.
"
1249,2019,Random Shuffling Beats SGD after Finite Epochs,Poster,"  A long-standing roblem in stochastic otimization is roving that \rsgd, the without-relacement version of \sgd, converges faster than the usual with-relacement \sgd. Building uon~\cite{gurbuzbalaban2015random}, we resent the \emh{first} (to our knowledge) non-asymtotic results for this roblem by roving that after a reasonable number of eochs \rsgd converges faster than \sgd. Secifically, we rove that for strongly convex, second-order smooth functions, the iterates of \rsgd converge to the otimal solution as $\mathcal{O}(\nicefrac{1}{T^2} + \nicefrac{n^3}{T^3})$, where $n$ is the number of comonents in the objective, and $T$ is number of iterations. This result imlies that after $\mathcal{O}(\sqrt{n})$ eochs, \rsgd is \emh{strictly better} than \sgd (which converges as $\mathcal{O}(\nicefrac{1}{T})$). The key ste toward showing this better deendence on $T$ is the introduction of $n$ into the bound; and as our analysis shows, in general a deendence on $n$ is unavoidable without further changes. To understand how \rsgd works in ractice, we further exlore two emirically useful settings: data sarsity and over-arameterization. For sarse data, \rsgd has the rate $\mathcal{O}\left(\frac{1}{T^2}\right)$, again strictly better than \sgd. Under a setting closely related to over-arameterization, \rsgd is shown to converge faster than \sgd after any \emh{arbitrary} number of iterations.  Finally, we extend the analysis of \rsgd to smooth non-convex and convex functions.
"
1250,2019,First-Order Algorithms Converge Faster than $O(1/k)$ on Convex Problems,Poster,"  It is well known that both gradient descent and
  stochastic coordinate descent achieve a global convergence rate of
  $O(1k)$ in the objective value, when alied to a scheme for
  minimizing a Lischitz-continuously differentiable, unconstrained
  convex function.  In this work, we imrove this rate to $o(1k)$.
  We extend the result to roximal gradient and roximal coordinate
  descent on regularized roblems to show similar $o(1k)$ convergence
  rates. The result is tight in the sense that a rate of
  $O(1k^{1+\esilon})$ is not generally attainable for any
  $\esilon0$, for any of these methods."
1251,2019,Improved Convergence for $\ell_1$ and $\ell_\infty$ Regression via Iteratively Reweighted Least Squares,Poster,"The iteratively reweighted least squares method (IRLS) is a oular technique used in ractice for solving regression roblems. Various versions of this method have been roosed, but their theoretical analyses failed to cature the good ractical erformance. 

In this aer we roose a simle and natural version of IRLS for solving $\ell_\infty$ and $\ell_1$ regression, which rovably converges to a $(1+\esilon)$-aroximate solution in $O(m^{13}\log(1\esilon)\esilon^{23} + \log m\esilon^2)$ iterations, where $m$ is the number of rows of the inut matrix. Interestingly, this running time is indeendent of the conditioning of the inut, and the dominant term of the running time deends sublinearly in $\esilon^{-1}$, which is atyical for the otimization of non-smooth functions.

This imroves uon the more comlex algorithms of Chin et al. (ITCS '12), and Christiano et al. (STOC '11) by a factor of at least $1\esilon^2$, and yields a truly efficient natural algorithm for the slime mold dynamics (Straszak-Vishnoi, SODA '16, ITCS '16, ITCS '17)."
1252,2019,Optimal Mini-Batch and Step Sizes for SAGA,Poster,"Recently it has been shown that the ste sizes of a family of variance reduced gradient methods called the JacSketch methods deend on the exected smoothness constant. In articular, if this exected smoothness constant could be calculated a riori, then one could safely set much larger ste sizes which would result in a much faster convergence rate. We fill in this ga, and rovide simle closed form exressions for the exected smoothness constant and careful numerical exeriments verifying these bounds. Using these bounds, and since the SAGA algorithm is art of this JacSketch family, we suggest a new standard ractice for setting the ste and mini-batch sizes for SAGA that are cometitive with a numerical grid search. Furthermore, we can now show that the total comlexity of the SAGA algorithm decreases linearly in the mini-batch size u to a re-defined value: the otimal mini-batch size. This is a rare result in the stochastic variance reduced literature, only reviously shown for the Katyusha algorithm. Finally we conjecture that this is the case for many other stochastic variance reduced methods and that our bounds and analysis of the exected smoothness constant is key to extending these results.
"
1253,2019,Differential Inclusions for Modeling Nonsmooth ADMM Variants: A Continuous Limit Theory,Poster,"Recently, there has been a great deal of research attention on understanding the convergence behavior of first-order methods. One line of this research focuses on analyzing the convergence behavior of first-order methods using tools from continuous dynamical systems such as ordinary differential equations and differential inclusions. These research results shed lights on better understanding first-order methods from a non-otimization oint of view. The alternating direction method of multiliers (ADMM) is a widely used first-order method for solving otimization roblems arising from machine learning and statistics, and it is imortant to investigate its behavior using these new techniques from dynamical systems. Existing works along this line have been mainly focusing on roblems with smooth objective functions, which exclude many imortant alications that are traditionally solved by ADMM variants. In this aer, we analyze some well-known and widely used ADMM variants for nonsmooth otimization roblems using tools of differential inclusions. In articular, we analyze the convergence behavior of linearized ADMM, gradient-based ADMM, generalized ADMM and accelerated generalized ADMM for nonsmooth roblems and show their connections with dynamical systems. We anticiate that these results will rovide new insights on understanding ADMM for solving nonsmooth roblems.
"
1254,2019,Distribution calibration for regression,Poster,"We are concerned with obtaining well-calibrated outut distributions from regression models. Such distributions allow us to quantify the uncertainty that the model has regarding the redicted target value. 
We introduce the novel concet of distribution calibration, and demonstrate its advantages over the existing definition of quantile calibration.
We further roose a ost-hoc aroach to imroving the redictions from reviously trained regression models, using multi-outut Gaussian Processes with a novel Beta link function.
The roosed method is exerimentally verified on a set of common regression models and shows imrovements for both distribution-level and quantile-level calibration.
"
1255,2019,Graph Convolutional Gaussian Processes,Poster,"We roose a novel Bayesian nonarametric method to learn translation-invariant relationshis on non-Euclidean domains. The resulting grah convolutional Gaussian rocesses can be alied to roblems in machine learning for which the inut observations are functions with domains on general grahs. The structure of these models allows for high dimensional inuts while retaining exressibility, as is the case with convolutional neural networks. We resent alications of grah convolutional Gaussian rocesses to images and triangular meshes, demonstrating their versatility and effectiveness, comaring favorably to existing methods, desite being relatively simle models.
"
1256,2019,Asynchronous Batch Bayesian Optimisation with Improved Local Penalisation,Poster,"Batch Bayesian otimisation (BO) has been successfully alied to hyerarameter tuning using arallel comuting, but it is wasteful of resources: workers that comlete jobs ahead of others are left idle. We address this roblem by develoing an aroach, Penalising Locally for Asynchronous Bayesian Otimisation on K Workers (PLAyBOOK), for asynchronous arallel BO. We demonstrate emirically the efficacy of PLAyBOOK and its variants on synthetic tasks and a real-world roblem. We undertake a comarison between synchronous and asynchronous BO, and show that asynchronous BO often outerforms synchronous batch BO in both wall-clock time and samle efficiency.
"
1257,2019,GOODE: A Gaussian Off-The-Shelf Ordinary Differential Equation Solver,Poster,"There are two tyes of ordinary differential equations (ODEs):
initial value roblems (IVPs) and boundary value roblems (BVPs).
While many robabilistic numerical methods for the solution
of IVPs have been resented to-date, there exists no efficient robabilistic
general-urose solver for nonlinear BVPs.
Our method based on iterated Gaussian rocess (GP) regression returns
a GP osterior over the solution of nonlinear ODEs,
which rovides a meaningful error estimation via
its redictive osterior standard deviation.
Our solver is fast (tyically of quadratic convergence rate)
and the theory of convergence can be transferred from 
rior non-robabilistic work. Our method erforms on ar with standard codes 
for an established benchmark of test roblems.
"
1258,2019,Overcoming Mean-Field Approximations in Recurrent Gaussian Process Models,Poster,"We identify a new variational inference scheme for dynamical systems whose transition function is modelled by a Gaussian rocess. Inference in this setting has either emloyed comutationally intensive MCMC methods, or relied on factorisations of the variational osterior. As we demonstrate in our exeriments, the factorisation between latent system states and transition function can lead to a miscalibrated osterior and to learning unnecessarily large noise terms. We eliminate this factorisation by exlicitly modelling the deendence between state trajectories and the low-rank reresentation of our Gaussian rocess osterior. Samles of the latent states can then be tractably generated by conditioning on this reresentation. The method we obtain gives better redictive erformance and more calibrated estimates of the transition function, yet maintains the same time and sace comlexities as mean-field methods.
"
1259,2019,AReS and MaRS - Adversarial and MMD-Minimizing Regression for SDEs,Poster,"Stochastic differential equations are an imortant modeling class in many discilines. Consequently, there exist many methods relying on various discretization and numerical integration schemes. In this aer, we roose a novel, robabilistic model for estimating the drift and diffusion given noisy observations of the underlying stochastic system. Using state-of-the-art adversarial and moment matching inference techniques, we avoid the discretization schemes of classical aroaches. This leads to significant imrovements in arameter accuracy and robustness given random initial guesses. On four established benchmark systems, we comare the erformance of our algorithms to state-of-the-art solutions based on extended Kalman filtering and Gaussian rocesses.
"
1260,2019,End-to-End Probabilistic Inference for Nonstationary Audio Analysis,Poster,"A tyical audio signal rocessing ieline includes multile disjoint analysis stages, including calculation of a time-frequency reresentation followed by sectrogram-based feature analysis. We show how time-frequency analysis and nonnegative matrix factorisation can be jointly formulated as a sectral mixture Gaussian rocess model with nonstationary riors over the amlitude variance arameters. Further, we formulate this nonlinear model's state sace reresentation, making it amenable to infinite-horizon Gaussian rocess regression with aroximate inference via exectation roagation, which scales linearly in the number of time stes and quadratically in the state dimensionality. By doing so, we are able to rocess audio signals with hundreds of thousands of data oints. We demonstrate, on various tasks with emirical data, how this inference scheme outerforms more standard techniques that rely on extended Kalman filtering.
"
1261,2019,Deep Gaussian Processes with Importance-Weighted Variational Inference,Poster,"Dee Gaussian rocesses (DGPs) can model comlex marginal densities as well as comlex maings. Non-Gaussian marginals are essential for modelling real-world data, and can be generated from the DGP by incororating uncorrelated variables to the model. Previous work in the DGP model has introduced noise additively, and used variational inference with a combination of sarse Gaussian rocesses and mean-field Gaussians for the aroximate osterior. Additive noise attenuates the signal, and the Gaussian form of variational distribution may lead to an inaccurate osterior. We instead incororate noisy variables as latent covariates, and roose a novel imortance-weighted objective, which leverages analytic results and rovides a mechanism to trade off comutation for imroved accuracy. Our results demonstrate that the imortance-weighted objective works well in ractice and consistently outerforms classical variational inference, esecially for deeer models.
"
1262,2019,Automated Model Selection with Bayesian Quadrature,Poster,"We resent a novel technique for tailoring Bayesian quadrature (BQ) to model selection. The state-of-the-art for comaring the evidence of multile models relies on Monte Carlo methods, which converge slowly and are unreliable for comutationally exensive models. Although revious research has shown that BQ offers samle efficiency suerior to Monte Carlo in comuting the evidence of an individual model, alying BQ directly to model comarison may waste comutation roducing an overly-accurate estimate for the evidence of a clearly oor model. We roose an automated and efficient algorithm for comuting the most-relevant quantity for model selection: the osterior model robability. Our technique maximizes the mutual information between this quantity and observations of the models' likelihoods, yielding efficient samle acquisition across disarate model saces when likelihood observations are limited. Our method roduces more-accurate osterior estimates using fewer likelihood evaluations than standard Bayesian quadrature and Monte Carlo estimators, as we demonstrate on synthetic and real-world examles.
"
1263,2019,Beyond the Chinese Restaurant and Pitman-Yor processes: Statistical Models with double power-law behavior,Poster,"Bayesian nonarametric aroaches, in articular the Pitman-Yor rocess and the associated two-arameter Chinese Restaurant rocess, have been successfully used in alications where the data exhibit a ower-law behavior. Examles include natural language rocessing, natural images or networks. There is also growing emirical evidence suggesting that some datasets exhibit a two-regime ower-law behavior: one regime for small frequencies, and a second regime, with a different exonent, for high frequencies. In this aer, we introduce a class of comletely random measures which are doubly regularly-varying. Contrary to the Pitman-Yor rocess, we show that when comletely random measures in this class are normalized to obtain random robability measures and associated random artitions, such artitions exhibit a double ower-law behavior. We resent two general constructions and discuss in articular two models within this class: the beta rime rocess (Broderick et al. (2015, 2018) and a novel rocess called generalized BFRY rocess. We derive efficient Markov chain Monte Carlo algorithms to estimate the arameters of these models. Finally, we show that the roosed models rovide a better fit than the Pitman-Yor rocess on various datasets.
"
1264,2019,DP-GP-LVM: A Bayesian Non-Parametric Model for Learning Multivariate Dependency Structures,Poster,"We resent a non-arametric Bayesian latent variable model caable of learning deendency structures across dimensions in a multivariate setting. Our aroach is based on flexible Gaussian rocess riors for the generative maings and interchangeable Dirichlet rocess riors to learn the structure. The introduction of the Dirichlet rocess as a secific structural rior allows our model to circumvent issues associated with revious Gaussian rocess latent variable models. Inference is erformed by deriving an efficient variational bound on the marginal log-likelihood of the model. We demonstrate the efficacy of our aroach via analysis of discovered structure and suerior quantitative erformance on missing data imutation.
"
1265,2019,Random Function Priors for Correlation Modeling,Poster,"The likelihood model of high dimensional data $X_n$ can often be exressed as $(X_n|Z_n,\theta)$, where $\theta\mathrel{\matho:}=(\theta_k)_{k\in[K]}$ is a collection of hidden features shared across objects, indexed by $n$, and $Z_n$ is a non-negative factor loading vector with $K$ entries where $Z_{nk}$ indicates the strength of $\theta_k$ used to exress $X_n$. In this aer, we introduce random function riors for $Z_n$ for modeling correlations among its $K$ dimensions $Z_{n1}$ through $Z_{nK}$, which we call \textit{oulation random measure embedding} (PRME). Our model can be viewed as a generalized aintbox model~\cite{Broderick13} using random functions, and can be learned efficiently with neural networks via amortized variational inference. We derive our Bayesian nonarametric method by alying a reresentation theorem on searately exchangeable discrete random measures."
1266,2019,Variational Russian Roulette for Deep Bayesian Nonparametrics,Poster,"Bayesian nonarametric models rovide a rinciled way to automatically adat the comlexity of a model to the amount of the data available, but comutation in such models is difficult. Amortized variational aroximations are aealing because of their comutational efficiency, but current methods rely on a fixed finite truncation of the infinite model. This truncation level can be difficult to set, and also interacts oorly with amortized methods due to the over-runing roblem. Instead, we roose a new variational aroximation, based on a method from statistical hysics called Russian roulette samling. This allows the variational distribution to adat its comlexity during inference, without relying on a fixed truncation level, and while still obtaining an unbiased estimate of the gradient of the original variational objective. We demonstrate this method on infinite sized variational auto-encoders using a Beta-Bernoulli (Indian buffet rocess) rior.
"
1267,2019,Incorporating Grouping Information into Bayesian Decision Tree Ensembles,Poster,"We consider the roblem of nonarametric regression in the high-dimensional setting in which $P \gg N$. We study the use of overlaing grou structures to imrove rediction and variable selection. These structures arise commonly when analyzing DNA microarray data, where genes can naturally be groued according to genetic athways. We incororate overlaing grou structure into a Bayesian additive regression trees model using a rior constructed so that, if a variable from some grou is used to construct a slit, this increases the robability that subsequent slits will use redictors from the same grou. We refer to our model as an overlaing grou Bayesian additive regression trees (OG-BART) model, and our rior on the slits an overlaing grou Dirichlet (OG-Dirichlet) rior. Like the sarse grou lasso, our rior encourages sarsity both within and between grous. We study the correlation structure of the rior, illustrate the roosed methodology on simulated data, and aly the methodology to gene exression data to learn which genetic athways are redictive of breast cancer tumor metastasis. "
1268,2019,Variational Implicit Processes,Poster,"We introduce the imlicit rocesses (IPs), a stochastic rocess that laces imlicitly defined multivariate distributions over any finite collections of random variables. IPs are therefore highly flexible imlicit riors over \emh{functions}, with examles including data simulators, Bayesian neural networks and non-linear transformations of stochastic rocesses. A novel and efficient aroximate inference algorithm for IPs, namely the variational imlicit rocesses (VIPs), is derived using generalised wake-slee udates.  This method returns simle udate equations and allows scalable hyer-arameter learning with stochastic otimization. Exeriments show that VIPs return better uncertainty estimates and lower errors over existing inference methods for challenging models such as Bayesian neural networks, and Gaussian rocesses. 
"
1269,2019,Discovering Latent Covariance Structures for Multiple Time Series,Poster,"Analyzing multivariate time series data is imortant to redict future events and changes of comlex systems in finance, manufacturing, and administrative decisions. The exressiveness ower of Gaussian Process (GP) regression methods has been significantly imroved by comositional covariance structures. In this aer, we resent a new GP model which naturally handles multile time series by lacing an Indian Buffet Process (IBP) rior on the resence of shared kernels. Our selective covariance structure decomosition allows exloiting shared arameters over a set of multile, selected time series. We also investigate the well-definedness of the models when infinite latent comonents are introduced. We resent a ragmatic search algorithm which exlores a larger structure sace efficiently. Exeriments conducted on five real-world data sets demonstrate that our new model outerforms existing methods in term of structure discoveries and redictive erformances.
"
1270,2019,Scalable Training of Inference Networks for Gaussian-Process Models,Poster,"Inference in Gaussian rocess (GP) models is comutationally challenging for large data, and often difficult to aroximate with a small number of inducing oints. We exlore an alternative aroximation that emloys stochastic inference networks for a flexible inference. Unfortunately, for such networks, minibatch training is difficult to be able to learn meaningful correlations over function oututs for a large dataset. We roose an algorithm that enables such training by tracking a stochastic, functional mirror-descent algorithm. At each iteration, this only requires considering a finite number of inut locations, resulting in a scalable and easy-to-imlement algorithm. Emirical results show comarable and, sometimes, suerior erformance to existing sarse variational GP methods.
"
1271,2019,Bayesian Optimization Meets Bayesian Optimal Stopping,Poster,"Bayesian otimization (BO) is a oular aradigm for otimizing the hyerarameters of machine learning (ML) models due to its samle efficiency. Many ML models require running an iterative training rocedure (e.g., stochastic gradient descent). This motivates the question whether information available during the training rocess (e.g., validation accuracy after each eoch) can be exloited for imroving the eoch efficiency of BO algorithms by early-stoing model training under hyerarameter settings that will end u under-erforming and hence eliminating unnecessary training eochs. This aer rooses to unify BO (secifically, Gaussian rocess-uer confidence bound (GP-UCB)) with Bayesian otimal stoing (BO-BOS) to boost the eoch efficiency of BO. To achieve this, while GP-UCB is samle-efficient in the number of function evaluations, BOS comlements it with eoch efficiency for each function evaluation by roviding a rinciled otimal stoing mechanism for early stoing. BO-BOS reserves the (asymtotic) no-regret erformance of GP-UCB using our secified choice of BOS arameters that is amenable to an elegant interretation in terms of the exloration-exloitation trade-off. We emirically evaluate the erformance of BO-BOS and demonstrate its generality in hyerarameter otimization of ML models and two other interesting alications.
"
1272,2019,Learning interpretable continuous-time models of latent stochastic dynamical systems,Poster,"We develo an aroach to learn an interretable semi-arametric model of a latent continuous-time stochastic dynamical system, assuming noisy high-dimensional oututs samled at uneven times. The dynamics are described by a nonlinear
stochastic differential equation (SDE) driven by a Wiener rocess, with a drift evolution function drawn from a Gaussian rocess (GP) conditioned on a set of learnt fixed oints and corresonding local Jacobian matrices.  This form yields a
flexible nonarametric model of the dynamics, with a reresentation corresonding directly to the interretable ortraits routinely emloyed in the study of nonlinear dynamical systems.  The learning algorithm combines inference of continuous latent aths underlying observed data with a sarse variational descrition of the dynamical rocess. We demonstrate our aroach on simulated data from different nonlinear dynamical systems.
"
1273,2019,A Tree-Based Method for Fast Repeated Sampling of Determinantal Point Processes,Poster,"It is often desirable in recommender systems and other information retrieval alications to rovide diverse results, and determinantal oint rocesses (DPPs) have become a oular way to cature the trade-off between the quality of individual results and the diversity of the overall set. However, samling from a DPP is inherently exensive: if the underlying collection contains N items, then generating each DPP samle requires time linear in N following a one-time rerocessing hase. Additionally, results often need to be ersonalized to a user, but standard aroaches to ersonalization invalidate the rerocessing, making ersonalized samles esecially exensive. In this work we address both of these shortcomings. First, we roose a new algorithm for generating DPP samles in time logarithmic in N, following a slightly more exensive rerocessing hase.  We then extend the algorithm to suort arbitrary query-time feature weights, allowing us to generate samles customized to individual users while still retaining logarithmic runtime; exeriments show our aroach runs over 300 times faster than traditional DPP samling on collections of 100,000 items for samles of size 10.
"
1274,2019,Nonlinear Stein Variational Gradient Descent for Learning Diversified Mixture Models,Poster,"Diversification has been shown to be a owerful mechanism for learning robust models in non-convex settings. A notable examle is learning mixture models, in which enforcing diversity between the different mixture comonents allows us to revent the model collasing henomenon and cature more atterns from the observed data. In this work, we resent a variational aroach for diversity-romoting learning, which leverages the entroy functional as a natural mechanism for enforcing diversity. We develo a simle and efficient functional gradient-based algorithm for otimizing the variational objective function, which rovides a significant generalization of Stein variational gradient descent (SVGD). We test our method on various challenging real world roblems, including dee embedded clustering and dee anomaly detection. Emirical results show that our method rovides an effective mechanism for diversity-romoting learning, achieving substantial imrovement over existing methods. 
"
1275,2019,Understanding and Accelerating Particle-Based Variational Inference,Poster,"Particle-based variational inference methods (ParVIs) have gained attention in the Bayesian inference literature, for their caacity to yield flexible and accurate aroximations. We exlore ParVIs from the ersective of Wasserstein gradient flows, and make both theoretical and ractical contributions. We unify various finite-article aroximations that existing ParVIs use, and recognize that the aroximation is essentially a comulsory smoothing treatment, in either of two equivalent forms. This novel understanding reveals the assumtions and relations of existing ParVIs, and also insires new ParVIs. We roose an acceleration framework and a rinciled bandwidth-selection method for general ParVIs; these are based on the develoed theory and leverage the geometry of the Wasserstein sace. Exerimental results show the imroved convergence by the acceleration framework and enhanced samle accuracy by the bandwidth-selection method.
"
1276,2019,Efficient learning of smooth probability functions from Bernoulli tests with guarantees,Poster,"We study the fundamental roblem of learning an unknown, smooth robability function via oint-wise Bernoulli tests. We rovide a scalable algorithm for efficiently solving this roblem with rigorous guarantees. In articular, we rove the convergence rate of our osterior udate rule to the true robability function in L2-norm. Moreover, we allow the Bernoulli tests to deend on contextual features, and rovide a modified inference engine with rovable guarantees for this novel setting. Numerical results show that the emirical convergence rates match the theory, and illustrate the sueriority of our aroach in handling contextual features over the state-of-the-art.
"
1277,2019,The Variational Predictive Natural Gradient,Poster,"Variational inference transforms osterior inference into arametric otimization thereby enabling the use of latent variable models where otherwise imractical. However, variational inference can be finicky when different variational arameters control variables that are strongly correlated under the model. Traditional natural gradients based on the variational aroximation fail to correct for correlations when the aroximation is not the true osterior. To address this, we construct a new natural gradient called the Variational Predictive Natural Gradient (VPNG). Unlike traditional natural gradients for variational inference, this natural gradient accounts for the relationshi between model arameters and variational arameters. We demonstrate the insight with a simle examle as well as the emirical value on a classification task, a dee generative model of images, and robabilistic matrix factorization for recommendation.
"
1278,2019,Scalable Nonparametric Sampling from Multimodal Posteriors with the Posterior Bootstrap,Poster,"Increasingly comlex datasets ose a number of challenges for Bayesian inference. Conventional osterior samling based on Markov chain Monte Carlo can be too comutationally intensive, is serial in nature and mixes oorly between osterior modes. Furthermore, all models are missecified, which brings into question the validity of the conventional Bayesian udate. We resent a scalable Bayesian nonarametric learning routine that enables osterior samling through the otimization of suitably randomized objective functions. A Dirichlet rocess rior on the unknown data distribution accounts for model missecification, and admits an embarrassingly arallel osterior bootstra algorithm that generates indeendent and exact samles from the nonarametric osterior distribution. Our method is articularly adet at samling from multimodal osterior distributions via a random restart mechanism, and we demonstrate this on Gaussian mixture model and sarse logistic regression examles.
"
1279,2019,An Instability in Variational Inference for Topic Models,Poster,"Naive mean field variational methods are the state of-the-art aroach to inference in toic modeling. We show that these methods suffer from an instability that can roduce misleading conclusions. Namely, for certain regimes of the model arameters, variational inference oututs a non-trivial decomosition into toics. However -for the same arameter values- the data contain no actual information
about the true toic decomosition, and the outut of the algorithm is uncorrelated with it. In articular, the estimated osterior mean is wrong, and estimated credible regions do not achieve the nominal coverage. We discuss how this instability is remedied by more accurate mean field aroximations.
"
1280,2019,Bayesian Optimization of Composite Functions,Poster,"We consider otimization of comosite objective functions, i.e., of the form $f(x)=g(h(x))$, where $h$ is a black-box derivative-free exensive-to-evaluate function with vector-valued oututs, and $g$ is a chea-to-evaluate real-valued function. While these roblems can be solved with standard Bayesian otimization, we roose a novel aroach that exloits the comosite structure of the objective function to substantially imrove samling efficiency. Our aroach models $h$ using a multi-outut Gaussian rocess and chooses where to samle using the exected imrovement evaluated on the imlied non-Gaussian osterior on $f$, which we call exected imrovement for comosite functions (EI-CF).  Although EI-CF cannot be comuted in closed form, we rovide a novel stochastic gradient estimator that allows its efficient maximization.  We also show that our aroach is asymtotically consistent, i.e., that it recovers a globally otimal solution as samling effort grows to infinity, generalizing revious convergence results for classical exected imrovement.
Numerical exeriments show that our aroach dramatically outerforms standard Bayesian otimization benchmarks, reducing simle regret by several orders of magnitude."
1281,2019,The Kernel Interaction Trick: Fast Bayesian Discovery of Pairwise Interactions in High Dimensions,Poster,"Discovering interaction effects on a resonse of interest is a fundamental roblem faced in biology, medicine, economics, and many other scientific discilines. In theory, Bayesian methods for discovering airwise interactions enjoy many benefits such as coherent uncertainty quantification, the ability to incororate background knowledge, and desirable shrinkage roerties. In ractice, however, Bayesian methods are often comutationally intractable for even moderate- dimensional roblems. Our key insight is that many hierarchical models of ractical interest admit a Gaussian rocess reresentation such that rather than maintaining a osterior over all O(^2) interactions, we need only maintain a vector of O() kernel hyer-arameters. This imlicit reresentation allows us to run Markov chain Monte Carlo (MCMC) over model hyer-arameters in time and memory linear in  er iteration. We focus on sarsity-inducing models and show on datasets with a variety of covariate behaviors that our method: (1) reduces runtime by orders of magnitude over naive alications of MCMC, (2) rovides lower Tye I and Tye II error relative to state-of-the-art LASSO-based aroaches, and (3) offers imroved comutational scaling in high dimensions relative to existing Bayesian and LASSO-based aroaches.
"
1282,2019,Quantile Stein Variational Gradient Descent for Batch  Bayesian Optimization,Poster,"Batch Bayesian otimization has been shown to be an efficient and successful aroach for black-box function otimization, esecially when the evaluation of cost function is highly exensive but can be efficiently arallelized. In this aer, we introduce a novel variational framework for batch query otimization, based on the argument that the query batch should be selected to have both high diversity and good worst case erformance. This motivates us to introduce a variational objective that combines a quantile-based risk measure (for worst case erformance) and entroy regularization (for enforcing diversity). We derive a gradient-based article-based algorithm for solving our quantile-based variational objective, which generalizes Stein variational gradient descent (SVGD). We evaluate our method on a number of real-world alications and show that it consistently outerforms other recent state-of-the-art batch Bayesian otimization methods. Extensive exerimental results indicate that our method achieves better or comarable erformance, comared to the existing methods.
"
1283,2019,Exploiting Worker Correlation for Label Aggregation in Crowdsourcing,Poster,"Crowdsourcing has emerged as a core comonent of data science ielines. From collected noisy worker labels, aggregation models that incororate worker reliability arameters aim to infer a latent true annotation. In this aer, we argue that existing crowdsourcing aroaches do not sufficiently model worker correlations observed in ractical settings; we roose in resonse an enhanced Bayesian classifier combination (EBCC) model, with inference based on a mean-field variational aroach. An introduced mixture of intra-class reliabilities---connected to tensor decomosition and item clustering---induces inter-worker correlation. EBCC does not suffer the limitations of existing correlation models: intractable marginalisation of missing labels and oor scaling to large worker cohorts. Extensive emirical comarison on 17 real-world datasets sees EBCC achieving the highest mean accuracy across 10 benchmark crowdsourcing methods.
"
1284,2019,Efficient Amortised Bayesian Inference for Hierarchical and Nonlinear Dynamical Systems,Poster,"We introduce a flexible, scalable Bayesian inference framework for nonlinear dynamical systems characterised by distinct and hierarchical variability at the individual, grou, and oulation levels. Our model class is a generalisation of nonlinear mixed-effects (NLME) dynamical systems, the statistical workhorse for many exerimental sciences. We cast arameter inference as stochastic otimisation of an end-to-end differentiable, block-conditional variational autoencoder. We secify the dynamics of the data-generating rocess as an ordinary differential equation (ODE) such that both the ODE and its solver are fully differentiable. This model class is highly flexible: the ODE right-hand sides can be a mixture of user-rescribed or ""white-box"" sub-comonents and neural network or ""black-box"" sub-comonents. Using stochastic otimisation, our amortised inference algorithm could seamlessly scale u to massive data collection ielines (common in labs with robotic automation). Finally, our framework suorts interretability with resect to the underlying dynamics, as well as redictive generalization to unseen combinations of grou comonents (also called ``zero-shot"" learning). We emirically validate our method by redicting the dynamic behaviour of bacteria that were genetically engineered to function as biosensors.
"
1285,2019,A Multitask Multiple Kernel Learning Algorithm for Survival Analysis with Application to Cancer Biology,Poster,"Predictive erformance of machine learning algorithms on related roblems can be imroved using multitask learning aroaches. Rather than erforming survival analysis on each data set to redict survival times of cancer atients, we develoed a novel multitask aroach based on multile kernel learning (MKL). Our multitask MKL algorithm both works on multile cancer data sets and integrates cancer-related athwaysgene sets into survival analysis. We tested our algorithm, which is named as Path2MSurv, on the Cancer Genome Atlas data sets analyzing gene exression rofiles of 7,655 atients from 20 cancer tyes together with cancer-secific athwaygene set collections. Path2MSurv obtained better or comarable redictive erformance when benchmarked against random survival forest, survival suort vector machine, and single-task variant of our algorithm. Path2MSurv has the ability to identify key athwaysgene sets in redicting survival times of atients from different cancer tyes.
"
1286,2019,Fast and Flexible Inference of Joint Distributions from their Marginals,Poster,"Across the social sciences and elsewhere, ractitioners frequently have to reason about relationshis between random variables, desite lacking joint observations of the variables. This is sometimes called an ""ecological"" inference; given samles from the marginal distributions of the variables, one attemts to infer their joint distribution. The roblem is inherently ill-osed, yet only a few models have been roosed for bringing rior information into the roblem, often relying on restrictive or unrealistic assumtions and lacking a unified aroach. In this aer, we treat the inference roblem generally and roose a unified class of models that encomasses some of those reviously roosed while including many new ones. Previous work has relied on either relaxation or aroximate inference via MCMC, with the latter known to mix rohibitively slowly for this tye of roblem. Here we instead give a single exact inference algorithm that works for the entire model class via an efficient fixed oint iteration called Dykstra's method. We investigate emirically both the comutational cost of our algorithm and the accuracy of the new models on real datasets, showing favorable erformance in both cases and illustrating the imact of increased flexibility in modeling enabled by this work.
"
1287,2019,Cognitive model priors for predicting human decisions,Poster,"Human decision-making underlies all economic behavior. For the ast four decades, human decision-making under uncertainty has continued to be exlained by theoretical models based on rosect theory, a framework that was awarded the Nobel Prize in Economic Sciences. However, theoretical models of this kind have develoed slowly, and robust, high-recision redictive models of human decisions remain a challenge. While machine learning is a natural candidate for solving these roblems, it is currently unclear to what extent it can imrove redictions obtained by current theories. We argue that this is mainly due to data scarcity, since noisy human behavior requires massive samle sizes to be accurately catured by off-the-shelf machine learning methods. To solve this roblem, what is needed are machine learning models with aroriate inductive biases for caturing human behavior, and larger datasets. We offer two contributions towards this end: first, we construct ``cognitive model riors'' by retraining neural networks with synthetic data generated by cognitive models (i.e., theoretical models develoed by cognitive sychologists). We find that fine-tuning these networks on small datasets of real human decisions results in unrecedented state-of-the-art imrovements on two benchmark datasets. Second, we resent the first large-scale dataset for human decision-making, containing over 240,000 human judgments across over 13,000 decision roblems. This dataset reveals the circumstances where cognitive model riors are useful, and rovides a new standard for benchmarking rediction of human decisions under uncertainty.
"
1288,2019,Conditioning by adaptive sampling for robust design,Poster,"We resent a method for design roblems wherein the goal is to maximize or secify the value of one or more roerties of interest (e.g. maximizing the fluorescence of a rotein). We assume access to black box, stochastic ``oracle"" redictive functions, each of which mas from design sace to a distribution over roerties of interest. Because many state-of-the-art redictive models are known to suffer from athologies, esecially for data far from the training distribution, the roblem becomes different from directly otimizing the oracles. Herein, we roose a method to solve this roblem that uses model-based adative samling to estimate a distribution over the design sace, conditioned on the desired roerties. 
"
1289,2019,Direct Uncertainty Prediction for Medical Second Opinions,Poster,"The issue of disagreements amongst human exerts is a ubiquitous one in both machine learning and medicine. In medicine, this often corresonds to doctor disagreements on a atient diagnosis. In this work, we show that machine learning models can be successfully trained to give uncertainty scores to data instances that result in high exert disagreements. In articular, they can identify atient cases that would benefit most from a medical second oinion. Our central methodological finding is that Direct Uncertainty Prediction (DUP), training a model to redict an uncertainty score directly from the raw atient features, works better than Uncertainty Via Classification, the two ste rocess of training a classifier and ostrocessing the outut distribution to give an uncertainty score. We show this both with a theoretical result, and on extensive evaluations on a large scale medical imaging alication. 
"
1290,2019,Dynamic Measurement Scheduling for Event Forecasting using Deep RL,Poster,"Imagine a atient in critical condition. What and when should be measured to forecast detrimental events, esecially under the budget constraints? We answer this question by dee reinforcement learning (RL) that jointly minimizes the measurement cost and maximizes redictive gain, by scheduling strategically-timed measurements. We learn our olicy to be dynamically deendent on the atient's health history. To scale our framework to exonentially large action sace, we distribute our reward in a sequential setting that makes the learning easier. In our simulation, our olicy outerforms heuristic-based scheduling with higher redictive gain and lower cost. In a real-world ICU mortality rediction task (MIMIC3), our olicies reduce the total number of measurements by 31% or imrove redictive gain by a factor of 3 as comared to hysicians, under the off-olicy olicy evaluation.
"
1291,2019,Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization,Poster,"Modern dee neural networks are tyically highly overarameterized.  Pruning techniques are able to remove a significant fraction of network arameters with little loss in accuracy.  Recently, techniques based on dynamic reallocation of non-zero arameters have emerged, allowing direct training of sarse networks without having to re-train a large dense model.  Here we resent a novel dynamic sarse rearameterization method that addresses the limitations of revious techniques such as high comutational cost and the need for manual configuration of the number of free arameters allocated to each layer.  We evaluate the erformance of dynamic reallocation methods in training dee convolutional networks and show that our method outerforms revious static and dynamic rearameterization methods, yielding the best accuracy for a fixed arameter budget, on ar with accuracies obtained by iteratively runing a re-trained dense model.  We further investigated the mechanisms underlying the suerior generalization erformance of the resultant sarse networks.  We found that neither the structure, nor the initialization of the non-zero arameters were sufficient to exlain the suerior erformance.  Rather, effective learning crucially deended on the continuous exloration of the sarse network structure sace during training.  Our work suggests that exloring structural degrees of freedom during training is more effective than adding extra arameters to the network.
"
1292,2019,DeepNose: Using artificial neural networks to represent the space of odorants,Poster,"The olfactory system emloys an ensemble of odorant recetors (ORs) to sense odorants and to derive olfactory ercets. We trained artificial neural networks to reresent the chemical sace of odorants and used this reresentation to redict human olfactory ercets. We hyothesized that ORs may be considered 3D convolutional filters that extract molecular features and, as such, can be trained using machine learning methods. First, we trained a convolutional autoencoder, called DeeNose, to deduce a low-dimensional reresentation of odorant molecules which were reresented by their 3D satial structure. Next, we tested the ability of DeeNose features in redicting hysical roerties and odorant ercets based on 3D molecular structure alone. We found that, desite the lack of human exertise, DeeNose features often outerformed molecular descritors used in comutational chemistry in redicting both hysical roerties and human ercetions. We roose that DeeNose network can extract {\it de novo} chemical features redictive of various bioactivities and can hel understand the factors influencing the comosition of ORs ensemble.
"
1293,2019,Domain Agnostic Learning with Disentangled Representations,Poster,"Unsuervised model transfer has the otential to greatly imrove the generalizability of dee models to novel domains. Yet the current literature assumes that the searation of target data into distinct domains is known a riori. In this aer, we roose the task of Domain-Agnostic Learning (DAL): How to transfer knowledge from a labeled source domain to unlabeled data from arbitrary target domains? To tackle this roblem, we devise a novel Dee Adversarial Disentangled Autoencoder (DADA) caable of disentangling domain-secific features from class identity. We demonstrate exerimentally that when the target domain labels are unknown, DADA leads to state-of-the-art erformance on several image classification datasets.
"
1294,2019,Composing Value Functions in Reinforcement Learning,Poster,"An imortant roerty for lifelong-learning agents is the ability to combine existing skills to solve new unseen tasks. In general, however, it is unclear how to comose existing skills in a rinciled manner. Under the assumtion of deterministic dynamics, we rove that otimal value function comosition can be achieved in entroy-regularised reinforcement learning (RL), and extend this result to the standard RL setting. Comosition is demonstrated in a high-dimensional video game, where an agent with an existing library of skills is immediately able to solve new tasks without the need for further learning.
"
1295,2019,Fast Context Adaptation via Meta-Learning,Poster,"We roose CAVIA for meta-learning, a simle extension to MAML that is less rone to meta-overfitting, easier to arallelise, and more interretable. CAVIA artitions the model arameters into two arts: context arameters that serve as additional inut to the model and are adated on individual tasks, and shared arameters that are meta-trained and shared across tasks. At test time, only the context arameters are udated, leading to a low-dimensional task reresentation. We show emirically that CAVIA outerforms MAML for regression, classification, and reinforcement learning. Our exeriments also highlight  weaknesses in current benchmarks, in that the amount of adatation needed in some cases is small.
"
1296,2019,Provable Guarantees for Gradient-Based Meta-Learning,Poster,"We study the roblem of meta-learning through the lens of online convex otimization, develoing a meta-algorithm bridging the ga between oular gradient-based meta-learning and classical regularization-based multi-task transfer methods. Our method is the first to simultaneously satisfy good samle efficiency guarantees in the convex setting, with generalization bounds that imrove with task-similarity, while also being comutationally scalable to modern dee learning architectures and the many-task setting. Desite its simlicity, the algorithm matches, u to a constant factor, a lower bound on the erformance of any such arameter-transfer method under natural task similarity assumtions. We use exeriments in both convex and dee learning settings to verify and demonstrate the alicability of our theory.
"
1297,2019,Towards Understanding Knowledge Distillation,Poster,"Knowledge distillation, i.e., one classifier being trained on the oututs of another classifier, is an emirically very successful technique for knowledge transfer between classifiers. It has even been observed that classifiers learn much faster and more reliably if trained with the oututs of another classifier as soft labels, instead of from ground truth data. So far, however, there is no satisfactory theoretical exlanation of this henomenon.
In this work, we rovide the first insights into the working mechanisms of distillation by studying the secial case of linear and dee linear classifiers. Secifically, we rove a generalization bound that establishes fast convergence of the exected risk of a distillation-trained linear classifier. From the bound and its roof we extract three key factors that determine the success of distillation: 
* data geometry -- geometric roerties of the data distribution, in articular class searation, has a direct influence on the convergence seed of the risk;
* otimization bias -- gradient descent otimization finds a very favorable minimum of the distillation objective; and
* strong monotonicity -- the exected risk of the student classifier always decreases when the size of the training set grows.
"
1298,2019,Transferable Adversarial Training: A General Approach to Adapting Deep Classifiers,Poster,"Domain adatation enables knowledge transfer from a labeled source domain to an unlabeled target domain. A mainstream aroach is adversarial feature adatation, which learns domain-invariant reresentations through aligning the feature distributions of both domains. However, a theoretical rerequisite of domain adatation is the adatability measured by the exected risk of an ideal joint hyothesis over the source and target domains. In this resect, adversarial feature adatation may otentially deteriorate the adatability, since it distorts the original feature distributions when suressing domain-secific variations. To this end, we roose Transferable Adversarial Training (TAT) to enable the adatation of dee classifiers. The aroach generates transferable examles to fill in the ga between the source and target domains, and adversarially trains the dee classifiers to make consistent redictions over the transferable examles. Without learning domain-invariant reresentations at the exense of distorting the feature distributions, the adatability in the theoretical learning bound is algorithmically guaranteed. A series of exeriments validate that our aroach advances the state of the arts on a variety of domain adatation tasks in vision and NLP, including object recognition, learning from synthetic to real data, and sentiment classification.
"
1299,2019,Transferability vs. Discriminability: Batch Spectral Penalization for Adversarial Domain Adaptation,Poster,"Adversarial domain adatation has made remarkable advances in learning transferable reresentations for knowledge transfer across domains. While adversarial learning strengthens the feature transferability which the community focuses on, its imact on the feature discriminability has not been fully exlored. In this aer, a series of exeriments based on sectral analysis of the feature reresentations have been conducted, revealing an unexected deterioration of the discriminability while learning transferable features adversarially. Our key finding is that the eigenvectors with the largest singular values will dominate the feature transferability. As a consequence, the transferability is enhanced at the exense of over enalization of other eigenvectors that embody rich structures crucial for discriminability. Towards this roblem, we resent Batch Sectral Penalization (BSP), a general aroach to enalizing the largest singular values so that other eigenvectors can be relatively strengthened to boost the feature discriminability. Exeriments show that the aroach significantly imroves uon reresentative adversarial domain adatation methods to yield state of the art results.
"
1300,2019,Learning-to-Learn Stochastic Gradient Descent with Biased Regularization,Poster,"We study the roblem of learning-to-learn: infer- ring a learning algorithm that works well on a family of tasks samled from an unknown distribution. As class of algorithms we consider Stochastic Gradient Descent (SGD) on the true risk regularized by the square euclidean distance from a bias vector. We resent an average excess risk bound for such a learning algorithm that quantifies the otential benefit of using a bias vector with resect to the unbiased case. We then roose a novel meta-algorithm to estimate the bias term online from a sequence of observed tasks. The small memory footrint and low time comlexity of our aroach makes it aealing in ractice while our theoretical analysis rovides guarantees on the generalization roerties of the meta-algorithm on new tasks. A key feature of our results is that, when the number of tasks grows and their vari- ance is relatively small, our learning-to-learn aroach has a significant advantage over learning each task in isolation by standard SGD without a bias term. Numerical exeriments demonstrate the effectiveness of our aroach in ractice.
"
1301,2019,BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning,Poster,"Multi-task learning shares information between related tasks, sometimes reducing the number of arameters required. State-of-the-art results across multile natural language understanding tasks in the GLUE benchmark have reviously used transfer from a single large task: unsuervised re-training with BERT, where a searate BERT model was fine-tuned for each task. We exlore multi-task aroaches that share a \hbox{single} BERT model with a small number of additional task-secific arameters. Using new adatation modules, PALs or `rojected attention layers', we match the erformance of searately fine-tuned models on the GLUE benchmark with $\arox$7 times fewer arameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset."
1302,2019,Towards Accurate Model Selection in Deep Unsupervised Domain Adaptation,Poster,"Dee unsuervised domain adatation (Dee UDA) methods successfully leverage rich labeled data in a source domain to boost the erformance on related but unlabeled data in a target domain. However, algorithm comarison is cumbersome in Dee UDA due to the absence of accurate and standardized model selection method, osing an obstacle to further advances in the field. Existing model selection methods for Dee UDA are either highly biased, restricted, unstable, or even controversial (requiring labeled target data). To this end, we roose Dee Embedded Validation (DEV), which embeds adated feature reresentation into the validation rocedure to obtain unbiased estimation of the target risk with bounded variance. The variance is further reduced by the technique of control variate. The efficacy of the method has been justified both theoretically and emirically. 
"
1303,2019,Active Embedding Search via Noisy Paired Comparisons,Poster,"Suose that we wish to estimate a user's reference vector $w$ from aired comarisons of the form ``does user $w$ refer item $$ or item $q$?,'' where both the user and items are embedded in a low-dimensional Euclidean sace with distances that reflect user and item similarities. Such observations arise in numerous settings, including sychometrics and sychology exeriments, search tasks, advertising, and recommender systems. In such tasks, queries can be extremely costly and subject to varying levels of resonse noise; thus, we aim to actively choose airs that are most informative given the results of revious comarisons. We rovide new theoretical insights into the benefits and challenges of greedy information maximization in this setting, and develo two novel strategies that maximize lower bounds on information gain and are simler to analyze and comute resectively. We use simulated resonses from a real-world dataset to validate our strategies through their similar erformance to greedy information maximization, and their suerior reference estimation over state-of-the-art selection methods as well as random queries."
1304,2019,Fast Direct Search in an Optimally Compressed Continuous Target Space for Efficient Multi-Label Active Learning,Poster,"Active learning for multi-label classification oses fundamental challenges given the comlex label correlations and a otentially large and sarse label sace. We roose a novel CS-BPCA rocess that integrates comressed sensing and Bayesian rincial comonent analysis to erform a two-level label transformation, resulting in an otimally comressed continuous target sace. Besides leveraging correlation and sarsity of a large label sace for effective comression, an otimal comressing rate and the relative imortance of the resultant targets are automatically determined through Bayesian inference. Furthermore, the orthogonality of the transformed sace comletely decoules the correlations among targets, which significantly simlifies multi-label samling in the target sace. We define a novel samling function that leverages  a multi-outut  Gaussian  Process  (MOGP). Gradient-free otimization strategies are develoed to achieve fast online hyer-arameter learning and model retraining for active learning. Exerimental results over multile real-world datasets and comarison with cometitive multi-label active learning models demonstrate the effectiveness of the roosed framework. 
"
1305,2019,Myopic Posterior Sampling for Adaptive Goal Oriented Design of Experiments,Poster,"Bayesian methods for adative decision-making, such as Bayesian otimisation,
active learning, and active search have seen great success in relevant alications.
However, real world data collection tasks are more broad and comlex, as we may need to
achieve a combination of the above goals andor alication secific goals.
In such scenarios, secialised methods have limited alicability.
In this work,
we design a new myoic strategy for a wide class of adative design of
exeriment (DOE) roblems, where we wish to collect data in order to fulfil a given goal.
Our aroach, Myoic Posterior Samling (MPS), 
which is insired by the classical osterior samling algorithm
for multi-armed bandits,
enables us to address a broad suite of DOE tasks where a ractitioner may
incororate domain exertise about the system and secify her
desired goal via a reward function.
Emirically, this general-urose strategy is cometitive with more
secialised methods in a wide array of synthetic and real world DOE tasks.
More imortantly, it enables
addressing comlex DOE goals where no existing method seems alicable.
On the theoretical side, we leverage ideas from adative submodularity and
reinforcement learning to derive conditions under which MPS achieves
sublinear regret against natural benchmark olicies.
"
1306,2019,Bayesian Generative Active Deep Learning,Poster,"Dee learning models have demonstrated outstanding erformance in several roblems, but their training rocess tends to require immense amounts of comutational and human resources for training and labeling, constraining the tyes of roblems that can be tackled.
Therefore, the design of effective training methods that require small labeled training sets is an imortant research direction that will allow a more effective use of resources.
Among current aroaches designed to address this issue, two are articularly interesting: data augmentation and active learning.  
Data augmentation achieves this goal by artificially generating new training oints, while active learning relies on the selection of the ``most informative'' subset of unlabeled training samles to be labelled by an oracle.
Although successful in ractice, data augmentation can waste comutational resources because it indiscriminately generates samles that are not guaranteed to be informative, and active learning selects a small subset of informative samles (from a large un-annotated set) that may be insufficient for the training rocess. 
In this aer, we roose a Bayesian generative active dee learning aroach that combines active learning with data augmentation -- we rovide theoretical and emirical evidence (MNIST, CIFAR-$\{10,100\}$, and SVHN) that our aroach has more efficient training and better classification results than data augmentation and active learning."
1307,2019,Active Learning for Probabilistic Structured Prediction of Cuts and Matchings,Poster,"Active learning methods, like uncertainty samling, combined with robabilistic rediction techniques  have  achieved  success  in  various  roblems like image classification and text classification.  For  more  comlex  multivariate  rediction tasks, the relationshis between labels lay an imortant role in designing structured classifiers with better erformance. However, comutational time comlexity limits revalent robabilistic methods from effectively suorting active learning. Secifically, while non-robabilistic methods based on structured suort vector ma-chines can be tractably alied to redicting cuts and biartite matchings, conditional random fields are intractable for these structures. We roose an adversarial aroach for active learning with structured rediction domains that is tractable for cuts  and  matching.  We  evaluate  this  aroach algorithmically in two imortant structured rediction roblems: multi-label classification and object tracking in videos. We demonstrate better accuracy and comutational efficiency for our roosed method.
"
1308,2019,Active Learning with Disagreement Graphs,Poster,"We resent two novel enhancements of an online imortance-weighted active learning algorithm IWAL, using the roerties of disagreements among hyotheses. The first enhancement, IWALD, runes the hyothesis set with a more aggressive strategy based on the disagreement grah. We show that IWAL-D imroves the generalization erformance and the label comlexity of the original IWAL, and quantify the imrovement in terms of the disagreement grah coefficient. The second enhancement, IZOOM, further imroves IWAL-D by adatively zooming into the current version sace and thus reducing the best-in-class error. We show that IZOOM admits favorable theoretical guarantees with the changing hyothesis set. We reort exerimental results on multile datasets and demonstrate that the roosed algorithms achieve better test erformances than IWAL given the same amount of labeling budget.
"
1309,2019,Multi-Frequency Vector Diffusion Maps,Poster,"We introduce multi-frequency vector diffusion mas (MFVDM), a new framework for organizing and analyzing high dimensional data sets. The new method is a mathematical and algorithmic generalization of vector diffusion mas (VDM) and other non-linear dimensionality reduction methods. The idea of MFVDM is to incororates multile unitary irreducible reresentations of the alignment grou which introduces robustness to noise. We illustrate the efficacy of MFVDM on synthetic and cryo-EM image datasets, achieving better nearest neighbors search and alignment estimation than other baselines as VDM and diffusion mas (DM), esecially on extremely noisy data.
"
1310,2019,Co-manifold learning with missing data,Poster,"Reresentation learning is tyically alied to only one mode of a data matrix, either its rows or columns. Yet in many alications, there is an underlying geometry to both the rows and the columns. We roose utilizing this couled structure to erform co-manifold learning: uncovering the underlying geometry of both the rows and the columns of a given matrix, where we focus on a missing data setting. Our unsuervised aroach consists of three comonents. We first solve a family of otimization roblems to estimate a comlete matrix at multile scales of smoothness. We then use this collection of smooth matrix estimates to comute airwise distances on the rows and columns based on a new multi-scale metric that imlicitly introduces a couling between the rows and the columns. Finally, we construct row and column reresentations from these multi-scale metrics.  We demonstrate that our aroach outerforms cometing methods in both data visualization and clustering.
"
1311,2019,Hybrid Models with Deep and Invertible Features,Poster,"We roose a neural hybrid model consisting of a linear model defined on a set of features comuted by a dee, invertible transformation (i.e. a normalizing flow). An attractive roerty of our model is that both (features),  the density of the features,  and  (targets|features), the redictive distribution, can be comuted exactly in a single feed-forward ass.   We show that our hybrid model, desite the invertibility constraints, achieves similar accuracy to urely redictive models.  Yet the generative comonent remains a good model of the inut features desite the hybrid otimization objective.  This offers additional caabilities such as detection of out-of-distribution inuts and enabling semi-suervised learning.  The availability of the exact joint density (targets, features) also allows us to comute many quantities readily, making our hybrid model a useful building block for downstream alications of robabilistic dee learning.
"
1312,2019,State-Reification Networks: Improving Generalization by Modeling the Distribution of Hidden Representations,Poster,"Machine learning romises methods that generalize well from finite labeled data. However, the brittleness of existing neural net aroaches is revealed by notable failures, such as the existence of adversarial examles that are misclassified desite being nearly identical to a training examle, or the inability of recurrent sequence-rocessing nets to stay on track without teacher forcing. We introduce a method, which we refer to as emstate reificationem, that involves modeling the distribution of hidden states over the training data and then rojecting hidden states observed during testing toward this distribution. Our intuition is that if the network can remain in a familiar manifold of hidden sace, subsequent layers of the net should be well trained to resond aroriately. We show that this state-reification method hels neural nets to generalize better, esecially when labeled data are sarse, and also hels overcome the challenge of achieving robust generalization with adversarial training.
"
1313,2019,Variational Laplace Autoencoders,Poster,"Variational autoencoders emloy an amortized inference model to aroximate the osterior of latent variables. However, such amortized variational inference faces two challenges: (1) the limited osterior exressiveness of fully-factorized Gaussian assumtion and (2) the amortization error of the inference model. We resent a novel aroach that addresses both challenges. First, we focus on ReLU networks with Gaussian outut and illustrate their connection to robabilistic PCA. Building on this observation, we derive an iterative algorithm that finds the mode of the osterior and aly fullcovariance Gaussian osterior aroximation centered on the mode. Subsequently, we resent a general framework named Variational Lalace Autoencoders (VLAEs) for training dee generative models. Based on the Lalace aroximation of the latent variable osterior, VLAEs enhance the exressiveness of the osterior while reducing the amortization error. Emirical results on MNIST, Omniglot, Fashion-MNIST, SVHN and CIFAR10 show that the roosed aroach significantly outerforms other recent amortized or iterative methods on the ReLU networks.
"
1314,2019,Latent Normalizing Flows for Discrete Sequences,Poster,"Normalizing flows are a owerful class of generative models for continuous random variables, showing both strong model flexibility and the otential for non-autoregressive generation. These benefits are also desired when modeling discrete random variables such as text, but directly alying normalizing flows to discrete sequences oses significant additional challenges. We roose a VAE-based generative model which jointly learns a normalizing flow-based distribution in the latent sace and a stochastic maing to an observed discrete sace. In this setting, we find that it is crucial for the flow-based distribution to be highly multimodal. To cature this roerty, we roose several normalizing flow architectures to maximize model flexibility. Exeriments consider common discrete sequence tasks of character-level language modeling and olyhonic music generation. Our results indicate that an autoregressive flow-based model can match the erformance of a comarable autoregressive baseline, and a non-autoregressive flow-based model can imrove generation seed with a enalty to erformance.
"
1315,2019,Multi-objective training of Generative Adversarial Networks with multiple discriminators,Poster,"Recent literature has demonstrated romising results for training Generative Adversarial Networks by emloying a set of discriminators, in contrast to the traditional game involving one generator against a single adversary. Such methods erform single-objective otimization on some simle consolidation of the losses, e.g. an arithmetic average. In this work, we revisit the multile-discriminator setting by framing the simultaneous minimization of losses rovided by different models as a multi-objective otimization roblem. Secifically, we evaluate the erformance of multile gradient descent and the hyervolume maximization algorithm on a number of different datasets. Moreover, we argue that the reviously roosed methods and hyervolume maximization can all be seen as variations of multile gradient descent in which the udate direction can be comuted efficiently. Our results indicate that hyervolume maximization resents a better comromise between samle quality and comutational cost than revious methods.
"
1316,2019,Learning Discrete and Continuous Factors of Data via Alternating Disentanglement,Poster,"We address the roblem of unsuervised disentanglement of discrete and continuous exlanatory factors of data. We first show a simle rocedure for minimizing the total correlation of the continuous latent variables without having to use a discriminator network or erform imortance samling, via cascading the information flow in the beta-VAE framework. Furthermore, we roose a method which avoids offloading the entire burden of jointly modeling the continuous and discrete factors to the variational encoder by emloying a searate discrete inference rocedure.

This leads to an interesting alternating minimization roblem which switches between finding the most likely discrete configuration given the continuous factors and udating the variational encoder based on the comuted discrete factors. Exeriments show that the roosed method clearly disentangles discrete factors and significantly outerforms current disentanglement methods based on the disentanglement score and inference network classification score. The source code is available at htts:github.comsnumllabDisentanglementICML19.
"
1317,2019,Bit-Swap: Recursive Bits-Back Coding for Lossless Compression with Hierarchical Latent Variables,Poster,"The bits-back argument suggests that latent variable models can be turned into lossless comression schemes. Translating the bits-back argument into efficient and ractical lossless comression schemes for general latent variable models, however, is still an oen roblem. Bits-Back with Asymmetric Numeral Systems (BB-ANS), recently roosed by Townsend et al,. 2019, makes bits-back coding ractically feasible for latent variable models with one latent layer, but it is inefficient for hierarchical latent variable models. In this aer we roose Bit-Swa, a new comression scheme that generalizes BB-ANS and achieves strictly better comression rates for hierarchical latent variable models with Markov chain structure. Through exeriments we verify that Bit-Swa results in lossless comression rates that are emirically suerior to existing techniques.
"
1318,2019,Graphite: Iterative Generative Modeling of Graphs,Poster,"Grahs are a fundamental abstraction for modeling relational data. However, grahs are discrete and combinatorial in nature, and learning reresentations suitable for machine learning tasks oses statistical and comutational challenges. In this work, we roose Grahite, an algorithmic framework for unsuervised learning of reresentations over nodes in large grahs using dee latent variable generative models. Our model arameterizes variational autoencoders (VAE) with grah neural networks, and uses a novel iterative grah refinement strategy insired by low-rank aroximations for decoding. On a wide variety of synthetic and benchmark datasets, Grahite outerforms cometing aroaches for the tasks of density estimation, link rediction, and node classification. Finally, we derive a theoretical connection between message assing in grah neural networks and mean-field variational inference.
"
1319,2019,MIWAE: Deep Generative Modelling and Imputation of Incomplete Data Sets,Poster,"We consider the roblem of handling missing data with dee latent variable models (DLVMs). First, we resent a simle technique to train DLVMs when the training set contains missing-at-random data. Our aroach, called MIWAE, is based on the imortance-weighted autoencoder (IWAE), and maximises a otentially tight lower bound of the log-likelihood of the observed data. Comared to the original IWAE, our algorithm does not induce any additional comutational overhead due to the missing data. We also develo Monte Carlo techniques for single and multile imutation using a DLVM trained on an incomlete data set. We illustrate our aroach by training a convolutional DLVM on incomlete static binarisations of MNIST. Moreover, on various continuous data sets, we show that MIWAE rovides extremely accurate single imutations, and is highly cometitive with state-of-the-art methods.
"
1320,2019,On Scalable and Efficient Computation of Large Scale Optimal Transport,Poster,"Otimal Transort (OT) naturally arises in many machine learning alications, yet the heavy comutational burden limits its wide-sread uses. To address the scalability issue, we roose an imlicit generative learning-based framework called SPOT (Scalable Push-forward of Otimal Transort). Secifically, we aroximate the otimal transort lan by a ushforward of a reference distribution, and cast the otimal transort roblem into a minimax roblem. We then can solve OT roblems efficiently using rimal dual stochastic gradient-tye algorithms. We also show that we can recover the density of the otimal transort lan using neural ordinary differential equations. Numerical exeriments on both synthetic and real datasets illustrate that SPOT is robust and has favorable convergence behavior. SPOT also allows us to efficiently samle from the otimal transort lan, which benefits downstream alications such as domain adatation.
"
1321,2019,Understanding and correcting pathologies in the training of learned optimizers,Poster,"Dee learning has shown that learned functions can dramatically outerform hand-designed functions on ercetual tasks. Analogously, this suggests that learned otimizers may similarly outerform current hand-designed otimizers, esecially for secific roblems. However, learned otimizers are notoriously difficult to train and have yet to demonstrate wall-clock seedus over hand-designed otimizers, and thus are rarely used in ractice. Tyically, learned otimizers are trained by truncated backroagation through an unrolled otimization rocess. The resulting gradients are either strongly biased (for short truncations) or have exloding norm (for long truncations). In this work we roose a training scheme which overcomes both of these difficulties, by dynamically weighting two unbiased gradient estimators for a variational loss on otimizer erformance. This allows us to train neural networks to erform otimization of a secific task faster than tuned first-order methods. Moreover, by training the otimizer against validation loss (as oosed to training loss), we are able to learn otimizers that train networks to generalize better than first order methods. We demonstrate these results on roblems where our learned otimizer trains convolutional networks faster in wall-clock time comared to tuned first-order methods and with an imrovement in test loss.
"
1322,2019,Demystifying Dropout,Poster,"Droout is a oular technique to train large-scale dee neural networks to alleviate the overfitting roblem. To disclose the underlying reasons for its gain, numerous works have tried to exlain it from different ersectives. In this aer, unlike existing works, we exlore it from a new ersective to rovide new insight into this line of research. In detail, we disentangle the forward and backward ass of droout. Then, we find that these two asses need different levels of noise to imrove the generalization erformance of dee neural networks. Based on this observation, we roose the augmented droout which emloys different droing strategies in the forward and backward ass. Exerimental results have verified the effectiveness of our roosed method. 
"
1323,2019,Ladder Capsule Network,Poster,"We roose a new architecture of the casule network called the ladder casule network, which has an alternative building block to the dynamic routing algorithm in the casule network (Sabour et al., 2017). Motivated by the need for using only imortant casules during training for robust erformance, we first introduce a new layer called the runing layer, which removes irrelevant casules. Based on the selected casules, we construct higher-level casule oututs. Subsequently, to cature the art-whole satial relationshis, we introduce another new layer called the ladder layer, the oututs of which are regressed lower-level casule oututs from higher-level casules. Unlike the casule network adoting the routing-by-agreement, the ladder casule network uses backroagation from a loss function to reconstruct the lower-level casule oututs from higher-level casules; thus, the ladder layer imlements the reverse directional inference of the agreementdisagreement mechanism of the casule network. The exeriments on MNIST demonstrate that the ladder casule network learns an equivariant reresentation and imroves the caability to extraolate or generalize to ose variations.
"
1324,2019,Unreproducible Research is Reproducible,Poster,"The aarent contradiction in the title is a wordlay on the different meanings attributed to the word reroducible across different scientific fields. What we imly is that unreroducible findings can be built uon reroducible methods. Without denying the imortance of facilitating the reroduction of methods, we deem imortant to reassert that reroduction of findings is a fundamental ste of the scientific inquiry. We argue that the commendable quest towards easy deterministic reroducibility of methods and numerical results should not have us forget the even more imortant necessity of ensuring the reroducibility of emirical findings and conclusions by roerly accounting for essential sources of variations. We rovide exeriments to exemlify the brittleness of current common ractice in the evaluation of models in the field of dee learning, showing that even if the results could be reroduced, a slightly different exeriment would not suort the findings. We hoe to hel clarify the distinction between exloratory and emirical research in the field of dee learning and believe more energy should be devoted to roer emirical research in our community. This work is an attemt to romote the use of more rigorous and diversified methodologies. It is not an attemt to imose a new methodology and it is not a critique on the nature of exloratory research.
"
1325,2019,Geometric Scattering for Graph Data Analysis,Poster,"We exlore the generalization of scattering transforms from traditional (e.g., image or audio) signals to grah data, analogous to the generalization of ConvNets in geometric dee learning, and the utility of extracted grah features in grah data analysis. In articular, we focus on the caacity of these features to retain informative variability and relations in the data (e.g., between individual grahs, or in aggregate), while relating our construction to revious theoretical results that establish the stability of similar transforms to families of grah deformations. We demonstrate the alication of our geometric scattering features in grah classification of social network data, and in data exloration of biochemistry data.
"
1326,2019,Robust Inference via Generative Classifiers for Handling Noisy Labels,Poster,"Large-scale datasets may contain significant roortions of noisy (incorrect) class labels, and it is well-known that modern dee neural networks (DNNs) oorly generalize from such noisy training datasets. To mitigate the issue, we roose a novel inference method, termed Robust Generative classifier (RoG), alicable to any discriminative (e.g., softmax) neural classifier re-trained on noisy datasets. In articular, we induce a generative classifier on to of hidden feature saces of the re-trained DNNs, for obtaining a more robust decision boundary. By estimating the arameters of generative classifier using the minimum covariance determinant estimator, we significantly imrove the classification accuracy with neither re-training of the dee model nor changing its architectures. With the assumtion of Gaussian distribution for features, we rove that RoG generalizes better than baselines under noisy labels. Finally, we roose the ensemble version of RoG to imrove its erformance by investigating the layer-wise characteristics of DNNs. Our extensive exerimental results demonstrate the sueriority of RoG given different learning models otimized by several training techniques to handle diverse scenarios of noisy labels. 
"
1327,2019,LIT: Learned Intermediate Representation Training for Model Compression,Poster,"Researchers have roosed a range of model comression techniques to reduce the
comutational and memory footrint of dee neural networks (DNNs). In
this work, we introduce Learned Intermediate reresentation
Training (LIT), a novel model comression technique that outerforms a
range of recent model comression techniques by leveraging the highly reetitive
structure of modern DNNs (e.g., ResNet). LIT uses a teacher DNN to train a
student DNN of reduced deth by leveraging two key ideas: 1) LIT directly
comares intermediate reresentations of the teacher and student model and 2)
LIT uses the intermediate reresentation from the teacher model's revious block
as inut to the current student block during training, imroving stability of
intermediate reresentations in the student network. We show that LIT can
substantially reduce network size without loss in accuracy on a range of DNN
architectures and datasets. For examle, LIT can comress ResNet on CIFAR10 by
3.4$\times$ outerforming network slimming and FitNets. Furthermore, LIT can
comress, by deth, ResNeXt 5.5$\times$ on CIFAR10 (image classification), VDCNN
by 1.7$\times$ on Amazon Reviews (sentiment analysis), and StarGAN by
1.8$\times$ on CelebA (style transfer, i.e., GANs)."
1328,2019,Analyzing and Improving Representations with the Soft Nearest Neighbor Loss,Poster,"We exlore and exand the Soft Nearest Neighbor Loss to measure the entanglement of class manifolds in reresentation sace: i.e., how close airs of oints from the same class are relative to airs of oints from different classes. We demonstrate several use cases of the loss. As an analytical tool, it rovides insights into the evolution of class similarity structures during learning. Surrisingly, we find that maximizing the entanglement of reresentations of different classes in the hidden layers is beneficial for discrimination in the final layer, ossibly because it encourages reresentations to identify class-indeendent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to better-calibrated estimates of uncertainty on outlier data but also marginally imroved generalization. Data that is not from the training distribution can be recognized by observing that in the hidden layers, it has fewer than the normal number of neighbors from the redicted class.
"
1329,2019,What is the Effect of Importance Weighting in Deep Learning?,Poster,"Imortance-weighted risk minimization is a key ingredient in many machine learning algorithms for causal inference, domain adatation, class imbalance, and off-olicy reinforcement learning. While the effect of imortance weighting is well-characterized for low-caacity missecified models, little is known about how it imacts over-arameterized, dee neural networks. This work is insired by recent theoretical results showing that on (linearly) searable data, dee linear networks otimized by SGD learn weight-agnostic solutions, romting us to ask, for realistic dee networks, for which many ractical datasets are searable, what is the effect of imortance weighting? We resent the surrising finding that while imortance weighting imacts models early in training, its effect diminishes over successive eochs. Moreover, while L2 regularization and batch normalization (but not droout), restore some of the imact of imortance weighting, they exress the effect via (seemingly) the wrong abstraction: why should ractitioners tweak the L2 regularization, and by how much, to roduce the correct weighting effect? Our exeriments confirm these findings across 
a range of architectures and datasets.
"
1330,2019,Similarity of Neural Network Representations Revisited,Poster,"Recent work has sought to understand the behavior of neural networks by comaring reresentations between layers and between different trained models. We examine methods for comaring neural network reresentations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between reresentations of higher dimension than the number of data oints. We introduce a similarity index that measures the relationshi between reresentational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify corresondences between reresentations in networks trained from different initializations.
"
1331,2019,Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations,Poster,"Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be reresented by dense matrix-vector multilication, yet each has a secialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and imlementations is necessary, what structural rior they encode, and how much knowledge is required to automatically learn a fast algorithm for a rovided structured transform. Motivated by a characterization of fast matrix-vector multilication as roducts of sarse matrices, we introduce a arameterization of divide-and-conquer methods that is caable of reresenting a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many imortant transforms; for examle, it recovers the $O(N \log N)$ Cooley-Tukey FFT algorithm to machine recision, for dimensions $N$ u to $1024$. Furthermore, our method can be incororated as a lightweight relacement of generic matrices in machine learning ielines to learn efficient and comressible transformations. On a standard task of comressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 oints---the first time a structured aroach has done so---with 4X faster inference seed and 40X fewer arameters."
1332,2019,"Distributed, Egocentric Representations of Graphs for Detecting Critical Structures",Poster,"We study the roblem of detecting critical structures using a grah embedding model. Existing grah embedding models lack the ability to recisely detect critical structures that are secific to a task at the global scale. In this aer, we roose a novel grah embedding model, called the Ego-CNNs, that emloys the ego-convolutions convolutions at each layer and stacks u layers using an ego-centric way to detects recise critical structures efficiently. An Ego-CNN can be jointly trained with a task model and hel exlaindiscover knowledge for the task. We conduct extensive exeriments and the results show that Ego-CNNs (1) can lead to comarable task erformance as the state-of-the-art grah embedding models, (2) works nicely with CNN visualization techniques to illustrate the detected structures, and (3) is efficient and can incororate with scale-free riors, which commonly occurs in social network datasets, to further imrove the training efficiency.
"
1333,2019,Breaking the Softmax Bottleneck via Learnable Monotonic Pointwise Non-linearities,Poster,"The Softmax function on to of a final linear layer is the de facto method to outut robability distributions in neural networks. In many alications such as language models or text generation, this model has to roduce distributions over large outut vocabularies. Recently, this has been shown to have limited reresentational caacity due to its connection with the rank bottleneck in matrix factorization. However, little is known about the limitations of Linear-Softmax for quantities of ractical interest such as cross entroy or mode estimation, a direction that we exlore here. As an efficient and effective solution to alleviate this issue, we roose to learn arametric monotonic functions on to of the logits. We theoretically investigate the rank increasing caabilities of such monotonic functions. Emirically, our method imroves in two different quality metrics over the traditional Linear-Softmax layer in synthetic and real language model exeriments, adding little time or memory overhead, while being comarable to the more comutationally exensive mixture of Softmaxes.
"
1334,2019,Multi-Object Representation Learning with Iterative Variational Inference,Poster,"Human ercetion is structured around objects which form the basis for our higher-level cognition and imressive systematic generalization abilities.
Yet most work on reresentation learning focuses on feature learning without even considering multile objects, or treats segmentation as an (often suervised) rerocessing ste.
Instead, we argue for the imortance of learning to segment and reresent objects jointly.
We demonstrate that, starting from the simle assumtion that a scene is comosed of multile entities, it is ossible to learn to segment images into interretable objects with disentangled reresentations.
Our method learns -- without suervision -- to inaint occluded arts, and extraolates to scenes with more objects and to unseen objects with novel feature combinations. 
We also show that, due to the use of iterative variational inference, our system is able to learn multi-modal osteriors for ambiguous inuts and extends naturally to sequences.
"
1335,2019,Cross-Domain 3D Equivariant Image Embeddings,Poster,"Sherical convolutional networks have been introduced recently as tools to learn owerful feature reresentations of 3D shaes. Sherical CNNs are equivariant to 3D rotations making them ideally suited to alications where 3D data may be observed in arbitrary orientations. In this aer we learn 2D image embeddings with a similar equivariant structure: embedding the image of a 3D object should commute with rotations of the object.  We introduce a cross-domain embedding from 2D images into a sherical CNN latent sace. This embedding encodes images with 3D shae roerties and is equivariant to 3D rotations of the observed object. The model is suervised only by target embeddings obtained from a sherical CNN retrained for 3D shae classification. We show that learning a rich embedding for images with aroriate geometric structure is sufficient for tackling varied alications, such as relative ose estimation and novel view synthesis, without requiring additional task-secific suervision.
"
1336,2019,Loss Landscapes of Regularized Linear Autoencoders,Poster,"Autoencoders are a dee learning model for reresentation learning. When trained to minimize the distance between the data and its reconstruction, linear autoencoders (LAEs) learn the subsace sanned by the to rincial directions but cannot learn the rincial directions themselves. In this aer, we rove that $L_2$-regularized LAEs are symmetric at all critical oints and learn the rincial directions as the left singular vectors of the decoder. We smoothly arameterize the critical manifold and relate the minima to the MAP estimate of robabilistic PCA. We illustrate these results emirically and consider imlications for PCA algorithms, comutational neuroscience, and the algebraic toology of learning."
1337,2019,Hyperbolic Disk Embeddings for Directed Acyclic Graphs,Poster,"Obtaining continuous reresentations of structural data such as directed acyclic grahs (DAGs) has gained attention in machine learning and artificial intelligence. However, embedding comlex DAGs in which both ancestors and descendants of nodes are exonentially increasing is difficult. Tackling in this roblem, we develo Disk Embeddings, which is a framework for embedding DAGs into quasi-metric saces. Existing state-of-the-art methods, Order Embeddings and Hyerbolic Entailment Cones, are instances of Disk Embedding in Euclidean sace and sheres resectively. Furthermore, we roose a novel method Hyerbolic Disk Embeddings to handle exonential growth of relations. The results of our exeriments show that our Disk Embedding models outerform existing methods esecially in comlex DAGs other than trees.
"
1338,2019,LatentGNN: Learning Efficient Non-local Relations for Visual Recognition,Poster,"Caturing long-range deendencies in feature reresentations is crucial for many visual recognition tasks. Desite recent successes of dee convolutional networks, it remains challenging to model non-local context relations between visual features. A romising strategy is to model the feature context by a fully-connected grah neural network (GNN), which augments traditional convolutional features with an estimated non-local context reresentation. However, most GNN-based aroaches require comuting a dense grah affinity matrix and hence have difficulty in scaling u to tackle comlex real-world visual roblems. In this work, we roose an efficient and yet flexible non-local relation reresentation based on a novel class of grah neural networks. Our key idea is to introduce a latent sace to reduce the comlexity of grah, which allows us to use a low-rank reresentation for the grah affinity matrix and to achieve a linear comlexity in comutation. Extensive exerimental evaluations on three major visual recognition tasks show that our method outerforms the rior works with a large margin while maintaining a low comutation cost.   
"
1339,2019,Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness,Poster,"The ability to learn disentangled reresentations that slit underlying sources of variation in high dimensional, unstructured data is imortant for data efficient and robust use of neural networks. While various aroaches aiming towards this goal have been roosed in recent times, a commonly acceted definition and validation rocedure is missing. We rovide a causal ersective on reresentation learning which covers disentanglement and domain shift robustness as secial cases. Our causal framework allows us to introduce a new metric for the quantitative evaluation of dee latent variable models. We show how this metric can be estimated from labeled observational data and further rovide an efficient estimation algorithm that scales linearly in the dataset size. 
"
1340,2019,Lorentzian Distance Learning for Hyperbolic Representations,Poster,"We introduce an aroach to learn reresentations based on the Lorentzian distance in hyerbolic geometry. Hyerbolic geometry is esecially suited to hierarchically-structured datasets, which are revalent in the real world. Current hyerbolic reresentation learning methods comare examles with the Poincar\'e distance. They try to minimize the distance of each node in a hierarchy with its descendants while maximizing its distance with other nodes. This formulation roduces node reresentations close to the centroid of their descendants. To obtain efficient and interretable algorithms, we exloit the fact that the centroid w.r.t the squared Lorentzian distance can be written in closed-form. We show that the Euclidean norm of such a centroid decreases as the curvature of the hyerbolic sace decreases. This roerty makes it aroriate to reresent hierarchies where arent nodes minimize the distances to their descendants and have smaller Euclidean norm than their children. Our aroach obtains state-of-the-art results in retrieval and classification tasks on different datasets.
"
1341,2019,Batch Policy Learning under Constraints,Poster,"When learning olicies for real-world domains, two imortant questions arise: (i) how to efficiently use re-collected off-olicy, non-otimal behavior data; and (ii) how to mediate among different cometing objectives and constraints. We thus study the roblem of batch olicy learning under multile constraints, and offer a systematic solution. We first roose a flexible meta-algorithm that admits any batch reinforcement learning and online learning rocedure as subroutines. We then resent a secific algorithmic instantiation and rovide erformance guarantees for the main objective and all constraints. As art of off-olicy learning, we roose a simle method for off-olicy olicy evaluation (OPE) and derive PAC-style bounds. Our algorithm achieves strong emirical results in different domains, including in a challenging roblem of simulated car driving subject to multile constraints such as lane keeing and smooth driving. We also show exerimentally that our OPE method outerforms other oular OPE techniques on a standalone basis, esecially in a high-dimensional setting.
"
1342,2019,Quantifying Generalization in Reinforcement Learning,Poster,"In this aer, we investigate the roblem of overfitting in dee reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This ractice offers relatively little insight into an agent's ability to generalize. We address this issue by using rocedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surrisingly large training sets. We then show that deeer convolutional architectures imrove generalization, as do methods traditionally found in suervised learning, including L2 regularization, droout, data augmentation and batch normalization.
"
1343,2019,Learning Latent Dynamics for Planning from Pixels,Poster,"Planning has been very successful for control tasks with known environment dynamics. To leverage lanning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for lanning has been a long-standing challenge, esecially in image-based domains. We roose the Dee Planning Network (PlaNet), a urely model-based agent that learns the environment dynamics from images and chooses actions through fast online lanning in latent sace. To achieve high erformance, the dynamics model must accurately redict the rewards ahead for multile time stes. We aroach this using a latent dynamics model with both deterministic and stochastic transition comonents. Moreover, we roose a multi-ste variational inference objective that we name latent overshooting. Using only ixel observations, our agent solves continuous control tasks with contact dynamics, artial observability, and sarse rewards, which exceed the difficulty of tasks that were reviously solved by lanning with learned models. PlaNet uses substantially fewer eisodes and reaches final erformance close to and sometimes higher than strong model-free algorithms.
"
1344,2019,Projections for Approximate Policy Iteration Algorithms,Poster,"Aroximate olicy iteration is a class of reinforcement learning (RL) algorithms where the olicy is encoded using a function aroximator and which has been esecially rominent in RL with continuous action saces. In this class of RL algorithms, ensuring increase of the olicy return during olicy udate often requires to constrain the change in action distribution. Several aroximations exist in the literature to solve this constrained olicy udate roblem. In this aer, we roose to imrove over such solutions by introducing a set of rojections that transform the constrained roblem into an unconstrained one which is then solved by standard gradient descent. Using these rojections, we emirically demonstrate that our aroach can imrove the olicy udate solution and the control over exloration of existing aroximate olicy iteration algorithms.
"
1345,2019,Learning Structured Decision Problems with Unawareness,Poster,"Structured models of decision making often assume an agent is aware of all ossible states and actions in advance. This assumtion is sometimes untenable. In this aer, we learn Bayesian Decision Networks from both domain exloration and exert assertions in a way which guarantees convergence to otimal behaviour, even when the agent starts unaware of actions or belief variables that are critical to success. Our exeriments show that our agent learns otimal behaviour on both small and large decision roblems, and that allowing an agent to conserve information uon making new discoveries results in faster convergence.
"
1346,2019,Calibrated Model-Based Deep Reinforcement Learning,Poster,"Estimates of redictive uncertainty are imortant for accurate model-based lanning and reinforcement learning. However, redictive uncertainties --- esecially ones derived from modern dee learning systems --- can be inaccurate and imose a bottleneck on erformance. This aer exlores which uncertainties are needed for model-based reinforcement learning and argues that ideal uncertainties should be calibrated, i.e. their robabilities should match emirical frequencies of redicted events. We describe a simle way to augment any model-based reinforcement learning agent with a calibrated model and show that doing so consistently imroves lanning, samle comlexity, and exloration. On the \textsc{HalfCheetah} MuJoCo task, our system achieves state-of-the-art erformance using 50\% fewer samles than the current leading aroach. Our findings suggest that calibration can imrove the erformance of model-based reinforcement learning with minimal comutational and imlementation overhead.
"
1347,2019,Reinforcement Learning in Configurable Continuous Environments,Poster,"Configurable Markov Decision Processes (Conf-MDPs) have been recently introduced as an extension of the usual MDP model to account for the ossibility of configuring the environment to imrove the agent's erformance. Currently, there is still no suitable algorithm to solve the learning roblem for real-world Conf-MDPs. In this aer, we fill this ga by roosing a trust-region method, Relative Entroy Model Policy Search (REMPS), able to learn both the olicy and the MDP configuration in continuous domains without requiring the knowledge of the true model of the environment. After introducing our aroach and roviding a finite-samle analysis, we emirically evaluate REMPS on both benchmark and realistic environments by comaring our results with those of the gradient methods.
"
1348,2019,Target-Based Temporal-Difference Learning,Poster,"The use of target networks has been a oular and key comonent of recent dee Q-learning algorithms for reinforcement learning, yet little is known from the theory side. In this work, we introduce a new family of target-based temoral difference (TD) learning algorithms that maintain two searate learning arameters – the target variable and online variable. We roose three members in the family, the averaging TD, double TD, and eriodic TD, where the target variable is udated through an averaging, symmetric, or eriodic fashion, resectively, mirroring those techniques used in dee Q-learning ractice. We establish asymtotic convergence analyses for both averaging TD and double TD and a finite samle
analysis for eriodic TD. In addition, we rovide some simulation results showing otentially suerior convergence of these target-based TD algorithms comared to the standard TD-learning. While this work focuses on linear function aroximation
and olicy evaluation setting, we consider this as a meaningful ste towards the theoretical understanding of dee Q-learning variants with target networks.
"
1349,2019,Iterative Linearized Control: Stable Algorithms and Complexity Guarantees,Poster,"We examine oular gradient-based algorithms for nonlinear control in the light of the modern comlexity analysis of first-order otimization algorithms. 
The examination reveals that the comlexity bounds can be clearly stated in terms of calls to a comutational oracle related to dynamic rogramming 
and imlementable by gradient back-roagation using machine learning software libraries such as PyTorch or TensorFlow. Finally, we roose a regularized Gauss-Newton algorithm enjoying worst-case comlexity bounds and imroved convergence behavior in ractice. The software library based on PyTorch is ublicly available. 
"
1350,2019,Finding Options that Minimize Planning Time,Poster,"We formalize the roblem of selecting the otimal set of otions for lanning as that of  comuting the smallest set of otions so that lanning converges in less than a given maximum of value-iteration asses. We first show that the roblem is  $\NP$-hard, even if the task is constrained to be deterministic---the first such comlexity result for otion discovery. We then resent the first olynomial-time boundedly subotimal aroximation algorithm for this setting, and emirically evaluate it against both the otimal otions and a reresentative collection of heuristic aroaches in simle grid-based domains."
1351,2019,Stochastic Beams and Where To Find Them: The Gumbel-Top-k Trick for Sampling Sequences Without Replacement,Poster,"The well-known Gumbel-Max trick for samling from a categorical distribution can be extended to samle $k$ elements without relacement. We show how to imlicitly aly this 'Gumbel-To-$k$' trick on a factorized distribution over sequences, allowing to draw exact samles without relacement using a Stochastic Beam Search. Even for exonentially large domains, the number of model evaluations grows only linear in $k$ and the maximum samled sequence length. The algorithm creates a theoretical connection between samling and (deterministic) beam search and can be used as a rinciled intermediate alternative. In a translation task, the roosed method comares favourably against alternatives to obtain diverse yet good quality translations. We show that sequences samled without relacement can be used to construct low-variance estimators for exected sentence-level BLEU score and model entroy."
1352,2019,Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs,Poster,"We study the roblem of knowledge grah (KG) embedding. A widely-established assumtion to this roblem is that similar entities are likely to have similar relational roles. However, existing related methods derive KG embeddings mainly based on trile-level learning, which lack the caability of caturing long-term relational deendencies of entities. Moreover, trile-level learning is insufficient for the roagation of semantic information among entities, esecially for the case of cross-KG embedding. In this aer, we roose recurrent skiing networks (RSNs), which emloy a skiing mechanism to bridge the gas between entities. RSNs integrate recurrent neural networks (RNNs) with residual learning to efficiently cature the long-term relational deendencies within and between KGs. We design an end-to-end framework to suort RSNs on different tasks. Our exerimental results showed that RSNs outerformed state-of-the-art embedding-based methods for entity alignment and achieved cometitive erformance for KG comletion.
"
1353,2019,Meta-Learning Neural Bloom Filters,Poster,"There has been a recent trend in training neural networks to relace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater comression. In this setting, a neural data structure is instantiated by training a network over many eochs of its inuts until convergence. In alications where inuts arrive at high throughut, or are ehemeral, training a network from scratch is not ractical. This motivates the need for few-shot neural data structures. In this aer we exlore the learning of aroximate set membershi over a set of data in one-shot via meta-learning. We roose a novel memory architecture, the Neural Bloom Filter, which is able to achieve significant comression gains over classical Bloom Filters and existing memory-augmented neural networks.
"
1354,2019,CoT: Cooperative Training for Generative Modeling of Discrete Data,Poster,"In this aer, we study the generative models of sequential discrete data. To tackle the exosure bias roblem inherent in maximum likelihood estimation (MLE), generative adversarial networks (GANs) are introduced to enalize the unrealistic generated samles. To exloit the suervision signal from the discriminator, most revious models leverage REINFORCE to address the non-differentiable roblem of sequential discrete data. However, because of the unstable roerty of the training signal during the dynamic rocess of adversarial training, the effectiveness of REINFORCE, in this case, is hardly guaranteed. To deal with such a roblem, we roose a novel aroach called Cooerative Training (CoT) to imrove the training of sequence generative models. CoT transforms the min-max game of GANs into a joint maximization framework and manages to exlicitly estimate and otimize Jensen-Shannon divergence. Moreover, CoT works without the necessity of re-training via MLE, which is crucial to the success of revious methods. In the exeriments, comared to existing state-of-the-art methods, CoT shows suerior or at least cometitive erformance on samle quality, diversity, as well as training stability. 
"
1355,2019,Non-Monotonic Sequential Text Generation,Poster,"Standard sequential generation methods assume a re-secified generation order, such as text generation methods which generate words from left to right. In this work, we roose a framework for training models of text generation that oerate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework oerates by generating a word at an arbitrary osition, and then recursively generating words to its left and then words to its right, yielding a binary tree. Learning is framed as imitation learning, including a coaching method which moves from imitating an oracle to reinforcing the olicy's own references. Exerimental results demonstrate that using the roosed method, it is ossible to learn olicies which generate text without re-secifying a generation order, while achieving cometitive erformance with conventional left-to-right generation.
"
1356,2019,Insertion Transformer: Flexible Sequence Generation via Insertion Operations,Poster,"We resent the Insertion Transformer, an iterative, artially autoregressive model for sequence generation based on insertion oerations. Unlike tyical autoregressive models which rely on a fixed, often left-to-right ordering of the outut, our aroach accommodates arbitrary orderings by allowing for tokens to be inserted anywhere in the sequence during decoding. This flexibility confers a number of advantages: for instance, not only can our model be trained to follow secific orderings such as left-to-right generation or a binary tree traversal, but it can also be trained to maximize entroy over all valid insertions for robustness. In addition, our model seamlessly accommodates both fully autoregressive generation (one insertion at a time) and artially autoregressive generation (simultaneous insertions at multile locations). We validate our aroach by analyzing its erformance on the WMT 2014 English-German machine translation task under various settings for training and decoding. We find that the Insertion Transformer outerforms many rior non-autoregressive aroaches to translation at comarable or better levels of arallelism, and successfully recovers the erformance of the original Transformer while requiring only logarithmically many iterations during decoding.
"
1357,2019,Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models,Poster,"Beam search is the most oular inference algorithm for decoding neural sequence models. Unlike greedy search, beam search allows for non-greedy local decisions that can otentially lead to a sequence with a higher overall robability. However, work on a number of alications has found that the quality of the highest robability hyothesis found by beam search degrades with large beam widths. We erform an emirical study of the behavior of beam search across three sequence synthesis tasks. We find that increasing the beam width leads to sequences that are disroortionately based on early, very low robability tokens that are followed by a sequence of tokens with higher (conditional) robability. We show that, emirically, such sequences are more likely to have a lower evaluation score than lower robability sequences without this attern. Using the notion of search discreancies from heuristic search, we hyothesize that large discreancies are the cause of the erformance degradation. We show that this hyothesis generalizes the revious ones in machine translation and image cationing. To validate our hyothesis, we show that constraining beam search to avoid large discreancies eliminates the erformance degradation.
"
1358,2019,Trainable Decoding of Sets of Sequences for Neural Sequence Models,Poster,"Many sequence rediction tasks admit multile correct oututs and so, it is often useful to decode a set of oututs that maximize some task-secific set-level metric. However, retooling standard sequence rediction rocedures tailored towards redicting the single best outut leads to the decoding of sets containing very similar sequences; failing to cature the variation in the outut sace. To address this, we roose $\nabla$BS, a trainable decoding rocedure that oututs a set of sequences, highly valued according to the metric. Our method tightly integrates the training and decoding hases and further allows for the otimization of the task-secific metric addressing the shortcomings of standard sequence rediction. Further, we discuss the trade-offs of commonly used set-level metrics and motivate a new set-level metric that naturally evaluates the notion of ``caturing the variation in the outut sace''. Finally, we show results on the image cationing task and find that our model outerforms standard techniques and natural ablations."
1359,2019,Learning to Generalize from Sparse and Underspecified Rewards,Poster,"We consider the roblem of learning from sarse and undersecified rewards, where an agent receives a comlex inut, such as a natural language instruction, and needs to generate a comlex resonse, such as an action sequence, while only receiving binary success-failure feedback.  Such success-failure rewards are often undersecified: they do not distinguish between uroseful and accidental
success. Generalization from undersecified rewards hinges on discounting surious trajectories that attain accidental success, while learning from sarse feedback requires effective exloration. We address exloration by using a mode covering direction of KL divergence to collect a diverse set of successful trajectories, followed by a mode seeking KL divergence to train a robust olicy.  We
roose Meta Reward Learning (MeRL) to construct an auxiliary reward function that rovides more refined feedback for learning.  The arameters of the auxiliary reward function are otimized with resect to the validation erformance of a trained olicy. The MeRL aroach outerforms an alternative method for reward learning based on Bayesian Otimization, and achieves the state-of-the-art on
weakly-suervised semantic arsing. It imroves revious work by 1.2% and 2.4% on WikiTableQuestions and WikiSQL datasets resectively.
"
1360,2019,Efficient Training of BERT by Progressively Stacking,Poster,"Unsuervised re-training is oularly used in natural language rocessing. By designing roer unsuervised rediction tasks, a dee neural network can be trained and shown to be effective in many downstream tasks. As the data is usually adequate, the model for re-training is generally huge and contains millions of arameters. Therefore, the training efficiency becomes a critical issue even when using high-erformance hardware. In this aer, we exlore an efficient training method for the state-of-the-art bidirectional Transformer (BERT) model. By visualizing the self-attention distribution of different layers at different ositions in a well-trained BERT model, we find that in most layers, the self-attention distribution will concentrate locally around its osition and the start-of-sentence token. Motivating from this, we roose the stacking algorithm to transfer knowledge from a shallow model to a dee model; then we aly stacking rogressively to accelerate BERT training. The exerimental results showed that the models trained by our training strategy achieve similar erformance to models trained from scratch, but our algorithm is much faster.
"
1361,2019,Decentralized Exploration in Multi-Armed Bandits,Poster,"We consider the decentralized exloration roblem: a set of layers collaborate to identify the best arm by asynchronously interacting with the same stochastic environment. The objective is to insure rivacy in the best arm identification roblem between asynchronous, collaborative, and thrifty layers. In the context of a digital service, we advocate that this decentralized aroach allows a good balance between conflicting interests: the roviders otimize their services, while
rotecting rivacy of users and saving resources. We define the rivacy level as the amount of information an adversary could infer by interceting
all the messages concerning a single user. We rovide a generic algorithm DECENTRALIZED ELIMINATION, which uses any best arm identification
algorithm as a subroutine. We rove that this algorithm insures rivacy, with a low communication cost, and that in comarison to the lower bound of the best arm identification roblem, its samle comlexity suffers from a enalty deending on the inverse of the robability of the most frequent layers. Then, thanks to the genericity of the aroach, we extend the roosed algorithm to the non-stationary bandits. Finally, exeriments illustrate and comlete the analysis.
"
1362,2019,Warm-starting Contextual Bandits: Robustly Combining Supervised and Bandit Feedback,Poster,"We investigate the feasibility of learning from both fully-labeled suervised data and contextual bandit data. We secifically consider settings in which the underlying learning signal may be different between these two data sources. Theoretically, we state and rove no-regret algorithms for learning that is robust to divergences between the two sources. Emirically, we evaluate some of these algorithms on a large selection of datasets, showing that our aroaches are feasible, and helful in ractice.
"
1363,2019,Exploiting structure of uncertainty for efficient matroid semi-bandits,Poster,"We imrove the efficiency of algorithms for stochastic combinatorial semi-bandits. In most interesting roblems, state-of-the-art algorithms take advantage of structural roerties of rewards, such as indeendence. However, while being minimax otimal in terms of regret, these algorithms are intractable. In our aer, we first reduce their imlementation to a secific submodular maximization. Then, in case of matroid constraints, we design adated aroximation routines, thereby roviding the first efficient algorithms that exloit the reward structure. In articular, we imrove the state-of-the-art efficient ga-free regret bound by a factor sqrt(k), where k is the maximum action size. Finally, we show how our imrovement translates to more general budgeted combinatorial semi-bandits.
"
1364,2019,PAC Identification of Many Good Arms in Stochastic Multi-Armed Bandits,Poster,"We consider the roblem of identifying any k out of the best m arms in an n-armed stochastic multi-armed bandit; framed in the PAC setting, this articular roblem generalises both the roblem of “best subset selection” (Kalyanakrishnan &am; Stone, 2010) and that of selecting “one out of the best m” arms (Roy Chaudhuri &am; Kalyanakrishnan, 2017). We resent a lower bound on the worst-case samle comlexity for general k, and a fully sequential PAC algorithm, LUCB-k-m, which is more samle-efficient on easy instances. Also, extending our analysis to infinite-armed bandits, we resent a PAC algorithm that is indeendent of n, which identifies an arm from the best ρ fraction of arms using at most an additive oly-log number of samles than comared to the lower bound, thereby imroving over Roy Chaudhuri &am; Kalyanakrishnan (2017) and Aziz et al. (2018). The roblem of identifying k &gt; 1 distinct arms from the best ρ fraction is not always well-defined; for a secial class of this roblem, we resent lower and uer bounds. Finally, through a reduction, we establish a relation between uer bounds for the “one out of the best ρ” roblem for infinite instances and the “one out of the best m” roblem for finite instances. We conjecture that it is more efficient to solve “small” finite instances using the latter formulation, rather than going through the former.
"
1365,2019,Contextual Multi-armed Bandit Algorithm for Semiparametric Reward Model,Poster,"Contextual multi-armed bandit (MAB) algorithms have been shown romising for maximizing cumulative rewards in sequential decision tasks such as news article recommendation systems, web age ad lacement algorithms, and mobile health. However, most of the roosed contextual MAB algorithms assume linear relationshis between the reward and the context of the action. This aer rooses a new contextual MAB algorithm for a relaxed, semiarametric reward model that suorts nonstationarity. The roosed method is less restrictive, easier to imlement and faster than two alternative algorithms that consider the same model, while achieving a tight regret uer bound. We rove that the high-robability uer bound of the regret incurred by the roosed algorithm has the same order as the Thomson samling algorithm for linear reward models. The roosed and existing algorithms are evaluated via simulation and also alied to Yahoo! news article recommendation log data.
"
1366,2019,Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning,Poster,"When observing the actions of others, humans make inferences about why they acted as they did, and what this imlies about the world; humans also use the fact that their actions will be interreted in this manner, allowing them to act informatively and thereby communicate efficiently with others. Although learning algorithms have recently achieved suerhuman erformance in a number of two-layer, zero-sum games, scalable multi-agent reinforcement learning algorithms that can discover effective strategies and conventions in comlex, artially observable settings have roven elusive. We resent the \emh{Bayesian action decoder} (BAD), a new multi-agent learning method that uses an aroximate Bayesian udate to obtain a ublic belief that conditions on the actions taken by all agents in the environment. BAD introduces a new Markov decision rocess, the \emh{ublic belief MDP}, in which the action sace consists of all deterministic artial olicies, and exloits the fact that an agent acting only on this ublic belief state can still learn to use its rivate information if the action sace is augmented to be over all artial olicies maing rivate information into environment actions. The Bayesian udate is closely related to the \emh{theory of mind} reasoning that humans carry out when observing others' actions. We first validate BAD on a roof-of-rincile two-ste matrix game, where it outerforms olicy gradient methods; we then evaluate BAD on the challenging, cooerative artial-information card game Hanabi, where, in the two-layer setting, it surasses all reviously ublished learning and hand-coded aroaches, establishing a new state of the art.
"
1367,2019,TarMAC: Targeted Multi-Agent Communication,Poster,"We roose a targeted communication architecture for multi-agent reinforcement learning, where agents learn both \emh{what} messages to send and \emh{whom} to address them to while erforming cooerative tasks in artially-observable environments. This targeting behavior is learnt solely from downstream task-secific reward without any communication suervision. We additionally augment this with a multi-round communication aroach where agents coordinate via multile rounds of communication before taking actions in the environment.

We evaluate our aroach on a diverse set of cooerative multi-agent tasks, of varying difficulties, with varying number of agents, in a variety of environments ranging from 2D grid layouts of shaes and simulated traffic junctions to 3D indoor environments, and demonstrate the benefits of targeted and multi-round communication. Moreover, we show that the targeted communication strategies learned by agents are interretable and intuitive.

Finally, we show that our architecture can be easily extended to mixed and cometitive environments, leading to imroved erformance and samle comlexity over recent state-of-the-art aroaches.
"
1368,2019,QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning,Poster,"We exlore value-based solutions for multi-agent reinforcement learning (MARL) tasks in the centralized training with decentralized execution (CTDE) regime oularized recently. However, VDN and QMIX are reresentative examles that use the idea of factorization of the joint action-value function into individual ones for decentralized execution. VDN and QMIX address only a fraction of factorizable MARL tasks due to their structural constraint in factorization such as additivity and monotonicity. In this aer, we roose a new factorization method for MARL, QTRAN, which is free from such structural constraints and takes on a new aroach to transforming the original joint action-value function into an easily factorizable one, with the same otimal actions. QTRAN guarantees more general factorization than VDN or QMIX, thus covering a much wider class of MARL tasks than does revious methods. Our exeriments for the tasks of multi-domain Gaussian-squeeze and modified redator-rey demonstrate  QTRAN's suerior erformance with esecially larger margins in games whose ayoffs enalize non-cooerative behavior more aggressively. 
"
1369,2019,Actor-Attention-Critic for Multi-Agent Reinforcement Learning,Poster,"Reinforcement learning in multi-agent scenarios is imortant for real-world alications but resents challenges beyond those seen in single-agent settings. We resent an actor-critic algorithm that trains decentralized olicies in multi-agent settings, using centrally comuted critics that share an attention mechanism which selects relevant information for each agent at every timeste. This attention mechanism enables more effective and scalable learning in comlex multi-agent environments, when comared to recent aroaches. Our aroach is alicable not only to cooerative settings with shared rewards, but also individualized reward settings, including adversarial settings, as well as settings that do not rovide global states, and it makes no assumtions about the action saces of the agents. As such, it is flexible enough to be alied to most multi-agent learning roblems.
"
1370,2019,Finite-Time Analysis of Distributed TD(0) with Linear Function Approximation on Multi-Agent Reinforcement Learning,Poster,"We study the olicy evaluation roblem in multi-agent reinforcement learning. In this roblem, a grou of agents works cooeratively to evaluate the value function for the global discounted accumulative reward roblem, which is comosed of local rewards observed by the agents. Over a series of time stes, the agents act, get rewarded, udate their local estimate of the value function, then communicate with their neighbors.  The local udate at each agent can be interreted as a distributed consensus-based variant of the oular temoral difference learning algorithm TD(0). While distributed reinforcement learning algorithms have been resented in the literature, almost nothing is known about their convergence rate.  Our main contribution is roviding a finite-time analysis for the convergence of the distributed TD(0) algorithm. We do this when the communication network between the agents is time-varying in general. We obtain an exlicit uer bound on the rate of convergence of this algorithm as a function of the network toology and the discount factor. Our results mirror what we would exect from using distributed stochastic gradient descent for solving convex otimization roblems. 
"
1371,2019,Neural Network Attributions: A Causal Perspective,Poster,"We roose a new attribution method for neural networks develoed using ﬁrst rinciles of causality (to the best of our knowledge, the ﬁrst such). The neural network architecture is viewed as a Structural Causal Model, and a methodology to comute the causal effect of each feature on the outut is resented. With reasonable assumtions on the causal structure of the inut data, we roose algorithms to efﬁciently comute the causal effects, as well as scale the aroach to data with large dimensionality. We also show how this method can be used for recurrent neural networks. We reort exerimental results on both simulated and real datasets showcasing the romise and usefulness of the roosed algorithm.
"
1372,2019,Towards a Deep and Unified Understanding of Deep Neural Models in NLP,Poster,"We define a unified information-based measure to rovide quantitative exlanations on how intermediate layers of dee Natural Language Processing
(NLP) models leverage information of inut words. Our method advances existing exlanation methods by addressing issues in coherency and generality. Exlanations generated by using our method are consistent and faithful across different timestams, layers, and models. We show how our method can be alied to four widely used models in NLP and exlain their erformances on three real-world benchmark datasets.
"
1373,2019,Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley Value Approximation,Poster,"The roblem of exlaining the behavior of dee neural networks has recently gained a lot of attention. While several attribution methods have been roosed, most come without strong theoretical foundations, which raises questions about their reliability. On the other hand, the literature on cooerative game theory suggests Shaley values as a unique way of assigning relevance scores such that certain desirable roerties are satisfied. Unfortunately, the exact evaluation of Shaley values is rohibitively exensive, exonential in the number of inut features. In this work, by leveraging recent results on uncertainty roagation, we roose a novel, olynomial-time aroximation of Shaley values in dee neural networks. We show that our method roduces significantly better aroximations of Shaley values than existing state-of-the-art attribution methods.
"
1374,2019,Functional Transparency for Structured Data: a Game-Theoretic Approach,Poster,"We rovide a new aroach to training neural models to exhibit transarency in a well-defined, functional manner. Our aroach naturally oerates over structured data and tailors the redictor, functionally, towards a chosen family of (local) witnesses. The estimation roblem is setu as a co-oerative game between an unrestricted \emh{redictor} such as a neural network, and a set of \emh{witnesses} chosen from the desired transarent family. The goal of the witnesses is to highlight, locally, how well the redictor conforms to the chosen family of functions, while the redictor is trained to minimize the highlighted discreancy. We emhasize that the redictor remains globally owerful as it is only encouraged to agree locally with locally adated witnesses. We analyze the effect of the roosed aroach, rovide examle formulations in the context of dee grah and sequence models, and emirically illustrate the idea in chemical roerty rediction, temoral modeling, and molecule reresentation learning.
"
1375,2019,Exploring interpretable LSTM neural networks over multi-variable data,Poster,"For recurrent neural networks trained on time series with target and exogenous variables, in addition to accurate rediction, it is also desired to rovide interretable insights into the data. 
In this aer, we exlore the structure of LSTM recurrent neural networks to learn variable-wise hidden states, with the aim to cature different dynamics in multi-variable time series and distinguish the contribution of variables to the rediction.
With these variable-wise hidden states, a mixture attention mechanism is roosed to model the generative rocess of the target. Then we develo associated training methods to jointly learn network arameters, variable and temoral imortance w.r.t the rediction of the target variable.
Extensive exeriments on real datasets demonstrate enhanced rediction erformance by caturing the dynamics of different variables.
Meanwhile, we evaluate the interretation results both qualitatively and quantitatively.
It exhibits the rosect as an end-to-end framework for both forecasting and knowledge extraction over multi-variable data.
"
1376,2019,TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing,Poster,"Neural networks are difficult to interret and debug. We introduce testing techniques for neural networks that
can discover errors occurring only for rare inuts. Secifically, we develo coverage-guided fuzzing (CGF)
methods for neural networks. In CGF, random mutations of inuts are guided by a coverage metric toward the
goal of satisfying user-secified constraints. We describe how aroximate nearest neighbor (ANN) algorithms
can rovide this coverage metric for neural networks. We then combine these methods with techniques for
roerty-based testing (PBT). In PBT, one asserts roerties that a function should satisfy and the system
automatically generates tests exercising those roerties. We then aly this system to ractical goals including
(but not limited to) surfacing broken loss functions in oular GitHub reositories and making erformance
imrovements to TensorFlow. Finally, we release an oen source library called TensorFuzz that imlements the
described techniques.
"
1377,2019,Gaining Free or Low-Cost Interpretability with Interpretable Partial Substitute,Poster,"This work addresses the situation where a black-box model with good redictive erformance is chosen over its interretable cometitors, and we show interretability is still achievable in this case. Our solution is to find an interretable substitute on a subset of data where the black-box model is \emh{overkill} or nearly overkill while leaving the rest to the black-box.  This transarency is obtained at minimal cost or no cost of the redictive erformance. Under this framework, we develo a Hybrid Rule Sets (HyRS)  model that uses decision rules to cature the subsace of data where the rules are as accurate or almost as accurate as the black-box rovided. To train a HyRS, we devise an efficient search algorithm that iteratively finds the otimal model and exloits theoretically grounded strategies to reduce comutation. Our framework is \emh{agnostic} to the black-box during training. Exeriments on structured and text data show that HyRS obtains an effective trade-off between transarency and interretability.
"
1378,2019,State-Regularized Recurrent Neural Networks,Poster,"Recurrent neural networks are a widely used class of neural architectures with two shortcomings. First, it is difficult to understand what exactly they learn. Second, they tend to work oorly on sequences requiring long-term memorization, desite having this caacity in rincile. We aim to address both shortcomings with a class of recurrent networks that use a stochastic state transition mechanism between cell alications. This mechanism, which we term state-regularization, makes RNNs transition between a finite set of learnable states. We evaluate state-regularized RNNs on (1) regular languages for the urose of automata extraction; (2) nonregular languages such as balanced arentheses, alindromes, and the coy task where external memory is required; and (3) real-word sequence learning tasks for sentiment analysis, visual object recognition, and language modeling. We show that state-regularization simlifies the extraction of finite state automata from the RNN's state transition dynamics; forces RNNs to oerate more like automata with external memory and less like finite state machines; and makes RNNs more interretable. 
"
1379,2019,Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation,Poster,"Current saliency ma interretations for neural networks generally rely on two key assumtions. First, they use first-order aroximations of the loss function, neglecting higher-order terms such as the loss curvature. Second, they evaluate each feature’s imortance in isolation, ignoring feature interdeendencies. This work studies the effect of relaxing these two assumtions. First, we characterize a closed-form formula for the inut Hessian matrix of a dee ReLU network. Using this formula, we show that, for classification roblems with many classes, if a rediction has high robability then including the Hessian term has a small imact on the interretation. We rove this result by demonstrating that these conditions cause the Hessian matrix to be aroximately rank one and its leading eigenvector to be almost arallel to the gradient of the loss. We emirically validate this theory by interreting ImageNet classifiers. Second, we incororate feature interdeendencies by calculating the imortance of grou-features using a sarsity regularization term. We use an L0 − L1 relaxation technique along with roximal gradient descent to efficiently comute grou-feature imortance values. Our emirical results show that our method significantly imroves dee learning interretations.
"
1380,2019,On the Connection Between Adversarial Robustness and Saliency Map Interpretability,Poster,"Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interretable saliency mas than their non-robust counterarts. We aim to quantify this behaviour by considering the alignment between inut image and saliency ma. We hyothesize that as the distance to the decision boundary grows, so does the alignment. This connection is strictly true in the case of linear models. We confirm these theoretical findings with exeriments based on models trained with a local Lischitz regularization and identify where the nonlinear nature of neural networks weakens the relation.
"
1381,2019,Why do Larger Models Generalize Better? A Theoretical Perspective via the XOR Problem,Poster,"Emirical evidence suggests that neural networks with ReLU activations generalize better with over-arameterization. However, there is currently no theoretical analysis that exlains this observation. In this work, we rovide theoretical and emirical evidence that, in certain cases, overarameterized convolutional networks generalize better than small networks because of an interlay between weight clustering and feature exloration at initialization.  We demonstrate this theoretically for a 3-layer convolutional neural network with max-ooling, in a novel setting which extends the XOR roblem. We show that this interlay imlies that with overaramterization, gradient descent converges to global minima with better generalization erformance comared to global minima of small networks. Emirically, we demonstrate these henomena for a 3-layer convolutional neural network in the MNIST task.
"
1382,2019,On the Spectral Bias of Neural Networks,Poster,"Neural networks are known to be a class of highly exressive functions able to fit even random inut-outut maings with 100% accuracy. In this work we resent roerties of neural networks that comlement this asect of exressivity. By using tools from Fourier analysis, we highlight a learning bias of dee networks towards low frequency functions -- i.e. functions that vary globally without local fluctuations -- which manifests itself as a frequency-deendent learning seed. Intuitively, this roerty is in line with the observation that over-arameterized networks rioritize learning simle atterns that generalize across data samles. We also investigate the role of the shae of the data manifold by resenting emirical and theoretical evidence that, somewhat counter-intuitively, learning higher frequencies gets easier with increasing manifold comlexity.
"
1383,2019,Recursive Sketches for Modular Deep Learning,Poster,"We resent a mechanism to comute a sketch (succinct summary) of how a comlex modular dee network rocesses its inuts. The sketch summarizes essential information about the inuts and oututs of the network and can be used to quickly identify key comonents and summary statistics of the inuts. Furthermore, the sketch is recursive and can be unrolled to identify sub-comonents of these comonents and so forth, caturing a otentially comlicated DAG structure. These sketches erase gracefully; even if we erase a fraction of the sketch at random, the remainder still retains the codehigh-weight'' information resent in the original sketch. The sketches can also be organized in a reository to imlicitly form acodeknowledge grah''; it is ossible to quickly retrieve sketches in the reository that are related to a sketch of interest; arranged in this fashion, the sketches can also be used to learn emerging concets by looking for new clusters in sketch sace. Finally, in the scenario where we want to learn a ground truth dee network, we show that augmenting inutoutut airs with these sketches can theoretically make it easier to do so.
"
1384,2019,Zero-Shot Knowledge Distillation in Deep Networks,Poster,"Knowledge distillation deals with the roblem of training a smaller model (\emh{Student}) from a high caacity source model (\emh{Teacher}) so as to retain most of its erformance. Existing aroaches use either the training data or meta-data extracted from it in order to train the \emh{Student}. However, accessing the dataset on which the \emh{Teacher} has been trained may not always be feasible if the dataset is very large or it oses rivacy or safety concerns (e.g., bio-metric or medical data). Hence, in this aer, we roose a novel data-free method to train the \emh{Student} from the \emh{Teacher}. Without even using any meta-data, we synthesize the \emh{Data Imressions} from the comlex \emh{Teacher} model and utilize these as surrogates for the original training data samles to transfer its learning to \emh{Student} via knowledge distillation. We, therefore, dub our method ``Zero-Shot Knowledge Distillation"" and demonstrate that our framework results in cometitive generalization erformance as achieved by distillation using the actual training data samles on multile benchmark datasets.
"
1385,2019,A Convergence Theory for Deep Learning via Over-Parameterization,Poster,"Dee neural networks (DNNs) have demonstrated dominating erformance in many fields; since AlexNet, networks used in ractice are going wider and deeer.
On the theoretical side, a long line of works have been focusing on why we can train neural networks when there is only one hidden layer. The theory of multi-layer networks remains unsettled. In this work, we rove simle algorithms such as stochastic gradient descent (SGD) can find Global Minima on the training objective of DNNs in Polynomial Time. We only make two assumtions: the inuts do not degenerate and the network is over-arameterized. The latter means the number of hidden neurons is sufficiently large: olynomial in L, the number of DNN layers and in n, the number of training samles. As concrete examles, starting from randomly initialized weights, we show that SGD attains 100% training accuracy in classification tasks, or minimizes regression loss in linear convergence seed es ~ e^{-T}, with running time olynomial in n and L. Our theory alies to the widely-used but non-smooth ReLU activation, and to any smooth and ossibly non-convex loss functions. In terms of network architectures, our theory at least alies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).
"
1386,2019,A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks,Poster,"The gradient noise (GN) in the stochastic gradient descent (SGD) algorithm is often considered to be Gaussian in the large data regime by assuming that the classical central limit theorem (CLT) kicks in. This assumtion is often made for mathematical convenience, since it enables SGD to be analyzed as a stochastic differential equation (SDE) driven by a Brownian motion. We argue that the Gaussianity assumtion might fail to hold in dee learning settings and hence render the Brownian motion-based analyses inaroriate. Insired by non-Gaussian natural henomena, we consider the GN in a more general context and invoke the generalized CLT (GCLT), which suggests that the GN converges to a heavy-tailed $\alha$-stable random variable. Accordingly, we roose to analyze SGD as an SDE driven by a L\'{e}vy motion. Such SDEs can incur `jums', which force the SDE transition from narrow minima to wider minima, as roven by existing metastability theory. To validate the $\alha$-stable assumtion, we conduct exeriments on common dee learning scenarios and show that in all settings, the GN is highly non-Gaussian and admits heavy-tails. We investigate the tail behavior in varying network architectures and sizes, loss functions, and datasets. Our results oen u a different ersective and shed more light on the belief that SGD refers wide minima."
1387,2019,Approximation and non-parametric estimation of ResNet-type convolutional neural networks,Poster,"Convolutional neural networks (CNNs) have been shown to achieve otimal aroximation and estimation error rates (in minimax sense) in several function classes. However, revious analyzed otimal CNNs are unrealistically wide and difficult to obtain via otimization due to sarse constraints in imortant function classes, including the H\""older class. We show a ResNet-tye CNN can attain the minimax otimal error rates in these classes in more lausible situations -- it can be dense, and its width, channel size, and filter size are constant with resect to samle size. The key idea is that we can relicate the learning ability of Fully-connected neural networks (FNNs) by tailored CNNs, as long as the FNNs have \textit{block-sarse} structures. Our theory is general in a sense that we can automatically translate any aroximation rate achieved by block-sarse FNNs into that by CNNs. As an alication, we derive aroximation and estimation error rates of the aformentioned tye of CNNs for the Barron and H\""older classes with the same strategy.
"
1388,2019,Global Convergence of Block Coordinate Descent in Deep Learning,Poster,"Dee learning has aroused extensive attention due to its great emirical success. The efficiency of the block coordinate descent (BCD) methods has been recently demonstrated in dee neural network (DNN) training. However, theoretical studies on their convergence roerties are limited due to the highly nonconvex nature of DNN training. In this aer, we aim at roviding a general methodology for rovable convergence guarantees for this tye of methods. In articular, for most of the commonly used DNN training models involving both two- and three-slitting schemes, we establish the global convergence to a critical oint at a rate of ${\cal O}(1k)$, where $k$ is the number of iterations. The results extend to general loss functions which have Lischitz continuous gradients and dee residual networks (ResNets). Our key develoment adds several new elements to the Kurdyka-Lojasiewicz inequality framework that enables us to carry out the global convergence analysis of BCD in the general scenario of dee learning. "
1389,2019,Measurements of Three-Level Hierarchical Structure in the Outliers in the Spectrum of Deepnet Hessians,Poster,"We exose a structure in dee classifying neural networks in the derivative of the logits with resect to the arameters of the model, which is used to exlain the existence of outliers in the sectrum of the Hessian. Previous works decomosed the Hessian into two comonents, attributing the outliers to one of them, the so-called Covariance of gradients. We show this term is not a Covariance but a second moment matrix, i.e., it is influenced by means of gradients. These means ossess an additive two-way structure that is the source of the outliers in the sectrum. This structure can be used to aroximate the rincial subsace of the Hessian using certain ""averaging"" oerations, avoiding the need for high-dimensional eigenanalysis. We corroborate this claim across different datasets, architectures and samle sizes.
"
1390,2019,On the Limitations of Representing Functions on Sets,Poster,"Recent work on the reresentation of functions on sets has considered the use of summation in a latent sace to enforce ermutation invariance. In articular, it has been conjectured that the dimension of this latent sace may remain fixed as the cardinality of the sets under consideration increases. However, we demonstrate that the analysis leading to this conjecture requires maings which are highly discontinuous and argue that this is only of limited ractical use. Motivated by this observation, we rove that an imlementation of this model via continuous maings (as rovided by e.g. neural networks or Gaussian rocesses) actually imoses a constraint on the dimensionality of the latent sace. Practical universal function reresentation for set inuts can only be achieved with a latent dimension at least the size of the maximum number of inut elements.
"
1391,2019,Probabilistic Neural Symbolic Models for Interpretable Visual Question Answering,Poster,"We roose a new class of robabilistic neural-symbolic models, that have symbolic functional rograms as a latent, stochastic variable. Instantiated in the context of visual question answering, our robabilistic formulation offers two key concetual advantages over rior neural-symbolic models for VQA. Firstly, the rograms generated by our model are more understandable while requiring less number of teaching examles. Secondly, we show that one can ose counterfactual scenarios to the model, to robe its beliefs on the rograms that could lead to a secified answer given an image. Our results on the CLEVR and SHAPES datasets verify our hyotheses, showing that the model gets better rogram (and answer) rediction accuracy even in the low data regime, and allows one to robe the coherence and consistency of reasoning erformed.
"
1392,2019,Nonparametric Bayesian Deep Networks with Local Competition,Poster,"The aim of this work is to enable inference of dee networks that retain high accuracy for the least ossible model comlexity, with the latter deduced from the data during inference.  To this end, we revisit dee networks that comrise cometing linear units, as oosed to nonlinear units that do not entail any form of (local) cometition. In this context, our main technical innovation consists in an inferential setu that leverages solid arguments from Bayesian nonarametrics. We infer both the needed set of connections or locally cometing sets of units, as well as the required floating-oint recision for storing the network arameters. Secifically, we introduce auxiliary discrete latent variables reresenting which initial network comonents are actually needed for modeling the data at hand, and erform Bayesian inference over them by imosing aroriate stick-breaking riors.  As we exerimentally show using benchmark datasets, our aroach yields networks with less comutational footrint than the state-of-the-art, and with no comromises in redictive accuracy.
"
1393,2019,Good Initializations of Variational Bayes for Deep Models,Poster,"Stochastic variational inference is an established way to carry out aroximate Bayesian inference for dee models flexibly and at scale. While there have been effective roosals for good initializations for loss minimization in dee learning, far less attention has been devoted to the issue of initialization of stochastic variational inference. We address this by roosing a novel layer-wise initialization strategy based on Bayesian linear models. The roosed method is extensively validated on regression and classification tasks, including Bayesian Dee Nets and Conv Nets, showing faster and better convergence comared to alternatives insired by the literature on initializations for loss minimization.
"
1394,2019,Dropout as a Structured Shrinkage Prior,Poster,"Droout regularization of dee neural networks has been a mysterious yet effective tool to revent overfitting.  Exlanations for its success range from the revention of ""co-adated"" weights to it being a form of chea Bayesian inference.  We roose a novel framework for understanding multilicative noise in neural networks, considering continuous distributions as well as Bernoulli noise (i.e. droout).  We show that multilicative noise induces structured shrinkage riors on a network's weights.  We derive the equivalence through rearametrization roerties of scale mixtures and without invoking any aroximations.  Given the equivalence, we then show that droout's Monte Carlo training objective aroximates marginal MAP estimation.  We leverage these insights to roose a novel shrinkage framework for resnets, terming the rior 'automatic deth determination' as it is the natural analog of automatic relevance determination for network deth.  Lastly, we investigate two inference strategies that imrove uon the aforementioned MAP aroximation in regression benchmarks.
"
1395,2019,ARSM: Augment-REINFORCE-Swap-Merge Estimator for Gradient Backpropagation Through Categorical Variables,Poster,"To address the challenge of backroagating the gradient through categorical variables, we roose the augment-REINFORCE-swa-merge (ARSM) gradient estimator that is unbiased and has low variance. ARSM first uses variable augmentation, REINFORCE, and Rao-Blackwellization to re-exress the gradient as an exectation under the Dirichlet distribution, then uses variable swaing to construct differently exressed but equivalent exectations, and finally shares common random numbers between these exectations to achieve significant variance reduction. Exerimental results show ARSM closely resembles the erformance of the true gradient for otimization in univariate settings; outerforms existing estimators by a large margin when alied to categorical variational auto-encoders; and rovides a ""try-and-see self-critic"" variance reduction method for discrete-action olicy gradient, which removes the need of estimating baselines by generating a random number of seudo actions and estimating their action-value functions. 
"
1396,2019,On Variational Bounds of Mutual Information,Poster,"Estimating and otimizing Mutual Information (MI) is core to many roblems in machine learning, but bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds arameterized by neural networks. However, the relationshis and tradeoffs between these bounds remains unclear. In this work, we unify these recent develoments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this roblem, we introduce a continuum of lower bounds that encomasses revious bounds and flexibly trades off bias and variance. On high-dimensional, controlled roblems, we emirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of these new bounds for estimation and reresentation learning.
"
1397,2019,Partially Exchangeable Networks and Architectures for Learning Summary Statistics in Approximate Bayesian Computation,Poster,"We resent a novel family of dee neural architectures, named artially exchangeable networks (PENs) that leverage robabilistic symmetries. By design, PENs are invariant to block-switch transformations, which characterize the artial exchangeability roerties of conditionally Markovian rocesses. Moreover, we show that any block-switch invariant function has a PEN-like reresentation. The DeeSets architecture is a secial case of PEN and we can therefore also target fully exchangeable data. We emloy PENs to learn summary statistics in aroximate Bayesian comutation (ABC). When comaring PENs to revious dee learning methods for learning summary statistics, our results are highly cometitive, both considering time series and static models. Indeed, PENs rovide more reliable osterior samles even when using less training data.
"
1398,2019,Hierarchical Importance Weighted Autoencoders,Poster,"Imortance weighted variational inference (Burda et al., 2015) uses multile i.i.d. samles to have a tighter variational lower bound. We believe a joint roosal has the otential of reducing the number of redundant samles, and introduce a hierarchical structure to induce correlation. The hoe is that the roosals would coordinate to make u for the error made by one another to reduce the variance of the imortance estimator.  Theoretically, we analyze the condition under which convergence of the estimator variance can be connected to convergence of the lower bound. Emirically, we confirm that maximization of the lower bound does imlicitly minimize variance.  Further analysis shows that this is a result of negative correlation induced by the roosed hierarchical meta samling scheme, and erformance of inference also imroves when the number of samles increases.
"
1399,2019,Faster Attend-Infer-Repeat with Tractable Probabilistic Models,Poster,"The recent Attend-Infer-Reeat (AIR) framework marks a milestone in structured robabilistic modeling, as it tackles the challenging roblem of unsuervised scene understanding via Bayesian inference. AIR exresses the comosition of visual scenes from individual objects, and uses variational autoencoders to model the aearance of those objects. However, inference in the overall model is highly intractable, which hamers its learning seed and makes it rone to subotimal solutions. In this aer, we show that the seed and robustness of learning in AIR can be considerably imroved by relacing the intractable object reresentations with tractable robabilistic models. In articular, we ot for sum-roduct networks (SPNs), exressive dee robabilistic models with a rich set of tractable inference routines. The resulting model, called SuPAIR, learns an order of magnitude faster than AIR, treats object occlusions in a consistent manner, and allows for the inclusion of a background noise model, imroving the robustness of Bayesian scene understanding.
"
1400,2019,Understanding Priors in Bayesian Neural Networks at the Unit Level,Poster,"We investigate dee Bayesian neural networks with Gaussian riors on the weights and a class of ReLU-like nonlinearities. Bayesian neural networks with Gaussian riors are well known to induce an L2,  ``weight decay'', regularization. Our results indicate a more intricate regularization effect at the level of the unit activations. Our main result  establishes that the induced rior distribution on the units before and after activation becomes increasingly heavy-tailed with the deth of the layer. 
We show that first layer units are Gaussian, second layer units are sub-exonential, and units in deeer layers are characterized by  sub-Weibull distributions. Our results  rovide new theoretical insight on dee Bayesian neural networks, which we corroborate with simulation exeriments. 
"
1401,2019,Defending Against Saddle Point Attack in Byzantine-Robust Distributed Learning,Poster,"We study robust distributed learning that involves minimizing a non-convex loss function with saddle oints.
We consider the Byzantine setting where some worker machines have abnormal or even arbitrary and adversarial behavior, and in this setting, the Byzantine machines may create fake local minima near a saddle oint that is far away from any true local minimum, even when robust gradient estimators are used.
We develo ByzantinePGD, a robust first-order algorithm that can rovably escae saddle oints and fake local minima, and converge to an aroximate true local minimizer with low iteration comlexity.
As a by-roduct, we give a simler algorithm and analysis for escaing saddle oints in the usual non-Byzantine setting.
We further discuss three robust gradient estimators that can be used in ByzantinePGD, including median, trimmed mean, and iterative filtering.
We characterize their erformance in concrete statistical settings, and argue for their near-otimality in low and high dimensional regimes. 
"
1402,2019,Stochastic Iterative Hard Thresholding for Graph-structured Sparsity Optimization,Poster,"Stochastic otimization algorithms udate models with chea er-iteration costs sequentially, which makes them amenable for large-scale data analysis. Such algorithms have been widely studied for structured sarse models where the sarsity information is very secific, e.g., convex sarsity-inducing norms or $\ell^0$-norm. However, these norms cannot be directly alied to the roblem of comlex (non-convex) grah-structured sarsity models, which have imortant alication in disease outbreak and social networks, etc. In this aer, we roose a stochastic gradient-based method for solving grah-structured sarsity constraint roblems, not restricted to the least square loss. We rove that our algorithm enjoys a linear convergence u to a constant error, which is cometitive with the counterarts in the batch learning setting. We conduct extensive exeriments to show the efficiency and effectiveness of the roosed algorithms. "
1403,2019,Neuron birth-death dynamics accelerates gradient descent and converges asymptotically,Poster,"Neural networks with a large number of arameters admit a mean-field descrition, which has recently served as a theoretical exlanation for the favorable training roerties of models with a large number of arameters. In this regime, gradient descent obeys a deterministic artial differential equation (PDE) that converges to a globally otimal solution for networks with a single hidden layer under aroriate assumtions. In this work, we roose a non-local mass transort dynamics that leads to a modified PDE with the same minimizer. We imlement this non-local dynamics as a stochastic neuronal birthdeath rocess and we rove that it accelerates the rate of convergence in the mean-field limit. We subsequently realize this PDE with two classes of numerical schemes that converge to the mean-field equation, each of which can easily be imlemented for neural networks with finite numbers of arameters. We illustrate our algorithms with two models to rovide intuition for the mechanism through which convergence is accelerated.
"
1404,2019,Width Provably Matters in Optimization for Deep Linear Neural Networks,Poster,"We rove that for an $L$-layer fully-connected linear neural network, if the width of every hidden layer is $\widetilde{\Omega}\left(L \cdot r \cdot d_{out} \cdot \kaa^3 \right)$, where $r$ and $\kaa$ are the rank and the condition number of the inut data, and $d_{out}$ is the outut dimension, then gradient descent with Gaussian random initialization converges to a global minimum at a linear rate. The number of iterations to find an  $\esilon$-subotimal solution is $O(\kaa \log(\frac{1}{\esilon}))$. Our olynomial uer bound on the total running time for wide dee linear networks and the  $\ex\left(\Omega\left(L\right)\right)$ lower bound for narrow dee linear neural networks [Shamir, 2018] together demonstrate that wide layers are necessary for otimizing dee models."
1405,2019,Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?,Poster,"Many modern learning tasks involve fitting nonlinear models which are trained in an overarameterized regime where the arameters of the model exceed the size of the training dataset. Due to this overarameterization, the training loss may have infinitely many global minima and it is critical to understand the roerties of the solutions found by first-order otimization schemes such as (stochastic) gradient descent starting from different initializations. In this aer we demonstrate that when the loss has certain roerties over a minimally small neighborhood of the initial oint, first order methods such as (stochastic) gradient descent have a few intriguing roerties: (1) the iterates converge at a geometric rate to a global otima even when the loss is nonconvex, (2) among all global otima of the loss the iterates converge to one with a near minimal distance to the initial oint, (3) the iterates take a near direct route from the initial oint to this global otimum. As art of our roof technique, we introduce a new otential function which catures the tradeoff between the loss function and the distance to the initial oint as the iterations rogress. The utility of our general theory is demonstrated for a variety of roblem domains sanning low-rank matrix recovery to shallow neural network training.
"
1406,2019,Power k-Means Clustering,Poster,"Clustering is a fundamental task in unsuervised machine learning. Lloyd's 1957 algorithm for k-means clustering remains one of the most widely used due to its seed and simlicity, but the greedy aroach is sensitive to initialization and often falls short at a oor solution. This aer exlores an alternative to Lloyd's algorithm that retains its simlicity and mitigates its tendency to get traed by local minima. Called ower k-means, our method embeds the k-means roblem in a continuous class of similar, better behaved roblems with fewer local minima. Power k-means anneals its way toward the solution of ordinary k-means by way of majorization-minimization (MM), sharing the aealing descent roerty and low comlexity of Lloyd's algorithm. Further, our method comlements widely used seeding strategies, reaing marked imrovements when used together as demonstrated on a suite of simulated and real data examles. 
"
1407,2019,Distributed Learning over Unreliable Networks,Poster,"Most of today's distributed machine learning systems assume {\em reliable networks}: whenever two machines exchange information (e.g., gradients or models), the network should guarantee the delivery of the message. At the same time, recent work exhibits the imressive tolerance of machine learning algorithms to errors or noise arising from relaxed communication or synchronization. In this aer, we connect these two trends, and consider the following question: {\em Can we design machine learning systems that are tolerant to network unreliability during training?} With this motivation, we focus on a theoretical roblem of indeendent interest---given a standard distributed arameter server architecture, if every communication between the worker and the server has a non-zero robability $$ of being droed, does there exist an algorithm that still converges, and at what seed? In the context of rior art, this roblem can be hrased as {\em distributed learning over random toologies}. The technical contribution of this aer is a novel theoretical analysis roving that distributed learning over random toologies can achieve comarable convergence rate to centralized or distributed learning over reliable networks. Further, we rove that the influence of the acket dro rate diminishes with the growth of the number of arameter servers. We ma this theoretical result onto a real-world scenario, training dee neural networks over an unreliable network layer, and conduct network simulation to validate the system imrovement by allowing the networks to be unreliable.
"
1408,2019,Escaping Saddle Points with Adaptive Gradient Methods,Poster,"Adative methods such as Adam and RMSPro are widely used in dee learning but are not well understood. In this aer, we seek a cris, clean and recise characterization of their behavior in nonconvex settings. To this end, we first rovide a novel view of adative methods as reconditioned SGD, where the reconditioner is estimated in an online manner. By studying the reconditioner on its own, we elucidate its urose: it rescales the stochastic gradient noise to be isotroic near stationary oints, which hels escae saddle oints. Furthermore, we show that adative methods can efficiently estimate the aforementioned reconditioner. By gluing together these two comonents, we rovide the first (to our knowledge) second-order convergence result for any adative method. The key insight from our analysis is that, comared to SGD, adative methods escae saddle oints faster, and can converge faster overall to second-order stationary oints.
"
1409,2019,$\texttt{DoubleSqueeze}$: Parallel Stochastic Gradient Descent with Double-pass Error-Compensated Compression,Poster,"A standard aroach in large scale machine learning is distributed stochastic gradient training, which requires the comutation of aggregated stochastic gradients over multile nodes on a network. Communication is a major bottleneck in such alications, and in recent years, comressed stochastic gradient methods such as QSGD (quantized SGD) and sarse SGD have been roosed to reduce communication. It was also shown that error comensation can be combined with comression to achieve better convergence in a scheme that each node comresses its local stochastic gradient and broadcast the result to all other nodes over the network in a single ass. However, such a single ass broadcast aroach is not realistic in many ractical imlementations. For examle, under the oular arameter-server model for distributed learning, the worker nodes need to send the comressed local gradients to the arameter server, which erforms the aggregation. The arameter server has to comress the aggregated stochastic gradient again before sending it back to the worker nodes. In this work, we rovide a detailed analysis on this two-ass communication model, with error-comensated comression both on the worker nodes and on the arameter server. We show that the error-comensated stochastic gradient algorithm admits three very nice roerties: 1) it is comatible with an \emh{arbitrary} comression technique; 2) it admits an imroved convergence rate than the non error-comensated stochastic gradient method such as QSGD and sarse SGD; 3) it admits linear seedu with resect to the number of workers. The emirical study is also conducted to validate our theoretical results.
"
1410,2019,Model Function Based Conditional Gradient Method with Armijo-like Line Search,Poster,"The Conditional Gradient Method is generalized to a class of non-smooth non-convex otimization roblems with many alications in machine learning. The roosed algorithm iterates by minimizing so-called model functions over the constraint set. Comlemented with an Armijo line search rocedure, we rove that subsequences converge to a stationary oint. The abstract framework of model functions rovides great flexibility in the design of concrete algorithms. As secial cases, for examle, we develo an algorithm for additive comosite roblems and an algorithm for non-linear comosite roblems which leads to a Gauss-Newton-tye algorithm. Both instances are novel in non-smooth non-convex otimization and come with numerous alications in machine learning. We erform an exeriment on a non-linear robust regression roblem and discuss the flexibility of the roosed framework in several matrix factorization formulations.
"
1411,2019,Analogies Explained: Towards Understanding Word Embeddings,Poster,"Word embeddings generated by neural network methods such as word2vec (W2V) are well known to exhibit seemingly linear behaviour, e.g. the embeddings of analogy ``woman is to queen as man is to king'' aroximately describe a arallelogram. This roerty is articularly intriguing since the embeddings are not trained to achieve it. Several exlanations have been roosed, but each introduces assumtions that do not hold in ractice.
We derive a robabilistically grounded definition of arahrasing that we re-interret as word transformation, a mathematical descrition of ``$w_x$ is to $w_y$''. From these concets we rove existence of linear relationshi between W2V-tye embeddings that underlie the analogical henomenon, identifying exlicit error terms."
1412,2019,Parameter-Efficient Transfer Learning for NLP,Poster,"Fine-tuning large retrained models is an effective transfer mechanism in NLP. However, in the resence of many downstream tasks, fine-tuning is arameter inefficient: an entire new model is required for every task. As an alternative, we roose transfer with adater modules. Adater modules yield a comact and extensible model; they add only a few trainable arameters er task, and new tasks can be added without revisiting revious ones. The arameters of the original network remain fixed, yielding a high degree of arameter sharing. To demonstrate adater's effectiveness, we transfer the recently roosed BERT Transformer model to $26$ diverse text classification tasks, including the GLUE benchmark. Adaters attain near state-of-the-art erformance, whilst adding only a few arameters er task. On GLUE, we attain within $0.8\%$ of the erformance of full fine-tuning, adding only $3.6\%$ arameters er task. By contrast, fine-tuning trains $100\%$ of the arameters er task."
1413,2019,Efficient On-Device Models using Neural Projections,Poster,"Many alications involving visual and language understanding can be effectively solved using dee neural networks. Even though these techniques achieve state-of-the-art results, it is very challenging to aly them on devices with limited memory and comutational caacity such as mobile hones, smart watches and IoT. We roose a neural rojection aroach for training comact on-device neural networks. We introduce ""rojection"" networks that use locality-sensitive rojections to generate comact binary reresentations and learn small neural networks with comutationally efficient oerations. We design a joint otimization framework where the rojection network can be trained from scratch or leverage existing larger neural networks such as feed-forward NNs, CNNs or RNNs. The trained neural rojection network can be directly used for inference on device at low memory and comutation cost. We demonstrate the effectiveness of this as a general-urose aroach for significantly shrinking memory requirements of different tyes of neural networks while reserving good accuracy on multile visual and text classification tasks.
"
1414,2019,Deep Residual Output Layers for Neural Language Generation,Poster,"Many tasks, including language generation, benefit from learning the structure of the outut sace, articularly when the sace of outut labels is large and the data is sarse.  State-of-the-art neural language models indirectly cature the outut sace structure in their classifier weights since they lack arameter sharing across outut labels. Learning shared outut label maings hels, but existing methods have limited exressivity and are rone to overfitting. In this aer, we investigate the usefulness of more owerful shared maings for outut labels, and roose a dee residual outut maing with droout between layers to better cature the structure of the outut sace and avoid overfitting. Evaluations on three language generation tasks show that our outut label maing can match or imrove state-of-the-art recurrent and self-attention architectures, and suggest that the classifier does not necessarily need to be high-rank to better model natural language if it is better at caturing the structure of the outut sace.
"
1415,2019,Improving Neural Language Modeling via Adversarial Training,Poster,"Recently, substantial rogress has been made in language modeling by using dee neural networks. However, in ractice, large scale neural language models have been shown to be rone to overfitting. In this aer, we resent a simle yet highly effective adversarial training mechanism for regularizing neural language models. The idea is to introduce adversarial noise to the outut embedding layer while training the models. We show that the otimal adversarial noise yields a simle closed form solution, thus allowing us to develo a simle and time efficient algorithm. Theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, heling to increase the robustness of models. Emirically, we show that our method imroves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test erlexity scores of 46.01 and 38.65, resectively. 
When alied to machine translation,  our method imroves over various transformer-based translation baselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English tasks.
"
1416,2019,Mixture Models for Diverse Machine Translation: Tricks of the Trade,Poster,"Mixture models trained via EM are among the simlest, most widely used and well understood latent variable models in the machine learning literature. Surrisingly, these models have been hardly exlored in text generation alications such as machine translation. In rincile, they rovide a latent variable to control generation and roduce a diverse set of hyotheses. In ractice, however, mixture models are rone to degeneracies---often only one comonent gets trained or the latent variable is simly ignored. We find that disabling droout noise in resonsibility comutation is critical to successful training. In addition, the design choices of arameterization, rior distribution, hard versus soft EM and online versus offline assignment can dramatically affect model erformance. We develo an evaluation rotocol to assess both quality and diversity of generations against multile references, and rovide an extensive emirical study of several mixture model variants. Our analysis shows that certain tyes of mixture models are more robust and offer the best trade-off between translation quality and diversity comared to variational models and diverse decoding aroaches.\footnote{Code to reroduce the results in this aer is available at \url{htts:github.comytorchfairseq}}
"
1417,2019,MASS: Masked Sequence to Sequence Pre-training for Language Generation,Poster,"Pre-training and fine-tuning, e.g., BERT~\cite{devlin2018bert}, have achieved great success in language understanding by transferring knowledge from rich-resource re-training task to the lowzero-resource downstream tasks. Insired by the success of BERT, we roose MAsked Sequence to Sequence re-training (MASS) for the encoder-decoder based language generation tasks. MASS adots the encoder-decoder framework to reconstruct a sentence fragment given the remaining art of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as inut, and its decoder tries to redict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develo the caability of reresentation extraction and language modeling. By further fine-tuning on a variety of zerolow-resource language generation tasks, including neural machine translation, text summarization and conversational resonse generation (3 tasks and totally 8 datasets), MASS achieves significant imrovements over the baselines without re-training or with other re-training methods. Esecially, we achieve the state-of-the-art accuracy (30.02 in terms of BLEU score) on the unsuervised English-French translation, even beating the early attention-based suervised model~\cite{bahdanau2015neural}.
"
1418,2019,Humor in Word Embeddings: Cockamamie Gobbledegook for Nincompoops,Poster,"While humor is often thought to be beyond the reach of Natural Language Processing, we show that several asects of single-word humor correlate with simle linear directions in Word Embeddings. In articular: (a) the word vectors cature multile asects discussed in humor theories from various discilines; (b) each individual's sense of humor can be reresented by a vector, which can redict differences in eole's senses of humor on new, unrated, words; and (c) uon clustering humor ratings of multile demograhic grous, different humor references emerge across the different grous. Humor ratings are taken from the work of Engelthaler and Hills (2017) as well as from an original crowdsourcing study of 120,000 words. Our dataset further includes annotations for the theoretically-motivated humor features we identify.
"
1419,2019,MeanSum: A Neural Model for Unsupervised Multi-Document Abstractive Summarization,Poster,"Abstractive summarization has been studied using neural sequence transduction methods with datasets of large, aired document-summary examles. However, such datasets are rare and the models trained from them do not generalize to other domains. Recently, some rogress has been made in learning sequence-to-sequence maings with only unaired examles. In our work, we consider the setting where there are only documents (roduct or business reviews) with no summaries rovided, and roose an end-to-end, neural model architecture to erform unsuervised abstractive summarization. Our roosed model consists of an auto-encoder where the mean of the reresentations of the inut reviews decodes to a reasonable summary-review. We consider variants of the roosed architecture and erform an ablation study to show the imortance of secific comonents. We show through metrics and human evaluation that the generated summaries are highly abstractive, fluent, relevant, and reresentative of the average sentiment of the inut reviews. Finally, we collect a ground-truth evaluation dataset and show that our model outerforms a strong extractive baseline.
"
1420,2019,CHiVE: Varying Prosody in Speech Synthesis with a Linguistically Driven Dynamic Hierarchical Conditional Variational Network,Poster,"The rosodic asects of seech signals roduced by current text-to-seech systems are tyically averaged over training material, and as such lack the variety and liveliness found in natural seech. To avoid monotony and averaged rosody contours, it is desirable to have a way of modeling the variation in the rosodic asects of seech, so audio signals can be synthesized in multile ways for a given text. We resent a new, hierarchically structured conditional variational auto-encoder to generate rosodic features (fundamental frequency, energy and duration) suitable for use with a vocoder or a generative model like WaveNet. At inference time, an embedding reresenting the rosody of a sentence may be samled from the variational layer to allow for rosodic variation. To efficiently cature the hierarchical nature of the linguistic inut (words, syllables and hones), both the encoder and decoder arts of the auto-encoder are hierarchical, in line with the linguistic structure, with layers being clocked dynamically at the resective rates. We show in our exeriments that our dynamic hierarchical network outerforms a non-hierarchical state-of-the-art baseline, and, additionally, that rosody transfer across sentences is ossible by emloying the rosody embedding of one sentence to generate the seech signal of another.
"
1421,2019,COMIC: Multi-view Clustering Without Parameter Selection,Poster,"In this aer, we study two challenges in clustering analysis, namely, how to cluster multi-view data and how to erform clustering without arameter selection on cluster size. To this end, we roose a novel objective function to roject raw data into one sace in which the rojection embraces the geometric consistency (GC) and the cluster assignment consistency (CAC). To be secific, the GC aims to learn a connection grah from a rojection sace wherein the data oints are connected if and only if they belong to the same cluster. The CAC aims to minimize the discreancy of airwise connection grahs induced from different views based on the view-consensus assumtion, \textit{i.e.}, different views could roduce the same cluster assignment structure as they are different ortraits of the same object. Thanks to the view-consensus derived from the connection grah, our method could achieve romising  erformance in learning view-secific reresentation and eliminating the heterogeneous gas across different views. Furthermore, with the roosed objective, it could learn almost all arameters including the cluster number from data without labor-intensive arameter selection. Extensive exerimental results show the romising erformance achieved by our method on five datasets comaring with nine state-of-the-art multi-view clustering aroaches. 
"
1422,2019,The Wasserstein Transform,Poster,"We introduce the Wasserstein transform, a method for enhancing and denoising datasets defined on general metric saces. The construction draws insiration from Otimal Transortation ideas. We establish the stability of our method under data erturbation and, when the dataset is assumed to be Euclidean, we also exhibit a recise connection between the Wasserstein transform and the mean shift family of algorithms. We then use this connection to rove that mean shift also inherits stability under erturbations. We study the erformance of the Wasserstein transform method on different datasets as a rerocessing ste rior to clustering and classification tasks.
"
1423,2019,Sequential Facility Location: Approximate Submodularity and Greedy Algorithm,Poster,"We develo and analyze a novel utility function and a fast otimization algorithm for subset selection in sequential data that incororates the dynamic model of data. We roose a cardinality-constrained sequential facility location function that finds a fixed number of reresentatives, where the sequence of reresentatives is comatible with the dynamic model and well encodes the data. As maximizing this new objective function is NP-hard, we develo a fast greedy algorithm based on submodular maximization. Unlike the conventional facility location, the comutation of the marginal gain in our case cannot be done by oerations on each item indeendently. We exloit the sequential structure of the roblem and develo an efficient dynamic rogramming-based algorithm that comutes the marginal gain exactly. We investigate conditions on the dynamic model, under which our utility function is  ($\esilon$-aroximately) submodualr, hence, the greedy algorithm comes with erformance guarantees. By exeriments on synthetic data and the roblem of rocedure learning from instructional videos, we show that our framework significantly imroves the comutational time, achieves better objective function values and obtains more coherent summaries. "
1424,2019,Neural Collaborative Subspace Clustering,Poster,"We introduce the Neural Collaborative Subsace Clustering, a neural model that discovers clusters of data oints drawn from a union of low-dimensional subsaces. In contrast to revious attemts, our model runs without the aid of sectral clustering. This makes our algorithm one of the kinds that can gracefully scale to large datasets. At its heart, our neural model benefits from a classifier which determines whether a air of oints lies on the same subsace or not. Essential to our model is the construction of two affinity matrices, one from the classifier and the other from a notion of subsace self-exressiveness, to suervise training in a collaborative scheme. We thoroughly assess and contrast the erformance of our model against various state-of-the-art  clustering algorithms including dee subsace-based ones. 
"
1425,2019,Unsupervised Deep Learning by Neighbourhood Discovery,Poster,"Dee convolutional neural networks (CNNs) have demonstrated remarkable success in comuter vision by suervisedly learning strong visual feature reresentations. However, training CNNs relies heavily on the availability of exhaustive training data annotations, limiting significantly their deloyment and scalability in many alication scenarios. In this work, we introduce a generic unsuervised dee learning aroach to training dee models without the need for any manual label suervision. Secifically, we rogressively discover samle anchoredcentred neighbourhoods to reason and learn the underlying class decision boundaries iteratively and accumulatively. Every single neighbourhood is secially formulated so that all the member samles can share the same unseen class labels at high robability for facilitating the extraction of class discriminative feature reresentations during training. Exeriments on image classification show the erformance advantages of the roosed method over the state-of-the-art unsuervised learning models on six benchmarks including both coarse-grained and fine-grained object image categorisation. 
"
1426,2019,Autoregressive Energy Machines,Poster,"Neural density estimators are flexible families of arametric models which have seen widesread use in unsuervised machine learning in recent years. Maximum-likelihood training tyically dictates that these models be constrained to secify an exlicit density. However, this limitation can be overcome by instead using a neural network to secify an energy function, or unnormalized density, which can subsequently be normalized to obtain a valid distribution. The challenge with this aroach lies in accurately estimating the normalizing constant of the high-dimensional energy function. We roose the Autoregressive Energy Machine, an energy-based model which simultaneously learns an unnormalized density and comutes an imortance-samling estimate of the normalizing constant for each conditional in an autoregressive decomosition. The Autoregressive Energy Machine achieves state-of-the-art erformance on a suite of density-estimation tasks.
"
1427,2019,Greedy Orthogonal Pivoting Algorithm for Non-Negative Matrix Factorization,Poster,"Non-negative matrix factorization is a owerful tool for learning useful reresentations in the data and has been widely alied in many roblems such as data mining and signal rocessing.  Orthogonal NMF, which  can imrove the locality of decomosition, has drawn considerable interest in solving clustering roblems in recent years. However, imosing simultaneous non-negative and orthogonal structure can be quite difficult, and so existing algorithms can only solve it aroximately. To address this challenge, we roose an innovative rocedure called Greedy Orthogonal Pivoting Algorithm (GOPA). The GOPA algorithm fully exloits the sarsity of non-negative orthogonal solutions to break the global roblem into a series of local otimizations, in which an adative subset of coordinates are udated in a greedy, closed-form manner. The biggest advantage of GOPA is that it romotes exact orthogonality and rovides solid emirical evidence that stronger orthogonality does contribute favorably to better  clustering erformance. On the other hand, we further design randomized and arallel version of GOPA, which can further reduce the comutational cost and imrove accuracy, making it suitable for large data.
"
1428,2019,Noise2Self: Blind Denoising by Self-Supervision,Poster,"We roose a general framework for denoising high-dimensional measurements which requires no rior on the signal, no estimate of the noise, and no clean training data. The only assumtion is that the noise exhibits statistical indeendence across different dimensions of the measurement, while the true signal exhibits some correlation. For a broad class of functions (``$\mathcal{J}$-invariant''), it is then ossible to estimate the erformance of a denoiser from noisy data alone. This allows us to calibrate $\mathcal{J}$-invariant versions of any arameterised denoising algorithm, from the single hyerarameter of a median filter to the millions of weights of a dee neural network. We demonstrate this on natural image and microscoy data, where we exloit noise indeendence between ixels, and on single-cell gene exression data, where we exloit indeendence between detections of individual molecules. This framework generalizes recent work on training neural nets from noisy images and on cross-validation for matrix factorization."
1429,2019,Learning Dependency Structures for Weak Supervision Models,Poster,"Labeling training data is a key bottleneck in the modern machine learning ieline. Recent weak suervision aroaches combine labels from multile noisy sources by estimating their accuracies without access to ground truth labels; however, estimating the deendencies among these sources is a critical challenge. We focus on a robust PCA-based algorithm for learning these deendency structures, establish imroved theoretical recovery rates, and outerform existing methods on various real-world tasks. Under certain conditions, we show that the amount of unlabeled data needed can scale sublinearly or even logarithmically with the number of sources m, imroving over revious efforts that ignore the sarsity attern in the deendency structure and scale linearly in m. We rovide an information-theoretic lower bound on the minimum samle comlexity of the weak suervision setting. Our method outerforms weak suervision aroaches that assume conditionally-indeendent sources by u to 4.64 F1 oints and revious structure learning aroaches by u to 4.41 F1 oints on real-world relation extraction and image classification tasks.
"
1430,2019,Geometry and Symmetry in Short-and-Sparse Deconvolution,Poster,"We study the Short-and-Sarse (SaS) deconvolution roblem of recovering a short signal a0 and a sarse signal x0 from their convolution. We roose a method based on nonconvex otimization, which under certain conditions recovers the target short and sarse signals, u to a signed shift symmetry which is intrinsic to this model. This symmetry lays a central role in shaing the otimization landscae for
deconvolution. We give a regional analysis, which characterizes this landscae geometrically, on a union of subsaces. Our geometric characterization holds when the length-0 short signal a0 has shift coherence µ, and x0 follows a random sarsity model with sarsity rate θ ∈ [c10, c2(0\sqrt{\mu}+\sqrt{0})]  (log^2(0)) . Based on this geometry, we give a rovable method that successfully solves SaS deconvolution with high robability.
"
1431,2019,On Sparse Linear Regression in the Local Differential Privacy Model,Poster,"In this aer, we study the sarse linear regression roblem under the Local Differential Privacy (LDP) model. We first show that olynomial deendency on the dimensionality $$ of the sace is unavoidable for the estimation error in both non-interactive and sequential interactive local models, if the rivacy of the whole dataset needs to be reserved.  Similar limitations also exist for other tyes of error measurements and in the relaxed local models. This indicates that differential rivacy in high dimensional sace is unlikely achievable for the roblem. With the understanding of this limitation, we then resent two algorithmic results. The first one is 
a sequential interactive LDP algorithm for the low dimensional sarse case, called Locally Differentially Private Iterative Hard Thresholding (LDP-IHT), which achieves a near otimal uer bound. This algorithm is actually rather general and can be used to solve quite a few other roblems, such as (Local) DP-ERM with sarsity constraints and sarse regression with non-linear measurements.  The second one is for the restricted (high dimensional) case where only  the rivacy  of the resonses (labels) needs to be reserved. For this case, 
we show that the otimal rate of the error estimation can be made logarithmically deending on $$ (i.e., $\log $) in the local model, 
where an uer bound is obtained by a label-rivacy version of LDP-IHT. Exeriments on real world and synthetic datasets confirm our theoretical analysis. "
1432,2019,Differentially Private Empirical Risk Minimization with Non-convex Loss Functions,Poster,"We study the roblem of Emirical Risk Minimization (ERM) with (smooth) non-convex loss functions under the differential-rivacy (DP) model. Existing aroaches for this roblem mainly adot gradient norms to measure the error, which in general cannot guarantee the quality of the solution. To address this issue, 
we first study the exected excess emirical (or oulation) risk, which was rimarily used as the utility to measure the quality for convex loss functions. Secifically, we show that
the excess emirical (or oulation) risk can be uer bounded by $\tilde{O}(\frac{d\log (1\delta)}{\log n\esilon^2})$ in the $(\esilon, \delta)$-DP settings, where $n$ is the data size and $d$ is the dimensionality of the sace. 
The $\frac{1}{\log n}$ term in the emirical risk bound can be further imroved to $\frac{1}{n^{\Omega(1)}}$ (when $d$ is a constant) by a highly non-trivial analysis on the time-average error. 
To obtain more efficient solutions, we also consider the connection between achieving differential rivacy and finding aroximate local minimum. 
Particularly, we show that when the size $n$ is large enough, there are $(\esilon, \delta)$-DP algorithms which can find an aroximate local minimum of the emirical risk with high robability in both the constrained and non-constrained settings. 
These results indicate that one can escae saddle oints rivately."
1433,2019,Bounding User Contributions: A Bias-Variance Trade-off in Differential Privacy,Poster,"Differentially rivate learning algorithms rotect individual articiants in the training dataset by guaranteeing that their resence does not significantly change the resulting model. In order to make this romise, such algorithms need to know the maximum contribution that can be made by a single user: the more data an individual can contribute, the more noise will need to be added to rotect them. While most existing analyses assume that the maximum contribution is known and fixed in advance—indeed, it is often assumed that each user contributes only a single examle—we argue that in ractice there is a meaningful choice to be made. On the one hand, if we allow users to contribute large amounts of data, we may end u adding excessive noise to rotect a few outliers, even when the majority contribute only modestly. On the other hand, limiting users to small contributions kees noise levels low at the cost of otentially discarding significant amounts of excess data, thus introducing bias. Here, we characterize this trade-off for an emirical risk minimization setting, showing that in general there is a “sweet sot” that deends on measurable roerties of the dataset, but that there is also a concrete cost to rivacy that cannot be avoided simly by collecting more data.
"
1434,2019,Differentially Private Learning of Geometric Concepts,Poster,"We resent differentially rivate efficient algorithms for learning union of olygons in the lane (which are not necessarily convex). Our algorithms achieve $(\alha,\beta)$-PAC learning and $(\esilon,\delta)$-differential rivacy using a samle of size $\tilde{O}\left(\frac{1}{\alha\esilon}k\log d\right)$, where the domain is $[d]\times[d]$ and $k$ is the number of edges in the union of olygons."
1435,2019,Toward Controlling Discrimination in Online Ad Auctions,Poster,"Online advertising latforms are thriving due to the customizable audiences they offer advertisers. However, recent studies show that advertisements can be discriminatory with resect to the gender or race of the audience that sees the ad, and may inadvertently cross ethical andor legal boundaries. To revent this, we roose a constrained ad auction framework that maximizes the latform’s revenue conditioned on ensuring that the audience seeing an advertiser’s ad is distributed aroriately across sensitive tyes such as gender or race. Building uon Myerson’s classic work, we first resent an otimal auction mechanism for a large class of fairness constraints. Finding the arameters of this otimal auction, however, turns out to be a non-convex roblem. We show that this non-convex roblem can be reformulated as a more structured non-convex roblem with no saddle oints or local-maxima; this allows us to develo a gradient-descent-based algorithm to solve it. Our emirical results on the A1 Yahoo! dataset demonstrate that our algorithm can obtain uniform coverage across different user tyes for each advertiser at a minor loss to the revenue of the latform, and a small change to the size of the audience each advertiser reaches.
"
1436,2019,Learning Optimal Fair Policies,Poster,"Systematic discriminatory biases resent in our society influence the way data is collected and stored, the way variables are defined, and the way scientific findings are ut into ractice as olicy. Automated decision rocedures and learning algorithms alied to such data may serve to eretuate existing injustice or unfairness in our society. In this aer, we consider how to make otimal but fair decisions, which “break the cycle of injustice” by correcting for the unfair deendence of both decisions and outcomes on sensitive features (e.g., variables that corresond to gender, race, disability, or other rotected attributes). We use methods from causal inference and constrained otimization to learn otimal olicies in a way that addresses multile otential biases which afflict data analysis in sensitive contexts, extending the aroach of Nabi &am; Shitser (2018). Our roosal comes equied with the theoretical guarantee that the chosen fair olicy will induce a joint distribution for new instances that satisfies given fairness constraints. We illustrate our aroach with both synthetic data and real criminal justice data. 
"
1437,2019,Fairness-Aware Learning for Continuous Attributes and Treatments,Poster,"We address the roblem of algorithmic fairness: ensuring that the outcome of a classifier is not biased towards certain values of sensitive variables such as age, race or gender. As common fairness metrics can be exressed as measures of (conditional) indeendence between variables, we roose to use the R\'enyi maximum correlation coefficient to generalize fairness measurement to continuous variables. We exloit Witsenhausen's characterization of the R\'enyi correlation coefficient to roose a differentiable imlementation linked to $f$-divergences. This allows us to generalize fairness-aware learning to continuous variables by using a enalty that uer bounds this coefficient. Theses allows fairness to be extented to variables such as mixed ethnic grous or financial status without thresholds effects. 
This  enalty can be estimated on  mini-batches allowing to use dee nets. Exeriments  show  favorable comarisons  to state of the art on binary variables and rove the ability to rotect continuous ones"
1438,2019,Fairness risk measures,Poster,"Ensuring that classifiers are non-discriminatory or fair with resect to a sensitive feature (e.g., race or gender) is a toical roblem. Progress in this task requires fixing a definition of fairness, and there have been several roosals in this regard over the ast few years. Several of these, however, assume either binary sensitive features (thus recluding categorical or real-valued sensitive grous), or result in non-convex objectives (thus adversely affecting the otimisation landscae). In this aer, we roose a new definition of fairness that generalises some existing roosals, while allowing for generic sensitive features and resulting in a convex objective. The key idea is to enforce that the exected losses (or risks) across each subgrou induced by the sensitive feature are commensurate. We show how this relates to the rich literature on risk measures from mathematical finance. As a secial case, this leads to a new convex fairness-aware objective based on minimising the conditional value at risk (CVaR).
"
1439,2019,Proportionally Fair Clustering,Poster,"We extend the fair machine learning literature by considering the roblem of roortional centroid clustering in a metric context. For clustering n oints with k centers, we define fairness as roortionality to mean that any nk oints are entitled to form their own cluster if there is another center that is closer in distance for all nk oints. We seek clustering solutions to which there are no such justified comlaints from any subsets of agents, without assuming any a riori notion of rotected subsets. We resent and analyze algorithms to efficiently comute, otimize, and audit roortional solutions. We conclude with an emirical examination of the tradeoff between roortional solutions and the k-means objective.
"
1440,2019,Stable and Fair Classification,Poster,"In a recent study, Friedler et al.  ointed out that several fair classification algorithms are not stable with resect to variations in the training set -- a crucial consideration in several alications. Motivated by their work, we study the roblem of designing classification algorithms that are both fair and stable. We roose an extended framework based on fair classification algorithms that are formulated as otimization roblems, by introducing a stability-focused regularization term. Theoretically, we rove an additional stability guarantee, that was lacking in fair classification algorithms, and also rovide an accuracy guarantee for our extended framework. Our accuracy guarantee can be used to inform the selection of the regularization arameter in our framework. We assess the benefits of our aroach emirically by extending several fair classification algorithms that are shown to achieve the best balance between fairness and accuracy over the \textbf{Adult} dataset. Our emirical results show that our extended framework indeed imroves the stability at only a slight sacrifice in accuracy.
"
1441,2019,Flexibly Fair Representation Learning by Disentanglement,Poster,"We consider the roblem of learning reresentations that achieve grou and subgrou fairness with resect to multile sensitive attributes. Taking insiration from the disentangled reresentation learning literature, we roose an algorithm for learning comact reresentations of datasets that are useful for reconstruction and rediction, but are also \emh{flexibly fair}, meaning they can be easily modified at test time to achieve subgrou demograhic arity with resect to multile sensitive attributes and their conjunctions. We show emirically that the resulting encoder---which does not require the sensitive attributes for inference---allows for the adatation of a single reresentation to a variety of fair classification tasks with new target labels and subgrou definitions.
"
1442,2019, Fair Regression: Quantitative Definitions and Reduction-Based Algorithms,Poster,"In this aer, we study the rediction of a real-valued target, such as a risk score or recidivism rate, while guaranteeing a quantitative notion of fairness with resect to a rotected attribute such as gender or race. We call this class of roblems fair regression. We roose general schemes for fair regression under two notions of fairness: (1) statistical arity, which asks that the rediction be statistically indeendent of the rotected attribute, and (2) bounded grou loss, which asks that the rediction error restricted to any rotected grou remain below some re-determined level. While we only study these two notions of fairness, our schemes are alicable to arbitrary Lischitz-continuous losses, and so they encomass least-squares regression, logistic regression, quantile regression, and many other tasks. Our schemes only require access to standard risk minimization algorithms (such as standard classification or least-squares regression) while roviding theoretical guarantees on the otimality and fairness of the obtained solutions. In addition to analyzing theoretical roerties of our schemes, we emirically demonstrate their ability to uncover fairness--accuracy frontiers on several standard datasets.
"
1443,2019,Fairness without Harm: Decoupled Classifiers with Preference Guarantees,Poster,"In domains such as medicine, it can be accetable for machine learning models to include {\em sensitive attributes} such as gender and ethnicity. In this work, we argue that when there is this kind of treatment disarity, then it should be in the best interest of each grou. Drawing on ethical rinciles such as beneficence (""do the best"") and non-maleficence (""do no harm""), we show how to use sensitive attributes to train decouled classifiers that satisfy reference guarantees. These guarantees ensure the majority of individuals in each grou refer their assigned classifier to (i) a ooled model that ignores grou membershi (rationality), and (ii) the model assigned to any other grou (envy-freeness). We introduce a  recursive  rocedure that adatively selects grou attributes for decouling, and resent formal conditions to ensure reference guarantees in terms of generalization error. We validate the effectiveness of the rocedure on real-world datasets, showing that it imroves accuracy without violating  reference guarantees on test data.
"
1444,2019,Differentially Private Fair Learning,Poster,"Motivated by settings in which redictive models may be required to be non-discriminatory with resect to certain attributes (such as race), but even collecting the sensitive attribute may be forbidden or restricted, we initiate the study of fair learning under the constraint of differential rivacy. Our first algorithm is a rivate imlementation of the equalized odds ost-rocessing aroach of (Hardt et al., 2016). This algorithm is aealingly simle, but must be able to use rotected grou membershi exlicitly at test time, which can be viewed as a form of “disarate treatment”. Our second algorithm is a differentially rivate version of the oracle-efficient in-rocessing aroach of (Agarwal et al., 2018) which is more comlex but need not have access to rotected grou membershi at test time. We identify new tradeoffs between fairness, accuracy, and rivacy that emerge only when requiring all three roerties, and show that these tradeoffs can be milder if grou membershi may be used at test time. We conclude with a brief exerimental evaluation.
"
1445,2019,Obtaining Fairness using Optimal Transport Theory,Poster,"In the fair classification setu, we recast the links between fairness and redictability in terms of robability metrics. We analyze reair methods based on maing conditional distributions to the Wasserstein barycenter. We roose a Random Reair which yields a tradeoff between minimal information loss and a certain amount of fairness.
"
1446,2019,Repairing without Retraining: Avoiding Disparate Impact with Counterfactual Distributions,Poster,"When the erformance of a machine learning model varies over grous defined by sensitive attributes (e.g., gender or ethnicity), the erformance  disarity can be exressed in terms of the robability distributions of the inut and outut variables over each grou. In this aer, we exloit this fact to reduce the disarate imact of a fixed classification model over a oulation of interest. Given a black-box classifier, we aim to eliminate the erformance ga by erturbing the distribution of inut variables for the disadvantaged grou. We refer to the erturbed distribution as a counterfactual distribution, and characterize its roerties for common fairness criteria. We introduce a descent algorithm to learn a counterfactual distribution from data. We then discuss how the estimated distribution can be used to build a data rerocessor that can reduce disarate imact without training a new model. We validate our aroach through exeriments on real-world datasets, showing that it can reair different forms of disarity without a significant dro in accuracy.
"
1447,2019,On the Long-term Impact of Algorithmic Decision Policies: Effort Unfairness and Feature Segregation through Social Learning,Poster,"Most existing notions of algorithmic fairness are one-shot: they ensure some form of allocative equality at the time of decision making, but do not account for the adverse imact of the algorithmic decisions today on the long-term welfare and roserity of certain segments of the oulation. We take a broader ersective on algorithmic fairness. We roose an effort-based measure of fairness and resent a data-driven framework for characterizing the long-term imact of algorithmic olicies on reshaing the underlying oulation. Motivated by the sychological literature on social learning and the economic literature on equality of oortunity, we roose a micro-scale model of how individuals may resond to decision-making algorithms. We emloy existing measures of segregation from sociology and economics to quantify the resulting macro- scale oulation-level change. Imortantly, we observe that different models may shift the grou- conditional distribution of qualifications in different directions. Our findings raise a number of imortant questions regarding the formalization of fairness for decision-making models.
"
1448,2019,Making Decisions that Reduce Discriminatory Impacts,Poster,"As machine learning algorithms move into real-world settings, it is crucial to ensure they are aligned with societal values. There has been much work on one asect of this, namely the discriminatory rediction roblem: How can we reduce discrimination in the redictions themselves? While an imortant question, solutions to this roblem only aly in a restricted setting, as we have full control over the redictions. Often we care about the non-discrimination of quantities we do not have full control over. Thus, we describe another key asect of this challenge, the discriminatory imact roblem: How can we reduce discrimination arising from the real-world imact of decisions? To address this, we describe causal methods that model the relevant arts of the real-world system in which the decisions are made. Unlike revious aroaches, these  models not only allow us to ma the causal athway of a single decision, but also to model the effect of interference--how the imact on an individual deends on decisions made about other eole. Often, the goal of decision olicies is to maximize a beneficial imact overall. To reduce the discrimination of these benefits, we devise a constraint insired by recent work in counterfactual fairness, and give an efficient rocedure to solve the constrained otimization roblem. We demonstrate our aroach with an examle: how to increase students taking college entrance exams in New York City ublic schools.
"
1449,2019,"Submodular Maximization beyond Non-negativity: Guarantees, Fast Algorithms, and Applications",Poster,"It is generally believed that submodular functions--and the more general class of 
$\gamma$-weakly submodular functions--may only be otimized under the non-negativity assumtion $f(S) \geq 0$. In this aer, we show that once the function is exressed as the difference $f = g - c$, where $g$ is monotone, non-negative, and $\gamma$-weakly submodular and $c$ is non-negative modular, then strong aroximation guarantees may be obtained. We resent an algorithm for maximizing $g - c$ under a $k$-cardinality constraint which roduces a random feasible set $S$ such that $\mathbb{E}[g(S) -c(S)]  \geq  (1  -  e^{-\gamma}  - \esilon) g(\ot) - c(\ot)$, 
whose running time is $O (\frac{n}{\esilon} \log^2 \frac{1}{\esilon})$, indeendent of $k$. We extend these results to the unconstrained setting by describing an algorithm with the same aroximation guarantees and faster $O(n \frac{1}{\esilon} \log\frac{1}{\esilon})$ runtime. The main techniques underlying our algorithms are two-fold: the use of a surrogate objective which varies the relative imortance between $g$ and $c$ throughout the algorithm, and a geometric swee over ossible $\gamma$ values. Our algorithmic guarantees are comlemented by a hardness result showing that no olynomial-time algorithm which accesses $g$ through a value oracle can do better. We emirically demonstrate the success of our algorithms by alying them to exerimental design on the Boston Housing dataset and directed vertex cover on the Email EU dataset."
1450,2019,Online Algorithms for Rent-Or-Buy with Expert Advice,Poster,"We study the use of redictions by multile exerts (such as machine learning algorithms) to imrove the erformance of online algorithms. In articular, we consider the classical rent-or-buy roblem (also called ski rental), and obtain algorithms that rovably imrove their erformance over the adversarial scenario by using these redictions. We also rove matching lower bounds to show that our algorithms are the best ossible, and erform exeriments to emirically validate their erformance in ractice
"
1451,2019,Non-monotone Submodular Maximization with Nearly Optimal Adaptivity and Query Complexity,Poster,"Submodular maximization is a general otimization roblem with a wide range of alications in machine learning (e.g., active learning, clustering, and feature selection). In large-scale otimization, the arallel running time of an algorithm is governed by its adativity, which measures the number of sequential rounds needed if the algorithm can execute olynomially-many indeendent oracle queries in arallel. While low adativity is ideal, it is not sufficient for an algorithm to be efficient in ractice---there are many alications of distributed submodular otimization where the number of function evaluations becomes rohibitively exensive. Motivated by these alications, we study the adativity and query comlexity of submodular maximization. In this aer, we give the first constant-factor aroximation algorithm for maximizing a non-monotone submodular function subject to a cardinality constraint $k$ that runs in $O(\log(n))$ adative rounds and makes $O(n \log(k))$ oracle queries in exectation. In our emirical study, we use three real-world alications to comare our algorithm with several benchmarks for non-monotone submodular maximization. The results demonstrate that our algorithm finds cometitive solutions using significantly fewer rounds and queries."
1452,2019,Categorical Feature Compression via Submodular Optimization,Poster,"In the era of big data, learning from categorical features with very large vocabularies (e.g., 28 million for the Criteo click rediction dataset) has become a ractical challenge for machine learning researchers and ractitioners.  We design a highly-scalable vocabulary comression algorithm that seeks to maximize the mutual information between the comressed categorical feature and the target binary labels and we furthermore show that its solution is guaranteed to be within a $1-1e \arox 63\%$ factor of the global otimal solution. Although in some settings, entroy-based set functions are known to be submodular, this is not the case for the mutual information objective we consider (mutual information with resect to the target labels).  To address this, we introduce a novel re-arametrization of the mutual information objective, which we rove is submodular, and also design a data structure to query the submodular function in amortized $O(\log n )$ time (where $n$ is the inut vocabulary size). Our comlete algorithm is shown to oerate in $O(n \log n )$ time. Additionally, we design a distributed imlementation in which the query data structure is decomosed across $O(k)$ machines such that each machine only requires $O(\frac n k)$ sace, while still reserving the aroximation guarantee and using only logarithmic rounds of comutation.  We also rovide analysis of simle alternative heuristic comression methods to demonstrate they cannot achieve any aroximation guarantee.  Using the large-scale Criteo learning task, we demonstrate better erformance in retaining mutual information and also verify cometitive learning erformance comared to other baseline methods."
1453,2019,Multi-Frequency Phase Synchronization,Poster,"We roose a novel formulation for hase synchronization---the statistical roblem of jointly estimating alignment angles from noisy airwise comarisons---as a nonconvex otimization roblem that enforces consistency among the airwise comarisons in multile frequency channels. Insired by harmonic retrieval in signal rocessing, we develo a simle yet efficient two-stage algorithm that leverages the
multi-frequency information. We demonstrate in theory and ractice that the roosed algorithm significantly outerforms state-of-the-art hase synchronization algorithms, at a mild comutational costs incurred by using the extra frequency channels. We also extend our algorithmic framework to general synchronization roblems over comact Lie grous.
"
1454,2019,Faster Algorithms for Binary Matrix Factorization,Poster,"We give faster aroximation algorithms for well-studied variants of Binary Matrix Factorization (BMF), where we are given a binary $m \times n$ matrix $A$ and would like to find binary rank-$k$ matrices $U, V$ to minimize the Frobenius norm of $U \cdot V - A$.   In the first setting, $U \cdot V$ denotes multilication over $\mathbb{Z}$, and we give a constant-factor aroximation algorithm that runs in $2^{O(k^2 \log k)} \textrm{oly}(mn)$ time, imroving uon the revious $\min(2^{2^k}, 2^n) \textrm{oly}(mn)$ time.  Our techniques generalize to minimizing $\|U \cdot V - A\|_$ for $ \geq 1$, in $2^{O(k^{\lceil 2 \rceil + 1}\log k)} \textrm{oly}(mn)$ time. For $ = 1$, this has a grah-theoretic consequence, namely, a $2^{O(k^2)} \oly(mn)$-time algorithm to aroximate a grah as a union of disjoint bicliques.  In the second setting, $U \cdot V$ is over $\GF(2)$, and we give a bicriteria constant-factor aroximation algorithm that runs in $2^{O(k^3)} \oly(mn)$ time
to find binary rank-$O(k \log m)$ matrices $U$, $V$ whose cost is as good as the best rank-$k$ aroximation, imroving uon $\min(2^{2^k}mn, \min(m,n)^{k^{O(1)}} \textrm{oly}(mn))$ time."
1455,2019,Guided evolutionary strategies: augmenting random search with surrogate gradients,Poster,"Many alications in machine learning require otimizing a function whose true gradient is unknown or comutationally exensive, but where surrogate gradient information, directions that may be correlated with the true gradient, is chealy available. For examle, this occurs when an aroximate gradient is easier to comute than the full gradient (e.g. in meta-learning or unrolled otimization), or when a true gradient is intractable and is relaced with a surrogate (e.g. in reinforcement learning or training networks with discrete variables). We roose Guided Evolutionary Strategies (GES), a method for otimally using surrogate gradient directions to accelerate random search. GES defines a search distribution for evolutionary strategies that is elongated along a subsace sanned by the surrogate gradients and estimates a descent direction which can then be assed to a first-order otimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subsace and use this to derive a setting of the hyerarameters that works well across roblems. We evaluate GES on several examle roblems, demonstrating an imrovement over both standard evolutionary strategies and first-order methods that directly follow the surrogate gradient.
"
1456,2019,Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional Subspaces,Poster,"Bayesian otimization is known to be difficult to scale to high dimensions, because the acquisition ste requires solving a non-convex otimization roblem in the same search sace. In order to scale the method and kee its benefits, we roose an algorithm (LineBO) that restricts the roblem to a sequence of iteratively chosen one-dimensional sub-roblems that can be solved efficiently. We show that our algorithm converges globally and obtains a fast local rate when the function is strongly convex. Further, if the objective has an invariant subsace, our method automatically adats to the effective dimension without changing the algorithm. When combined with the SafeOt algorithm to solve the sub-roblems, we obtain the first safe Bayesian otimization algorithm with theoretical guarantees alicable in high-dimensional settings. We evaluate our method on multile synthetic benchmarks, where we obtain cometitive erformance. Further, we deloy our algorithm to otimize the beam intensity of the Swiss Free Electron Laser with u to 40 arameters while satisfying safe oeration constraints.
"
1457,2019,Semi-Cyclic Stochastic Gradient Descent,Poster,"We consider convex SGD udates with a block-cyclic structure, i.e., where each cycle consists of a small number of blocks, each with many samles from a ossibly different, block-secific, distribution.  This situation arises, e.g., in Federated Learning where the mobile devices available for udates at different times during the day have different characteristics. We show that such block-cyclic structure can significantly deteriorate the erformance of SGD, but roose a simle  aroach that allows rediction with the same guarantees as for i.i.d., non-cyclic, samling.
"
1458,2019,Matrix-Free Preconditioning in Online Learning,Poster,"We rovide an online convex otimization algorithm with regret that interolates between the regret of an algorithm using an otimal reconditioning matrix and one using a diagonal reconditioning matrix. Our regret bound is never worse than that obtained by diagonal reconditioning, and in certain setting even surasses that of algorithms with full-matrix reconditioning. Imortantly, our algorithm runs in the same time and sace comlexity as online gradient descent. Along the way we incororate new techniques that mildly streamline and imrove logarithmic factors in rior regret analyses. We conclude by benchmarking our algorithm on synthetic data and dee learning tasks.
"
1459,2019,Online Convex Optimization in Adversarial Markov Decision Processes,Poster,"We consider online learning in eisodic loo-free Markov decision rocesses (MDPs), where the  loss function can change arbitrarily between eisodes, and the transition function is not known to the learner.
We show $\tilde{O}(L|X|\sqrt{|A|T})$ regret bound, where $T$ is the number of eisodes, $X$ is the state sace, $A$ is the action sace, and $L$ is the length of each eisode. 
Our online algorithm is imlemented using entroic regularization methodology, which allows to extend the original adversarial MDP model to handle convex erformance criteria (different ways to aggregate the losses of a single eisode) , as well as imrove revious regret bounds."
1460,2019,Competing Against Nash Equilibria in Adversarially Changing Zero-Sum Games,Poster,"We study the roblem of reeated lay in a zero-sum game in which the ayoff matrix may change, in a ossibly adversarial fashion, on each round; we call these Online Matrix Games. Finding the Nash Equilibrium (NE) of a two layer zero-sum game is core to many roblems in statistics, otimization, and economics, and for a fixed game matrix this can be easily reduced to solving a linear rogram. But when the ayoff matrix evolves over time our goal is to find a sequential algorithm that can comete with, in a certain sense, the NE of the long-term-averaged ayoff matrix. We design an algorithm with small NE regret--that is, we ensure that the long-term ayoff of both layers is close to minimax otimum in hindsight. Our algorithm achieves near-otimal deendence with resect to the number of rounds and deends oly-logarithmically on the number of available actions of the layers. Additionally, we show that the naive reduction, where each layer simly minimizes its own regret, fails to achieve the stated objective regardless of which algorithm is used. Lastly, we consider the so-called bandit setting, where the feedback is significantly limited, and we rovide an algorithm with small NE regret using one-oint estimates of each ayoff matrix.
"
1461,2019,Online Learning with Sleeping Experts and Feedback Graphs,Poster,"We consider the scenario of online learning with sleeing exerts, where not all exerts are available at each round, and analyze the general framework of learning with feedback grahs, where the loss observations associated with each exert are characterized by a grah. A critical assumtion in this framework is that the loss observations and the set of sleeing exerts at each round are indeendent. We first extend the classical sleeing exerts algorithm of Kleinberg et al. 2008 to the feedback grahs scenario, and rove matching uer and lower bounds for the sleeing regret of the resulting algorithm under the indeendence assumtion. Our main contribution is then to relax this assumtion, resent a more general notion of sleeing regret, and derive a general algorithm with strong theoretical guarantees. We aly this new framework to the imortant scenario of online learning with abstention, where a learner can elect to abstain from making a rediction at the rice of a certain cost. We emirically validate our algorithm against multile online abstention algorithms on several real-world datasets, showing substantial erformance imrovements.
"
1462,2019,Incremental Randomized Sketching for Online Kernel Learning,Poster,"Randomized sketching has been used in offline kernel learning, but it cannot be alied directly to online kernel learning due to the lack of incremental maintenances for randomized sketches with regret guarantees. To address these issues, we roose a novel incremental randomized sketching aroach for online kernel learning, which has efficient incremental maintenances with theoretical guarantees. We construct two incremental randomized sketches using the sarse transform matrix and the samling matrix for kernel matrix aroximation, udate the incremental randomized sketches using rank-$1$ modifications, and construct an time-varying exlicit feature maing for online kernel learning. We rove that the roosed incremental randomized sketching is statistically unbiased for the matrix roduct aroximation, obtains a $1 + \esilon$ relative-error bound for the kernel matrix aroximation, enjoys a sublinear regret bound for online kernel learning, and has constant time and sace comlexities at each round for incremental maintenances. Exerimental results demonstrate that the incremental randomized sketching achieves a better learning erformance in terms of accuracy and efficiency even in adversarial environments."
1463,2019,Adaptive Scale-Invariant Online Algorithms for Learning Linear Models,Poster,"We consider online learning with linear models, where the algorithm redicts on sequentially revealed instances (feature vectors), and is comared against the best linear function (comarator) in hindsight. Poular algorithms in this framework, such as Online Gradient Descent (OGD), have arameters (learning rates), which ideally should be tuned based on the scales of the features and the otimal comarator, but these quantities only become available at the end of the learning rocess. In this aer, we resolve the tuning roblem by roosing online algorithms making redictions which are invariant under arbitrary rescaling of the features. The algorithms have no arameters to tune, do not require any rior knowledge on the scale of the instances or the comarator, and achieve regret bounds matching (u to a logarithmic factor) that of OGD with otimally tuned searate learning rates er dimension, while retaining comarable runtime erformance.
"
1464,2019,Online Control with Adversarial Disturbances,Poster,"We study the control of linear dynamical systems with adversarial disturbances, as oosed to statistical noise. We resent an efficient algorithm that achieves nearly-tight regret bounds in this setting. Our result generalizes uon revious work in two main asects: the algorithm can accommodate adversarial noise in the dynamics, and can handle general convex costs.
"
1465,2019,Adversarial Online Learning with noise,Poster,"We resent and study models of adversarial online learning where the feedback observed by the learner is noisy, and the feedback is either full information feedback or bandit feedback. Secifically, we consider binary losses xored with the noise, which is a Bernoulli random variable. We consider both a constant noise rate and a variable noise rate. Our main results are tight regret bounds for learning with noise in the adversarial online learning model.
"
1466,2019,Online Variance Reduction with Mixtures,Poster,"Adative imortance samling for stochastic otimization is a romising aroach that offers imroved convergence through variance reduction. In this work, we roose a new framework for variance reduction that enables the use of mixtures over redefined samling distributions, which can naturally encode rior knowledge about the data. While these samling distributions are fixed, the mixture weights are adated during the otimization rocess. We roose VRM, a novel and efficient adative scheme that asymtotically recovers the best mixture weights in hindsight and can also accommodate samling distributions over sets of oints. We emirically demonstrate the versatility of VRM in a range of alications.
"
1467,2019,Bandit Multiclass Linear Classification: Efficient Algorithms for the Separable Case,Poster,"We study the roblem of efficient online multiclass linear classification with
bandit feedback, where all examles belong to one of $K$ classes and lie in the
$d$-dimensional Euclidean sace. Previous works have left oen the challenge of
designing efficient algorithms with finite mistake bounds when the data is
linearly searable by a margin $\gamma$. In this work, we take a first ste
towards this roblem. We consider two notions of linear searability:
strong and weak.

1. Under the strong linear searability condition, we design an efficient
algorithm that achieves a near-otimal mistake bound of
$O\left(\frac{K}{\gamma^2} \right)$.

2. Under the more challenging weak linear searability condition, we design
an efficient algorithm with a mistake bound of $2^{\widetilde{O}(\min(K \log^2
\frac{1}{\gamma}, \sqrt{\frac{1}{\gamma}} \log K))}$. Our algorithm
is based on kernel Percetron, which is insired by the work
of Klivans & Servedio (2008) on imroerly learning intersection of halfsaces."
1468,2019,Learning Linear-Quadratic Regulators Efficiently with only $\sqrt{T}$ Regret,Poster,"We resent the first comutationally-efficient algorithm with $\widetilde{O}(\sqrt{T})$ regret for learning in Linear Quadratic Control systems with unknown dynamics.
By that, we resolve an oen question of Abbasi-Yadkori and Szeesvari (2011) and Dean,Mania, Matni, Recht, and Tu (2018)."
1469,2019,Learning from Delayed Outcomes via Proxies with Applications to Recommender Systems,Poster,"Predicting delayed outcomes is an imortant roblem in recommender systems (e.g., if customers will finish reading an ebook). We formalize the roblem as an adversarial, delayed online learning roblem and consider how a roxy for the delayed outcome (e.g., if customers read a third of the book in 24 hours) can hel minimize regret, even though the roxy is not available when making a rediction. Motivated by our regret analysis, we roose two neural network architectures: Factored Forecaster (FF) which is ideal if the roxy is informative of the outcome in hindsight, and Residual Factored Forecaster (RFF)  that is robust to a non-informative roxy. Exeriments on two real-world datasets for redicting human behavior show that RFF outerforms both FF and a direct forecaster that does not make use of the roxy. Our results suggest that exloiting roxies by factorization is a romising way to mitigate the imact of long delays in human-behavior rediction tasks.
"
1470,2019,Adaptive Regret of Convex and Smooth Functions,Poster,"We investigate online convex otimization in changing environments, and choose the adative regret as the erformance measure. The goal is to achieve a small regret over every interval so that the comarator is allowed to change over time. Different from revious works that only utilize the convexity condition, this aer further exloits smoothness to imrove the adative regret. To this end, we develo novel adative algorithms for convex and smooth functions, and establish roblem-deendent regret bounds over any interval. Our regret bounds are comarable to existing results in the worst case, and become much tighter when the comarator has a small loss.
"
1471,2019,Online Adaptive Principal Component Analysis and Its extensions,Poster,"We roose algorithms for online rincial comonent analysis (PCA)
and variance minimization for adative settings.
Previous literature has focused on uer bounding the static adversarial regret,
whose comarator is the otimal fixed action in hindsight.
However, static regret is not an aroriate metric when the underlying environment is changing.
Instead, we adot the adative regret metric from the revious literature 
and roose online adative algorithms for PCA and variance minimization, 
that have sub-linear adative regret guarantees.
We demonstrate both theoretically and exerimentally that
the roosed algorithms can adat to the changing environments.
"
1472,2019,POLITEX: Regret Bounds for Policy Iteration using Expert Prediction,Poster,"We resent POLITEX (POLicy ITeration with EXert advice), a variant of olicy iteration where each olicy is a Boltzmann distribution over the sum of action-value function estimates of the revious olicies, and analyze its regret in continuing RL roblems.  We assume that the value function error after running a olicy for $\tau$ time stes scales as $\es(\tau) = \es_0 + O(\sqrt{d\tau})$, where $\es_0$ is the worst-case aroximation error and $d$ is the number of features in a comressed reresentation of the state-action sace. We establish that this condition is satisfied by the LSPE algorithm under certain assumtions on the MDP and olicies. Under the error assumtion, we show that the regret of POLITEX  in uniformly mixing MDPs scales  as  $O(d^{12}T^{34} + \es_0T)$, where $O(\cdot)$ hides logarithmic terms and roblem-deendent constants. Thus, we rovide the first regret bound for a fully ractical model-free method which only scales in the number of features, and not in the size of the underlying MDP. Exeriments on a queuing roblem confirm that POLITEX is cometitive with some of its alternatives, while reliminary results on Ms Pacman (one of the standard Atari benchmark roblems) confirm the viability of POLITEX beyond linear function aroximation."
1473,2019,"Anytime Online-to-Batch, Optimism and Acceleration",Poster,"A standard way to obtain convergence guarantees in stochastic convex otimization is to run an online learning algorithm and then outut the average of its iterates: the actual iterates of the online learning algorithm do not come with individual guarantees. We close this ga by introducing a black-box modification to any online learning algorithm whose iterates converge to the otimum in stochastic scenarios. We then consider the case of smooth losses, and show that combining our aroach with otimistic online learning algorithms immediately yields a fast convergence rate of $O(LT^{32}+\sigma\sqrt{T})$ on $L$-smooth roblems with $\sigma^2$ variance in the gradients. Finally, we rovide a reduction that converts any adative online algorithm into one that obtains the otimal accelerated rate of $\tilde O(LT^2 + \sigma\sqrt{T})$, while still maintaining $\tilde O(1\sqrt{T})$ convergence in the non-smooth setting. Imortantly, our algorithms adat to $L$ and $\sigma$ automatically: they do not need to know either to obtain these rates."
1474,2019,Cautious Regret Minimization: Online Optimization with Long-Term Budget Constraints,Poster,"We study a class of online convex otimization roblems with long-term budget constraints that arise naturally as reliability guarantees or total consumtion constraints. In this general setting, rior work by Mannor et al. (2009) has shown that achieving no regret is imossible if the functions defining the agent's budget are chosen by an adversary. To overcome this obstacle, we refine the agent's regret metric by introducing the notion of a ""K-benchmark"", i.e., a comarator which meets the roblem's allotted budget over any window of length K. The imossibility analysis of Mannor et al. (2009) is recovered when K=T; however, for K=o(T), we show that it is ossible to minimize regret while still meeting the roblem's long-term budget constraints. We achieve this via an online learning olicy based on Cautious Online Lagrangiant Descent (COLD) for which we derive exlicit bounds, in terms of both the incurred regret and the residual budget violations.
"
1475,2019,Optimal Kronecker-Sum Approximation of Real Time Recurrent Learning,Poster,"One of the central goals of Recurrent Neural Networks (RNNs) is to learn long-term deendencies in sequential data. 
Nevertheless, the most oular training method, Truncated Backroagation through Time (TBPTT), categorically forbids learning deendencies beyond the truncation horizon.
In contrast, the online training algorithm Real Time Recurrent Learning (RTRL) rovides untruncated gradients, with the disadvantage of imractically large comutational costs. 
Recently ublished aroaches reduce these costs by roviding noisy aroximations of RTRL. 
We resent a new aroximation algorithm of RTRL, Otimal Kronecker-Sum Aroximation (OK).
We rove that OK is otimal for a class of aroximations of RTRL, which includes all aroaches ublished so far. 
Additionally, we show that OK has emirically negligible noise: Unlike revious algorithms it matches TBPTT in a real world task (character-level Penn TreeBank) and can exloit online arameter udates to outerform TBPTT in a synthetic 
string memorization task.
Code available at GitHub.
"
1476,2019,Adaptive Sensor Placement for Continuous Spaces,Poster,We consider the roblem of adatively lacing sensors along an interval to detect stochastically-generated events. We resent a new formulation of the roblem as a continuum-armed bandit roblem with feedback in the form of artial observations of realisations of an inhomogeneous Poisson rocess. We design a solution method by combining Thomson samling with nonarametric inference via increasingly granular Bayesian histograms and derive an $\tilde{O}(T^{23})$ bound on the Bayesian regret in $T$ rounds. This is couled with the design of an efficent otimisation aroach to select actions in olynomial time. In simulations we demonstrate our aroach to have substantially lower and less variable regret than cometitor algorithms.
1477,2019,Scale-free adaptive planning for deterministic dynamics & discounted rewards,Poster,"We address the roblem of lanning in an environment with deterministic dynamics and stochastic discounted rewards under a limited numerical budget where the ranges of both rewards and noise are unknown.  We introduce PlaTyOOS, an adative, robust, and efficient alternative to the OLOP (oen-loo otimistic lanning) algorithm.  Whereas OLOP requires a riori knowledge of the ranges of both rewards and noise, PlaTyOOS dynamically adats its behavior to both.  This allows PlaTyOOS to be immune to two vulnerabilities of OLOP: failure when given underestimated ranges of noise and rewards and inefficiency when these are overestimated. PlaTyOOS additionally adats to the global smoothness of the value function. PlaTyOOS acts in a rovably more efficient manner vs. OLOP when OLOP is given an overestimated reward and show that in the case of no noise, PlaTyOOS learns exonentially faster.
"
1478,2019,Communication-Constrained Inference and the Role of Shared Randomness,Poster,"A central server needs to erform statistical inference based on samles that are distributed over multile users  who can each send a message of limited length to the center. We study  roblems of distribution learning and identity testing in this distributed inference setting and examine the role of shared randomness as a resource. We roose a general urose \textit{simulate-and-infer} strategy that  uses only rivate-coin communication rotocols and is samle-otimal for distribution learning. This general strategy turns out to be samle-otimal even for distribution testing among rivate-coin rotocols. Interestingly, we roose a ublic-coin rotocol that outerforms simulate-and-infer for distribution testing and is, in fact, samle-otimal. Underlying our ublic-coin rotocol is a random hash that when alied to the samles minimally contracts the chi-squared distance of their distribution from the uniform distribution. 
"
1479,2019,Learning and Data Selection in Big Datasets,Poster,"Finding a dataset of minimal cardinality to characterize the otimal arameters of a model is of aramount imortance in machine learning and distributed otimization over a network. This aer investigates the comressibility of large datasets. More secifically, we roose a framework that jointly learns the inut-outut maing
as well as the most reresentative samles of the dataset (sufficient dataset). Our analytical results show that the cardinality of the sufficient dataset increases sub-linearly with resect to the original dataset size. Numerical evaluations of real
datasets reveal a large comressibility, u to 95%, without a noticeable dro in the learnability erformance, measured by the generalization error.
"
1480,2019,Sublinear quantum algorithms for training linear and kernel-based classifiers,Poster,"We investigate quantum algorithms for classification, a fundamental roblem in machine learning, with rovable guarantees. Given $n$ $d$-dimensional data oints, the state-of-the-art (and otimal) classical algorithm for training classifiers with constant margin by Clarkson et al. runs in $\tilde{O}(n +d)$, which is also otimal in its inutoutut model. We design sublinear quantum algorithms for the same task running in $\tilde{O}(\sqrt{n} +\sqrt{d})$, a quadratic imrovement in both $n$ and $d$. Moreover, our algorithms use the standard quantization of the classical inut and generate the same classical outut, suggesting minimal overheads when used as subroutines for end-to-end alications. We also demonstrate a tight lower bound (u to oly-log factors) and discuss the ossibility of imlementation on near-term quantum machines."
1481,2019,Agnostic Federated Learning,Poster,"A key learning scenario in large-scale alications is that of federated learning, where a centralized model is trained based on data originating from a large number of clients. We argue that, with the existing training and inference, federated models can be biased towards different clients. Instead, we roose a new framework of agnostic federated learning, where the centralized model is otimized for any target distribution formed by a mixture of the client distributions. We further show that this   framework naturally yields a notion of fairness. We resent data-deendent Rademacher comlexity guarantees for learning with this objective, which guide the definition of an algorithm for  agnostic federated learning. We also give a fast stochastic otimization algorithm for solving the corresonding otimization  roblem, for which we rove convergence bounds, assuming a convex loss function and a convex hyothesis set. We further emirically demonstrate the benefits of our aroach in several datasets. Beyond federated learning, our framework and algorithm can be of interest to other learning scenarios such as cloud comuting, domain adatation, drifting, and other contexts where the training and test distributions do not coincide.
"
1482,2019,Discovering Conditionally Salient Features with Statistical Guarantees,Poster,"The goal of feature selection is to identify imortant features that are relevant to exlain a outcome variable. Most of the work in this domain has focused on identifying \emh{globally} relevant features, which are features that are related to the outcome using evidence across the entire dataset. We study a more fine-grained statistical roblem: \emh{conditional feature selection}, where a feature may be relevant deending on the values of the other features. For examle in genetic association studies, variant $A$ could be associated with the henotye in the entire dataset, but conditioned on variant $B$ being resent it might be indeendent of the henotye. In this sense, variant $A$ is globally relevant, but conditioned on $B$ it is  no longer locally relevant in that region of the feature sace.  We resent a generalization of the knockoff rocedure that erforms \emh{conditional feature selection} while controlling a generalization of the false discovery rate (FDR) to the conditional setting. By exloiting the featureresonse model-free framework of the knockoffs, the quality of the statistical FDR guarantee is not degraded even when we erform conditional feature selections. We imlement this method and resent an algorithm that automatically artitions the feature sace such that it enhances the differences between selected sets in different regions, and validate the statistical theoretical results with exeriments."
1483,2019,A Theoretical Analysis of Contrastive Unsupervised Representation Learning,Poster,"Recent emirical works have successfully used unlabeled data to learn feature reresentations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of the well-known word2vec embedding algorithm: leveraging availability of airs of semantically codesimilar""  data oints andcodenegative samles,"" the learner forces the inner roduct of reresentations of similar airs with each other to be higher on average than with negative samles. The current aer uses the term {\em contrastive learning} for such algorithms and resents a theoretical framework for analyzing them by introducing {\em latent classes} and hyothesizing that semantically similar oints are samled from the same latent class. This framework allows us to show rovable guarantees on the erformance of the learned reresentations on the average classification task that is comrised of a subset of the same set of latent classes. Our generalization bound also shows that learned reresentations can reduce (labeled) samle comlexity on downstream tasks. We conduct controlled exeriments in both the text and image domains to suort the theory.
"
1484,2019,The information-theoretic value of unlabeled data in semi-supervised learning,Poster,"We quantify the searation between the numbers of labeled examles required to
learn in two settings: Settings with and without the knowledge of
the distribution of the unlabeled data. More secifically, we rove a searation
by $\Theta(\log n)$ multilicative factor for the class of rojections over
the Boolean hyercube of dimension $n$. We rove that there is no searation
for the class of all functions on domain of any size. Learning with the knowledge of the distribution (a.k.a. fixed-distribution
learning) can be viewed as an idealized scenario of semi-suervised learning
where the number of unlabeled data oints is so great that the unlabeled
distribution is known exactly. For this reason, we call the searation the
value of unlabeled data."
1485,2019,Unsupervised Label Noise Modeling and Loss Correction,Poster,"Desite being robust to small amounts of label noise, convolutional neural networks trained with stochastic gradient methods have been shown to easily fit random labels. When there are a mixture of correct and mislabelled targets, networks tend to fit the former before the latter. This suggests using a suitable two-comonent mixture model as an unsuervised generative model of samle loss values during training to allow online estimation of the robability that a samle is mislabelled. Secifically, we roose a beta mixture to estimate this robability and correct the loss by relying on the network rediction (the so-called bootstraing loss). We further adat mixu augmentation to drive our aroach a ste further. 
Exeriments on CIFAR-10100 and TinyImageNet demonstrate a robustness to label noise that substantially outerforms recent state-of-the-art. Source code is available at htts:git.iofjsvE and Aendix at htts:arxiv.orgabs1904.11238.
"
1486,2019,Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment,Poster,"Domain adatation addresses the common situation in which the target distribution generating our test data differs from the source distribution generating our training data. While absent assumtions, domain adatation is imossible, strict conditions, e.g. covariate or label shift, enable rinciled algorithms. Recently-roosed domain-adversarial aroaches consist of aligning source and target encodings, an aroach often motivated as minimizing two (of three) terms in a theoretical bound on target error. Unfortunately, this minimization can cause arbitrary increases in the third term, a roblem guaranteed to arise under shifting label distributions. We roose asymmetrically-relaxed distribution alignment, a new aroach that overcomes some limitations of standard domain-adversarial algorithms. Moreover, we characterize recise assumtions under which our algorithm is theoretically rinciled and demonstrate emirical benefits on both synthetic and real datasets.
"
1487,2019,Pareto Optimal Streaming Unsupervised Classification,Poster,"We study an online and streaming unsuervised classification system. Our setting consists of a collection of classifiers (with unknown confusion matrices) each of which can classify one samle er unit time, and which are accessed by a stream of unlabeled samles. Each samle is disatched to one or more classifiers, and deending on the labels collected from these classifiers, may be sent to other classifiers to collect additional labels. The labels are continually aggregated. Once the aggregated label has high enough accuracy (a re-secified threshold for accuracy) or the samle is sent to all the classifiers, the now labeled samle is ejected from the system. For any given re-secified threshold for accuracy, the objective is to sustain the maximum ossible rate of arrival of new samles, such that the number of samles in memory does not grow unbounded. In this aer, we characterize the Pareto-otimal region of accuracy and arrival rate, and develo an algorithm that can oerate at any oint within this region. Our algorithm uses queueing-based routing and scheduling aroaches combined with novel online tensor decomosition method to learn the hidden arameters, to Pareto-otimality guarantees. We finally verify our theoretical results through simulations on two ensembles formed using AlexNet, VGG, and ResNet dee image classifiers.
"
1488,2019,Geometric Losses for Distributional Learning,Poster,"Building uon recent advances in entroy-regularized otimal transort, and uon Fenchel duality between measures and continuous functions, we roose a generalization of the logistic loss that incororates a metric or cost between classes. Unlike revious attemts to use otimal transort distances for learning, our loss results in unconstrained convex objective functions, suorts infinite (or very large) class saces, and naturally defines a geometric generalization of the softmax oerator. The geometric roerties of this loss make it suitable for redicting sarse and singular distributions, for instance suorted on curves or hyer-surfaces. We study the theoretical roerties of our loss and showcase its effectiveness on two alications: ordinal regression and drawing generation.
"
1489,2019,"Classification from Positive, Unlabeled and Biased Negative Data",Poster,"In binary classification, there are situations where negative (N) data are too diverse to be fully labeled and we often resort to ositive-unlabeled (PU) learning in these scenarios. However, collecting a non-reresentative N set that contains only a small ortion of all ossible N data can often be much easier in ractice. This aer studies a novel classification framework which incororates such biased N (bN) data in PU learning. We rovide a method based on emirical risk minimization to address this PUbN classification roblem. Our aroach can be regarded as a novel examle-weighting algorithm, with the weight of each examle comuted through a reliminary ste that draws insiration from PU learning. We also derive an estimation error bound for the roosed method. Exerimental results demonstrate the effectiveness of our algorithm in not only PUbN learning scenarios but also ordinary PU learning scenarios on several benchmark datasets.
"
1490,2019,Complementary-Label Learning for Arbitrary Losses and Models,Poster,"In contrast to the standard classification aradigm where the true class is given to each training attern, comlementary-label learning only uses training atterns each equied with a comlementary label, which only secifies one of the classes that the attern does not belong to. The goal of this aer is to derive a novel framework of comlementary-label learning with an unbiased estimator of the classification risk, for arbitrary losses and models---all existing methods have failed to achieve this goal. Not only is this beneficial for the learning stage, it also makes modelhyer-arameter selection (through cross-validation) ossible without the need of any ordinarily labeled validation data, while using any linearnon-linear models or convexnon-convex loss functions. We further imrove the risk estimator by a non-negative correction and gradient ascent trick, and demonstrate its sueriority through exeriments.
"
1491,2019,Learning to Infer Program Sketches,Poster,"Our goal is to build systems which write code automatically from the kinds of secifications humans can most easily rovide, such as examles and natural language instruction. The key idea of this work is that a flexible combination of attern recognition and exlicit reasoning can be used to solve these comlex rogramming roblems. We roose a method for dynamically integrating these tyes of information. Our novel intermediate reresentation and training algorithm allow a rogram synthesis system to learn, without direct suervision, when to rely on attern recognition and when to erform symbolic search. Our model matches the memorization and generalization erformance of neural synthesis and symbolic search, resectively, and achieves state-of-the-art erformance on a dataset of simle English descrition-to-code rogramming roblems.
"
1492,2019,Hierarchically Structured Meta-learning,Poster,"In order to learn quickly with few samles, meta-learning utilizes rior knowledge learned from revious tasks. However, a critical challenge in meta-learning is task uncertainty and heterogeneity, which can not be handled via globally sharing knowledge among tasks. In this aer, based on gradient-based meta-learning, we roose a hierarchically structured meta-learning (HSML) algorithm that exlicitly tailors the transferable knowledge to different clusters of tasks. Insired by the way human beings organize knowledge, we resort to a hierarchical task clustering structure to cluster tasks. As a result, the roosed aroach not only addresses the challenge via the knowledge customization to different clusters of tasks, but also reserves knowledge generalization among a cluster of similar tasks. To tackle the changing of task relationshi, in addition, we extend the hierarchical structure to a continual learning environment. The exerimental results show that our aroach can achieve state-of-the-art erformance in both toy-regression and few-shot image classification roblems.
"
1493,2019,Bridging Theory and Algorithm for Domain Adaptation,Poster,"This aer addresses the roblem of unsuervised domain adation from theoretical and algorithmic ersectives. Existing domain adatation theories naturally imly minimax otimization algorithms, which connect well with the domain adatation methods based on adversarial learning. However, several disconnections still exist and form the ga between theory and algorithm. We extend revious theories (Mansour et al., 2009c; Ben-David et al., 2010) to multiclass classification in domain adatation, where classifiers based on the scoring functions and margin loss are standard choices in algorithm design. We introduce Margin Disarity Discreancy, a novel measurement with rigorous generalization bounds, tailored to the distribution comarison with the asymmetric margin loss, and to the minimax otimization for easier training. Our theory can be seamlessly transformed into an adversarial learning algorithm for domain adatation, successfully bridging the ga between theory and algorithm. A series of emirical studies show that our algorithm achieves the state of the art accuracies on challenging domain adatation tasks.
"
1494,2019,Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation,Poster,"Desite the remarkable success of Dee RL in learning control olicies from raw ixels, the resulting models do not generalize. We demonstrate that a trained agent fails comletely when facing small visual changes, and that fine-tuning---the common transfer learning aradigm---fails to adat to these changes, to the extent that it is faster to re-train the model from scratch. We show that by searating the visual transfer task from the control olicy we achieve substantially better samle efficiency and transfer behavior, allowing an agent trained on the source task to transfer well to the target tasks. The visual maing from the target to the source domain is erformed using unaligned GANs, resulting in a control olicy that can be further imroved using imitation learning from imerfect demonstrations. We demonstrate the aroach on synthetic visual variants of the Breakout game, as well as on transfer between subsequent levels of Road Fighter, a Nintendo car-driving game. A visualization of our aroach can be seen in \url{htts:youtu.be4mnkzYyXMn4} and \url{htts:youtu.beKCGTrQi6Ogo}.
"
1495,2019,Learning What and Where to Transfer,Poster,"As the alication of dee learning has exanded to real-world roblems with insufficient volume of training data, transfer learning recently has gained much attention as means of imroving the erformance in such small-data regime. However, when existing methods are alied between heterogeneous architectures and tasks, it becomes more imortant to manage their detailed configurations and often requires exhaustive tuning on them for the desired erformance. To address the issue, we roose a novel transfer learning aroach based on meta-learning that can automatically learn what knowledge to transfer from the source network to where in the target network. Given source and target networks, we roose an efficient training scheme to learn meta-networks that decide (a) which airs of layers between the source and target networks should be matched for knowledge transfer and (b) which features and how much knowledge from each feature should be transferred. We validate our meta-transfer aroach against recent transfer learning methods on various datasets and network architectures, on which our automated scheme significantly outerforms the rior baselines that find “what and where to transfer” in a hand-crafted manner.
"
1496,2019,DBSCAN++: Towards fast and scalable density clustering,Poster,"DBSCAN is a classical density-based clustering rocedure with tremendous ractical relevance. However, DBSCAN imlicitly needs to comute the emirical density for each samle oint, leading to a quadratic  worst-case  time  comlexity, which is too slow on large datasets. We roose DBSCAN++, a simle modification of DBSCAN which only requires comuting the densities for a chosen subset of oints. We show emirically that, comared to traditional DBSCAN, DBSCAN++ can rovide not only cometitive erformance but also added robustness in the bandwidth hyerarameter while taking a fraction of the runtime. We also resent statistical consistency guarantees showing the trade-off between comutational cost and estimation rates.  Surrisingly, u to a certain oint, we can enjoy the same estimation rates while lowering comutational cost, showing that DBSCAN++ is a sub-quadratic algorithm that attains minimax otimal rates for level-set estimation, a quality that may be of indeendent interest.
"
1497,2019,Concrete Autoencoders: Differentiable Feature Selection and Reconstruction,Poster,"We introduce the concrete autoencoder, an end-to-end differentiable method for global feature selection, which efficiently identifies a subset of the most informative features and simultaneously learns a neural network to reconstruct the inut data from the selected features. Our method is unsuervised, and is based on using a concrete selector layer as the encoder and using a standard neural network as the decoder. During the training hase, the temerature of the concrete selector layer is gradually decreased, which encourages a user-secified number of discrete features to be learned; during test time, the selected features can be used with the decoder network to reconstruct the remaining inut features.  We evaluate concrete autoencoders on a variety of datasets, where they significantly outerform state-of-the-art methods for feature selection and data reconstruction. In articular, on a large-scale gene exression dataset, the concrete autoencoder selects a small subset of genes whose exression levels can be used to imute the exression levels of the remaining genes; in doing so, it imroves on the current widely-used exert-curated L1000 landmark genes, otentially reducing measurement costs by 20%.  The concrete autoencoder can be imlemented by adding just a few lines of code to a standard autoencoder, and the code for the algorithm and exeriments is ublicly available.
"
1498,2019,Gromov-Wasserstein Learning for Graph Matching and Node Embedding,Poster,"A novel Gromov-Wasserstein learning framework is roosed to jointly match (align) grahs and learn embedding vectors for the associated grah nodes. 
Using Gromov-Wasserstein discreancy, we measure the dissimilarity between two grahs and find their corresondence, according to the learned otimal transort. 
The node embeddings associated with the two grahs are learned under the guidance of the otimal transort, the distance of which not only reflects the toological structure of each grah but also yields the corresondence across the grahs. 
These two learning stes are mutually-beneficial, and are unified here by minimizing the Gromov-Wasserstein discreancy with structural regularizers. This framework leads to an otimization roblem that is solved by a roximal oint method.
We aly the roosed method to matching roblems in real-world networks, and demonstrate its suerior erformance comared to alternative aroaches.
"
1499,2019,Spectral Clustering of Signed Graphs via Matrix Power Means,Poster,"Signed grahs encode ositive (attractive) and negative (reulsive) relations between nodes. We extend sectral clustering to signed grahs  via the one-arameter family of Signed Power Mean Lalacians, defined as the matrix ower mean of normalized standard and signless Lalacians of ositive and negative edges. We rovide a thorough analysis of the roosed aroach in the setting of a general Stochastic Block Model that includes models such as the Labeled Stochastic Block Model and the Censored Block Model. We show that in exectation the signed ower mean Lalacian catures the  ground truth clusters under reasonable settings where state-of-the-art aroaches fail. Moreover, we rove that the eigenvalues and  eigenvector of the signed ower mean Lalacian concentrate around their exectation under reasonable conditions in the general Stochastic Block Model. Extensive exeriments on random grahs and real world datasets confirm the theoretically redicted behaviour of the signed ower mean Lalacian and show that it comares favourably with state-of-the-art methods. 
"
1500,2019,Coresets for Ordered Weighted Clustering,Poster,"We design coresets for Ordered k-Median, a generalization of classical clustering roblems such as k-Median and k-Center. Its objective function is defined via the Ordered Weighted Averaging (OWA) aradigm of Yager (1988), where data oints are weighted according to a redefined weight vector, but in order of their contribution to the objective (distance from the centers). A owerful data-reduction technique, called a coreset, is to summarize a oint set $X$ in $\mathbb{R}^d$ into a small (weighted) oint set $X'$, such that for every set of $k$ otential centers, the objective value of the coreset $X'$ aroximates that of $X$ within factor $1\m \esilon$. When there are multile objectives (weights), the above standard coreset might have limited usefulness, whereas in a \emh{simultaneous} coreset, the above aroximation holds for all weights (in addition to all centers). Our main result is a construction of a simultaneous coreset of size $O_{\esilon, d}(k^2 \log^2 |X|)$ for Ordered k-Median. We validate our algorithm on a real geograhical data set, and we find our coreset leads to a massive seedu of clustering comutations, while maintaining high accuracy for a range of weights. "
1501,2019,Fair k-Center Clustering for Data Summarization,Poster,"In data summarization we want to choose $k$ rototyes in order to summarize a data set. We study a setting where the data set comrises several demograhic grous and we are restricted to choose $k_i$ rototyes belonging to grou $i$. A common aroach to the roblem without the fairness constraint is to otimize a centroid-based clustering objective such as $k$-center. A natural extension then is to incororate the fairness constraint into the clustering roblem. Existing algorithms for doing so run in time suer-quadratic in the size of the data set, which is in contrast to the standard $k$-center roblem being aroximable in linear time. In this aer, we resolve this ga by roviding a simle aroximation algorithm for the $k$-center roblem under the fairness constraint with running time linear in the size of the data set and $k$. If the number of demograhic grous is small, the aroximation guarantee of our algorithm only incurs a constant-factor overhead."
1502,2019,A Better k-means++ Algorithm via Local Search,Poster,"In this aer, we develo a new variant of k-means++ seeding that in exectation achieves a constant aroximation guarantee. We obtain this result by a simle combination of k-means++ samling with a local search strategy.
We evaluate our algorithm emirically and show that it also imroves the quality of a solution in ractice.
"
1503,2019,Kernel Normalized Cut: a Theoretical Revisit,Poster,"In this aer, we study the theoretical roerties of clustering based on the kernel normalized cut. Our first contribution is to derive a nonasymtotic uer bound on the exected distortion rate of the kernel normalized cut. From this result, we show that the solution of the kernel normalized cut converges to that of the oulation-level weighted k-means clustering on a certain reroducing kernel Hilbert sace (RKHS). Our second contribution is the discover of the interesting fact that the oulation-level weighted k-means clustering in the RKHS is equivalent to the oulation-level normalized cut. Combining these results, we can see that the kernel normalized cut converges to the oulation-level normalized cut. The criterion of the oulation-level normalized cut can be considered as an indivisibility of the oulation distribution, and this criterion lays an imortant role in the theoretical analysis of sectral clustering in Schiebinger et al. (2015). We believe that our results will rovide dee insights into the behavior of both normalized cut and sectral clustering.
"
1504,2019,Guarantees for Spectral Clustering with Fairness Constraints,Poster,"Given the widesread oularity of sectral clustering (SC) for artitioning grah data, we study a version of constrained SC in which we try to incororate the fairness notion roosed by Chierichetti et al. (2017). According to this notion, a clustering is fair if every demograhic grou is aroximately roortionally reresented in each cluster. To this end, we develo variants of both normalized and unnormalized constrained SC and show that they hel find fairer clusterings on both synthetic and real data. We also rovide a rigorous theoretical analysis of our algorithms on a natural variant of the stochastic block model, where $h$ grous have strong inter-grou connectivity, but also exhibit a “natural” clustering structure which is fair. We rove that our algorithms can recover this fair clustering with high robability."
1505,2019,Supervised Hierarchical Clustering with Exponential Linkage,Poster,"In suervised clustering, standard techniques for learning a airwise dissimilarity function often suffer from a discreancy between the training and clustering objectives, leading to oor cluster quality. Rectifying this discreancy necessitates matching the rocedure for training the  dissimilarity function to the clustering algorithm. In this aer, we introduce a method for training the dissimilarity function in a way that is tightly couled with hierarchical clustering, in articular single linkage. However, the aroriate clustering algorithm for a given dataset is often unknown. Thus we introduce an aroach to suervised hierarchical clustering that smoothly interolates between single, average, and comlete linkage, and we give a training rocedure that simultaneously learns a linkage function and a dissimilarity function. We accomlish this with a novel Exonential Linkage function that has a learnable arameter that controls the interolation. In exeriments on four datasets, our joint training rocedure consistently matches or outerforms the next best training rocedurelinkage function air and gives u to 8 oints imrovement in dendrogram urity over discreant airs.
"
1506,2019,Sliced-Wasserstein Flows: Nonparametric Generative Modeling via Optimal Transport and Diffusions,Poster,"By building uon the recent theory that established the connection between imlicit generative modeling (IGM) and otimal transort, in this study, we roose a novel arameter-free algorithm for learning the underlying distributions of comlicated datasets and samling from them. The roosed algorithm is based on a functional otimization roblem, which aims at finding a measure that is close to the data distribution as much as ossible and also exressive enough for generative modeling uroses. We formulate the roblem as a gradient flow in the sace of robability measures. The connections between gradient flows and stochastic differential equations let us develo a comutationally efficient algorithm for solving the otimization roblem. We rovide formal theoretical analysis where we rove finite-time error guarantees for the roosed algorithm. To the best of our knowledge, the roosed algorithm is the first nonarametric IGM algorithm with exlicit theoretical guarantees. Our exerimental results suort our theory and show that our algorithm is able to successfully cature the structure of different tyes of data distributions.
"
1507,2019,Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for Non-Convex Optimization,Poster,"Recent studies on diffusion-based samling methods have shown that Langevin Monte Carlo (LMC) algorithms can be beneficial for non-convex otimization, and rigorous theoretical guarantees have been roven for both asymtotic and finite-time regimes. Algorithmically, LMC-based algorithms resemble the well-known gradient descent (GD) algorithm, where the GD recursion is erturbed by an additive Gaussian noise whose variance has a articular form. Fractional Langevin Monte Carlo (FLMC) is a recently roosed extension of LMC, where the Gaussian noise is relaced by a heavy-tailed α-stable noise. As oosed to its Gaussian counterart, these heavy-tailed erturbations can incur large jums and it has been emirically demonstrated that the choice of α-stable noise can rovide several advantages in modern machine learning roblems, both in otimization and samling contexts. However, as oosed to LMC, only asymtotic convergence roerties of FLMC have been yet established. In this study, we analyze the non-asymtotic behavior of FLMC for non-convex otimization and rove finite-time bounds for its exected subotimality. Our results show that the weak-error of FLMC increases faster than LMC, which suggests using smaller ste-sizes in FLMC. We finally extend our results to the case where the exact gradients are relaced by stochastic gradients and show that similar results hold in this setting as well.
"
1508,2019,Unifying Orthogonal Monte Carlo Methods,Poster,"Many machine learning methods making use of Monte Carlo samling in vector saces have been shown to be imroved by conditioning samles to be mutually orthogonal. Exact orthogonal couling of samles is comutationally intensive, hence aroximate methods have been of great interest. In this aer, we resent a unifying ersective of many aroximate methods by considering Givens transformations, roose new aroximate methods based on this framework, and demonstrate the ﬁrst statistical guarantees for families of aroximate methods in kernel aroximation. We rovide extensive emirical evaluations with guidance for ractitioners.
"
1509,2019,Adaptive Monte Carlo Multiple Testing via Multi-Armed Bandits,Poster,"Monte Carlo (MC) ermutation test is considered the gold standard for statistical hyothesis testing, esecially when standard arametric assumtions are not clear or likely to fail. However, in modern data science settings where a large number of hyothesis tests need to be erformed simultaneously, it is rarely used due to its rohibitive comutational cost. In genome-wide association studies, for examle, the number of hyothesis tests $m$ is around $10^6$ while the number of MC samles $n$ for each test could be greater than $10^8$, totaling more than $nm$=$10^{14}$ samles. In this aer, we roose  \texttt{A}dative \texttt{M}C multile \texttt{T}esting (\texttt{AMT}) to estimate MC -values and control false discovery rate in multile testing. The algorithm oututs the same result as the standard full MC aroach with high robability while requiring only $\tilde{O}(\sqrt{n}m)$ samles. This samle comlexity is shown to be otimal. On a Parkinson GWAS dataset, the algorithm reduces the running time from 2 months for full MC to an hour. The \texttt{AMT} algorithm is derived based on the theory of multi-armed bandits."
1510,2019,Metropolis-Hastings Generative Adversarial Networks,Poster,"We introduce the Metroolis-Hastings generative adversarial network (MH-GAN), which combines asects of Markov chain Monte Carlo and GANs. The MH-GAN draws samles from the distribution imlicitly defined by a GAN's discriminator-generator air, as oosed to standard GANs which draw samles from the distribution defined only by the generator. It uses the discriminator from GAN training to build a wraer around the generator for imroved samling. With a erfect discriminator, this wraed generator samles from the true distribution on the data exactly even when the generator is imerfect. We demonstrate the benefits of the imroved generator on multile benchmark datasets, including CIFAR-10 and CelebA, using the DCGAN, WGAN, and rogressive GAN.
"
1511,2019,Scalable Metropolis-Hastings for Exact Bayesian Inference with Large Datasets,Poster,"Bayesian inference via standard Markov Chain Monte Carlo (MCMC) methods such as Metroolis-Hastings is too comutationally intensive to handle large datasets, since the cost er ste usually scales like $O(n)$ in the number of data oints $n$. We roose the Scalable Metroolis-Hastings (SMH) kernel that only requires rocessing on average $O(1)$ or even $O(1\sqrt{n})$ data oints er ste. This scheme is based on a combination of factorized accetance robabilities, rocedures for fast simulation of Bernoulli rocesses, and control variate ideas. Contrary to many MCMC subsamling schemes such as fixed ste-size Stochastic Gradient Langevin Dynamics, our aroach is exact insofar as the invariant distribution is the true osterior and not an aroximation to it. We characterise the erformance of our algorithm theoretically, and give realistic and verifiable conditions under which it is geometrically ergodic. This theory is borne out by emirical results that demonstrate overall erformance benefits over standard Metroolis-Hastings and various subsamling algorithms."
1512,2019,Replica Conditional Sequential Monte Carlo,Poster,"We roose a Markov chain Monte Carlo (MCMC) scheme to erform state inference in non-linear non-Gaussian state-sace models. Current state-of-the-art methods to address this roblem rely on article MCMC techniques and its variants, such as the iterated conditional Sequential Monte Carlo (cSMC) scheme, which uses a Sequential Monte Carlo (SMC) tye roosal within MCMC. A deficiency of standard SMC roosals is that they only use observations u to time $t$ to roose states at time $t$ when an entire observation sequence is available. More sohisticated SMC based on lookahead techniques could be used but they can be difficult to ut in ractice. We roose here relica cSMC where we build SMC roosals for one relica using information from the entire observation sequence by conditioning on the states of the other relicas. This aroach is easily arallelizable and we demonstrate its excellent emirical erformance when comared to the standard iterated cSMC scheme at fixed comutational comlexity.
"
1513,2019,A Polynomial Time MCMC Method for  Sampling from Continuous Determinantal Point Processes,Poster,"We study the Gibbs samling algorithm for discrete and continuous $k$-determinantal oint rocesses. We show that in both cases, the sectral ga of the chain is bounded by a olynomial of $k$ and it is   indeendent of the  size of the domain.
As an immediate corollary, we obtain sublinear time algorithms for samling from discrete $k$-DPPs given access to olynomially many rocessors. 
In the continuous setting, our result leads to the first class of rigorously analyzed efficient algorithms to generate random samles of continuous $k$-DPPs.
We achieve this by showing that the Gibbs samler for a large family of continuous $k$-DPPs can be simulated efficiently when the sectrum is not concentrated on the to $k$ eigenvalues."
1514,2019,Adaptive Antithetic Sampling for Variance Reduction,Poster,"Variance reduction is crucial in stochastic estimation and otimization roblems. Antithetic samling reduces the variance of a Monte Carlo estimator by drawing correlated, rather than indeendent, samles. However, designing an effective correlation structure is challenging and alication secific, thus limiting the ractical alicability of these methods. In this aer, we roose a general-urose adative antithetic samling framework. We rovide gradient-based and gradient-free methods to train the samlers such that they reduce variance while ensuring that the underlying Monte Carlo estimator is rovably unbiased. We demonstrate the effectiveness of our aroach on Bayesian inference and generative model training, where it reduces variance and imroves task erformance with little comutational overhead.
"
1515,2019,Accelerated Flow for Probability Distributions,Poster,"This aer resents a methodology and numerical algorithms for constructing accelerated gradient flows on the sace of robability distributions. In articular, we extend the recent variational formulation of accelerated methods in (Wibisono et al., 2016) from vector valued variables to robability distributions. The variational roblem  is modeled as a mean-field otimal control roblem. A quantitative estimate on the asymtotic convergence rate is rovided based on a Lyaunov function construction, when the objective functional is dislacement convex.  An imortant secial case is considered where the objective functional is the relative entroy. For this case, two numerical aroximations are resented to imlement the Hamilton's equations as a system of N interacting articles. The algorithm is numerically illustrated and comared with the MCMC and Hamiltonian MCMC algorithms. 
"
1516,2019,Tight Kernel Query Complexity of Kernel Ridge Regression and Kernel $k$-means Clustering,Poster,"Kernel methods generalize machine learning algorithms that only deend on the airwise inner roducts of the dataset by relacing inner roducts with kernel evaluations, a function that asses inut oints through a nonlinear feature ma before taking the inner roduct in a higher dimensional sace. In this work, we resent nearly tight lower bounds on the number of kernel evaluations required to aroximately solve kernel ridge regression (KRR) and kernel $k$-means clustering (KKMC) on $n$ inut oints. For KRR, our bound for relative error aroximation the argmin of the objective function is $\Omega(nd_{\mathrm{eff}}^\lambda\varesilon)$ where $d_{\mathrm{eff}}^\lambda$ is the effective statistical dimension, tight u to a $\log(d_{\mathrm{eff}}^\lambda\varesilon)$ factor. For KKMC, our bound for finding a $k$-clustering achieving a relative error aroximation of the objective function is $\Omega(nk\varesilon)$, tight u to a $\log(k\varesilon)$ factor. Our KRR result resolves a variant of an oen question of El Alaoui and Mahoney, asking whether the effective statistical dimension is a lower bound on the samling comlexity or not. Furthermore, for the imortant inut distribution case of mixtures of Gaussians, we rovide algorithms that byass the above lower bounds."
1517,2019,Dimensionality Reduction for Tukey Regression,Poster,"We give the first dimensionality reduction methods for the overconstrained Tukey regression roblem. The Tukey loss function $\|y\|_M = \sum_i M(y_i)$ has $M(y_i) \arox |y_i|^$ for residual errors $y_i$ smaller than a rescribed threshold $\tau$, but $M(y_i)$ becomes constant for errors $|y_i|  \tau$. Our results deend on a new structural result, roven constructively, showing that for any $d$-dimensional subsace $L \subset \mathbb{R}^n$, there is a fixed bounded-size subset of coordinates containing, for every $y \in L$, all the large coordinates, with resect to the Tukey loss function, of $y$. Our methods reduce a given Tukey regression roblem to a smaller weighted version, whose solution is a rovably good aroximate solution to the original roblem. Our reductions are fast, simle and easy to imlement, and we give emirical results demonstrating their racticality, using existing heuristic solvers for the small versions. We also give exonential-time algorithms giving rovably good solutions, and hardness results suggesting that a significant seedu in the worst case is unlikely. "
1518,2019,Efficient Full-Matrix Adaptive Regularization,Poster,"Adative regularization methods re-multily a descent direction by a reconditioning matrix. Due to the large number of arameters of machine learning roblems, full-matrix reconditioning methods are rohibitively exensive. We show how to modify full-matrix adative regularization in order to make it ractical and effective. We also rovide a novel theoretical analysis for adative regularization in {\em non-convex} otimization settings. The core of our algorithm, termed GGT, consists of the efficient comutation of the inverse square root of a low-rank matrix. Our reliminary exeriments show imroved iteration-wise convergence rates across synthetic tasks and standard dee learning benchmarks, and that the more carefully-reconditioned stes sometimes lead to a better solution.
"
1519,2019,Breaking the gridlock in Mixture-of-Experts: Consistent and Efficient Algorithms,Poster,"Mixture-of-Exerts (MoE) is a widely oular model for ensemble learning and is a basic building block of highly successful modern neural networks as well as a comonent in Gated Recurrent Units (GRU) and Attention networks. However, resent algorithms for learning MoE, including the EM algorithm and gradient descent, are known to get stuck in local otima. From a theoretical viewoint, finding an efficient and rovably consistent algorithm to learn the arameters remains a long standing oen roblem for more than two decades. In this aer, we introduce the first algorithm that learns the true arameters of a MoE model for a wide class of non-linearities with global consistency guarantees. While existing algorithms jointly or iteratively estimate the exert arameters and the gating arameters in the MoE, we roose a novel algorithm that breaks the deadlock and can directly estimate the exert arameters by sensing its echo in a  carefully designed cross-moment tensor between the inuts and the outut. Once the exerts are known, the recovery of gating arameters still requires an EM algorithm; however, we show that the EM algorithm for this simlified roblem, unlike the joint EM algorithm, converges to the true arameters. We emirically validate our algorithm on both the synthetic and real data sets in a variety of settings, and show suerior erformance to standard baselines.
"
1520,2019,Efficient Nonconvex Regularized Tensor Completion with Structure-aware Proximal Iterations,Poster,"Nonconvex regularizers have been successfully used in low-rank matrix learning. In this aer, we extend this to the more challenging roblem of low-rank tensor comletion. Based on the roximal average algorithm,
we develo an efficient solver that avoids exensive tensor folding and unfolding. A secial ``sarse lus low-rank"" structure, which is essential for fast comutation of individual roximal stes,  is maintained
throughout the iterations. We also incororate adative momentum to further seed u emirical convergence. Convergence results to critical oints are rovided under smoothness and Kurdyka-Lojasiewicz conditions. Exerimental results on a number of synthetic and real-world data sets show that the roosed algorithm is more efficient in both time and sace, and is also more accurate than existing aroaches.
"
1521,2019,Robust Estimation of Tree Structured Gaussian Graphical Models,Poster,"Consider jointly Gaussian random variables whose conditional indeendence structure is secified by a grahical model. If we observe realizations of the variables, we can comute the covariance matrix, and it is well known that the suort of the inverse covariance matrix corresonds to the edges of the grahical model. Instead, suose we only have noisy observations. If the noise at each node is indeendent, we can comute the sum of the covariance matrix and an unknown diagonal. The inverse of this sum is (in general) dense. We ask: can the original indeendence structure be recovered? We address this question for tree structured grahical models. We rove that this roblem is unidentifiable, but show that this unidentifiability is limited to a small class of candidate trees. We further resent additional constraints under which the roblem is identifiable. Finally, we rovide an O(n^3) algorithm to find this equivalence class of trees.
"
1522,2019,Spectral Approximate Inference,Poster,"Given a grahical model (GM), comuting its artition function is the most essential inference task, but it is comutationally intractable in general. To address the issue, iterative aroximation algorithms exloring certain local structureconsistency of GM have been investigated as oular choices in ractice. However, due to their localiterative nature, they often outut oor aroximations or even do not converge, e.g., in low-temerature regimes (hard instances of large arameters). To overcome the limitation, we roose a novel aroach utilizing the global sectral feature of GM. Our contribution is two-fold: (a) we first roose a fully olynomial-time aroximation scheme (FPTAS) for aroximating the artition function of GM  associating with a low-rank couling matrix; (b) for general high-rank GMs, we design a sectral mean-field scheme utilizing (a) as a subroutine, where it aroximates a high-rank GM into a roduct of rank-1 GMs for an efficient aroximation of the artition function. The roosed algorithm is more robust in its running time and accuracy than rior methods, i.e., neither suffers from the convergence issue nor deends on hard local structures, as demonstrated in our exeriments.
"
1523,2019,Partially Linear Additive Gaussian Graphical Models,Poster,"We roose a artially linear additive Gaussian grahical model (PLA-GGM) for the estimation of associations between random variables distorted by observed confounders. Model arameters are estimated  using an $L_1$-regularized maximal seudo-rofile likelihood estimator (MaPPLE) for which we rove a $\sqrt{n}$-sarsistency. Imortantly, our aroach avoids arametric constraints on the effects of confounders on the estimated grahical model structure. Emirically, the PLA-GGM is alied to both synthetic and real-world datasets, demonstrating suerior erformance comared to cometing methods."
1524,2019,DAG-GNN: DAG Structure Learning with Graph Neural Networks,Poster,"Learning a faithful directed acyclic grah (DAG) from samles of a joint distribution is a challenging combinatorial roblem, owing to the intractable search sace suerexonential in the number of grah nodes. A recent breakthrough formulates the roblem as a continuous otimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors aly the aroach to the linear structural equation model (SEM) and the least-squares loss function that are statistically well justified but nevertheless limited. Motivated by the widesread success of dee learning that is caable of caturing comlex nonlinear maings, in this work we roose a dee generative model and aly a variant of the structural constraint to learn the DAG. At the heart of the generative model is a variational autoencoder arameterized by a novel grah neural network architecture, which we coin DAG-GNN. In addition to the richer caacity, an advantage of the roosed model is that it naturally handles discrete variables as well as vector-valued ones. We demonstrate that on synthetic data sets, the roosed method learns more accurate grahs for nonlinearly generated samles; and on benchmark data sets with discrete variables, the learned grahs are reasonably close to the global otima. The code is available at \url{htts:github.comfishmoon1234DAG-GNN}.
"
1525,2019,Random Walks on Hypergraphs with Edge-Dependent Vertex Weights,Poster,"Hyergrahs are used in machine learning to model higher-order relationshis in data. While sectral methods for grahs are well-established, sectral theory for hyergrahs remains an active area of research. In this aer, we use random walks to develo a sectral theory for hyergrahs with edge-deendent vertex weights: hyergrahs where every vertex v has a weight $\gamma_e(v)$ for each incident hyeredge e that describes the contribution of v to the hyeredge e. We derive a random walk-based hyergrah Lalacian, and bound the mixing time of random walks on such hyergrahs. Moreover, we give conditions under which random walks on such hyergrahs are equivalent to random walks on grahs. As a corollary, we show that current machine learning methods that rely on Lalacians derived from random walks on hyergrahs with edge-indeendent vertex weights do not utilize higher-order relationshis in the data. Finally, we demonstrate the advantages of hyergrahs with edge-deendent vertex weights on ranking alications using real-world datasets."
1526,2019,Doubly Robust Joint Learning for Recommendation on Data Missing Not at Random,Poster,"In recommender systems, usually the ratings of a user to most items are missing and a critical roblem is that the missing ratings are often missing not at random (MNAR) in reality.
It is widely acknowledged that MNAR ratings make it difficult to accurately redict the ratings and unbiasedly estimate the erformance of rating rediction.
Recent aroaches use imuted errors to recover the rediction errors for missing ratings, or weight observed ratings with the roensities of being observed.
These aroaches can still be severely biased in erformance estimation or suffer from the variance of the roensities. 
To overcome these limitations, we first roose an estimator that integrates the imuted errors and roensities in a doubly robust way to obtain unbiased erformance estimation and alleviate the effect of the roensity variance.
To achieve good erformance guarantees, based on this estimator, we roose joint learning of rating rediction and error imutation, which outerforms the state-of-the-art aroaches on four real-world datasets.
"
1527,2019,Linear-Complexity Data-Parallel Earth Mover's Distance Approximations,Poster,"The Earth Mover's Distance (EMD) is a state-of-the art metric for comaring discrete robability distributions, but its high distinguishability comes at a high cost in comutational comlexity. Even though linear-comlexity aroximation algorithms have been roosed to imrove its scalability, these algorithms are either limited to vector saces with only a few dimensions or they become ineffective when the degree of overla between the robability distributions is high. We roose novel aroximation algorithms that overcome both of these limitations, yet still achieve linear time comlexity. All our algorithms are data arallel, and thus, we take advantage of massively arallel comuting engines, such as Grahics Processing Units (GPUs). On the oular text-based 20 Newsgrous dataset, the new algorithms are four orders of magnitude faster than a multi-threaded CPU imlementation of Word Mover's Distance and match its nearest-neighbors-search accuracy. On MNIST images, the new algorithms are four orders of magnitude faster than a GPU imlementation of the Sinkhorn's algorithm while offering a slightly higher nearest-neighbors-search accuracy.
"
1528,2019,Model Comparison for Semantic Grouping,Poster,"We introduce a robabilistic framework for quantifying the semantic similarity between two grous of embeddings. We formulate the task of semantic similarity as a model comarison task in which we contrast a generative model which jointly models two sentences versus one that does not. We illustrate how this framework can be used for the Semantic Textual Similarity tasks using clear assumtions about how the embeddings of words are generated. We aly model comarison that utilises information criteria to address some of the shortcomings of Bayesian model comarison, whilst still enalising model comlexity. We achieve cometitive results by alying the roosed framework with an aroriate choice of likelihood on the STS datasets.
"
1529,2019,RaFM: Rank-Aware Factorization Machines,Poster,"Fatorization machines (FM) are a oular model class to learn airwise interactions by a low-rank aroximation. Different from existing FM-based aroaches which use a fixed rank for all features, this aer rooses a Rank-Aware FM (RaFM) model which adots airwise interactions from embeddings with different ranks. The roosed model achieves a better erformance on real-world datasets where different features have significantly varying frequencies of occurrences. Moreover, we rove that the RaFM model can be stored, evaluated, and trained as efficiently as one single FM, and under some reasonable conditions it can be even significantly more efficient than FM. RaFM imroves the erformance of FMs in both regression tasks and classification tasks while incurring less comutational burden, therefore also has attractive otential in industrial alications.
"
1530,2019,CAB: Continuous Adaptive Blending for Policy Evaluation and Learning,Poster,"The ability to erform offline AB-testing and off-olicy learning using logged contextual bandit feedback is highly desirable in a broad range of alications, including recommender systems, search engines, ad lacement, and ersonalized health care. Both offline AB-testing and off-olicy learning require a counterfactual estimator that evaluates how some new olicy would have erformed, if it had been used instead of the logging olicy. In this aer, we identify a family of counterfactual estimators which subsumes most such estimators roosed to date. Our analysis of this family identifies a new estimator - called Continuous Adative Blending (CAB) - which enjoys many advantageous theoretical and ractical roerties. In articular, it can be substantially less biased than clied Inverse Proensity Score (IPS) weighting and the Direct Method, and it can have less variance than Doubly Robust and IPS estimators. In addition, it is sub-differentiable such that it can be used for learning, unlike the SWITCH estimator. Exerimental results show that CAB rovides excellent evaluation accuracy and outerforms other counterfactual estimators in terms of learning erformance.
"
1531,2019,MetricGAN: Generative Adversarial Networks based Black-box Metric Scores Optimization for Speech Enhancement,Poster,"Adversarial loss in a conditional generative adversarial network (GAN) is not designed to directly otimize evaluation metrics of a target task, and thus, may not always guide the generator in a GAN to generate data with imroved metric scores. To overcome this issue, we roose a novel MetricGAN aroach with an aim to otimize the generator with resect to one or multile evaluation metrics. Moreover, based on MetricGAN, the metric scores of the generated data can also be arbitrarily secified by users. We tested the roosed MetricGAN on a seech enhancement task, which is articularly suitable to verify the roosed aroach because there are multile metrics measuring different asects of seech signals. Moreover, these metrics are generally comlex and could not be fully otimized by L or conventional adversarial losses.
"
1532,2019,Neural Separation of Observed and Unobserved Distributions,Poster,"Searating mixed distributions is a long standing challenge for machine learning and signal rocessing. Most current methods either rely on making strong assumtions on the source distributions or rely on having training samles of each source in the mixture. In this work, we introduce a new method---Neural Egg Searation---to tackle the scenario of extracting a signal from an unobserved distribution additively mixed with a signal from an observed distribution. Our method iteratively learns to searate the known distribution from rogressively finer estimates of the unknown distribution. In some settings, Neural Egg Searation is initialization sensitive, we therefore introduce Latent Mixture Masking which ensures a good initialization. Extensive exeriments on audio and image searation tasks show that our method outerforms current methods that use the same level of suervision, and often achieves similar erformance to full suervision.
"
1533,2019,Almost Unsupervised Text to Speech and Automatic Speech Recognition,Poster,"Text to seech (TTS) and automatic seech recognition (ASR) are two dual tasks in seech rocessing and both achieve imressive erformance thanks to the recent advance in dee learning and large amount of aligned seech and text data. However, the lack of aligned data oses a major ractical roblem for TTS and ASR on low-resource languages. In this aer, by leveraging the dual nature of the two tasks, we roose an almost unsuervised learning method that only leverages few hundreds of aired data and extra unaired data for TTS and ASR. Our method consists of the following comonents: (1) denoising auto-encoder, which reconstructs seech and text sequences resectively to develo the caability of language modeling both in seech and text domain; (2) dual transformation, where the TTS model transforms the text $y$ into seech $\hat{x}$, and the ASR model leverages the transformed air $(\hat{x},y)$ for training, and vice versa, to boost the accuracy of the two tasks; (3) bidirectional sequence modeling, which address the error roagation roblem esecially in the long seech and text sequence when training with few aired data; (4) a unified model structure, which combines all the above comonents for TTS and ASR based on Transformer model. Our method achieves 99.84\% in terms of word level intelligible rate and 2.68 MOS for TTS, and 11.7\% PER for ASR on LJSeech dataset, by leveraging only 200 aired seech and text data (about 20 minutes audio), together with extra unaired seech and text data. "
1534,2019,AutoVC:  Zero-Shot Voice Style Transfer with Only Autoencoder Loss,Poster,"Desite the rogress in voice conversion, many-to-many voice conversion trained on non-arallel data, as well as zero-shot voice conversion, remains under-exlored. Dee style transfer algorithms, generative adversarial networks (GAN) in articular, are being alied as new solutions in this field. However, GAN training is very sohisticated and difficult, and there is no strong evidence that its generated seech is of good ercetual quality. In this aer, we roose a new style transfer scheme that involves only an autoencoder with a carefully designed bottleneck. We formally show that this scheme can achieve distribution-matching style transfer by training only on self-reconstruction loss. Based on this scheme, we roosed AutoVC, which achieves state-of-the-art results in many-to-many voice conversion with non-arallel data, and which is the first to erform zero-shot voice conversion.
"
1535,2019,A fully differentiable beam search decoder,Poster,"We introduce a new beam search decoder that is fully differentiable, making it ossible to otimize at training time through the inference rocedure. Our decoder allows us to combine models which oerate at different granularities (e.g. acoustic and language models). It can be used when target sequences are not aligned to inut sequences by considering all ossible alignments between the two. We demonstrate our aroach scales by alying it to seech recognition, jointly training acoustic and word-level language models. The system is end-to-end, with gradients flowing through the whole architecture from the word-level transcritions. Recent research efforts have shown that dee neural networks with attention-based mechanisms can successfully train an acoustic model from the final transcrition, while imlicitly learning a language model.  Instead, we show that it is ossible to discriminatively train an acoustic  model jointly with an \emh{exlicit} and ossibly re-trained language model.
"
1536,2019,Scaling Up Ordinal Embedding: A Landmark Approach,Poster,"Ordinal Embedding is the roblem of lacing n objects into R^d to satisfy constraints like ""object a is closer to b than to c."" It can accommodate data that embeddings from features or distances cannot, but is a more difficult roblem. We roose a novel landmark-based method as a artial solution. At small to medium scales, we resent a novel combination of existing methods with some new theoretical justification. For very large values of n otimizing over an entire embedding breaks down, so we roose a novel method which first embeds a subset of m &lt;&lt; n objects and then embeds the remaining objects indeendently and in arallel. We rove a distance error bound for our method in terms of m and that it has O(dn log m) time comlexity, and show emirically that it is able to roduce high quality embeddings in a fraction of the time needed for any ublished method.
"
1537,2019,Learning to select for a predefined ranking,Poster,"In this aer, we formulate a novel roblem of learning to select a set of items maximizing the quality of their ordered list, where the order is redefined by some exlicit rule. Unlike the classic information retrieval roblem, in our setting, the redefined order of items in the list may not corresond to their quality in general. For examle, this is a dominant scenario in ersonalized news and social media feeds, where items are ordered by ublication time in a user interface. We roose new theoretically grounded algorithms based on direct otimization of the resulting list quality. Our offline and online exeriments with a large-scale roduct search engine demonstrate the overwhelming advantage of our methods over the baselines in terms of all key quality metrics.
"
1538,2019,Mallows ranking models: maximum likelihood estimate and regeneration,Poster,"This aer is concerned with various Mallows ranking models. We study the statistical roerties of the MLE of Mallows' $\hi$ model. We also make connections of various Mallows ranking models, encomassing recent rogress in mathematics. Motivated by the infinite to-$t$ ranking model, we roose an algorithm to select the model size $t$ automatically. The key idea relies on the renewal roerty of such an infinite random ermutation. Our algorithm shows good erformance on several data sets."
1539,2019,Fast and Stable Maximum Likelihood Estimation for Incomplete Multinomial Models,Poster,"We roose a fixed-oint iteration aroach to the maximum likelihood estimation for the incomlete multinomial model, which rovides a unified framework for ranking data analysis. Incomlete observations tyically fall in a subset of categories, and thus cannot be distinguished as belonging to a unique category. We develo a minorization--maximization (MM) tye of algorithm, which requires relatively fewer iterations and shorter time to achieve convergence. Under such a general framework, incomlete multinomial models can be reformulated to include several well-known ranking models as secial cases, such as the Bradley--Terry, Plackett--Luce models and their variants. The simle form of iteratively udating equations in our algorithm involves only basic matrix oerations, which makes it efficient and easy to imlement with large data. Exerimental results show that our algorithm runs faster than existing methods on synthetic data and real data.
"
1540,2019,Fast Algorithm for Generalized Multinomial Models with Ranking Data,Poster,"We develo a framework of generalized multinomial models, which includes both the oular Plackett--Luce model and Bradley--Terry model as secial cases. From a theoretical ersective, we rove that the maximum likelihood estimator (MLE) under generalized multinomial models corresonds to the stationary distribution of an inhomogeneous Markov chain uniquely. Based on this roerty, we roose an iterative algorithm that is easy to imlement and interret, and is guaranteed to converge. Numerical exeriments on synthetic data and real data demonstrate the advantages of our Markov chain based algorithm over existing ones. Our algorithm converges to the MLE with fewer iterations and at a faster convergence rate. The new algorithm is readily alicable to roblems such as age ranking or sorts ranking data.
"
1541,2019,Graph Resistance and Learning from Pairwise Comparisons,Poster,"We consider the roblem of learning the qualities of a collection of items by erforming noisy comarisons among them. Following the standard aradigm, we assume there is a fixed ``comarison grah'' and every neighboring air of items in this grah is comared k times according to the Bradley-Terry-Luce model (where the robability than an item wins a comarison is roortional the item quality). We are interested in how the relative error in quality estimation scales with the comarison grah in the regime where k is large. We show that, asymtotically, the relevant grah-theoretic quantity is the square root of the resistance of the comarison grah. Secifically, we rovide an algorithm with relative error decay that scales with the square root of the grah resistance, and rovide a lower bound showing that (u to log factors) a better scaling is imossible. The erformance guarantee of our algorithm, both in terms of the grah and the skewness of the item quality distribution, significantly outerforms earlier results. 
"
1542,2019,Learning Context-dependent Label Permutations for Multi-label Classification,Poster,"A key roblem in multi-label classification is to utilize deendencies among the labels. Chaining classifiers are a simle technique for addressing this roblem but current algorithms all assume a fixed, static label ordering. In this work, we roose a multi-label classification aroach which allows to choose a dynamic, context-deendent label ordering. Our roosed aroach consists of two sub-comonents: a simle EM-like algorithm which bootstras the learned model, and a more elaborate aroach based on reinforcement learning. Our exeriments on three ublic multi-label classification benchmarks show that our roosed dynamic label ordering aroach based on reinforcement learning outerforms recurrent neural networks with fixed label ordering across both biartition and ranking measures on all the three datasets. As a result, we obtain a owerful sequence rediction-based algorithm for multi-label classification, which is able to efficiently and exlicitly exloit label deendencies.
"
1543,2019,Discovering Context Effects from Raw Choice Data,Poster,"Many alications in reference learning assume that decisions come from the maximization of a stable utility function. Yet a large exerimental literature shows that individual choices and judgements can be affected by ``irrelevant'' asects of the context in which they are made. An imortant class of such contexts is the comosition of the choice set. In this work, our goal is to discover such choice set effects from raw choice data. We introduce an extension of the Multinomial Logit (MNL) model, called the context deendent random utility model (CDM), which allows for a articular class of choice set effects. We show that the CDM can be thought of as a second-order aroximation to a general choice system, can be inferred otimally using maximum likelihood and, imortantly, is easily interretable. We aly the CDM to both real and simulated choice data to erform rinciled exloratory analyses for the resence of choice set effects.
"
1544,2019,"On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference",Poster,"Our goal is for agents to otimize the right reward function, desite how difficult it is for us to secify what that is. Inverse Reinforcement Learning (IRL) enables us to infer reward functions from demonstrations, but it usually assumes that the exert is noisily otimal. Real eole, on the other hand, often have systematic biases: risk-aversion, myoia, etc. One otion is to try to characterize these biases and account for them exlicitly during learning. But in the era of dee learning, a natural suggestion researchers make is to avoid mathematical models of human behavior that are fraught with secific assumtions, and instead use a urely data-driven aroach. We decided to ut this to the test -- rather than relying on assumtions about which secific bias the demonstrator has when lanning, we instead learn the demonstrator's lanning algorithm that they use to generate demonstrations, as a differentiable lanner. Our exloration yielded mixed findings: on the one hand, learning the lanner can lead to better reward inference than relying on the wrong assumtion; on the other hand, this benefit is dwarfed by the loss we incur by going from an exact to a differentiable lanner. This suggest that at least for the foreseeable future, agents need a middle ground between the flexibility of data-driven methods and the useful bias of known human biases. Code is available at htts:tinyurl.comlearningbiases.
"
1545,2019,Learning Distance for Sequences by Learning a Ground Metric,Poster,"Learning distances that oerate directly on multi-dimensional sequences is challenging because such distances are structural by nature and the vectors in sequences are not indeendent. Generally, distances for sequences heavily deend on the ground metric between the vectors in sequences. We roose to learn the distance for sequences through learning a ground Mahalanobis metric for the vectors in sequences. The learning samles are sequences of vectors for which how the ground metric between vectors induces the overall distance is given, and the objective is that the distance induced by the learned ground metric roduces large values for sequences from different classes and small values for those from the same class. We formulate the metric as a arameter of the distance, bring closer each sequence to an associated virtual sequence w.r.t. the distance to reduce the number of constraints, and develo a general iterative solution for any ground-metric-based sequence distance. Exeriments on several sequence datasets demonstrate the effectiveness and efficiency of our method.
"
