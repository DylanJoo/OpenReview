,Year,Title,Decision,Abstract
0,2017,Decoupled Neural Interfaces using Synthetic Gradients,Oral/Poster,"Training directed neural networks tyically requires forward-roagating data through a comutation grah, followed by backroagating error signal, to roduce weight udates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and roagate error backwards before they can be udated. In this work we break this constraint by decouling modules by introducing a model of the future comutation of the network grah. These models redict what the result of the modelled subgrah will roduce using only local information. In articular we focus on modelling error gradients: by using the modelled \emh{synthetic gradient} in lace of true backroagated error gradients we decoule subgrahs, and can udate them indeendently and asynchronously \ie~we realise \emh{decouled neural interfaces}. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where redicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to redicting gradients, the same framework can be used to redict inuts, resulting in models which are decouled in both the forward and backwards ass -- amounting to indeendent networks which co-learn such that they can be comosed into a single functioning cororation.
"
1,2017,PixelCNN Models with Auxiliary Variables for Natural Image Modeling,Oral/Poster,"We study robabilistic models of natural images and extend the autoregressive family of PixelCNN models by incororating auxiliary variables. Subsequently, we describe two new generative image models that exloit different image transformations as auxiliary variables: a quantized grayscale view of the image or a multi-resolution image yramid. The roosed models tackle two known shortcomings of existing PixelCNN models: 1) their tendency to focus on low-level image details, while largely ignoring high-level image information, such as object shaes, and 2) their comutationally costly rocedure for image samling. We exerimentally demonstrate benefits of our models, in articular showing that they roduce much more realistically looking image samles than revious state-of-the-art robabilistic models.
"
2,2017,Tight Bounds for Approximate Carathéodory and Beyond,Oral/Poster,"We resent a deterministic nearly-linear time algorithm for aroximating any oint inside a convex olytoe with a sarse convex combination of the olytoe's vertices. Our result rovides a constructive roof for the Aroximate Carathéodory Problem, which states that any oint inside a olytoe contained in the $\ell_$ ball of radius $D$ can be aroximated to within $\esilon$ in $\ell_$ norm by a convex combination of $O\left(D^2 \esilon^2\right)$ vertices of the olytoe for $ \geq 2$. While for the articular case of $=2$, this can be achieved by the well-known Percetron algorithm, we follow a more rinciled aroach which generalizes to arbitrary $\geq 2$; furthermore, this naturally extends to domains with more comlicated geometry, as it is the case for roviding an aroximate Birkhoff-von Neumann decomosition. Secondly, we show that the sarsity bound is tight for $\ell_$ norms, using an argument based on anti-concentration for the binomial distribution, thus resolving an oen question osed by Barman. Exerimentally, we verify that our deterministic otimization-based algorithms achieve in ractice much better sarsity than reviously known samling-based algorithms. We also show how to aly our techniques to SVM training and rounding fractional oints in matroid and flow olytoes."
3,2017,Robust Adversarial Reinforcement Learning,Oral/Poster,"Dee neural networks couled with fast simulation and imroved comutational seeds have led to recent successes in the field of reinforcement learning (RL). However, most current RL-based aroaches fail to generalize since: (a) the ga between simulation and real world is so large that olicy-learning aroaches fail to transfer; (b) even if olicy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses). Insired from H-infinity control methods, we note that both modeling errors and differences in training and test scenarios can just be viewed as extra forcesdisturbances in the system. This aer rooses the idea of robust adversarial reinforcement learning (RARL), where we train an agent to oerate in the resence of a destabilizing adversary that alies disturbance forces to the system. The jointly trained adversary is reinforced -- that is, it learns an otimal destabilization olicy. 
We formulate the olicy learning as a zero-sum, minimax objective function. Extensive exeriments in multile environments (InvertedPendulum, HalfCheetah, Swimmer, Hoer, Walker2d and Ant) conclusively demonstrate that our method  (a) imroves training stability;  (b) is robust to differences in trainingtest conditions; and c) outerform the baseline even in the absence of the adversary.
"
4,2017,Robust Probabilistic Modeling with Bayesian Data Reweighting,Oral/Poster,"Probabilistic models analyze data by relying on a set of assumtions.
Data that exhibit deviations from these assumtions can undermine
inference and rediction quality. Robust models offer rotection
against mismatch between a model's assumtions and reality. We
roose a way to systematically detect and mitigate mismatch of a
large class of robabilistic models. The idea is to raise the
likelihood of each observation to a weight and then to infer both the
latent variables and the weights from data. Inferring the weights
allows a model to identify observations that match its assumtions
and down-weight others. This enables robust inference and imroves
redictive accuracy. We study four different forms of mismatch with
reality, ranging from missing latent grous to structure
missecification. A Poisson factorization analysis of the Movielens
1M dataset shows the benefits of this aroach in a ractical
scenario.
"
5,2017,Multi-objective Bandits: Optimizing the Generalized Gini Index,Oral/Poster,"We study the multi-armed bandit (MAB) roblem where the agent receives a vectorial feedback that encodes many ossibly cometing objectives to be otimized. The goal of the agent is to find a olicy, which can otimize these objectives simultaneously in a fair way. This multi-objective online otimization roblem is formalized by using the Generalized Gini Index (GGI) aggregation function. We roose an online gradient descent algorithm which exloits the convexity of the GGI aggregation function, and controls the exloration in a careful way achieving a distribution-free regret $\tilde{\bigO} (T^{-12} )$ with high robability. We test our algorithm on synthetic data as well as on an electric battery control roblem where the goal is to trade off the use of the different cells of a battery in order to balance their resective degradation rates."
6,2017,Communication-efficient Algorithms for Distributed Stochastic Principal Component Analysis,Oral/Poster,"We study the fundamental roblem of Princial Comonent Analysis in a statistical distributed setting in which each machine out of m stores a samle of n oints samled i.i.d. from a single unknown distribution. We study algorithms for estimating the leading rincial comonent of the oulation covariance matrix that are both communication-efficient and achieve estimation error of the order of the centralized ERM solution that uses all mn samles. On the negative side, we show that in contrast to results obtained for distributed estimation under convexity assumtions, for the PCA objective, simly averaging the local ERM solutions cannot guarantee error that is consistent with the centralized ERM. We show that this unfortunate henomena can be remedied by erforming a simle correction ste which correlates between the individual solutions, and rovides an estimator that is consistent with the centralized ERM for sufficiently-large n. 
We also introduce an iterative distributed algorithm that is alicable in any regime of n, which is based on distributed matrix-vector roducts. The algorithm gives significant acceleration in terms of communication rounds over revious distributed algorithms, in a wide regime of arameters.
"
7,2017,Enumerating Distinct Decision Trees,Oral/Poster,"The search sace for the feature selection roblem in decision tree learning is the lattice of subsets of the available features. We rovide an exact enumeration rocedure of the subsets that lead to all and only the distinct decision trees. The rocedure can be adoted to rune the search sace of comlete and heuristics search methods in wraer models for feature selection. Based on this, we design a comutational otimization of the sequential backward elimination heuristics with a erformance imrovement of u to 100X.
"
8,2017,Understanding Synthetic Gradients and Decoupled Neural Interfaces,Oral/Poster,"When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without udate locking - without waiting for a true error gradient to be backroagated -resulting in Decouled Neural Interfaces (DNIs). This unlocked ability of being able to udate arts of a neural network asynchronously and with only local information was demonstrated to work emirically in Jaderberg et al (2016). However, there has been very little demonstration of what changes DNIs and SGs imose from a functional, reresentational, and learning dynamics oint of view. In this aer, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on otimisation. We show that the incororation of SGs does not affect the reresentational strength of the learning system for a neural network, and rove the convergence of the learning system for linear and dee linear models. On ractical roblems we investigate the mechanism by which synthetic gradient estimators aroximate the true loss, and, surrisingly, how that leads to drastically different layer-wise reresentations. Finally, we also exose the relationshi of using synthetic gradients to other error aroximation techniques and find a unifying language for discussion and comarison. 
"
9,2017,Parallel Multiscale Autoregressive Density Estimation,Oral/Poster,"PixelCNN achieves state-of-the-art results in density estimation for natural images. Although training is fast, inference is costly, requiring one network evaluation er ixel; O(N) for N ixels. This can be sed u by caching activations, but still involves generating each ixel sequentially. In this work, we roose a arallelized PixelCNN that allows more efficient inference by modeling certain ixel grous as conditionally indeendent. Our new PixelCNN model achieves cometitive density estimation and orders of magnitude seedu - O(log N) samling instead of O(N) - enabling the ractical generation of 512x512 images. We evaluate the model on class-conditional image generation, text-to-image synthesis, and action-conditional video generation, showing that our model achieves the best results among non-ixel-autoregressive density models that allow efficient samling.
"
10,2017,Oracle Complexity of Second-Order Methods for Finite-Sum Problems,Oral/Poster,"Finite-sum otimization roblems are ubiquitous in machine learning, and are commonly solved using first-order methods which rely on gradient comutations. Recently, there has been growing interest in \emh{second-order} methods, which rely on both gradients and Hessians. In rincile, second-order methods can require much fewer iterations than first-order methods, and hold the romise for more efficient algorithms. Although comuting and maniulating Hessians is rohibitive for high-dimensional roblems in general, the Hessians of individual functions in finite-sum roblems can often be efficiently comuted, e.g. because they ossess a low-rank structure. Can second-order information indeed be used to solve such roblems more efficiently? In this aer, we rovide evidence that the answer -- erhas surrisingly -- is negative, at least in terms of worst-case guarantees. However, we also discuss what additional assumtions and algorithmic aroaches might otentially circumvent this negative result. 
"
11,2017,Minimax Regret Bounds for Reinforcement Learning,Oral/Poster,"We consider the roblem of rovably otimal exloration in reinforcement learning for finite horizon MDPs.
We show that an otimistic modification to value iteration achieves a regret bound of $\tilde {O}( \sqrt{HSAT} + H^2S^2A+H\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states, $A$ the number of actions and $T$ the number of time-stes.
This result imroves over the best revious known bound $\tilde {O}(HS \sqrt{AT})$ achieved by the UCRL2 algorithm. 
The key significance of our new results is that when  $T\geq H^3S^3A$ and $SA\geq H$, it leads to a regret of $\tilde{O}(\sqrt{HSAT})$ that matches the established lower bound of $\Omega(\sqrt{HSAT})$ u to a logarithmic factor.
Our analysis contain two key insights.
We use careful alication of concentration inequalities to the otimal value function as a whole, rather than to the transitions robabilities (to imrove scaling in $S$), and we use ""exloration bonuses"" built from Bernstein's  inequality, together with using a recursive -Bellman-tye- Law of Total Variance (to imrove scaling in $H$)."
12,2017,Post-Inference Prior Swapping,Oral/Poster,"While Bayesian methods are raised for their ability to incororate useful rior knowledge, in ractice, convenient riors that allow for comutationally chea or tractable inference are commonly used. In this aer, we investigate the following question: for a given model, is it ossible to comute an inference result with any convenient false rior, and afterwards, given any target rior of interest, quickly transform this result into the target osterior? A otential solution is to use imortance samling (IS). However, we demonstrate that IS will fail for many choices of the target rior, deending on its arametric form and similarity to the false rior. Instead, we roose rior swaing, a method that leverages the re-inferred false osterior to efficiently generate accurate osterior samles under arbitrary target riors. Prior swaing lets us aly less-costly inference algorithms to certain models, and incororate new or udated rior information “ost-inference”. We give theoretical guarantees about our method, and demonstrate it emirically on a number of models and riors.
"
13,2017,Online Learning with Local Permutations and Delayed Feedback,Oral/Poster,"We roose an Online Learning with Local Permutations (OLLP) setting, in which the learner is allowed to slightly ermute the \emh{order} of the loss functions generated by an adversary. On one hand, this models natural situations where the exact order of the learner's resonses is not crucial, and on the other hand, might allow better learning and regret erformance, by mitigating highly adversarial loss sequences. Also, with random ermutations, this can be seen as a setting interolating between adversarial and stochastic losses. In this aer, we consider the 
alicability of this setting to convex online learning with delayed feedback, in which the feedback on the rediction made in round $t$ arrives with some delay $\tau$. With such delayed feedback, the best ossible regret bound is well-known to be $O(\sqrt{\tau T})$. We rove that by being able to ermute losses by a distance of at most $M$ (for $M\geq \tau$), the regret can be imroved to $O(\sqrt{T}(1+\sqrt{\tau^2M}))$, using a Mirror-Descent based algorithm which can be alied for both Euclidean and non-Euclidean geometries. We also rove a lower bound, showing that for $M\tau3$, it is imossible to imrove the standard $O(\sqrt{\tau T})$ regret bound by more than constant factors. Finally, we rovide some exeriments validating the erformance of our algorithm."
14,2017,SPLICE: Fully Tractable Hierarchical Extension of ICA with Pooling,Oral/Poster,"We resent a novel robabilistic framework for a hierarchical extension of indeendent comonent analysis (ICA), with a articular motivation in neuroscientific data analysis and modeling. The framework incororates a general subsace ooling with linear ICA-like layers stacked recursively. Unlike related revious models, our generative model is fully tractable: both the likelihood and the osterior estimates of latent variables can readily be comuted with analytically simle formulae. The model is articularly simle in the case of comlex-valued data since the ooling can be reduced to taking the modulus of comlex numbers. Exeriments on electroencehalograhy (EEG) and natural images demonstrate the validity of the method.
"
15,2017,Simultaneous Learning of Trees and Representations for Extreme Classification and Density Estimation,Oral/Poster,"We consider multi-class classification where the redictor has a hierarchical structure that allows for a very large number of labels both at train and test time. The redictive ower of such models can heavily deend on the structure of the tree, and although ast work showed how to learn the tree structure, it exected that the feature vectors remained static. We rovide a novel algorithm to simultaneously erform reresentation learning for the inut data and learning of the hierarchical redictor. Our aroach otimizes an objective function which favors balanced and easily-searable multi-way node artitions. We theoretically analyze this objective, showing that it gives rise to a boosting style roerty and a bound on classification error. We next show how to extend the algorithm to conditional density estimation. We emirically validate both variants of the algorithm on text classification and language modeling, resectively, and show that they comare favorably to common baselines in terms of accuracy and running time.
"
16,2017,meProp: Sparsified Back Propagation for Accelerated Deep Learning with Reduced Overfitting,Oral/Poster,"We roose a simle yet effective technique for neural network learning. The forward roagation is comuted as usual. In back roagation, only a small subset of the full gradient is comuted to udate the model arameters. The gradient vectors are sarsified in such a way that only the to-$k$ elements (in terms of magnitude) are ket. As a result, only $k$ rows or columns (deending on the layout) of the weight matrix are modified, leading to a linear reduction ($k$ divided by the vector dimension) in the comutational cost. Surrisingly, exerimental results demonstrate that we can udate only 1--4\% of the weights at each back roagation ass. This does not result in a larger number of training iterations. More interestingly, the accuracy of the resulting models is actually imroved rather than degraded, and a detailed analysis is given."
17,2017,Video Pixel Networks,Oral/Poster,"We  roose  a  robabilistic  video  model, the Video Pixel Network (VPN), that estimates the  discrete  joint  distribution  of  the  raw  ixel  values  in  a  video.   The  model  and  the  neural  architecture reflect the time, sace and color structure  of  video  tensors  and  encode  it  as  a   four-dimensional  deendency  chain.  The  VPN  aroaches  the  best  ossible  erformance  on  the Moving MNIST benchmark, a lea over the revious state of the art,  and the generated  videos show  only  minor  deviations from  the  ground truth. The  VPN  also  roduces  detailed  samles  on  the  action-conditional Robotic  Pushing benchmark  and  generalizes  to  the  motion of novel objects.
"
18,2017,Global optimization of Lipschitz functions,Oral/Poster,"The goal of the aer is to design sequential strategies which lead to efficient otimization of an unknown function under the only assumtion that it has a finite Lischitz constant. We first identify sufficient conditions for the consistency of generic sequential algorithms and formulate the exected minimax rate for their erformance. We introduce and analyze a first algorithm called LIPO which assumes the Lischitz constant to be known. Consistency, minimax rates for LIPO are roved, as well as fast rates under an additional H\""older like condition. An adative version of LIPO is also introduced for the more realistic setu where Lischitz constant is unknown and has to be estimated along with the otimization. Similar theoretical guarantees are shown to hold for the adative LIPO algorithm and a numerical assessment is rovided at the end of the aer to illustrate the otential of this strategy with resect to state-of-the-art methods over tyical benchmark roblems for global otimization.
"
19,2017,Fairness in Reinforcement Learning,Oral/Poster,"We initiate the study of fairness in reinforcement learning, where the actions of a learning algorithm may affect its environment and future rewards. Our fairness constraint requires that an algorithm never refers one action over another if the long-term (discounted) reward of choosing the latter action is higher. Our first result is negative: desite the fact that fairness is consistent with the otimal olicy, any learning algorithm satisfying fairness must take time exonential in the number of states
to achieve non-trivial aroximation to the otimal olicy.  We then rovide a rovably fair olynomial time algorithm under an aroximate notion of fairness, thus establishing an exonential ga between exact and aroximate fairness.
"
20,2017,Evaluating Bayesian Models with Posterior Dispersion Indices,Oral/Poster,"Probabilistic modeling is cyclical: we secify a model, infer its osterior, and evaluate its erformance. Evaluation drives the cycle, as we revise our model based on how it erforms. This requires a metric. Traditionally, redictive accuracy revails. Yet, redictive accuracy does not tell the whole story. We roose to evaluate a model through osterior disersion. The idea is to analyze how each dataoint fares in relation to osterior uncertainty around the hidden structure. This highlights dataoints the model struggles to exlain and rovides comlimentary insight to dataoints with low redictive accuracy. We resent a family of osterior disersion indices (PDI) that cature this idea. We show how a PDI identifies atterns of model mismatch in three real data examles: voting references, suermarket shoing, and oulation genetics.
"
21,2017,Model-Independent Online Learning for Influence Maximization,Oral/Poster,"We consider \emh{influence maximization} (IM) in social networks, which is the roblem of maximizing the number of users that become aware of a roduct by selecting a set of ``seed'' users to exose the roduct to. While rior work assumes a known model of information diffusion, we roose a novel arametrization that not only makes our framework agnostic to the underlying diffusion model, but also statistically efficient to learn from data. We give a corresonding monotone, submodular surrogate function, and show that it is a good aroximation to the original IM objective. We also consider the case of a new marketer looking to exloit an existing social network, while simultaneously learning the factors governing information roagation. For this, we roose a airwise-influence semi-bandit feedback model and develo a LinUCB-based bandit algorithm. Our model-indeendent analysis shows that our regret bound has a better (as comared to revious work) deendence on the size of the network. Exerimental evaluation suggests that our framework is robust to the underlying diffusion model and can efficiently learn a near-otimal solution.
"
22,2017,Latent Feature Lasso,Oral/Poster,"The latent feature model (LFM), roosed in \cite{griffiths2005infinite}, but ossibly with earlier origins, is a generalization of a mixture model, where each instance is generated not from a single latent class but from a combination of \emh{latent features}. Thus, each instance has an associated  latent binary feature incidence vector indicating the resence or absence of a feature. Due to its combinatorial nature, inference of LFMs is considerably intractable, and accordingly, most of the attention has focused on nonarametric LFMs, with riors such as the Indian Buffet Process (IBP) on infinite binary matrices. Recent efforts to tackle this comlexity either still have comutational comlexity that is exonential, or samle comlexity that is high-order olynomial w.r.t. the number of latent features. In this aer, we address this outstanding roblem of tractable estimation of LFMs via a novel atomic-norm regularization, which gives an algorithm with olynomial run-time and samle comlexity without imractical assumtions on the data distribution.
"
23,2017,Resource-efficient Machine Learning in 2 KB RAM for the Internet of Things,Oral/Poster,"This aer develos a novel tree-based algorithm, called Bonsai, for efficient rediction on IoT devices – such as those based on the Arduino Uno board having an 8 bit ATmega328P microcontroller oerating at 16 MHz with no native floating oint suort, 2 KB RAM and 32 KB read-only flash. Bonsai maintains rediction accuracy while minimizing model size and rediction costs by: (a) develoing a tree model which learns a single, shallow, sarse tree with owerful nodes; (b) sarsely rojecting all data into a low-dimensional sace in which the tree is learnt; and (c) jointly learning all tree and rojection arameters. Exerimental results on multile benchmark datasets demonstrate that Bonsai can make redictions in milliseconds even on slow microcontrollers, can fit in KB of memory, has lower battery consumtion than all other algorithms while achieving rediction accuracies that can be as much as 30% higher than state-of-the-art methods for resource-efficient machine learning. Bonsai is also shown to generalize to other resource constrained settings beyond IoT by generating significantly better search results as comared to Bing’s L3 ranker when the model size is restricted to 300 bytes. Bonsai’s code can be downloaded from (htt:www.manikvarma.orgcodeBonsaidownload.html).
"
24,2017,Learning Important Features Through Propagating Activation Differences,Oral/Poster,"The urorted ""black box"" nature of neural networks is a barrier to adotion in alications where interretability is essential. Here we resent DeeLIFT (Dee Learning Imortant FeaTures), a method for decomosing the outut rediction of a neural network on a secific inut by backroagating the contributions of all neurons in the network to every feature of the inut. DeeLIFT comares the activation of each neuron to its `reference activation' and assigns contribution scores according to the difference. By otionally giving searate consideration to ositive and negative contributions, DeeLIFT can also reveal deendencies which are missed by other aroaches. Scores can be comuted efficiently in a single backward ass. We aly DeeLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: \url{htt:goo.glqKb7L}, code: \url{htt:goo.glRM8jvH}.
"
25,2017,Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks,Oral/Poster,"Variational Autoencoders (VAEs) are exressive latent variable models that can be used to learn comlex robability distributions from training data. However, the quality of the resulting model crucially relies on the exressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders  with arbitrarily exressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rehrase the maximum-likelihood-roblem as a two-layer game, hence establishing a rinciled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonarametric limit our method yields an exact maximum-likelihood assignment for the arameters of the generative model, as well as the exact osterior distribution over the latent variables given an observation. Contrary to cometing aroaches which combine VAEs with GANs, our aroach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to imlement.
"
26,2017,Strong NP-Hardness for Sparse Optimization with Concave Penalty Functions,Oral/Poster,"Consider the regularized sarse minimization roblem, which involves emirical sums of loss functions for $n$ data oints (each of dimension $d$) and a nonconvex sarsity enalty. We rove that finding an $\mathcal{O}(n^{c_1}d^{c_2})$-otimal solution to the regularized sarse otimization roblem is strongly NP-hard for any $c_1, c_2\in [0,1)$ such that $c_1+c_2$ less than 1. We also rove strong NP-hardness results for the sarsity-constrained otimizaiotn roblem. These results aly to a broad class of loss functions and sarse enalty functions. They suggest that one cannot even aroximately solve the sarse otimization roblem in olynomial time, unless P $=$ NP."
27,2017,Boosted Fitted Q-Iteration,Oral/Poster,"This aer is about the study of B-FQI, an Aroximated Value Iteration (AVI) algorithm that exloits a boosting rocedure to estimate the action-value function in reinforcement learning roblems. B-FQI is an iterative off-line algorithm that, given a dataset of transitions, builds an aroximation of the otimal action-value function by summing the aroximations of the Bellman residuals across all iterations. The advantage of such aroach w.r.t. to other AVI methods is twofold: (1) while keeing the same function sace at each iteration, B-FQI can reresent more comlex functions by considering an
additive model; (2) since the Bellman residual decreases as the otimal value function is aroached, regression roblems become easier as iterations roceed. We study B-FQI both theoretically, roviding also a finite-samle error uer bound for it, and emirically, by comaring its erformance to the one of FQI in different domains and using different regression techniques.
"
28,2017,Automatic Discovery of the Statistical Types of Variables in a Dataset,Oral/Poster,"A common ractice in statistics and machine learning is to assume that the statistical data tyes (e.g., ordinal, categorical or real-valued) of variables, and usually also the likelihood model, is known. However, as the availability of real-world data increases, this assumtion becomes too restrictive. Data are often heterogeneous, comlex, and imroerly or incomletely documented. Surrisingly, desite their ractical imortance, there is still a lack of tools to automatically discover the statistical tyes of, as well as aroriate likelihood (noise) models for, the variables in a dataset. In this aer, we fill this ga by roosing a Bayesian method, which accurately discovers the statistical data tyes in both synthetic and real data.
"
29,2017,Online Learning to Rank in Stochastic Click Models,Oral/Poster,"Online learning to rank is a core roblem in information retrieval and machine learning. Many rovably efficient algorithms have been recently roosed for this roblem in secific click models. The click model is a model of how the user interacts with a list of documents. Though these results are significant, their imact on ractice is limited, because all roosed algorithms are designed for secific click models and lack convergence guarantees in other models. In this work, we roose BatchRank, the first online learning to rank algorithm for a broad class of click models. The class encomasses two most fundamental click models, the cascade and osition-based models. We derive a ga-deendent uer bound on the T-ste regret of BatchRank and evaluate it on a range of web search queries. We observe that BatchRank outerforms ranked bandits and is more robust than CascadeKL-UCB, an existing algorithm for the cascade model.
"
30,2017,Online Partial Least Square Optimization: Dropping Convexity for Better Efficiency and Scalability,Oral/Poster,"Multiview reresentation learning is oular for latent factor analysis. Many existing aroaches formulate the multiview reresentation learning as convex otimization roblems, where global otima can be obtained by certain algorithms in olynomial time. However, many evidences have corroborated that heuristic nonconvex aroaches also have good emirical comutational erformance and convergence to the global otima, although there is a lack of theoretical justification. Such a ga between theory and ractice motivates us to study a nonconvex formulation for multiview reresentation learning, which can be efficiently solved by a simle stochastic gradient descent method. By analyzing the dynamics of the algorithm based on diffusion rocesses, we establish a global rate of convergence to the global otima. Numerical exeriments are rovided to suort our theory.
"
31,2017,Multi-Class Optimal Margin Distribution Machine,Oral/Poster,"Recent studies disclose that maximizing the minimum margin like suort vector machines does not necessarily lead to better generalization erformances, and instead, it is crucial to otimize the margin distribution. Although it has been shown that for binary classification, characterizing the margin distribution by the first- and second-order statistics can achieve suerior erformance. It still remains oen for multi-class classification, and due to the comlexity of margin for multi-class classification, otimizing its distribution by mean and variance can also be difficult. In this aer, we roose mcODM (multi-class Otimal margin Distribution Machine), which can solve this roblem efficiently. We also give a theoretical analysis for our method, which verifies the significance of margin distribution for multi-class classification. Emirical study further shows that mcODM always outerforms all four versions of multi-class SVMs on all exerimental data sets.
"
32,2017,Evaluating the Variance of Likelihood-Ratio Gradient Estimators,Oral/Poster,"The likelihood-ratio method is often used to estimate gradients of stochastic comutations, for which baselines are required to reduce the estimation variance. Many tyes of baselines have been roosed, although their degree of otimality is not well understood. In this study, we establish a novel framework of gradient estimation that includes most of the common gradient estimators as secial cases. The framework gives a natural derivation of the otimal estimator that can be interreted as a secial case of the likelihood-ratio method so that we can evaluate the otimal degree of ractical techniques with it. It bridges the likelihood-ratio method and the rearameterization trick while still suorting discrete variables. It is derived from the exchange roerty of the differentiation and integration. To be more secific, it is derived by the rearameterization trick and local marginalization analogous to the local exectation gradient. We evaluate various baselines and the otimal estimator for variational learning and show that the erformance of the modern estimators is close to the otimal estimator.
"
33,2017,Learning Texture Manifolds with the Periodic Spatial GAN,Oral/Poster,"This aer introduces a novel aroach to texture synthesis based on generative adversarial networks (GAN) (Goodfellow et al., 2014), and call this technique Periodic Satial GAN (PSGAN). The PSGAN has several novel abilities  which surass the current state of the art in texture synthesis. First, we can learn multile textures, eriodic or non-eriodic, from datasets of one or more comlex large images. Second, we show that the image generation with PSGANs has roerties of a texture manifold: we can smoothly interolate between samles in the structured noise sace and generate novel samles, which lie ercetually between the textures of the original dataset. We make multile exeriments which show that PSGANs can flexibly handle diverse texture and image data sources, and the method is highly scalable and can generate outut images of arbitrary large size.
"
34,2017,Stochastic Convex Optimization: Faster Local Growth Implies Faster Global Convergence,Oral/Poster,"In this aer, a new theory is develoed for first-order stochastic convex otimization, showing that the global convergence rate is sufficiently  quantified by a local growth rate of the objective function in a neighborhood of the otimal solutions. In articular, if the objective function $F(\w)$ in the $\esilon$-sublevel set grows as fast as $\|\w - \w_*\|_2^{1\theta}$, where $\w_*$ reresents the closest otimal solution to $\w$ and $\theta\in(0,1]$ quantifies the local growth rate,  the iteration comlexity of first-order stochastic otimization for achieving  an $\esilon$-otimal solution can be $\widetilde O(1\esilon^{2(1-\theta)})$, which is {\it otimal at most} u to a logarithmic factor. This result is fundamentally better in contrast with the revious works that either assume a global growth condition in the entire domain or achieve a local faster convergence under the local faster growth condition. To achieve the faster global convergence, we develo two different {\bf accelerated stochastic subgradient} methods by iteratively solving the original roblem aroximately in a local region around a historical solution with the size of the local region gradually decreasing as the solution aroaches the otimal set. Besides the theoretical imrovements, this work also include new contributions towards making the roosed algorithms ractical: (i) we resent ractical variants of accelerated stochastic subgradient methods that can run without the knowledge of  multilicative growth constant and even the growth rate $\theta$; (ii) we consider a broad family of roblems in machine learning to demonstrate that the roosed algorithms enjoy faster convergence than traditional stochastic subgradient method.   For examle, when alied to the $\ell_1$ regularized emirical olyhedral loss minimization (e.g., hinge loss, absolute loss), the roosed stochastic methods have a logarithmic iteration comlexity. "
35,2017,Why is Posterior Sampling Better than Optimism for Reinforcement Learning?,Oral/Poster,"Comutational results demonstrate that osterior samling for reinforcement learning (PSRL) dramatically outerforms existing algorithms driven by otimism, such as UCRL2. We rovide insight into the extent of this erformance boost and the henomenon that drives it. We leverage this insight to establish an $\tilde{O}(H\sqrt{SAT})$ Bayesian regret bound for PSRL in finite-horizon eisodic Markov decision rocesses. This imroves uon the best revious Bayesian regret bound of $\tilde{O}(H S \sqrt{AT})$ for any reinforcement learning algorithm. Our theoretical results are suorted by extensive emirical evaluation."
36,2017,Bayesian Models of Data Streams with Hierarchical Power Priors,Oral/Poster,"Making inferences from data streams is a ervasive roblem in many modern data analysis alications. But it requires to address the roblem of continuous model udating, and adat to changes or drifts in the underlying data generating distribution. In this aer, we aroach these roblems from a Bayesian ersective covering general conjugate exonential models. Our roosal makes use of non-conjugate hierarchical riors to exlicitly model temoral changes of the model arameters. We also derive a novel variational inference scheme which overcomes the use of non-conjugate riors while maintaining the comutational efficiency of variational methods over conjugate models. The aroach is validated on three real data sets over three latent variable models. 
"
37,2017,The Sample Complexity of Online One-Class Collaborative Filtering,Oral/Poster,"We consider the online one-class collaborative filtering (CF) roblem that consist of recommending items to users over time in an online fashion based on ositive ratings only. 
This roblem arises when users resond only occasionally to a recommendation with a ositive rating, and never with a negative one. 
We study the imact of the robability of a user resonding to a recommendation, 
emf, on the samle comlexity, and ask whether receiving ositive and negative ratings, instead of ositive ratings only, imroves the samle comlexity. 
Both questions arise in the design of recommender systems. 
We introduce a simle robabilistic user model, and analyze the erformance of an online user-based CF algorithm. 
We rove that after an initial cold start hase, 
where recommendations are invested in exloring the user's references, 
this algorithm makes---u to a fraction of the recommendations required for udating the user's references---erfect recommendations. 
The number of ratings required for the cold start hase 
is nearly roortional to 1emf, 
and that for 
udating the user's references is essentially indeendent of emf. As a consequence we find that, 
receiving ositive and negative ratings instead of only ositive ones imroves the number of ratings required for initial exloration 
by a factor of 1emf, which can be significant. 
"
38,2017,Kernelized Support Tensor Machines,Oral/Poster,"In the context of suervised tensor learning, reserving the structural information and exloiting the discriminative nonlinear relationshis of tensor data are crucial for imroving the erformance of learning tasks. Based on tensor factorization theory and kernel methods, we roose a novel Kernelized Suort Tensor Machine (KSTM) which integrates kernelized tensor factorization with maximum-margin criterion. Secifically, the kernelized factorization technique is introduced to aroximate the tensor data in kernel sace such that the comlex nonlinear relationshis within tensor data can be exlored. Further, dual structural reserving kernels are devised to learn the nonlinear boundary between tensor data. As a result of joint otimization, the kernels obtained in KSTM exhibit better generalization ower to discriminative analysis. The exerimental results on real-world neuroimaging datasets show the sueriority of KSTM over the state-of-the-art techniques.
"
39,2017,Equivariance Through Parameter-Sharing,Oral/Poster,"We roose to study equivariance in dee neural networks through arameter symmetries. In articular, given a grou G that acts discretely on the inut and outut of a standard neural network layer,
we show that its equivariance is linked to the symmetry grou of network arameters. We then roose two arameter-sharing scheme to induce the desirable symmetry on the arameters of the neural network. Under some conditions on the action of G, our rocedure for tying the arameters achieves G-equivariance and guarantees sensitivity  to all other ermutation grous outside of G.
"
40,2017,Generalization and Equilibrium in Generative Adversarial Nets (GANs),Oral/Poster,"It is shown that training of generative adversarial network (GAN) may not have good generalization roerties; e.g., training may aear successful but the trained distribution may be far from target distribution in standard metrics. However, generalization does occur for a weaker metric called neural net distance. It is also shown that an aroximate ure equilibrium exists in the discriminatorgenerator game for a natural training objective (Wasserstein) when generator caacity and training set sizes are moderate. This existence of equilibrium insires MIX+GAN rotocol, which can be combined with any existing GAN training, and emirically shown to imrove some of them.
"
41,2017,GSOS: Gauss-Seidel Operator Splitting Algorithm for Multi-Term Nonsmooth Convex Composite Optimization,Oral/Poster,"In this aer, we roose a fast {\bf{G}}auss-{\bf{S}}eidel {\bf{O}}erator {\bf{S}}litting (GSOS) algorithm for addressing multi-term nonsmooth convex comosite otimization, which has wide alications in machine learning, signal rocessing and statistics. The roosed GSOS algorithm inherits the advantage of the Gauss-Seidel technique to accelerate the otimization rocedure, and leverages the oerator slitting technique to reduce the comutational comlexity. In addition, we develo a new technique to establish the global convergence of the GSOS algorithm. To be secific, we first reformulate the iterations of GSOS as a two-ste iterations algorithm by emloying the tool of oerator otimization theory. Subsequently, we establish the convergence of GSOS based on the two-ste iterations algorithm reformulation. At last, we aly the roosed GSOS algorithm to solve overlaing grou Lasso and grah-guided fused Lasso roblems. Numerical exeriments show that our roosed GSOS algorithm is suerior to the state-of-the-art algorithms in terms of both efficiency and effectiveness.
"
42,2017,Constrained Policy Optimization,Oral/Poster,"For many alications of reinforcement learning it can be more convenient to secify both a reward function and constraints, rather than trying to design behavior through the reward function. For examle, systems that hysically interact with or around humans should satisfy safety constraints. Recent advances in olicy search algorithms (Mnih et al., 2016, Schulman et al., 2015, Lillicra et al., 2016, Levine et al., 2016) have enabled new caabilities in high-dimensional control, but do not consider the constrained setting.
We roose Constrained Policy Otimization (CPO), the first general-urose olicy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network olicies for high-dimensional control while making guarantees about olicy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of indeendent interest: we rove a bound relating the exected returns of two olicies to an average divergence between them. We demonstrate the effectiveness of our aroach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety. 
"
43,2017,Ordinal Graphical Models: A Tale of Two Approaches,Oral/Poster,"Undirected grahical models or Markov random fields (MRFs) are widely used for modeling multivariate robability distributions. Much of the work on MRFs has focused on continuous variables, and nominal variables (that is, unordered categorical variables). However, data from many real world alications involve ordered categorical variables also known as ordinal variables, e.g., movie ratings on Netflix which can be ordered from 1 to 5 stars. 
With resect to univariate ordinal distributions, as we detail in the aer, there are two main categories of distributions; while there have been efforts to extend these to multivariate ordinal distributions, the resulting distributions are tyically very comlex, with either a large number of arameters, or with non-convex likelihoods. While there have been some work on tractable aroximations, these do not come with strong statistical guarantees, and moreover are relatively comutationally exensive.
In this aer, we theoretically investigate two classes of grahical models for ordinal data, corresonding to the two main categories of univariate ordinal distributions. In contrast to revious work, our theoretical develoments allow us to rovide corresondingly two classes of estimators that are not only comutationally efficient but also have strong statistical guarantees.
"
44,2017,Efficient Regret Minimization in Non-Convex Games,Oral/Poster,"We consider regret minimization in reeated games with non-convex loss functions. Minimizing the standard notion of regret is comutationally intractable. Thus, we define a natural notion of regret which ermits efficient otimization and generalizes offline guarantees for convergence to an aroximate local otimum. We give gradient-based methods that achieve otimal regret, which in turn guarantee convergence to equilibrium in this framework.
"
45,2017,Coresets for Vector Summarization with Applications to Network Graphs,Oral/Poster,"We rovide a deterministic data summarization algorithm that aroximates the mean $\bar{}=\frac{1}{n}\sum_{\in P} $ of a set $P$ of $n$ vectors in $\REAL^d$, by a weighted mean $\tilde{}$ of a \emh{subset} of $O(1\es)$ vectors, i.e., indeendent of both $n$ and $d$. We rove that the squared Euclidean distance between $\bar{}$ and $\tilde{}$ is at most $\es$ multilied by the variance of $P$. We use this algorithm to maintain an aroximated sum of vectors from an unbounded stream, using memory that is indeendent of $d$, and logarithmic in the $n$ vectors seen so far. Our main alication is to extract and reresent in a comact way friend grous and activity summaries of users from underlying data exchanges. For examle, in the case of mobile networks, we can use GPS traces to identify meetings; in the case of social networks, we can use information exchange to identify friend grous. Our algorithm rovably identifies the {\it Heavy Hitter} entries in a roximity (adjacency) matrix. The Heavy Hitters can be used to extract and reresent in a comact way friend grous and activity summaries of users from underlying data exchanges. We evaluate the algorithm on several large data sets. "
46,2017,Recovery Guarantees for One-hidden-layer Neural Networks,Oral/Poster,"In this aer, we consider regression roblems with one-hidden-layer neural networks (1NNs). We distill some roerties of activation functions that lead to            \emh{local strong convexity} in the neighborhood of the ground-truth arameters for the 1NN squared-loss objective and most oular nonlinear activation functions     satisfy the distilled roerties, including rectified linear units (\ReLU s), leaky \ReLU s, squared \ReLU s and sigmoids. For activation functions that are also       smooth, we show \emh{local linear convergence} guarantees of gradient descent under a resamling rule. For homogeneous activations, we show tensor methods are able to initialize the arameters to fall into the local strong convexity region. As a result, tensor initialization followed by gradient descent is guaranteed to recover the  ground truth with samle comlexity $ d \cdot \log(1\esilon) \cdot \oly(k,\lambda )$ and comutational comlexity $n\cdot d \cdot \oly(k,\lambda) $ for smooth      homogeneous activations with high robability, where $d$ is the dimension of the inut, $k$ ($k\leq d$) is the number of hidden nodes, $\lambda$ is a conditioning      roerty of the ground-truth arameter matrix between the inut layer and the hidden layer, $\esilon$ is the targeted recision and $n$ is the number of samles. To   the best of our knowledge, this is the first work that rovides recovery guarantees for 1NNs with both samle comlexity and comutational comlexity \emh{linear} in  the inut dimension and \emh{logarithmic} in the recision."
47,2017,Dual Supervised Learning,Oral/Poster,"Many suervised learning tasks are emerged in dual forms, e.g., English-to-French translation vs. French-to-English translation, seech recognition vs. text to seech, and image classification vs. image generation. Two dual tasks have intrinsic connections with each other due to the robabilistic correlation between their models. This connection is, however, not effectively utilized today, since eole usually train the models of two dual tasks searately and indeendently. In this work, we roose training the models of two dual tasks simultaneously, and exlicitly exloiting the robabilistic correlation between them to regularize the training rocess. For ease of reference, we call the roosed aroach dual suervised learning. We demonstrate that dual suervised learning can imrove the ractical erformances of both tasks, for various alications including machine translation, image rocessing, and sentiment analysis.
"
48,2017,Warped Convolutions: Efficient Invariance to Spatial Transformations,Oral/Poster,"Convolutional Neural Networks (CNNs) are extremely efficient, since they exloit the inherent translation-invariance of natural images. However, translation is just one of a myriad of useful satial transformations. Can the same efficiency be attained when considering other satial invariances? Such generalized convolutions have been considered in the ast, but at a high comutational cost. We resent a construction that is simle and exact, yet has the same comutational comlexity that standard convolutions enjoy. It consists of a constant image war followed by a simle convolution, which are standard blocks in dee learning toolboxes. With a carefully crafted war, the resulting architecture can be made equivariant to a wide range of two-arameter satial transformations. We show encouraging results in realistic scenarios, including the estimation of vehicle oses in the Google Earth dataset (rotation and scale), and face oses in Annotated Facial Landmarks in the Wild (3D rotations under ersective).
"
49,2017,McGan: Mean and Covariance Feature Matching GAN,Oral/Poster,"We introduce new families of Integral Probability
Metrics (IPM) for training Generative Adversarial
Networks (GAN). Our IPMs are based on
matching statistics of distributions embedded in
a finite dimensional feature sace. Mean and covariance feature matching IPMs allow for stable
training of GANs, which we will call McGan. McGan minimizes a meaningful loss between distributions.
"
50,2017,Breaking Locality Accelerates Block Gauss-Seidel,Oral/Poster,"Recent work by Nesterov and Stich (2016) showed that momentum can be used to accelerate the rate of convergence for block Gauss-Seidel in the setting where a fixed artitioning of the coordinates is chosen ahead of time.  We show that this setting is too restrictive, constructing instances where breaking locality by running non-accelerated Gauss-Seidel with randomly samled coordinates substantially outerforms accelerated Gauss-Seidel with any fixed artitioning.  Motivated by this finding, we analyze the accelerated block Gauss-Seidel algorithm in the random coordinate samling setting. Our analysis catures the benefit of acceleration with a new data-deendent arameter which is well behaved when the matrix sub-blocks are well-conditioned.  Emirically, we show that accelerated Gauss-Seidel with random coordinate samling rovides seedus for large scale machine learning tasks when comared to non-accelerated Gauss-Seidel and the classical conjugate-gradient algorithm.
"
51,2017,Reinforcement Learning with Deep Energy-Based Policies,Oral/Poster,"We roose a method for learning exressive energy-based olicies for continuous states and actions, which has been feasible only in tabular domains before. We aly our method to learning maximum entroy olicies, resulting into a new algorithm, called soft Q-learning, that exresses the otimal olicy via a Boltzmann distribution. We use the recently roosed amortized Stein variational gradient descent to learn a stochastic samling network that aroximates samles from this distribution. The benefits of the roosed algorithm include imroved exloration and comositionality that allows transferring skills between tasks, which we confirm in simulated exeriments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed erforming aroximate inference on the corresonding energy-based model.
"
52,2017,Scalable Bayesian Rule Lists,Oral/Poster,"We resent an algorithm for building robabilistic rule lists that is two orders of magnitude faster than revious work. Rule list algorithms are cometitors for decision tree algorithms. They are associative classifiers, in that they are built from re-mined association rules. They have a logical structure that is a sequence of IF-THEN rules, identical to a decision list or one-sided decision tree. Instead of using greedy slitting and runing like decision tree algorithms, we aim to fully otimize over rule lists, striking a ractical balance between accuracy, interretability, and comutational seed. The algorithm resented here uses a mixture of theoretical bounds (tight enough to have ractical imlications as a screening or bounding rocedure), comutational reuse, and highly tuned language libraries to achieve comutational efficiency. Currently, for many ractical roblems, this method achieves better accuracy and sarsity than decision trees. In many cases, the comutational time is ractical and often less than that of decision trees.
"
53,2017,Identify the Nash Equilibrium in Static Games with Random Payoffs,Oral/Poster,"We study the roblem on how to learn the ure Nash Equilibrium of a two-layer zero-sum static game with random ayoffs under unknown distributions via efficient ayoff queries. We introduce a multi-armed bandit model to this roblem due to its ability to find the best arm efficiently among random arms and roose two algorithms for this roblem---LUCB-G based on the confidence bounds and a racing algorithm based on successive action elimination. We rovide an analysis on the samle comlexity lower bound when the Nash Equilibrium exists.
"
54,2017,Partitioned Tensor Factorizations for Learning Mixed Membership Models,Oral/Poster,"We resent an efficient algorithm for learning mixed membershi models when the number of variables  is much larger than the number of hidden comonents k. This algorithm reduces the comutational comlexity of state-of-the-art tensor methods, which require decomosing an O(^3) tensor, to factorizing O (k) sub-tensors each of size O(k^3). In addition, we address the issue of negative entries in the emirical method of moments based estimators. We rovide sufficient conditions under which our aroach has rovable guarantees. Our aroach obtains cometitive emirical results on both simulated and real data.
"
55,2017,Failures of Gradient-Based Deep Learning,Oral/Poster,"In recent years, Dee Learning has become the go-to solution for a
  broad range of alications, often outerforming
  state-of-the-art. However, it is imortant, for both theoreticians
  and ractitioners, to gain a deeer understanding of the
  difficulties and limitations associated with common aroaches and
  algorithms. We describe four tyes of simle roblems, for which the
  gradient-based algorithms commonly used in dee learning either fail
  or suffer from significant difficulties. We illustrate the failures
  through ractical exeriments, and rovide theoretical insights
  exlaining their source, and how they might be
  remedied.
"
56,2017,Learning Infinite Layer Networks without the Kernel Trick,Oral/Poster,"Infinite Layer Networks (ILN) have been roosed as an architecture that mimics neural networks while enjoying some of the advantages of kernel methods. ILN  are networks that integrate over infinitely many nodes within a single hidden layer. It has been demonstrated by several authors that the roblem of learning ILN can be reduced to the kernel trick, imlying that whenever a certain integral can be comuted analytically they are efficiently learnable. 
In this work we give an online algorithm for ILN, which avoids the kernel trick assumtion. More generally and of indeendent interest, we show that kernel methods in general can be exloited even when the kernel cannot be efficiently comuted but can only be estimated via samling. We rovide a regret analysis for our algorithm, showing that it matches the samle comlexity of methods which have access to kernel values.  Thus, our method is the first to demonstrate that the kernel trick is not necessary, as such, and random features suffice to obtain comarable erformance.
"
57,2017,Graph-based Isometry Invariant Representation Learning,Oral/Poster,"Learning transformation invariant reresentations of visual data is an imortant roblem in comuter vision. Dee convolutional networks have demonstrated remarkable results for image and video classification tasks. However,  they have achieved only limited success in the classification of images that undergo geometric transformations. In this work we resent a novel Transformation Invariant Grah-based Network (TIGraNet), which learns grah-based features that are inherently invariant to isometric transformations such as rotation and translation of inut images. In articular, images are reresented as signals on grahs, which ermits to relace classical convolution and ooling layers in dee networks with grah sectral convolution and dynamic grah ooling layers that together contribute to invariance to isometric transformation. Our exeriments show high erformance on rotated and translated images from the test set comared to classical architectures that are very sensitive to transformations in the data. The inherent invariance roerties of our framework rovide key advantages, such as increased resiliency to data variability and sustained erformance with limited training sets.
"
58,2017,Conditional Image Synthesis with Auxiliary Classifier GANs,Oral/Poster,"In this aer we introduce new
methods for the imroved training of generative
adversarial networks (GANs) for image synthesis.
We construct a variant of GANs emloying
label conditioning that results in 128 × 128 resolution
image samles exhibiting global coherence.
We exand on revious work for image
quality assessment to rovide two new analyses
for assessing the discriminability and diversity of
samles from class-conditional image synthesis
models. These analyses demonstrate that high
resolution samles rovide class information not
resent in low resolution samles. Across 1000
ImageNet classes, 128 × 128 samles are more
than twice as discriminable as artificially resized
32×32 samles. In addition, 84.7% of the classes
have samles exhibiting diversity comarable to
real ImageNet data.
"
59,2017,Stochastic DCA for the Large-sum of Non-convex Functions Problem and its Application to Group Variable Selection in Classification,Oral/Poster,"In this aer, we resent a stochastic version of DCA (Difference of Convex functions Algorithm) to solve a class of otimization roblems whose objective function is a large sum of non-convex functions and a regularization term. We consider the $\ell_{2,0}$ regularization to deal with the grou variables selection. By exloiting the secial structure of the roblem, we roose an efficient DC decomosition for which the corresonding stochastic DCA scheme is very inexensive: it only requires the rojection of oints onto balls that is exlicitly comuted. As an alication, we alied our algorithm for the grou variables selection in multiclass logistic regression. Numerical exeriments on several benchmark datasets and synthetic datasets illustrate the efficiency of our algorithm and its sueriority over well-known methods, with resect to classification accuracy, sarsity of solution as well as running time."
60,2017,Prediction and Control with Temporal Segment Models,Oral/Poster,"We introduce a method for learning the dynamics of comlex nonlinear systems based on dee generative models over temoral segments of states and actions. Unlike dynamics models that oerate over individual discrete timestes, we learn the distribution over future state trajectories conditioned on ast state, ast action, and lanned future action trajectories, as well as a latent rior over action trajectories. Our aroach is based on convolutional autoregressive models and variational autoencoders. It makes stable and accurate redictions over long horizons for comlex, stochastic systems, effectively exressing uncertainty and modeling the effects of collisions, sensory noise, and action delays. The learned dynamics model and action rior can be used for end-to-end, fully differentiable trajectory otimization and model-based olicy otimization, which we use to evaluate the erformance and samle-efficiency of our method.
"
61,2017,Learning Determinantal Point Processes with Moments and Cycles,Oral/Poster,"Determinantal Point Processes (DPPs) are a family of robabilistic models that have a reulsive behavior, and lend themselves naturally to many tasks in machine learning where returning a diverse set of objects is imortant. While there are fast algorithms for samling, marginalization and conditioning, much less is known about learning the arameters of a DPP. Our contribution is twofold: (i) we establish the otimal samle comlexity achievable in this roblem and show that it is governed by a natural arameter, which we call the cycle sarsity; (ii) we roose a rovably fast combinatorial algorithm that imlements the method of moments efficiently and achieves otimal samle comlexity. Finally, we give exerimental results that confirm our theoretical findings. 
"
62,2017,Follow the Compressed Leader: Faster Online Learning of Eigenvectors and Faster MMWU,Oral/Poster,"The online roblem of comuting the to eigenvector is fundamental to machine learning. The famous matrix-multilicative-weight-udate (MMWU) framework solves this online roblem and gives otimal regret. However, since MMWU runs very slow due to the comutation of matrix exonentials, researchers roosed the follow-the-erturbed-leader (FTPL) framework which is faster, but a factor $\sqrt{d}$ worse than the otimal regret for dimension-$d$ matrices.

We roose a \emh{follow-the-comressed-leader} framework which, not only matches the otimal regret of MMWU (u to olylog factors), but runs no slower than FTPL.

Our main idea is to ``comress'' the MMWU strategy to dimension 3 in the adversarial setting, or dimension 1 in the stochastic setting. This resolves an oen question regarding how to obtain both (nearly) otimal and efficient algorithms for the online eigenvector roblem."
63,2017,On Mixed Memberships and Symmetric Nonnegative Matrix Factorizations,Oral/Poster,"The roblem of finding overlaing communities in networks has gained much attention recently. Otimization-based aroaches use non-negative matrix factorization (NMF) or variants, but the global otimum cannot be rovably attained in general. Model-based aroaches, such as the oular mixed-membershi stochastic blockmodel or MMSB (Airoldi et al., 2008), use arameters for each node to secify the overlaing communities, but standard inference techniques cannot guarantee consistency. We link the two aroaches, by (a) establishing sufficient conditions for the symmetric NMF otimization to have a unique solution under MMSB, and (b) roosing a comutationally efficient algorithm called GeoNMF that is rovably otimal and hence consistent for a broad arameter regime. We demonstrate its accuracy on both simulated and real-world datasets.
"
64,2017,Analytical Guarantees on Numerical Precision of Deep Neural Networks,Oral/Poster,"The acclaimed successes of neural networks often overshadow their tremendous comlexity. We focus on numerical recision - a key arameter defining the comlexity of neural networks. First, we resent theoretical bounds on the accuracy in resence of limited recision. Interestingly, these bounds can be comuted via the back-roagation algorithm. Hence, by combining our theoretical analysis and the back-roagation algorithm, we are able to readily determine the minimum recision needed to reserve accuracy without having to resort to  time-consuming fixed-oint simulations. We rovide numerical evidence showing how our aroach allows us to maintain high accuracy but with lower comlexity than state-of-the-art binary networks.
"
65,2017,Random Fourier Features for Kernel Ridge Regression: Approximation Bounds and Statistical Guarantees,Oral/Poster,"Random Fourier features is one of the most oular techniques for scaling u kernel methods, such as kernel ridge regression. However, desite imressive emirical results, the statistical roerties of random Fourier features are still not well understood. In this aer we take stes toward filling this ga. Secifically, we aroach random Fourier features from a sectral matrix aroximation oint of view, give tight bounds on the number of Fourier features required to achieve a sectral aroximation, and show how sectral matrix aroximation bounds imly statistical guarantees for kernel ridge regression.
"
66,2017,Deriving Neural Architectures from Sequence and Graph Kernels,Oral/Poster,"The design of neural architectures for structured objects is tyically guided by exerimental insights rather than a formal rocess.
In this work, we aeal to kernels over combinatorial structures, such as sequences and grahs, to derive aroriate neural oerations.
We introduce a class of dee recurrent neural oerations and formally characterize their associated kernel saces. Our recurrent modules comare the inut to virtual reference objects (cf. filters in CNN) via the kernels. 
Similar to traditional neural oerations, these reference objects are arameterized and directly otimized in end-to-end training.
We emirically evaluate the roosed class of neural architectures on standard alications such as language modeling and molecular grah regression, achieving state-of-the-art results across these alications.
"
67,2017,Learning to Discover Cross-Domain Relations with Generative Adversarial Networks,Oral/Poster,"While humans easily recognize relations between data from different domains without any suervision, learning to automatically discover them is in general very challenging and needs many ground-truth airs that illustrate the relations. To avoid costly airing, we address the task of discovering cross-domain relations given unaired data. We roose a method based on a generative adversarial network that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our roosed network successfully transfers style from one domain to another while reserving key attributes such as orientation and face identity.
"
68,2017,Gradient Projection Iterative Sketch for Large-Scale Constrained Least-Squares,Oral/Poster,"We roose a randomized first order otimization algorithm Gradient Projection Iterative Sketch (GPIS) and an accelerated variant for efficiently solving large scale constrained Least Squares (LS). We rovide the first theoretical convergence analysis for both algorithms. An efficient imlementation using a tailored line-search scheme is also roosed. We demonstrate our methods' comutational efficiency comared to the classical accelerated gradient method, and the variance-reduced stochastic gradient methods through numerical exeriments in various large syntheticreal data sets.
"
69,2017,An Alternative Softmax Operator for Reinforcement Learning,Oral/Poster,"A softmax oerator alied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against roblems that arise from utting all of one’s weight behind a single maximum utility decision. The Boltzmann softmax oerator is the most commonly used softmax oerator in this setting, but we show that this oerator is rone to misbehavior. In this work, we study a differentiable softmax oerator that, among other roerties, is a non-exansion ensuring a convergent behavior in learning and lanning. We introduce a variant of SARSA algorithm that, by utilizing the new oerator, comutes a Boltzmann olicy with a state-deendent temerature arameter. We show that the algorithm is convergent and that it erforms favorably in ractice.
"
70,2017,Deep Bayesian Active Learning with Image Data,Oral/Poster,"Even though active learning forms an imortant illar of machine learning, dee learning tools are not revalent within it. Dee learning oses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and udate models from small amounts of data. Recent advances in dee learning, on the other hand, are notorious for their deendence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet dee learning methods rarely reresent such model uncertainty. In this aer we combine recent advances in Bayesian dee learning into the active learning framework in a ractical way. We develo an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sarse existing literature. Taking advantage of secialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant imrovement on existing active learning aroaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).
"
71,2017,On Kernelized Multi-armed Bandits,Oral/Poster,"We consider the stochastic bandit roblem with a continuous set of arms, with the exected reward function over the arms assumed to be fixed but unknown. We rovide two new Gaussian rocess-based algorithms for continuous bandit otimization -- Imroved GP-UCB (IGP-UCB) and GP-Thomson samling (GP-TS), and derive corresonding regret bounds. Secifically, the bounds hold when the exected reward function belongs to the reroducing kernel Hilbert sace (RKHS) that naturally corresonds to a Gaussian rocess kernel used as inut by the algorithms. Along the way, we derive a new self-normalized concentration inequality for vector-valued martingales of arbitrary, ossibly infinite, dimension. Finally, exerimental evaluation and comarisons to existing algorithms on synthetic and real-world environments are carried out that highlight the favourable gains of the roosed strategies in many cases. 
"
72,2017,Nonnegative Matrix Factorization for Time Series Recovery From a Few Temporal Aggregates,Oral/Poster,"Motivated by electricity consumtion reconstitution, we roose a new matrix recovery method using nonnegative matrix factorization (NMF).
The task tackled here is to reconstitute electricity consumtion time series at a fine temoral scale from measures that are temoral aggregates of individual consumtion. 
Contrary to existing NMF algorithms, the roosed method uses temoral aggregates as inut data, instead of matrix entries.
Furthermore, the roosed method is extended to take into account individual autocorrelation to rovide better estimation, using a recent convex relaxation of quadratically constrained quadratic rograms. 
Extensive exeriments on synthetic and real-world electricity consumtion datasets illustrate the effectiveness of the roosed method.
"
73,2017,Follow the Moving Leader in Deep Learning,Oral/Poster,"Dee networks are highly nonlinear and difficult to otimize. During training, the arameter iterate may move from one local basin to another, or the data distribution may even change. Insired by the close connection between stochastic otimization and online learning, we roose a variant of the {\em follow the regularized leader} (FTRL) algorithm called {\em follow the moving leader} (FTML). Unlike the FTRL family of algorithms, the recent samles are weighted more heavily in each iteration and so FTML can adat more quickly to changes. We show that FTML enjoys the nice roerties of RMSro and Adam, while avoiding their itfalls.  Exerimental results on a number of dee learning models and tasks demonstrate that FTML converges quickly, and outerforms other state-of-the-art otimizers.
"
74,2017,Logarithmic Time One-Against-Some,Oral/Poster,"We create a new online reduction of multiclass classification to binary classification for which training and rediction time scale logarithmically with the number of classes. We show that several simle techniques give rise to an algorithm which is suerior to revious logarithmic time classification aroaches while cometing with one-against-all in sace. The core construction is based on using a tree to select a small subset of labels with high recall, which are then scored using a one-against-some structure with high recision.
"
75,2017,Unsupervised Learning by Predicting Noise,Oral/Poster,"Convolutional neural networks rovide visual features that erform remarkably well in many comuter vision alications. However, training these networks requires significant amounts of suervision; this aer introduces a generic framework to train such networks, end-to-end, with no suervision. We roose to fix a set of target reresentations, called Noise As Targets (NAT), and to constrain the dee features to align to them. This domain agnostic aroach avoids the standard unsuervised learning issues of trivial solutions and collasing of the features. Thanks to a stochastic batch reassignment strategy and a searable square loss function, it scales to millions of images. The roosed aroach roduces reresentations that erform on ar with the state-of-the-arts among unsuervised methods on ImageNet and Pascal VOC.
"
76,2017,Wasserstein Generative Adversarial Networks,Oral/Poster,"We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can imrove the stability of learning, get rid of roblems like mode collase, and rovide meaningful learning curves useful for debugging and hyerarameter searches. Furthermore, we show that the corresonding otimization roblem is sound, and rovide extensive theoretical work highlighting the dee connections to different distances between distributions.
"
77,2017,Connected Subgraph Detection with Mirror Descent on SDPs,Oral/Poster,"We roose a novel, comutationally efficient mirror-descent based otimization framework for subgrah detection in grah-structured data. Our aim is to discover anomalous atterns resent in a connected subgrah of a given grah. This roblem arises in many alications such as detection of network intrusions, community detection, detection of anomalous events in surveillance videos or disease outbreaks. 
Since otimization over connected subgrahs is a combinatorial and comutationally difficult roblem, we roose a convex relaxation that offers a rinciled aroach to incororating connectivity and conductance constraints on candidate subgrahs. We develo a novel efficient algorithm to solve the relaxed roblem, establish convergence  guarantees and demonstrate its feasibility and erformance with exeriments on real and very large simulated networks.
"
78,2017,Fake News Mitigation via Point Process Based Intervention,Oral/Poster,"We roose the first multistage intervention framework that tackles fake news in social networks by combining reinforcement learning with a oint rocess network activity model. The sread of fake news and mitigation events within the network is modeled by a multivariate Hawkes rocess with additional exogenous control terms. By choosing a feature reresentation of states, defining mitigation actions and constructing reward functions to measure the effectiveness of mitigation activities, we ma the roblem of fake news mitigation into the reinforcement learning framework. We develo a olicy iteration method unique to the multivariate networked oint rocess, with the goal of otimizing the actions for maximal reward under budget constraints. Our method shows romising erformance in real-time intervention exeriments on a Twitter network to mitigate a surrogate fake news camaign, and outerforms alternatives on synthetic datasets.
"
79,2017,Bayesian Boolean Matrix Factorisation,Oral/Poster,"Boolean matrix factorisation aims to decomose a binary data matrix into an aroximate Boolean roduct of two low rank, binary matrices: one containing meaningful atterns, the other quantifying how the observations can be exressed as a combination of these atterns. We introduce the OrMachine, a robabilistic generative model for Boolean matrix factorisation and derive a Metroolised Gibbs samler that facilitates efficient arallel osterior inference. On real world and simulated data, our method outerforms all currently existing aroaches for Boolean matrix factorisation and comletion. This is the first method to rovide full osterior inference for Boolean Matrix factorisation which is relevant in alications, e.g. for controlling false ositive rates in collaborative filtering and, crucially, imroves the interretability of the inferred atterns. The roosed algorithm scales to large datasets as we demonstrate by analysing single cell gene exression data in 1.3 million mouse brain cells across 11 thousand genes on commodity hardware.
"
80,2017,Second-Order Kernel Online Convex Optimization with Adaptive Sketching,Oral/Poster,"Kernel online convex otimization (KOCO) is a framework combining the exressiveness of non-arametric kernel models with the regret guarantees of online learning. First-order KOCO methods such as functional gradient descent require only $O(t)$ time and sace er iteration, and, when the only information on the losses is their convexity, achieve a minimax otimal $O(\sqrt{T})$ regret. Nonetheless, many common losses in kernel roblems, such as squared loss, logistic loss, and squared hinge loss osses stronger curvature that can be exloited. In this case, second-order KOCO methods achieve $O(\log(\Det(K)))$ regret, which we show scales as $O(deff \log T)$, where $deff$ is the effective dimension of the roblem and is usually much smaller than $O(\sqrt{T})$. The main drawback of second-order methods is their much higher $O(t^2)$ sace and time comlexity.
In this aer, we introduce kernel online Newton ste (KONS), a new second-order KOCO method that also achieves $O(deff\log T)$ regret. To address the comutational comlexity of second-order methods, we introduce a new matrix sketching algorithm for the kernel matrix~$K$, and show that for a chosen arameter $\gamma \leq 1$ our Sketched-KONS reduces the sace and time comlexity by a factor of $\gamma^2$ to $O(t^2\gamma^2)$ sace and time er iteration, while incurring only $1\gamma$ times more regret.
"
81,2017,Frame-based Data Factorizations,Oral/Poster,"Archetyal Analysis is the method of choice to comute interretable matrix factorizations. Every data oint is reresented as a convex combination of factors, i.e., oints on the boundary of the convex hull of the data. This renders comutation inefficient. In this aer, we show that the set of vertices of a convex hull, the so-called frame, can be efficiently comuted by a quadratic rogram. We rovide theoretical and emirical results  for our roosed aroach and make use of the frame to accelerate Archetyal Analysis. The novel method yields similar reconstruction errors as baseline cometitors but is much faster to comute.
"
82,2017,Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank,Oral/Poster,"Recently low dislacement rank (LDR) matrices, or so-called structured matrices, have been roosed to comress large-scale neural networks. Emirical results have shown that neural networks with weight matrices of LDR matrices, referred as LDR neural networks, can achieve significant reduction in sace and comutational comlexity while retaining high accuracy. This aer gives theoretical study on LDR neural networks. First, we rove the universal aroximation roerty of LDR neural networks with a mild condition on the dislacement oerators. We then show that the error bounds of LDR neural networks are as efficient as general neural networks with both single-layer and multile-layer structure. Finally, we roose back-roagation based training algorithm for general LDR neural networks.
"
83,2017,Understanding Black-box Predictions via Influence Functions,Oral/Poster,"How can we exlain the redictions of a black-box model? In this aer, we use influence functions --- a classic technique from robust statistics --- to trace a model's rediction through the learning algorithm and back to its training data, thereby identifying training oints most resonsible for a given rediction. To scale u influence functions to modern machine learning settings, we develo a simle, efficient imlementation that requires only oracle access to gradients and Hessian-vector roducts. We show that even on non-convex and non-differentiable models where the theory breaks down, aroximations to influence functions can still rovide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multile uroses: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.
"
84,2017,Deep Transfer Learning with Joint Adaptation Networks,Oral/Poster,"Dee networks have been successfully alied to learn transferable features for adating models from a source domain to a different target domain. In this aer, we resent joint adatation networks (JAN), which learn a transfer network by aligning the joint distributions of multile domain-secific layers across domains based on a joint maximum mean discreancy (JMMD) criterion. Adversarial training strategy is adoted to maximize JMMD such that the distributions of the source and target domains are made more distinguishable. Learning can be erformed by stochastic gradient descent with the gradients comuted by back-roagation in linear-time. Exeriments testify that our model yields state of the art results on standard datasets.
"
85,2017,Learning Hierarchical Features from Deep Generative Models,Oral/Poster,"Dee neural networks have been shown to be very successful at learning feature hierarchies in suervised learning tasks. Generative models, on the other hand, have benefited less from hierarchical models with multile layers of latent variables. In this aer, we rove that hierarchical latent variable models do not take advantage of the hierarchical structure when trained with existing variational methods, and rovide some limitations on the kind of features existing models can learn. Finally we roose an alternative architecture that do not suffer from these limitations. Our model is able to learn highly interretable and disentangled hierarchical features on several natural image datasets with no task secific regularization or rior knowledge. 
"
86,2017,Prox-PDA: The Proximal Primal-Dual Algorithm for Fast Distributed Nonconvex Optimization and Learning Over Networks,Oral/Poster,"In this aer we consider  nonconvex otimization and learning over a network of distributed nodes.  We develo a  Proximal Primal-Dual Algorithm (Prox-PDA),  which enables the network nodes to distributedly and collectively comute the set of first-order stationary solutions in a global sublinear manner  [with a  rate of $\cO(1r)$, where $r$ is the iteration counter]. To the best of our knowledge, this is the first algorithm that enables distributed nonconvex otimization with global rate guarantees.  Our numerical exeriments also demonstrate the effectiveness of the roosed algorithm.   "
87,2017,Curiosity-driven Exploration by Self-supervised Prediction,Oral/Poster,"In many real-world scenarios, rewards extrinsic to the agent are extremely sarse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to exlore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to redict the consequence of its own actions in a visual feature sace learned by a self-suervised inverse dynamics model. Our formulation scales to high-dimensional continuous state saces like images, byasses the difficulties of directly redicting ixels, and, critically, ignores the asects of the environment that cannot affect the agent. The roosed aroach is evaluated in two environments: VizDoom and Suer Mario Bros. Three broad settings are investigated: 1) sarse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exloration with no extrinsic reward, where curiosity ushes the agent to exlore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier exerience hels the agent exlore new laces much faster than starting from scratch.
"
88,2017,Learning the Structure of Generative Models without Labeled Data,Oral/Poster,"Curating labeled training data has become the rimary bottleneck in machine learning. Recent frameworks address this bottleneck with generative models to synthesize labels at scale from weak suervision sources. The generative model's deendency structure directly affects the quality of the estimated labels, but selecting a structure automatically without any labeled data is a distinct challenge. We roose a structure estimation method that maximizes the l1-regularized marginal seudolikelihood of the observed data. Our analysis shows that the amount of unlabeled data required to identify the true structure scales sublinearly in the number of ossible deendencies for a broad class of models. Simulations show that our method is 100x faster than a maximum likelihood aroach and selects 14 as many extraneous deendencies. We also show that our method rovides an average of 1.5 F1 oints of imrovement over existing, user-develoed information extraction alications on real-world data such as PubMed journal abstracts.
"
89,2017,Dueling Bandits with Weak Regret,Oral/Poster,"We consider online content recommendation with imlicit feedback through airwise comarisons, formalized as the so-called dueling bandit roblem. We study the dueling bandit roblem in the Condorcet winner setting, and consider two notions of regret: the more well-studied strong regret, which is 0 only when both arms ulled are the Condorcet winner; and the less well-studied weak regret, which is 0 if either arm ulled is the Condorcet winner. We roose a new algorithm for this roblem, Winner Stays (WS), with variations for each kind of regret:
WS for weak regret (WS-W) has exected cumulative weak regret that is $O(N^2)$, and $O(N\log(N))$ if arms have a total order; WS for strong regret (WS-S) has exected cumulative strong regret of $O(N^2 + N \log(T))$, and $O(N\log(N)+N\log(T))$ if arms have a total order.
WS-W is the first dueling bandit algorithm with weak regret that is constant in time.
WS is simle to comute, even for roblems with many arms, and we demonstrate through numerical exeriments on simulated and real data that WS has significantly smaller regret than existing algorithms in both the weak- and strong-regret settings."
90,2017,Nearly Optimal Robust Matrix Completion,Oral/Poster,"In this aer, we consider the roblem of Robust Matrix Comletion (RMC) where the goal is to recover a low-rank matrix by observing a small number of its entries out of which a few can be arbitrarily corruted. We roose a simle rojected gradient descent-based method to estimate the low-rank matrix that alternately erforms a rojected gradient descent ste and cleans u a few of the corruted entries using hard-thresholding. Our algorithm solves RMC using nearly otimal number of observations while tolerating a nearly otimal number of corrutions. Our result also imlies significant imrovement over the existing time comlexity bounds for the low-rank matrix comletion roblem. Finally, an alication of our result to the robust PCA roblem (low-rank+sarse matrix searation)  leads to nearly linear time (in matrix dimensions) algorithm for the same; existing state-of-the-art methods require quadratic time. Our emirical results corroborate our theoretical results and show that even for moderate sized roblems, our method for robust PCA is an order of magnitude faster than the existing methods.
"
91,2017,Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs,Oral/Poster,"Dee learning models are often successfully trained using gradient descent, desite the worst case hardness of the underlying non-convex otimization roblem. The key question is then under what conditions can one rove that otimization will succeed. Here we rovide a strong result of this kind. We consider a neural net with one hidden layer and a convolutional structure with no overla and a ReLU activation function. For this architecture we show that learning is NP-comlete in the general case, but that when the inut distribution is Gaussian, gradient descent converges to the global otimum in olynomial time. To the best of our knowledge, this is the first global otimality guarantee of gradient descent on a convolutional neural network with ReLU activations.
"
92,2017,Re-revisiting Learning on Hypergraphs: Confidence Interval and Subgradient Method,Oral/Poster,"We revisit semi-suervised learning on hyergrahs. Same as revious aroaches, our method uses a convex rogram whose objective function is not everywhere differentiable.  We exloit the non-uniqueness of the otimal solutions, and consider  confidence intervals which give the exact ranges that unlabeled vertices take in any otimal solution. Moreover, we give a much simler aroach for solving the convex rogram based on the subgradient method. Our exeriments on real-world datasets confirm that our confidence interval aroach on hyergrahs outerforms existing methods, and our sub-gradient method gives faster running times when the number of vertices is much larger than the number of edges.
"
93,2017,Meta Networks,Oral/Poster,"Neural networks have been successfully alied in alications with a large amount of labeled data. However, the task of raid generalization on new concets with small training data while reserving erformances on reviously learned ones still resents a significant challenge to neural network models. In this work, we introduce a novel meta learning method, Meta Networks (MetaNet), that learns a meta-level knowledge across tasks and shifts its inductive biases via fast arameterization for raid generalization. When evaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve a near human-level erformance and outerform the baseline aroaches by u to 6% accuracy. We demonstrate several aealing roerties of MetaNet relating to generalization and continual learning.
"
94,2017,Bottleneck Conditional Density Estimation,Oral/Poster,"We introduce a new framework for training dee generative models for high-dimensional conditional density estimation. The Bottleneck Conditional Density Estimator (BCDE) is a variant of the conditional variational autoencoder (CVAE) that emloys layer(s) of stochastic variables as the bottleneck between the inut x and target y, where both are high-dimensional. Crucially, we roose a new hybrid training method that blends the conditional generative model with a joint generative model. Hybrid blending is the key to effective training of the BCDE, which avoids overfitting and rovides a novel mechanism for leveraging unlabeled data. We show that our hybrid training rocedure enables models to achieve cometitive results in the MNIST quadrant rediction task in the fully-suervised setting, and sets new benchmarks in the semi-suervised regime for MNIST, SVHN, and CelebA.
"
95,2017,Exploiting Strong Convexity from Data with Primal-Dual First-Order Algorithms,Oral/Poster,"We consider emirical risk minimization of linear redictors with convex loss functions. Such roblems can be reformulated as convex-concave saddle oint roblems and solved by rimal-dual first-order algorithms. However, rimal-dual algorithms often require exlicit strongly convex 
regularization in order to obtain fast linear convergence, and the required dual roximal maing may not admit closed-form or efficient solution.  In this aer, we develo both batch and randomized rimal-dual algorithms  that can exloit strong convexity from data adatively
and are caable of achieving linear convergence even without regularization. We also resent dual-free variants of adative rimal-dual algorithms 
that do not need the dual roximal maing, 
which are esecially suitable for logistic regression.
"
96,2017,Interactive Learning from Policy-Dependent Human Feedback,Oral/Poster,"This aer investigates the roblem of interactively learning behaviors communicated by a human teacher using ositive and negative feedback. Much revious work on this roblem has made the assumtion that eole rovide feedback for decisions that is deendent on the behavior they are teaching and is indeendent from the learner's current olicy. We resent emirical results that show this assumtion to be false---whether human trainers give a ositive or negative feedback for a decision is influenced by the learner's current olicy. Based on this insight, we introduce Convergent Actor-Critic by Humans (COACH), an algorithm for learning from olicy-deendent feedback that converges to a local otimum. Finally, we demonstrate that COACH can successfully learn multile behaviors on a hysical robot.
"
97,2017,Learning to Discover Sparse Graphical Models,Oral/Poster,"We consider structure discovery of undirected grahical models from observational data. Inferring likely structures from few examles is a comlex task often requiring the formulation of riors and sohisticated inference rocedures.  Poular methods rely on estimating a enalized maximum likelihood of the recision matrix. However, in these aroaches structure recovery is an indirect consequence of the data-fit term, the enalty can be difficult to adat for domain-secific knowledge, and the inference is comutationally demanding.
By contrast, it may be easier to generate training samles of data that arise from grahs with the desired structure roerties. We roose here to leverage this
latter source of information as training data to learn a function, arametrized by a neural network, that
mas emirical covariance matrices to estimated grah structures.  Learning this function brings two benefits: it imlicitly models the desired
structure or sarsity roerties to form suitable riors, and it can be
tailored to the secific roblem of edge structure discovery,
rather than maximizing data likelihood. Alying this framework, we find our learnable grah-discovery method trained on synthetic data generalizes
well: identifying relevant edges in both synthetic and real data,
comletely unknown at training time. We find that on
genetics, brain imaging, and simulation data we obtain erformance generally suerior to analytical methods.
"
98,2017,On Context-Dependent Clustering of Bandits,Oral/Poster,"We investigate a novel cluster-of-bandit algorithm CAB for collaborative recommendation tasks that imlements the underlying feedback sharing mechanism by estimating user neighborhoods in a context-deendent manner. CAB makes shar deartures from the state of the art by incororating collaborative effects into inference, as well as learning rocesses in a manner that seamlessly interleaves exlore-exloit tradeoffs and collaborative stes. We rove regret bounds for CAB under various data-deendent assumtions which exhibit a cris deendence on the exected number of clusters over the users, a natural measure of the statistical difficulty of the learning task. Exeriments on roduction and real-world datasets show that CAB offers significantly increased rediction erformance against a reresentative ool of state-of-the-art methods.
"
99,2017,Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations,Oral/Poster,"Non-negative matrix factorization is a basic tool for decomosing data into the feature and weight matrices under non-negativity constraints, and in ractice is often solved in the alternating minimization framework. 
However, it is unclear whether such algorithms can recover the ground-truth feature matrix when the weights for different features are highly correlated, which is common in alications.
This aer rooses a simle and natural alternating gradient descent based algorithm, and shows that with a mild initialization it rovably recovers the ground-truth in the resence of strong correlations. 
In most interesting cases, the correlation can be in the same order as the highest ossible. 
Our analysis also reveals its several favorable features including robustness to noise. 
We comlement our theoretical results with emirical studies on semi-synthetic datasets, demonstrating its advantage over several oular methods in recovering the ground-truth.
"
100,2017,Convexified Convolutional Neural Networks,Oral/Poster,"We describe the class of convexified convolutional neural networks (CCNNs), which cature the arameter sharing of convolutional neural networks in a convex manner. By reresenting the nonlinear convolutional filters as vectors in a reroducing kernel Hilbert sace, the CNN arameters can be reresented as a low-rank matrix, which can be relaxed to obtain a convex otimization roblem.  For learning two-layer convolutional neural networks, we rove that the generalization error obtained by a convexified CNN converges to that of the best ossible CNN.  For learning deeer networks, we train CCNNs in a layer-wise manner. Emirically, CCNNs achieve cometitive or better erformance than CNNs trained by backroagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.
"
101,2017,Self-Paced Co-training,Oral/Poster,"Co-training is a well-known semi-suervised learning aroach which trains classifiers on two different views and exchanges labels of unlabeled instances in an iterative way. During co-training rocess, labels of unlabeled instances in the training ool are very likely to be false esecially in the initial training rounds, while the standard co-training algorithm utilizes a ""draw without relacement"" manner and does not remove these false labeled instances from training. This issue not only tends to degenerate its erformance but also hamers its fundamental theory. Besides, there is no otimization model to exlain what objective a cotraining rocess otimizes. To these issues, in this study we design a new co-training algorithm named self-aced cotraining (SPaCo) with a ``draw with relacement"" learning mode. The rationality of SPaCo can be roved under theoretical assumtions utilized in traditional co-training research, and furthermore, the algorithm exactly  comlies with the alternative otimization rocess for an otimization model of self-aced curriculum learning, which can be finely exlained in robust learning manner. Exerimental results substantiate the sueriority of the roosed method as comared with current state-of-the-art co-training methods.
"
102,2017,SplitNet: Learning to Semantically Split Deep Networks for Parameter Reduction and Model Parallelization,Oral/Poster,"We roose a novel dee neural network that is both lightweight and effectively structured for model arallelization. Our network, which we name as SlitNet, automatically learns to slit the network weights into either a set or a hierarchy of multile grous that use disjoint sets of features, by learning both the class-to-grou and feature-to-grou assignment matrices along with the network weights. This roduces a tree-structured network that involves no connection between branched subtrees of semantically disarate class grous. SlitNet thus greatly reduces the number of arameters and requires significantly less comutations, and is also embarrassingly model arallelizable at test time, since the network evaluation for each subnetwork is comletely indeendent excet for the shared lower layer weights that can be dulicated over multile rocessors. We validate our method with two dee network models (ResNet and AlexNet) on two different datasets (CIFAR-100 and ILSVRC 2012) for image classification, on which our method obtains networks with significantly reduced number of arameters while achieving comarable or suerior classification accuracies over original full dee networks, and accelerated test seed with multile GPUs.
"
103,2017,Learning Deep Latent Gaussian Models with Markov Chain Monte Carlo,Oral/Poster,"Dee latent Gaussian models are owerful and oular robabilistic models of high-dimensional data. These models are almost always fit using variational exectation-maximization, an aroximation to true maximum-marginal-likelihood estimation.  In this aer, we roose a different aroach: rather than use a variational aroximation (which roduces biased gradient signals), we use Markov chain Monte Carlo (MCMC, which allows us to trade bias for comutation). We find that our MCMC-based aroach has several advantages: it yields higher held-out likelihoods, roduces sharer images, and
 does not suffer from the variational overruning effect. MCMC's additional comutational overhead roves to be 
significant, but not rohibitive.
"
104,2017,Doubly Greedy Primal-Dual Coordinate Descent for Sparse Empirical Risk Minimization,Oral/Poster,"We consider the oular roblem of sarse emirical risk minimization with linear redictors and a large number of both features and observations. With a convex-concave saddle oint objective reformulation, we roose a Doubly Greedy Primal-Dual Coordinate Descent algorithm that is able to exloit sarsity in both rimal and dual variables. It enjoys a low cost er iteration and our theoretical analysis shows that it converges linearly with a good iteration comlexity, rovided that the set of rimal variables is sarse. We then extend this algorithm further to leverage active sets. The resulting new algorithm is even faster, and exeriments on large-scale Multi-class data sets show that our algorithm achieves u to 30 times seedu on several state-of-the-art otimization methods.
"
105,2017,End-to-End Differentiable Adversarial Imitation Learning,Oral/Poster,"Generative Adversarial Networks (GANs) have been successfully alied to the roblem of \emh{olicy imitation} in a model-free setu. However, the comutation grah of GANs, that include a stochastic olicy as the generative model, is no longer differentiable end-to-end, which requires the use of high-variance gradient estimation. In this aer, we introduce the Model-based Generative Adversarial Imitation Learning (MGAIL) algorithm. We show how to use a forward model to make the comutation fully differentiable, which enables training olicies using the exact gradient of the discriminator. The resulting algorithm trains cometent olicies using relatively fewer exert samles and interactions with the environment. We test it on both discrete and continuous action domains and reort results that surass the state-of-the-art.
"
106,2017,Local-to-Global Bayesian Network Structure Learning,Oral/Poster,"We introduce a new local-to-global structure learning algorithm, called grah growing  structure learning (GGSL), to learn Bayesian network (BN) structures.  GGSL starts at a (random) node and then gradually exands the learned structure through a series of local learning stes. At each local learning ste, the roosed algorithm only needs to revisit a subset of the learned nodes, consisting of the local neighborhood of a target, and therefore imroves on both memory and time efficiency comared to traditional global structure learning aroaches. GGSL also imroves on the existing local-to-global learning aroaches by removing the need for conflict-resolving AND-rules, and achieves better learning accuracy. We rovide theoretical analysis for the local learning ste, and show  that GGSL outerforms existing algorithms on benchmark datasets. Overall, GGSL demonstrates a novel direction to scale u BN structure learning while limiting accuracy loss.
"
107,2017,Provably Optimal Algorithms for Generalized Linear Contextual Bandits,Oral/Poster,"Contextual bandits are widely used in Internet services from news recommendation to advertising, and to Web search. Generalized linear models (logistical regression in articular) have demonstrated stronger erformance than linear models in many alications where rewards are binary. However, most theoretical analyses on contextual bandits so far are on linear bandits. In this work, we roose an uer confidence bound based algorithm for generalized linear contextual bandits, which achieves an ~O(sqrt{dT}) regret over T rounds with d dimensional feature vectors.  This regret matches the minimax lower bound, u to logarithmic terms, and imroves on the best revious result by a sqrt{d} factor, assuming the number of arms is fixed. A key comonent in our analysis is to establish a new, shar finite-samle confidence bound for maximum likelihood estimates in generalized linear models, which may be of indeendent interest. We also analyze a simler uer confidence bound algorithm, which is useful in ractice, and rove it to have otimal regret for certain cases.
"
108,2017,No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis,Oral/Poster,"In this aer we develo a new framework that catures the common landscae underlying the common non-convex low-rank matrix roblems including matrix sensing, matrix comletion and robust PCA. In articular, we show for all above roblems (including asymmetric cases): 1) all local minima are also globally otimal; 2) no high-order saddle oints exists. These results exlain why simle algorithms such as stochastic gradient descent have global converge, and efficiently otimize these non-convex objective functions in ractice. Our framework connects and simlifies the existing analyses on otimization landscaes for matrix sensing and symmetric matrix comletion. The framework naturally leads to new results for asymmetric matrix comletion and robust PCA.
"
109,2017,On the Expressive Power of Deep Neural Networks,Oral/Poster,"We roose a new aroach to the roblem of neural network exressivity, which seeks to characterize how structural roerties of a neural network family affect the functions it is able to comute. Our aroach is based on an interrelated set of measures of exressivity, unified by the novel notion of trajectory length, which measures how the outut of a network changes as the inut swees along a one-dimensional ath. Our findings show that: (1) The comlexity of the comuted function grows exonentially with deth (2) All weights are not equal: trained networks are more sensitive to their lower (initial) layer weights (3) Trajectory regularization is a simler alternative to batch normalization, with the same erformance.
"
110,2017,Semi-Supervised Classification Based on Classification from Positive and Unlabeled Data,Oral/Poster,"Most of the semi-suervised classification methods develoed so far use unlabeled data for regularization uroses under articular distributional assumtions such as the cluster assumtion. In contrast, recently develoed methods of classification from ositive and unlabeled data (PU classification) use unlabeled data for risk evaluation, i.e., label information is directly extracted from unlabeled data. In this aer, we extend PU classification to also incororate negative data and roose a novel semi-suervised learning aroach. We establish generalization error bounds for our novel methods and show that the bounds decrease with resect to the number of unlabeled data without the distributional assumtions that are required in existing semi-suervised learning methods. Through exeriments, we demonstrate the usefulness of the roosed methods.
"
111,2017,Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,Oral/Poster,"We roose an algorithm for meta-learning that is model-agnostic, in the sense that it is comatible with any model trained with gradient descent and alicable to a variety of different learning roblems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samles. In our aroach, the arameters of the model are exlicitly trained such that a small number of gradient stes with a small amount of training data from a new task will roduce good generalization erformance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this aroach leads to state-of-the-art erformance on two few-shot image classification benchmarks, roduces good results on few-shot regression, and accelerates fine-tuning for olicy gradient reinforcement learning with neural network olicies.
"
112,2017,Zero-Inflated Exponential Family Embeddings,Oral/Poster,"Word embeddings are a widely-used tool to analyze language, and exonential family embeddings (Rudolh et al., 2016) generalize the technique to other tyes of data. One challenge to fitting embedding methods is sarse data, such as a documentterm matrix that contains many zeros. To address this issue, ractitioners tyically downweight or subsamle the zeros, thus focusing learning on the non-zero entries. In this aer, we develo zero-inflated embeddings, a new embedding method that is designed to learn from sarse observations. In a zero-inflated embedding (ZIE), a zero in the data can come from an interaction to other data (i.e., an embedding) or from a searate rocess by which many observations are equal to zero (i.e. a robability mass at zero). Fitting a ZIE naturally downweights the zeros and damens their influence on the model. Across many tyes of data---language, movie ratings, shoing histories, and bird watching logs---we found that zero-inflated embeddings rovide imroved redictive erformance over standard aroaches and find better vector reresentation of items.
"
113,2017,A Richer Theory of Convex Constrained Optimization with Reduced Projections and Improved Rates,Oral/Poster,"This aer focuses on  convex constrained otimization roblems, where the solution is subject to a convex inequality constraint. In articular, we aim at challenging roblems for which both rojection into the constrained domain and a linear otimization under the inequality constraint are  time-consuming, which render both rojected gradient methods and conditional gradient methods (a.k.a. the Frank-Wolfe algorithm) exensive. In this aer, we develo rojection reduced otimization algorithms for both smooth and non-smooth otimization with imroved convergence rates under a certain regularity condition of the constraint function. We first resent a general theory of otimization with only one rojection. Its alication to smooth otimization with only one rojection yields $O(1\esilon)$ iteration comlexity, which  imroves over the $O(1\esilon^2)$ iteration comlexity established before for non-smooth otimization  and can be further reduced under strong convexity. Then we introduce a local error bound condition and develo faster algorithms for non-strongly convex otimization at the rice of a logarithmic number of rojections. In articular, we achieve  an iteration comlexity  of $\widetilde O(1\esilon^{2(1-\theta)})$ for non-smooth otimization and $\widetilde O(1\esilon^{1-\theta})$ for smooth otimization, where $\theta\in(0,1]$ aearing the local error bound condition characterizes the functional local growth rate around the otimal solutions. Novel alications in solving the constrained $\ell_1$ minimization roblem  and a ositive semi-definite constrained distance metric learning roblem demonstrate that the roosed algorithms achieve significant seed-u comared with revious algorithms. "
114,2017,Learning in POMDPs with Monte Carlo Tree Search,Oral/Poster,"The POMDP is a owerful framework for reasoning under outcome and information uncertainty, but constructing an accurate POMDP model is difficult. Bayes-Adative Partially Observable Markov Decision Processes (BA-POMDPs) extend POMDPs to allow the model to be learned during execution. BA-POMDPs are a Bayesian RL aroach that, in rincile, allows for an otimal trade-off between exloitation and exloration. Unfortunately, BA-POMDPs are currently imractical to solve for any non-trivial domain. In this aer, we extend the Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting method, which we call BA-POMCP, is able to tackle roblems that revious solution methods have been unable to solve. Additionally, we introduce several
techniques that exloit the BA-POMDP structure to imrove the efficiency of BA-POMCP along with roof of their convergence.
"
115,2017,Composing Tree Graphical Models with Persistent Homology Features for Clustering Mixed-Type Data,Oral/Poster,"Clustering data with both continuous and discrete attributes is a challenging task. Existing methods lack a rinciled robabilistic formulation. In this aer, we roose a clustering method based on a tree-structured grahical model to describe the generation rocess of mixed-tye data. Our tree-structured model factorized into a roduct of airwise interactions, and thus localizes the interaction between feature variables of different tyes. To rovide a robust clustering method based on the tree-model, we adot a toograhical view and comute eaks of the density function and their attractive basins for clustering. Furthermore, we leverage the theory from toology data analysis to adatively merge trivial eaks into large ones in order to achieve meaningful clusterings. Our method outerforms state-of-the-art methods on mixed-tye data.
"
116,2017,Safety-Aware Algorithms for Adversarial Contextual Bandit,Oral/Poster,"In this work we study the safe sequential decision making roblem under the setting of adversarial contextual bandits with sequential risk constraints. At each round, nature reares a context, a cost for each arm, and additionally a risk for each arm. The learner leverages the context to ull an arm and receives the corresonding cost and risk associated with the ulled arm. In addition to minimizing the cumulative cost, for safety uroses, the learner needs to make safe decisions such that the average of the cumulative risk from all ulled arms should not be larger than a re-defined threshold. To address this roblem, we first study online convex rogramming in the full information setting where in each round the learner receives an adversarial convex loss and a convex constraint. We develo a meta algorithm leveraging online mirror descent for the full information setting and then extend it to contextual bandit with sequential risk constraints setting using exert advice. Our algorithms can achieve near-otimal regret in terms of minimizing the total cost, while successfully maintaining a sub- linear growth of accumulative risk constraint violation. We suort our theoretical results by demonstrating our algorithm on a simle simulated robotics reactive control task.
"
117,2017,"Coherence Pursuit: Fast, Simple, and Robust Subspace Recovery",Oral/Poster,"This aer resents a remarkably simle, yet owerful, algorithm for robust Princial Comonent Analysis (PCA). In the roosed aroach, an outlier is set aart from an inlier by comaring their coherence with the rest of the data oints. As inliers lie on a low dimensional subsace, they are likely to have strong mutual coherence rovided there are enough inliers. By contrast, outliers do not tyically admit low dimensional structures, wherefore an outlier is unlikely to bear strong resemblance with a large number of data oints. The mutual coherences are comuted by forming the Gram matrix of normalized data oints. Subsequently, the subsace is recovered from the san of a small subset of the data oints that exhibit strong coherence with the rest of the data. As coherence ursuit only involves one simle matrix multilication, it is significantly faster than the state of-the-art robust PCA algorithms. We rovide a mathematical analysis of the roosed algorithm under a random model for the distribution of the inliers and outliers. It is shown that the roosed method can recover the correct subsace even if the data is redominantly outliers. To the best of our knowledge, this is the first rovable robust PCA algorithm that is simultaneously non-iterative, can tolerate a large number of outliers and is robust to linearly deendent outliers.
"
118,2017,Depth-Width Tradeoffs in Approximating Natural Functions With Neural Networks,Oral/Poster,"We rovide several new deth-based searation results for feed-forward neural networks, roving that various tyes of simle and natural functions can be better aroximated using deeer networks than shallower ones, even if the shallower networks are much larger. This includes indicators of balls and ellises; non-linear functions which are radial with resect to the $L_1$ norm; and smooth non-linear functions. We also show that these gas can be observed exerimentally: Increasing the deth indeed allows better learning than increasing width, when training neural networks to learn an indicator of a unit ball."
119,2017,Iterative Machine Teaching,Oral/Poster,"In this aer, we consider the roblem of machine teaching, the inverse roblem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new aradigm where the learner uses an iterative algorithm and a teacher can feed examles sequentially and intelligently based on the current erformance of the learner. We show that the teaching comlexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Deending on the level of information the teacher has from the learner model, we design teaching algorithms which can rovably reduce the number of teaching examles and achieve faster convergence than learning without teachers. We also validate our theoretical findings with extensive exeriments on different data distribution and real image datasets.
"
120,2017,AdaNet: Adaptive Structural Learning of Artificial Neural Networks,Oral/Poster,"We resent a new framework for analyzing and learning artificial neural networks. Our aroach simultaneously and adatively learns both the structure of the network as well as its weights. The methodology is based uon and accomanied by strong data-deendent theoretical learning guarantees, so that the final network architecture rovably adats to the comlexity of any given roblem.
"
121,2017,Convex Phase Retrieval without Lifting via PhaseMax,Oral/Poster,"Semidefinite relaxation methods transform a variety of non-convex otimization roblems into convex roblems, but square the number of variables.  We study a new tye of convex relaxation for hase retrieval roblems, called PhaseMax, that convexifies the underlying roblem without lifting. The resulting roblem formulation can be solved using standard convex otimization routines, while still working in the original, low-dimensional variable sace.  We rove, using a random sherical distribution measurement model, that PhaseMax succeeds with high robability for a sufficiently large number of measurements.  We comare our aroach to other hase retrieval methods and demonstrate that our theory accurately redicts the success of PhaseMax.  
"
122,2017,DARLA: Improving Zero-Shot Transfer in Reinforcement Learning,Oral/Poster,"Domain adatation is an imortant oen roblem in dee reinforcement learning (RL). In many scenarios of interest data is hard to obtain, so agents may learn a source olicy in a setting where data is readily available, with the hoe that it generalises well to the target domain. We roose a new multi-stage RL agent, DARLA (DisentAngled Reresentation Learning Agent), which learns to see before learning to act. DARLA’s vision is based on learning a disentangled reresentation of the observed environment. Once DARLA can see, it is able to acquire source olicies that are robust to many domain shifts - even with no access to the target domain. DARLA significantly outerforms conventional baselines in zero-shot domain adatation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeeMind Lab) and base RL algorithms (DQN, A3C and EC).
"
123,2017,On Relaxing Determinism in Arithmetic Circuits,Oral/Poster,"The ast decade has seen a significant interest in learning tractable robabilistic reresentations. Arithmetic circuits (ACs) were among the first roosed tractable reresentations, with some subsequent reresentations being instances of ACs with weaker or stronger roerties.  In this aer, we rovide a formal basis under which variants on ACs can be comared, and where the recise roles and semantics of their various roerties can be made more transarent.  This allows us to lace some recent develoments on ACs in a clearer ersective and to also derive new results for ACs. This includes an exonential searation between ACs with and without determinism; comleteness and incomleteness results; and tractability results (or lack thereof) when comuting most robable exlanations (MPEs).
"
124,2017,Adaptive Multiple-Arm Identification,Oral/Poster,"We study the roblem of selecting K arms with the highest exected rewards in a stochastic n-armed bandit game. This roblem has a wide range of alications, e.g., AB testing, crowdsourcing, simulation otimization. Our goal is to develo a PAC algorithm, which, with robability at least 1-\delta, identifies a set of K arms with the aggregate regret at most \esilon. The notion of aggregate regret for multile-arm identification was first introduced in Zhou et. al. (2014), which is defined as the difference of the averaged exected rewards between the selected set of arms and the best K arms. In contrast to Zhou et. al. (2014) that only rovides instance-indeendent samle comlexity, we introduce a new hardness arameter for characterizing the difficulty of any given instance. We further develo two algorithms and establish the corresonding samle comlexity in terms of this hardness arameter. The derived samle comlexity can be significantly smaller than state-of-the-art results for a large class of instances and matches the instance-indeendent lower bound uto a \log(\esilon^{-1}) factor in the worst case. We also rove a lower bound result showing that the extra \log(\esilon^{-1}) is necessary for instance-deendent algorithms using the introduced hardness arameter.
"
125,2017,Tensor Decomposition with Smoothness,Oral/Poster,"Real data tensors are usually high dimensional but their intrinsic information is reserved in low-dimensional sace, which motivates to use tensor decomositions such as Tucker decomosition. Often, real data tensors are not only low dimensional, but also smooth, meaning that the adjacent elements are similar or continuously changing, which tyically aear as satial or temoral data. To incororate the smoothness roerty, we roose the smoothed Tucker decomosition (STD). STD leverages the smoothness by the sum of a few basis functions, which reduces the number of arameters. The objective function is formulated as a convex roblem and, to solve that, an algorithm based on the alternating direction method of multiliers is derived. We theoretically show that, under the smoothness assumtion, STD achieves a better error bound. The theoretical result and erformances of STD are numerically verified.
"
126,2017,Automated Curriculum Learning for Neural Networks,Oral/Poster,"We introduce a method for automatically selecting the ath, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency.
A measure of the amount that the network learns from each data samle is rovided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus.
We consider a range of signals derived from two distinct indicators of learning rogress: rate of increase in rediction accuracy, and rate of increase in network comlexity.
Exerimental results for LSTM networks on three curricula demonstrate that our aroach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory erformance level.
"
127,2017,Attentive Recurrent Comparators,Oral/Poster,"Raid learning requires flexible reresentations to quickly adot to new evidence. We develo a novel class of models called Attentive Recurrent Comarators (ARCs) that form reresentations of objects by cycling through them and making observations. Using the reresentations extracted by ARCs, we develo a way of aroximating a \textit{dynamic reresentation sace} and use it for one-shot learning. In the task of one-shot classification on the Omniglot dataset, we achieve the state of the art erformance with an error rate of 1.5\%. This reresents the first suer-human result achieved for this task with a generic model that uses only ixel information.
"
128,2017,An Infinite Hidden Markov Model With Similarity-Biased Transitions,Oral/Poster,"We describe a generalization of the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) which is able to encode rior information that state transitions are more likely between ""nearby"" states.  This is accomlished by defining a similarity function on the state sace and scaling transition robabilities by airwise similarities, thereby inducing correlations among the transition distributions. We resent an augmented data reresentation of the model as a Markov Jum Process in which: (1) some jum attemts fail, and (2) the robability of success is roortional to the similarity between the source and destination states.  This augmentation restores conditional conjugacy and admits a simle Gibbs samler. We evaluate the model and inference method on a seaker diarization task and a ""harmonic arsing"" task using four-art chorale data, as well as on several synthetic datasets, achieving favorable comarisons to existing models.
"
129,2017,Efficient Nonmyopic Active Search,Oral/Poster,"Active search is an active learning setting with the goal of identifying as many members of a given class as ossible under a labeling budget. In this work, we first establish a theoretical hardness of active search, roving that no olynomial-time olicy can achieve a constant factor aroximation ratio with resect to the exected utility of the otimal olicy. We also roose a novel, comutationally efficient active search olicy achieving excetional erformance on several real-world tasks. Our olicy is nonmyoic, always considering the entire remaining search budget. It also automatically and dynamically balances exloration and exloitation consistent with the remaining budget, without relying on a arameter to control this tradeoff. We conduct exeriments on diverse datasets from several domains: drug discovery, materials science, and a citation network. Our efficient nonmyoic olicy recovers significantly more valuable oints with the same budget than several alternatives from the literature, including myoic aroximations to the otimal olicy.
"
130,2017,Asymmetric Tri-training for Unsupervised Domain Adaptation,Oral/Poster,"It is imortant to aly models trained on a large number of labeled samles to different domains because collecting many labeled samles in various domains is exensive. To learn discriminative reresentations for the target domain, we assume that artificially labeling the target samles can result in a good reresentation. Tri-training leverages three classifiers equally to rovide seudo-labels to unlabeled samles; however, the method does not assume labeling samles generated from a different domain. In this aer, we roose the use of an \textit{asymmetric} tri-training method for unsuervised domain adatation, where we assign seudo-labels to unlabeled samles and train the neural networks as if they are true labels. In our work, we use three networks \textit{asymmetrically}, and by \textit{asymmetric}, we mean that two networks are used to label unlabeled target samles, and one network is trained by the seudo-labeled samles to obtain target-discriminative reresentations. Our roosed method was shown to achieve a state-of-the-art erformance on the benchmark digit recognition datasets for domain adatation.
"
131,2017,State-Frequency Memory Recurrent Neural Networks,Oral/Poster,"Modeling temoral sequences lays a fundamental role in various modern alications and has drawn more and more attentions in the machine learning community. Among those efforts on imroving the caability to reresent temoral data, the Long Short-Term Memory (LSTM) has achieved great success in many areas. Although the LSTM can cature long-range deendency in the time domain, it does not exlicitly model the attern occurrences in the frequency domain that lays an imortant role in tracking and redicting data oints over various time cycles. We roose the State-Frequency Memory (SFM), a novel recurrent architecture that allows to searate dynamic atterns across different frequency comonents and their imacts on modeling the temoral contexts of inut sequences. By jointly decomosing memorized dynamics into state-frequency comonents, the SFM is able to offer a fine-grained analysis of temoral sequences by caturing the deendency of uncovered atterns in both time and frequency domains. Evaluations on several temoral modeling tasks demonstrate the SFM can yield cometitive erformances, in articular as comared with the state-of-the-art LSTM models.
"
132,2017,Batched High-dimensional Bayesian Optimization via Structural Kernel Learning,Oral/Poster,"Otimization of high-dimensional black-box functions is an extremely challenging roblem. While Bayesian otimization has emerged as a oular aroach for otimizing black-box functions, its alicability has been limited to low-dimensional roblems due to its comutational and statistical challenges arising from high-dimensional settings. In this aer, we roose to tackle these challenges by (1) assuming a latent additive structure in the function and inferring it roerly for more efficient and effective BO, and (2) erforming multile evaluations in arallel to reduce the number of iterations required by the method. Our novel aroach learns the latent structure with Gibbs samling and constructs batched queries using determinantal oint rocesses. Exerimental validations on both synthetic and real-world functions demonstrate that the roosed method outerforms the existing state-of-the-art aroaches.
"
133,2017,Leveraging Union of Subspace Structure to Improve Constrained Clustering,Oral/Poster,"Many clustering roblems in comuter vision and other contexts are also classification roblems, where each cluster shares a meaningful label. Subsace clustering algorithms in articular are often alied to roblems that fit this descrition, for examle with face images or handwritten digits. While it is straightforward to request human inut on these datasets, our goal is to reduce this inut as much as ossible. We resent a airwise-constrained clustering algorithm that actively selects queries based on the union-of-subsaces model. The central ste of the algorithm is in querying oints of minimum margin between estimated subsaces; analogous to classifier margin, these lie near the decision boundary. We rove that oints lying near the intersection of subsaces are oints with low margin. Our rocedure can be used after any subsace clustering algorithm that oututs an affinity matrix. We demonstrate on several datasets that our algorithm drives the clustering error down considerably faster than the state-of-the-art active query algorithms on datasets with subsace structure and is cometitive on other datasets. 
"
134,2017,Source-Target Similarity Modelings for Multi-Source Transfer Gaussian Process Regression,Oral/Poster,"A key challenge in multi-source transfer learning is to cature the diverse inter-domain similarities. In this aer, we study different aroaches based on Gaussian rocess models to solve the multi-source transfer regression roblem. Precisely, we first investigate the feasibility and erformance of a family of transfer covariance functions that reresent the airwise similarity of each source and the target domain. We theoretically show that using such a transfer covariance function for general Gaussian rocess modelling can only cature the same similarity coefficient for all the sources, and thus may result in unsatisfactory transfer erformance.
This leads us to roose \emh{TC$_{MS}$Stack}, an integrated strategy incororating the benefits of the transfer covariance function and stacking. Extensive exeriments on one synthetic and two real-world datasets, with learning settings of u to 11 sources for the latter, demonstrate the effectiveness of our roosed \emh{TC$_{MS}$Stack}."
135,2017,Delta Networks for Optimized Recurrent Network Computation,Oral/Poster,"Many neural networks exhibit stability in their activation atterns over time in resonse to inuts from sensors oerating under real-world conditions. By caitalizing on this roerty of natural signals, we roose a Recurrent Neural Network (RNN) architecture called a delta network in which each neuron transmits its value only when the change in its activation exceeds a threshold. The execution of RNNs as delta networks is attractive because their states must be stored and fetched at every timeste, unlike in convolutional neural networks (CNNs). We show that a naive run-time delta network imlementation offers modest imrovements on the number of memory accesses and comutes, but otimized training techniques confer higher accuracy at higher seedu. With these otimizations, we demonstrate a 9X reduction in cost with negligible loss of accuracy for the TIDIGITS audio digit recognition benchmark.  Similarly, on the large Wall Street Journal (WSJ) seech recognition benchmark, retrained networks can also be greatly accelerated as delta networks and trained delta networks show a 5.7x imrovement with negligible loss of accuracy. Finally, on an end-to-end CNN-RNN network trained for steering angle rediction in a driving dataset, the RNN cost can be reduced by a substantial 100X.
"
136,2017,From Patches to Images: A Nonparametric Generative Model,Oral/Poster,"We roose a hierarchical generative model that catures the self-similar structure of image regions as well as how this structure is shared across image collections. Our model is based on a novel, variational interretation of the oular exected atch log-likelihood (EPLL) method as a model for randomly ositioned grids of image atches. While revious EPLL methods modeled image atches with finite Gaussian mixtures, we use nonarametric Dirichlet rocess (DP) mixtures to create models whose comlexity grows as additional images are observed. An extension based on the hierarchical DP then catures reetitive and self-similar structure via image-secific variations in cluster frequencies. We derive a structured variational inference algorithm that adatively creates new atch clusters to more accurately model novel image textures. Our denoising erformance on standard benchmarks is suerior to EPLL and comarable to the state-of-the-art, and rovides novel statistical justifications for common image rocessing heuristics. We also show accurate image inainting results.
"
137,2017,Active Heteroscedastic Regression,Oral/Poster,"An active learner is given a model class $\Theta$, a large samle of unlabeled data drawn from an underlying distribution and access to a labeling oracle which can rovide a label for any of the unlabeled instances. The goal of the learner is to find a model $\theta \in \Theta$ that fits the data to a given accuracy while making as few label queries to the oracle as ossible. In this work, we consider a theoretical analysis of the label requirement of active learning for regression under a heteroscedastic noise model.

Previous work has looked at active regression either with no model mismatch~\cite{chaudhuri2015convergence} or with arbitrary model mismatch~\cite{sabato2014active}. In the first case, active learning rovided no imrovement even in the simle case where the unlabeled examles were drawn from Gaussians. In the second case, under arbitrary model mismatch, the algorithm either required a very high running time or a large number of labels. We rovide bounds on the convergence rates of active and assive learning for heteroscedastic regression, where the noise deends on the instance. Our results illustrate that just like in binary classification, some artial knowledge of the nature of the noise can lead to significant gains in the label requirement of active learning. "
138,2017,Multi-task Learning with Labeled and Unlabeled Tasks,Oral/Poster,"In multi-task learning, a learner is given a collection of rediction tasks and needs to solve all of them.  In contrast to revious work, which required that annotated training data must be available for all tasks, we consider a new setting, in which for some tasks, otentially most of them, only unlabeled training data is rovided. Consequently, to solve all tasks, information must be transferred between tasks with labels and tasks without labels.  Focusing on an instance-based transfer method we analyze two variants of this setting: when the set of labeled tasks is fixed, and when it can be actively selected by the learner. We state and rove a generalization bound that covers both scenarios and derive from it an algorithm for making the choice of labeled tasks (in the active case) and for transferring information between the tasks in a rinciled way. We also illustrate the effectiveness of the algorithm on synthetic and real data.
"
139,2017,Recurrent Highway Networks,Oral/Poster,"Many sequential rocessing tasks require comlex nonlinear transition functions from one ste to the next. However, recurrent neural networks with ""dee"" transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin’s circle theorem that illuminates several modeling and otimization issues and imroves our understanding of the LSTM cell. Based on this analysis we roose Recurrent Highway Networks, which extend the LSTM architecture to allow ste-to-ste transition deths larger than one. Several language modeling exeriments demonstrate that the roosed architecture results in owerful and efficient models. On the Penn Treebank corus, solely increasing the transition deth from 1 to 10 imroves word-level erlexity from 90.6 to 65.4 using the same number of arameters. On the larger Wikiedia datasets for character rediction (text8 and enwik8), RHNs outerform all revious results and achieve an entroy of 1.27 bits er character.
"
140,2017,Fast Bayesian Intensity Estimation for the Permanental Process,Oral/Poster,"The Cox rocess is a stochastic rocess which generalises the Poisson rocess by letting the underlying intensity function itself be a stochastic rocess. In this aer we resent a fast Bayesian inference scheme for the ermanental rocess, a Cox rocess under which the square root of the intensity is a Gaussian rocess. In articular we exloit connections with reroducing kernel Hilbert saces, to derive efficient aroximate Bayesian inference algorithms based on the Lalace aroximation to the redictive distribution and marginal likelihood. We obtain a simle algorithm which we aly to toy and real-world roblems, obtaining orders of magnitude seed imrovements over revious work.
"
141,2017,Active Learning for Cost-Sensitive Classification,Oral/Poster,"We design an active learning algorithm for cost-sensitive multiclass classification: roblems where different errors have different costs. Our algorithm, COAL, makes redictions by regressing to each label’s cost and redicting the smallest. On a new examle, it uses a set of regressors that erform well on ast data to estimate ossible costs for each label. It queries only the labels that could be the best, ignoring the sure losers. We rove COAL can be efficiently imlemented for any regression family that admits squared loss otimization; it also enjoys strong guarantees with resect to redictive erformance and labeling effort. Our exeriment with COAL show significant imrovements in labeling effort and test cost over assive and active baselines.
"
142,2017,Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics,Oral/Poster,"The recent adatation of dee neural network-based methods to reinforcement learning and lanning domains has yielded remarkable rogress on individual tasks. Nonetheless, rogress on task-to-task transfer remains limited. In ursuit of efficient and robust generalization, we introduce the Schema Network, an object-oriented generative hysics simulator caable of disentangling multile causes of events and reasoning backward through causes to achieve goals. The richly structured architecture of the Schema Network can learn the dynamics of an environment directly from data. We comare Schema Networks with Asynchronous Advantage Actor-Critic and Progressive Networks on a suite of Breakout variations, reorting results on training efficiency and zero-shot generalization, consistently demonstrating faster, more robust learning and better transfer. We argue that generalizing from limited data and learning causal relationshis are essential abilities on the ath toward generally intelligent systems.
"
143,2017,A Birth-Death Process for Feature Allocation,Oral/Poster,"We roose a Bayesian nonarametric rior over feature allocations 
for sequential data, the birth-death feature allocation rocess (BDFP). The BDFP models 
the evolution of the feature allocation of a set of N objects across a covariate (e.g.~time) 
by creating and deleting features. A BDFP is exchangeable, rojective, stationary and reversible, 
and its equilibrium distribution is given by the Indian buffet rocess (IBP). We show that the Beta rocess 
on an extended sace is the de Finetti mixing distribution underlying the BDFP. Finally, we resent the finite aroximation of the BDFP, the Beta Event Process (BEP), that ermits simlified inference. The utility of the BDFP as a rior is demonstrated on real world dynamic genomics and social network data. 
"
144,2017,Diameter-Based Active Learning,Oral/Poster,"To date, the tightest uer and lower-bounds for the active learning of general concet classes have been in terms of a arameter of the learning roblem called the slitting index. We rovide, for the first time, an efficient algorithm that is able to realize this uer bound, and we emirically demonstrate its good erformance.
"
145,2017,Risk Bounds for Transferring Representations With and Without Fine-Tuning,Oral/Poster,"A oular machine learning strategy is the transfer of a reresentation (i.e. a feature extraction function) learned on a source task to a target task. Examles include the re-use of neural network weights or word embeddings. We develo sufficient conditions for the success of this aroach. If the reresentation learned from the source task is fixed, we identify conditions on how the tasks relate to obtain an uer bound on target task risk via a VC dimension-based argument. We then consider using the reresentation from the source task to construct a rior, which is fine-tuned using target task data. We give a PAC-Bayes target task risk bound in this setting under suitable conditions. We show examles of our bounds using feedforward neural networks. Our results motivate a ractical aroach to weight transfer, which we validate with exeriments.
"
146,2017,The loss surface of deep and wide neural networks,Oral/Poster,"While the otimization roblem behind dee neural networks is highly non-convex, it is frequently observed in ractice that training dee networks seems ossible without getting stuck in subotimal oints. It has been argued that this is the case as all local minima are close to being globally otimal. We show that this is (almost) true, in fact almost all local minima are globally otimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training oints and the network structure from this layer on is yramidal.
"
147,2017,Neural Taylor Approximations: Convergence and Exploration in Rectifier Networks,Oral/Poster,"Modern convolutional networks, incororating rectifiers and max-ooling, are neither smooth nor convex; standard guarantees therefore do not aly. Nevertheless, methods from convex otimization such as gradient descent and Adam are widely used as building blocks for dee learning algorithms. This aer rovides the first convergence guarantee alicable to modern convnets, which furthermore matches a lower bound for convex nonsmooth functions. The key technical tool is the neural Taylor aroximation -- a straightforward alication of Taylor exansions to neural networks -- and the associated Taylor loss. Exeriments on a range of otimizers, layers, and tasks rovide evidence that the analysis accurately catures the dynamics of neural otimization. The second half of the aer alies the Taylor aroximation to isolate the main difficulty in training rectifier nets -- that gradients are shattered --  and investigates the hyothesis that, by exloring the sace of activation configurations more thoroughly, adative otimizers such as RMSPro and Adam are able to converge to better solutions.
"
148,2017,Sharp Minima Can Generalize For Deep Nets,Oral/Poster,"Desite their overwhelming caacity to overfit, dee learning architectures tend to generalize relatively well to unseen data, allowing them to be deloyed in ractice. However, exlaining why this is the case is still an oen area of research. One standing hyothesis that is gaining oularity, e.g. \citet{hochreiter1997flat, keskar2016large}, is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This aer argues that most notions of flatness are roblematic for dee models and can not be directly alied to exlain generalization.  Secifically, when focusing on dee networks with rectifier units, we can exloit the articular geometry of arameter sace induced by the inherent symmetries that these architectures exhibit to build equivalent models corresonding to arbitrarily sharer minima. Or, deending on the definition of flatness, it is the same for any given minimum. Furthermore, if we allow to rearametrize a function, the geometry of its arameters can change drastically without affecting its generalization roerties.
"
149,2017,Geometry of Neural Network Loss Surfaces via Random Matrix Theory,Oral/Poster,"Understanding the geometry of neural network loss surfaces is imortant for the develoment of imroved otimization algorithms and for building a theoretical understanding of why dee learning works. In this aer, we study the geometry in terms of the distribution of eigenvalues of the Hessian matrix at critical oints of varying energy. We introduce an analytical framework and a set of tools from random matrix theory that allow us to comute an aroximation of this distribution under a set of simlifying assumtions. The shae of the sectrum deends strongly on the energy and another key arameter, $\hi$, which measures the ratio of arameters to data oints. Our analysis redicts and numerical simulations suort that for critical oints of small index, the number of negative eigenvalues scales like the 32 ower of the energy. We leave as an oen roblem an exlanation for our observation that, in the context of a certain memorization task, the energy of minimizers is well-aroximated by the function $12(1-\hi)^2$."
150,2017,"The Shattered Gradients Problem: If resnets are the answer, then what is the question?",Oral/Poster,"A long-standing obstacle to rogress in dee learning is the roblem of vanishing and exloding gradients. Although, the roblem has largely been overcome via carefully constructed initializations and batch normalization, architectures incororating ski-connections such as highway and resnets erform much better than standard feedforward architectures desite well-chosen initialization and batch normalization. In this aer, we identify the shattered gradients roblem. Secifically, we show that the correlation between gradients in standard feedforward networks decays exonentially with deth resulting in gradients that resemble white noise whereas, in contrast, the gradients in architectures with ski-connections are far more resistant to shattering, decaying sublinearly. Detailed emirical evidence is resented in suort of the analysis, on both fully-connected networks and convnets. Finally, we resent a new ``looks linear'' (LL) initialization that revents shattering, with reliminary exeriments showing the new initialization allows to train very dee networks without the addition of ski-connections.
"
151,2017,Learning to Learn without Gradient Descent by Gradient Descent,Oral/Poster,"We learn recurrent neural network otimizers trained on simle synthetic functions by gradient descent. We show that these learned otimizers exhibit a remarkable degree of transfer in that they can be used to efficiently otimize a broad range of derivative-free black-box functions, including Gaussian rocess bandits, simle control objectives, global otimization benchmarks and hyer-arameter tuning tasks.  U to the training horizon, the learned otimizers learn to trade-off exloration and exloitation, and comare favourably with heavily engineered Bayesian otimization ackages for hyer-arameter tuning.
"
152,2017,"A Semismooth Newton Method for Fast, Generic Convex Programming",Oral/Poster,"We introduce Newton-ADMM, a method for fast
conic otimization.  The basic idea is to view the residuals of consecutive iterates generated by the
alternating direction method of multiliers (ADMM) as a set of fixed oint
equations, and then use a nonsmooth Newton method to find a solution; we aly the basic idea to the Slitting Cone Solver (SCS), a state-of-the-art method for solving generic conic otimization
roblems.  We demonstrate theoretically, by extending the theory of 
semismooth oerators, that Newton-ADMM converges raidly (i.e.,
quadratically) to a solution; emirically, Newton-ADMM is significantly faster
than SCS on a number of roblems.  The method also has essentially no tuning arameters, generates certificates of rimal or dual infeasibility, when aroriate, and can be secialized to solve secific convex roblems. 
"
153,2017,Unifying task specification in reinforcement learning,Oral/Poster,"Reinforcement learning tasks are tyically secified as Markov decision rocesses. This formalism has been highly successful, though secifications often coule the dynamics of the environment and the learning objective. This lack of modularity can comlicate generalization of the task secification, as well as obfuscate connections between different task settings, such as eisodic and continuing. In this work, we introduce the RL task formalism, that rovides a unification through simle constructs including a generalization to transition-based discounting. Through a series of examles, we demonstrate the generality and utility of this formalism. Finally, we extend standard learning constructs, including Bellman oerators, and extend some seminal theoretical results, including aroximation errors bounds. Overall, we rovide a well-understood and sound formalism on which to build theoretical results and simlify algorithm use and develoment.
"
154,2017,Efficient Online Bandit Multiclass Learning with O(sqrt{T}) Regret,Oral/Poster,"We resent an efficient second-order algorithm with tilde{O}(1eta sqrt{T}) regret for the bandit online multiclass roblem.  The regret bound holds simultaneously with resect to a family of loss functions arameterized by eta, ranging from hinge loss (eta=0) to squared hinge loss (eta=1).  This rovides a solution to the oen roblem of (Abernethy, J. and Rakhlin, A. An efficient bandit algorithm for sqrt{T}-regret in online multiclass rediction? In COLT, 2009).  We test our algorithm exerimentally, showing that it erforms favorably against earlier algorithms.
"
155,2017,Orthogonalized ALS: A Theoretically Principled Tensor Decomposition Algorithm for Practical Use,Oral/Poster,"The oular Alternating Least Squares (ALS) algorithm for tensor decomosition is efficient and easy to imlement, but often converges to oor local otima---articularly when the weights of the factors are non-uniform. We roose a modification of the ALS aroach that is as efficient as standard ALS, but rovably recovers the true factors with random initialization under standard incoherence assumtions on the factors of the tensor. We demonstrate the significant ractical sueriority of our aroach over traditional ALS for a variety of tasks on synthetic data---including tensor factorization on exact, noisy and over-comlete tensors, as well as tensor comletion---and for comuting word embeddings from a third-order word tri-occurrence tensor.
"
156,2017,Learned Optimizers that Scale and Generalize,Oral/Poster,"Learning to learn has emerged as an imortant direction for achieving artificial intelligence. Two of the rimary barriers to its adotion are an inability to scale to larger roblems and a limited ability to generalize to new tasks. We introduce a learned gradient descent otimizer that generalizes well to new tasks, and which has significantly reduced memory and comutation overhead. We achieve this by introducing a novel hierarchical RNN architecture, with minimal er-arameter overhead, augmented with additional architectural features that mirror the known structure of otimization tasks. We also develo a meta-training ensemble of small, diverse, otimization tasks caturing common roerties of loss landscaes. The otimizer learns to outerform RMSProADAM on roblems in this corus. More imortantly, it erforms comarably or better when alied to small convolutional neural networks, desite seeing no neural networks in its meta-training set. Finally, it generalizes to train Incetion V3 and ResNet V2 architectures on the ImageNet dataset for thousands of stes, otimization roblems that are of a vastly different scale than those it was trained on.
"
157,2017,Approximate Newton Methods and Their Local Convergence,Oral/Poster,"Many machine learning models are reformulated as otimization roblems. Thus, it is imortant to solve a large-scale otimization roblem in big data alications. 
Recently, subsamled Newton methods have emerged to attract much attention for otimization
due to their efficiency at
each iteration, rectified a weakness in the ordinary Newton method of suffering a high cost in each
iteration while commanding a high convergence rate. Other efficient stochastic second order methods are also roosed. However, the convergence roerties of these methods are still not well understood. There are also several imortant gas between the current convergence theory and erformance in real alications. In this aer, we aim to fill these gas. We roose a unifying framework to analyze local convergence roerties of second order methods. Based on this framework, our theoretical analysis matches the erformance in real alications.
"
158,2017,A Distributional Perspective on Reinforcement Learning,Oral/Poster,"In this aer we argue for the fundamental imortance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common aroach to reinforcement learning which models the exectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a secific urose such as imlementing risk-aware behaviour. We begin with theoretical results in both the olicy evaluation and control settings, exosing a significant distributional instability in the latter. We then use the distributional ersective to design a new algorithm which alies Bellman's equation to the learning of aroximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the imortance of the value distribution in aroximate reinforcement learning. Finally, we combine theoretical and emirical evidence to highlight the ways in which the value distribution imacts learning in the aroximate setting.
"
159,2017,Active Learning for Accurate Estimation of Linear Models,Oral/Poster,"We exlore the sequential decision making roblem where the goal is to estimate uniformly well a number of linear models, given a shared budget of random contexts indeendently samled from a known distribution. The decision maker must query one of the linear models for each incoming context, and receives an observation corruted by noise levels that are unknown, and deend on the model instance. We resent Trace-UCB, an adative allocation algorithm that learns the noise levels while balancing contexts accordingly across the different linear functions, and derive guarantees for simle regret in both exectation and high-robability. Finally, we extend the algorithm and its guarantees to high dimensional settings, where the number of linear models times the dimension of the contextual sace is higher than the total budget of samles. Simulations with real data suggest that Trace-UCB is remarkably robust, outerforming a number of baselines even when its assumtions are violated.
"
160,2017,Tensor Decomposition via Simultaneous Power Iteration,Oral/Poster,"Tensor decomosition is an imortant roblem with many alications across several discilines, and a oular aroach for this roblem
is the tensor ower method. However, revious works with theoretical guarantee based on this aroach can only find the to eigenvectors one after one, unlike the case for matrices. In this aer, we show how to find the eigenvectors simultaneously with the hel of a new initialization rocedure. This allows us to achieve a better running
time in the batch setting, as well as a lower samle comlexity in the streaming setting.
"
161,2017,Learning Gradient Descent: Better Generalization and Longer Horizons,Oral/Poster,"Training dee neural networks is a highly nontrivial task, involving carefully selecting aroriate training algorithms, scheduling ste sizes and tuning other hyerarameters. Trying different combinations can be quite labor-intensive and time consuming. Recently, researchers have tried to use dee learning algorithms to exloit the landscae of the loss function of the training roblem of interest, and learn how to otimize over it in an automatic way. In this aer, we roose a new learning-to-learn model and some useful and ractical tricks. Our otimizer outerforms generic, hand-crafted otimization algorithms and state-of-the-art learning-to-learn otimizers by DeeMind in many tasks. We demonstrate the effectiveness of our algorithms on a number of tasks, including dee MLPs, CNNs, and simle LSTMs.
"
162,2017,Stochastic Adaptive Quasi-Newton Methods for Minimizing Expected Values,Oral/Poster,"We roose a novel class of stochastic, adative methods for minimizing self-concordant functions which can be exressed as an exected value. These methods generate an estimate of the true objective function by taking the emirical mean over a samle drawn at each ste, making the roblem tractable.
The use of adative ste sizes eliminates the need for the user to suly a ste size. Methods in this class include extensions of gradient descent (GD) and BFGS. We show that,
given a suitable amount of samling, the stochastic adative GD attains linear convergence in exectation, and with further samling, the stochastic adative BFGS attains R-suerlinear convergence. We resent exeriments showing that these methods comare favorably to SGD.
"
163,2017,Hierarchy Through Composition with Multitask LMDPs,Oral/Poster,"Hierarchical architectures are critical to the scalability of reinforcement learning methods. Most current hierarchical frameworks execute actions serially, with macro-actions comrising sequences of rimitive actions. We roose a novel alternative to these control hierarchies based on concurrent execution of many actions in arallel. Our scheme exloits the guaranteed concurrent comositionality rovided by the linearly solvable Markov decision rocess (LMDP) framework, which naturally enables a learning agent to draw on several macro-actions simultaneously to solve new tasks. We introduce the Multitask LMDP module, which maintains a arallel distributed reresentation of tasks and may be stacked to form dee hierarchies abstracted in sace and time.
"
164,2017,Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP,Oral/Poster,"Online sarse linear regression is an online roblem where an algorithm reeatedly chooses a subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued rediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret to the best sarse linear redictor in hindsight. Without any assumtions, this roblem is known to be comutationally intractable. In this aer, we make the assumtion that data matrix satisfies restricted isometry roerty, and show that this assumtion leads to comutationally efficient algorithms with sublinear regret for two variants of the roblem. In the first variant, the true label is generated according to a sarse linear model with additive Gaussian noise. In the second, the true label is chosen adversarially.
"
165,2017,A Unified Variance Reduction-Based Framework for Nonconvex Low-Rank Matrix Recovery,Oral/Poster,"We roose a generic framework based on a new stochastic variance-reduced gradient descent algorithm for accelerating nonconvex low-rank matrix recovery. Starting from an aroriate initial estimator, our roosed algorithm erforms rojected gradient descent based on a novel semi-stochastic gradient secifically designed for low-rank matrix recovery. Based uon the mild restricted strong convexity and smoothness conditions, we derive a  rojected notion of the restricted Lischitz continuous gradient roerty, and rove that our algorithm enjoys linear convergence rate to the unknown low-rank matrix with an imroved comutational comlexity. Moreover, our algorithm can be emloyed to both noiseless and noisy observations, where the (near) otimal samle comlexity and statistical rate can be attained resectively. We further illustrate the sueriority of our generic framework through several secific examles, both theoretically and exerimentally. 
"
166,2017,Learning Algorithms for Active Learning,Oral/Poster,"We introduce a model that learns active learning algorithms via metalearning. For a distribution of related tasks, our model jointly learns: a data reresentation, an item selection heuristic, and a rediction function. Our model uses the item selection heuristic to construct a labeled suort set for training the rediction function. Using the Omniglot and MovieLens datasets, we test our model in synthetic and ractical settings.
"
167,2017,Practical Gauss-Newton Optimisation for Deep Learning,Oral/Poster,"We resent an efficient block-diagonal aroximation to the Gauss-Newton matrix for feedforward neural networks. Our resulting algorithm is cometitive against state-of-the-art first-order otimisation methods, with sometimes significant imrovement in otimisation erformance. Unlike first-order methods, for which hyerarameter tuning of the otimisation arameters is often a laborious rocess, our aroach can rovide good erformance even when used with default settings. A side result of our work is that for iecewise linear transfer functions, the network objective function can have no differentiable local maxima, which may artially exlain why such transfer functions facilitate effective otimisation.
"
168,2017,A Laplacian Framework for Option Discovery in Reinforcement Learning,Oral/Poster,"Reresentation learning and otion discovery are two of the biggest challenges in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known aroach for reresentation learning in MDPs. In this aer we address the otion discovery roblem by showing how PVFs imlicitly define otions. We do it by introducing eigenuroses, intrinsic reward functions derived from the learned reresentations. The otions discovered from eigenuroses traverse the rincial directions of the state sace. They are useful for multile tasks because they are discovered without taking the environment's rewards into consideration. Moreover, different otions act at different time scales, making them helful for exloration. We demonstrate features of eigenuroses in traditional tabular domains as well as in Atari 2600 games.
"
169,2017,Emulating the Expert: Inverse Optimization through Online Learning,Oral/Poster,"In this aer, we demonstrate how to learn the objective function of a decision maker while only observing the roblem inut data and the decision maker's corresonding decisions over multile rounds. Our aroach is based on online learning techniques and works for linear objectives over arbitrary sets for which we have a linear otimization oracle and as such generalizes revious work based on KKT-system decomosition and dualization aroaches. The alicability of our framework for learning linear constraints is also discussed briefly. Our algorithm converges at a rate of O(1sqrt(T)), and we demonstrate its effectiveness and alications in reliminary comutational results. 
"
170,2017,"An Efficient, Sparsity-Preserving, Online Algorithm for Low-Rank Approximation",Oral/Poster,"Low-rank matrix aroximation is a fundamental tool in data analysis for rocessing large datasets, reducing noise, and finding imortant signals.  In this work, we resent a novel truncated LU factorization called Sectrum-Revealing LU (SRLU) for effective low-rank matrix aroximation, and develo a fast algorithm to comute an SRLU factorization. We rovide both matrix and singular value aroximation error bounds for the SRLU aroximation comuted by our algorithm. Our analysis suggests that SRLU is cometitive with the best low-rank matrix aroximation methods, deterministic or randomized, in both comutational comlexity and aroximation quality. Numeric exeriments illustrate that SRLU reserves sarsity, highlights imortant data features and variables, can be efficiently udated, and calculates data aroximations nearly as accurately as the best ossible.  To the best of our knowledge this is the first ractical variant of the LU factorization for effective  and efficient low-rank matrix aroximation. 
"
171,2017,Tensor Balancing on Statistical Manifold,Oral/Poster,"We solve tensor balancing, rescaling an Nth order nonnegative tensor by multilying N tensors of order N - 1 so that every fiber sums to one. This generalizes a fundamental rocess of matrix balancing used to comare matrices in a wide range of alications from biology to economics. We resent an efficient balancing algorithm with quadratic convergence using Newton's method and show in numerical exeriments that the roosed algorithm is several orders of magnitude faster than existing ones. To theoretically rove the correctness of the algorithm, we model tensors as robability distributions in a statistical manifold and realize tensor balancing as rojection onto a submanifold. The key to our algorithm is that the gradient of the manifold, used as a Jacobian matrix in Newton's method, can be analytically obtained using the Möbius inversion formula, the essential of combinatorial mathematics. Our model is not limited to tensor balancing, but has a wide alicability as it includes various statistical and machine learning models such as weighted DAGs and Boltzmann machines.
"
172,2017,Modular Multitask Reinforcement Learning with Policy Sketches,Oral/Poster,"We describe a framework for multitask dee reinforcement learning guided by olicy sketches. Sketches annotate tasks with sequences of named subtasks, roviding information about high-level structural relationshis among tasks but not how to imlement them---secifically not roviding the detailed guidance used by much revious work on learning olicy abstractions for RL (e.g. intermediate rewards, subtask comletion signals, or intrinsic motivations).  To learn from sketches, we resent a model that associates every subtask with a modular subolicy, and jointly maximizes reward over full task-secific olicies by tying arameters across shared subolicies.  Otimization is accomlished via a decouled actor--critic training objective that facilitates learning common behaviors from multile dissimilar reward functions.  We evaluate the effectiveness of our aroach in three environments featuring both discrete and continuous control, and with sarse rewards that can be obtained only after comleting a number of high-level subgoals.  Exeriments show that using our aroach to learn olicies guided by sketches gives better erformance than existing techniques for learning task-secific or shared olicies, while naturally inducing a library of interretable rimitive behaviors that can be recombined to raidly adat to new tasks.
"
173,2017,Variants of RMSProp and Adagrad with Logarithmic Regret Bounds,Oral/Poster,"Adative gradient methods have become recently very oular, in articular as they have been shown to be useful in the training of dee neural networks. In this aer we have analyzed RMSPro, originally roosed for the training of dee neural networks, in the context of online convex otimization and show $\sqrt{T}$-tye regret bounds. Moreover, we roose two variants SC-Adagrad and SC-RMSPro for which we show logarithmic regret bounds for strongly convex functions. Finally, we demonstrate in the exeriments that these new variants outerform other adative gradient techniques or stochastic gradient descent in the otimization of strongly convex functions as well as in training of dee neural networks."
174,2017,Algorithms for $\ell_p$ Low-Rank Approximation,Oral/Poster,"  We consider the roblem of aroximating a given matrix by a
  low-rank matrix so as to minimize the entrywise $\ell_$-aroximation error,
  for any $ \geq 1$; the case $ = 2$ is the classical SVD roblem.
  We obtain the first rovably good aroximation algorithms for this
  robust version of low-rank aroximation that work for
  every value of $$.
  Our algorithms are simle, easy to imlement, work well in
  ractice, and illustrate interesting tradeoffs between the
  aroximation quality, the running time, and the rank of the
  aroximating matrix.
"
175,2017,Relative Fisher Information and Natural Gradient for Learning Large Modular Models,Oral/Poster,"Fisher information and natural gradient rovided dee insights and owerful tools to artificial neural networks. However related analysis becomes more and more difficult as the learner's structure turns large and comlex. This aer makes a reliminary ste towards a new direction. We extract a local comonent from a large neural system, and define its relative Fisher information metric that describes accurately this small comonent, and is invariant to the other arts of the system. This concet is imortant because the geometry structure is much simlified and it can be easily alied to guide the learning of neural networks. We rovide an analysis on a list of commonly used comonents, and demonstrate how to use this concet to further imrove otimization.
"
176,2017,Efficient Orthogonal Parametrisation of Recurrent Neural Networks Using Householder Reflections,Oral/Poster,"The roblem of learning long-term deendencies in sequences using Recurrent Neural Networks (RNNs) is still a major challenge. Recent methods have been suggested to solve this roblem by constraining the transition matrix to be unitary during training which ensures that its norm is equal to one and revents exloding gradients. These methods either have limited exressiveness or scale oorly with the size of the network when comared with the simle RNN case, esecially when using stochastic gradient descent with a small mini-batch size. Our contributions are as follows; we first show that constraining the transition matrix to be unitary is a secial case of an orthogonal constraint. Then we resent a new arametrisation of the transition matrix which allows efficient training of an RNN while ensuring that the matrix is always orthogonal. Our results show that the orthogonal constraint on the transition matrix alied through our arametrisation gives similar benefits to the unitary constraint, without the time comlexity limitations. 
"
177,2017,Lazifying Conditional Gradient Algorithms,Oral/Poster,"Conditional gradient algorithms (also often called Frank-Wolfe algorithms) are oular due to their simlicity of only requiring a linear otimization oracle and more recently they also gained significant traction for online learning. While simle in rincile, in many cases the actual imlementation of the linear otimization oracle is costly. We show a general method to lazify various conditional gradient algorithms, which in actual comutations leads to several orders of magnitude of seedu in wall-clock time. This is achieved by using a faster searation oracle instead of a linear otimization oracle, relying only on few linear otimization oracle calls.
"
178,2017,Data-Efficient Policy Evaluation Through Behavior Policy Search,Oral/Poster,"We consider the task of evaluating a olicy for a Markov decision rocess (MDP). The standard unbiased technique for evaluating a olicy is to deloy the olicy and observe its erformance. We show that the data collected from deloying a different olicy, commonly called the behavior olicy, can be used to roduce unbiased estimates with lower mean squared error than this standard technique. We derive an analytic exression for the otimal behavior olicy --- the behavior olicy that minimizes the mean squared error of the resulting estimates. Because this exression deends on terms that are unknown in ractice, we roose a novel olicy evaluation sub-roblem, behavior olicy search: searching for a behavior olicy that reduces mean squared error. We resent a behavior olicy search algorithm and emirically demonstrate its effectiveness in lowering the mean squared error of olicy erformance estimates.
"
179,2017,Exact MAP Inference by Avoiding Fractional Vertices,Oral/Poster,"Given a grahical model, one essential roblem is MAP inference, that is, finding the most likely configuration of states according to the model. Although this roblem is NP-hard, large instances can be solved in ractice and it is a major oen question is to exlain why this is true. We give a natural condition under which we can rovably erform MAP inference in olynomial time---we require that the number of fractional vertices in the LP relaxation exceeding the otimal solution is bounded by a olynomial in the roblem size. This resolves an oen question by Dimakis, Gohari, and Wainwright. In contrast, for general LP relaxations of integer rograms, known techniques can only handle a constant number of fractional vertices whose value exceeds the otimal solution. We exerimentally verify this condition and demonstrate how efficient various integer rogramming methods are at removing fractional solutions.
"
180,2017,Leveraging Node Attributes for Incomplete Relational Data,Oral/Poster,"Relational data are usually highly incomlete in ractice, which insires us to leverage side information to imrove the erformance of community detection and link rediction. This aer resents a Bayesian robabilistic aroach that incororates various kinds of node attributes encoded in binary form in relational models with Poisson likelihood. Our method works flexibly with both directed and undirected relational networks. The inference can be done by efficient Gibbs samling which leverages sarsity of both networks and node attributes. Extensive exeriments show that our models achieve the state-of-the-art link rediction results, esecially with highly incomlete relational data.
"
181,2017,How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?,Oral/Poster,"How many samles are sufficient to guarantee that the eigenvectors of the samle covariance matrix are close to those of the actual covariance matrix? For a wide family of distributions, including distributions with finite second moment and sub-gaussian distributions suorted in a centered Euclidean ball, we rove that the inner roduct between eigenvectors of the samle and actual covariance matrices decreases roortionally to the resective eigenvalue distance and the number of samles. Our findings imly \textit{non-asymtotic} concentration bounds for eigenvectors and eigenvalues and carry strong consequences for the non-asymtotic analysis of PCA and its alications. For instance, they rovide conditions for searating comonents estimated from $O(1)$ samles and show that even few samles can be sufficient to erform dimensionality reduction, esecially for low-rank covariances. 
"
182,2017,Distributed and Provably Good Seedings for k-Means in Constant Rounds,Oral/Poster,"The k-Means++ algorithm is the state of the art algorithm to solve k-Means clustering roblems as the comuted clusterings are O(log k) cometitive in exectation. However, its seeding ste requires k inherently sequential asses through the full data set making it hard to scale to massive data sets. The standard remedy is to use the k-Means|| algorithm which reduces the number of sequential rounds and is thus suitable for a distributed setting. In this aer, we rovide a novel analysis of the k-Means|| algorithm that bounds the exected solution quality for any number of rounds and oversamling factors greater than k, the two arameters one needs to choose in ractice. In articular, we show that k-Means|| rovides emrovably goodem clusterings even for a small, constant number of iterations. This theoretical finding exlains the common observation that k-Means|| erforms extremely well in ractice even if the number of rounds is low. We further rovide a hard instance that shows that an emadditiveem error term as encountered in our analysis is inevitable if less than k-1 rounds are emloyed.
"
183,2017,Learning Deep Architectures via Generalized Whitened Neural Networks,Oral/Poster,"Whitened Neural Network (WNN) is a recent advanced dee architecture, which imroves convergence and generalization of canonical neural
networks by whitening their internal hidden reresentation. However, the whitening transformation increases comutation time. Unlike WNN that reduced runtime by erforming whitening every thousand iterations, which degenerates
convergence due to the ill conditioning, we resent generalized WNN (GWNN), which has three aealing roerties. First, GWNN is able to learn comact reresentation to reduce comutations. Second, it enables whitening transformation to be erformed in a short eriod,
reserving good conditioning. Third, we roose a data-indeendent estimation of the covariance matrix to further imrove comutational efficiency. Extensive exeriments on various datasets demonstrate the benefits of GWNN.
"
184,2017,On orthogonality and learning RNNs with long term dependencies,Oral/Poster,"It is well known that it is challenging to train dee neural networks and recurrent neural networks for tasks that exhibit long term deendencies. The vanishing or exloding gradient roblem is a well known issue associated with these challenges. One aroach to addressing vanishing and exloding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices reserve gradient norm during backroagation and may therefore be a desirable roerty. This aer exlores issues with otimization convergence, seed and gradient stability when encouraging or enforcing orthogonality. To erform this analysis, we roose a weight matrix factorization and arameterization strategy through which we can bound matrix norms and therein control the degree of exansivity induced during backroagation. We find that hard constraints on orthogonality can negatively affect the seed of convergence and model erformance.
"
185,2017,Conditional Accelerated Lazy Stochastic Gradient Descent,Oral/Poster,"In this work we introduce a conditional accelerated lazy stochastic gradient descent algorithm with otimal number of calls to a stochastic first-order oracle and convergence rate O(1  esilon^2) imroving over the rojection-free, Online Frank-Wolfe based stochastic gradient descent of (Hazan and Kale, 2012) with convergence rate O(1  esilon^4). 
"
186,2017,Stochastic Variance Reduction Methods for Policy Evaluation,Oral/Poster,"Policy evaluation is concerned with estimating the value function that redicts long-term values of states under a given olicy.  It is a crucial ste in many reinforcement-learning algorithms.  In this aer, we focus on olicy evaluation with linear function aroximation over a fixed dataset. We first transform the emirical olicy evaluation roblem into a (quadratic) convex-concave saddle-oint roblem, and then resent a rimal-dual batch gradient method, as well as two stochastic variance reduction methods for solving the roblem. These algorithms scale linearly in both samle size and feature dimension. Moreover, they achieve linear convergence even when the saddle-oint roblem has only strong concavity in the dual variables but no strong convexity in the rimal variables. Numerical exeriments on benchmark roblems demonstrate the effectiveness of our methods.
"
187,2017,Exact Inference for Integer Latent-Variable Models,Oral/Poster,"Grahical models with latent count variables arise in a number of areas. However, standard inference algorithms do not aly to these models due to the infinite suort of the latent variables. Winner and Sheldon (2016) recently develoed a new technique using robability generating functions (PGFs) to erform efficient, exact inference for certain Poisson latent variable models. However, the method relies on symbolic maniulation of PGFs, and it is unclear whether this can be extended to more general models. In this aer we introduce a new aroach for inference with PGFs: instead of maniulating PGFs symbolically, we adat techniques from the autodiff literature to comute the higher-order derivatives necessary for inference. This substantially generalizes the class of models for which efficient, exact inference algorithms are available. Secifically, our results aly to a class of models that includes branching rocesses, which are widely used in alied mathematics and oulation ecology, and autoregressive models for integer data. Exeriments show that our techniques are more scalable than existing aroximate methods and enable new alications.
"
188,2017,Bayesian inference on random simple graphs with power law degree distributions,Oral/Poster,"We resent a model for random simle grahs with ower law (i.e., heavy-tailed) degree distributions. To attain this behavior, the edge
robabilities in the grah are constructed from Bertoin–Fujita–Roynette–Yor (BFRY) random variables, which have been recently utilized in
Bayesian statistics for the construction of ower law models in several alications. Our construction readily extends to cature the structure of latent factors, similarly to stochastic block-models, while maintaining its ower law degree distribution. The BFRY random variables are well aroximated by gamma random variables in a variational Bayesian inference routine, which we aly to several network datasets for which ower law degree distributions are a natural assumtion. By learning the arameters of the BFRY distribution via robabilistic inference, we are able to automatically select the aroriate ower law behavior from the data. In order to further scale our inference rocedure, we adot
stochastic gradient ascent routines where the gradients are comuted on minibatches (i.e., subsets) of the edges in the grah.
"
189,2017,Faster Principal Component Regression and Stable Matrix Chebyshev Approximation,Oral/Poster,"We solve rincial comonent regression (PCR), u to a multilicative accuracy $1+\gamma$, by reducing the roblem to $\tilde{O}(\gamma^{-1})$ black-box calls of ridge regression. Therefore, our algorithm does not require any exlicit construction of the to rincial comonents, and is suitable for large-scale PCR instances. In contrast, revious result requires $\tilde{O}(\gamma^{-2})$ such black-box calls.

We obtain this result by develoing a general stable recurrence formula for matrix Chebyshev olynomials, and a degree-otimal olynomial aroximation to the matrix sign function. Our techniques may be of indeendent interests, esecially when designing iterative methods.
"
190,2017,Consistent k-Clustering,Oral/Poster,"The study of online algorithms and cometitive analysis rovides a solid foundation for studying the quality of irrevocable decision making when the data arrives in an online manner. 
While in some scenarios the decisions are indeed irrevocable, there are many ractical situations when changing a revious decision is not imossible, but simly exensive.  In this work we formalize this notion and introduce the consistent k-clustering roblem. With oints arriving online, the goal is to maintain a constant aroximate solution, while minimizing the number of reclusterings necessary. We rove a lower bound, showing that \Omega(k \log n) changes are necessary in the worst case for a wide range of objective functions. On the ositive side, we give an algorithm that needs only O(k^2 \log^4n) changes to maintain a constant cometitive solution, an exonential imrovement on the naive solution of reclustering at every time ste. Finally, we show exerimentally that our aroach erforms much better than the theoretical bound, with the number of changes growing aroximately as O(\log n). 
"
191,2017,Continual Learning Through Synaptic Intelligence,Oral/Poster,"While dee learning has led to remarkable advances across diverse alications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adat to changing domains, ossibly by leveraging comlex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synases that bring some of this biological comlexity into artificial neural networks. Each synase accumulates task relevant information over time, and exloits this information to raidly store new memories without forgetting old ones. We evaluate our aroach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining comutational efficiency.
"
192,2017,Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNNs,Oral/Poster,"Using unitary (instead of general) matrices in artificial neural networks (ANNs) is a romising way to solve the gradient exlosionvanishing roblem, as well as to enable ANNs to learn long-term correlations in the data. This aroach aears articularly romising for Recurrent Neural Networks (RNNs). In this work, we resent a new architecture for imlementing an Efficient Unitary Neural Network (EUNNs); its main advantages can be summarized as follows. Firstly, the reresentation caacity of the unitary sace in an EUNN is fully tunable, ranging from a subsace of SU(N) to the entire unitary sace. Secondly, the comutational comlexity for training an EUNN is merely $\mathcal{O}(1)$ er arameter. Finally, we test the erformance of EUNNs on the standard coying task, the ixel-ermuted MNIST digit recognition benchmark as well as the Seech Prediction Test (TIMIT). We find that our architecture significantly outerforms both other state-of-the-art unitary RNNs and the LSTM architecture, in terms of the final erformance andor the wall-clock training seed. EUNNs are thus romising alternatives to RNNs and LSTMs for a wide variety of alications."
193,2017,SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient,Oral/Poster,"In this aer, we roose a StochAstic Recursive grAdient algoritHm (SARAH), as well as its ractical variant SARAH+, as a novel aroach to the finite-sum minimization roblems. Different from the vanilla SGD and other modern stochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simle recursive framework for udating stochastic gradient estimates; when comaring to SAGSAGA, SARAH does not require a storage of ast gradients. The linear convergence rate of SARAH is roven under strong convexity assumtion. We also rove a linear convergence rate (in the strongly convex case) for an inner loo of SARAH, the roerty that SVRG does not ossess. Numerical exeriments demonstrate the efficiency of our algorithm.
"
194,2017,Optimal and Adaptive Off-policy Evaluation in Contextual Bandits,Oral/Poster,"We study the off-olicy evaluation roblem---estimating the value of a target olicy using data collected by another olicy---under the contextual bandit model. We consider the general (agnostic) setting without access to a consistent model of rewards and establish a minimax lower bound on the mean squared error (MSE). The bound is matched u to constants by the inverse roensity scoring (IPS) and doubly robust (DR) estimators. This highlights the difficulty of the agnostic contextual setting, in contrast with multi-armed bandits and contextual bandits with access to a consistent reward model, where IPS is subotimal. We then roose the SWITCH estimator, which can use an existing reward model (not necessarily consistent) to achieve a better bias-variance tradeoff than IPS and DR. We rove an uer bound on its MSE and demonstrate its benefits emirically on a diverse collection of datasets, often outerforming rior work by orders of magnitude.
"
195,2017,Improving Viterbi is Hard: Better Runtimes Imply Faster Clique Algorithms,Oral/Poster,"The classic algorithm of Viterbi comutes the most likely ath in a Hidden Markov Model (HMM) that results in a given sequence of observations. It runs in time O(Tn^2) given a sequence of T observations from a HMM with n states. Desite significant interest in the roblem and rolonged effort by different communities, no known algorithm achieves more than a olylogarithmic seedu. In this aer, we exlain this difficulty by roviding matching conditional lower bounds. Our lower bounds are based on assumtions that the best known algorithms for the All-Pairs Shortest Paths roblem (APSP) and for the Max-Weight k-Clique roblem in edge-weighted grahs are essentially tight. Finally, using a recent algorithm by Green Larsen and Williams for online Boolean matrix-vector multilication, we get a 2^{Omega(sqrt{log n})} seedu for the Viterbi algorithm when there are few distinct transition robabilities in the HMM.
"
196,2017,Analogical Inference for Multi-relational Embeddings,Oral/Poster,"Large-scale multi-relational embedding refers to the task of learning the latent reresentations for entities and relations in large knowledge grahs. An effective and scalable solution for this roblem is crucial for the true success of knowledge-based inference in a broad range of alications. This aer rooses a novel framework for otimizing the latent reresentations with resect to the \textit{analogical} roerties of the embedded entities and relations. By formulating the objective function in a differentiable fashion, our model enjoys both its theoretical ower and comutational scalability, and significantly outerformed a large number of reresentative baseline methods on benchmark datasets. Furthermore, the model offers an elegant unification of several well-known methods in multi-relational embedding, which can be roven to be secial instantiations of our framework.
"
197,2017,Spectral Learning from a Single Trajectory under Finite-State Policies,Oral/Poster,"We resent sectral methods of moments for learning sequential models from a single trajectory, in stark contrast with the classical literature that assumes the availability of multile i.i.d. trajectories. Our aroach leverages an efficient SVD-based learning algorithm for weighted automata and rovides the first rigorous analysis for learning many imortant models using deendent data. We state and analyze the algorithm under three increasingly difficult scenarios: robabilistic automata, stochastic weighted automata, and reactive redictive state reresentations controlled by a finite-state olicy. Our roofs include novel tools for studying mixing roerties of stochastic weighted automata.
"
198,2017,Towards K-means-friendly Spaces: Simultaneous Deep Learning and Clustering,Oral/Poster,"Most learning aroaches treat dimensionality reduction (DR) and clustering searately (i.e., sequentially), but recent research has shown that otimizing the two tasks jointly can substantially imrove the erformance of both. The remise behind the latter genre is that the data samles are obtained via linear transformation of latent reresentations that are easy to cluster; but in ractice, the transformation from the latent sace to the data can be more comlicated. In this work, we assume that this transformation is an unknown and ossibly nonlinear function. To recover the ‘clustering-friendly’ latent reresentations and to better cluster the data, we roose a joint DR and K-means clustering aroach in which DR is accomlished via learning a dee neural network (DNN). The motivation is to kee the advantages of jointly otimizing the two tasks, while exloiting the dee neural network’s ability to aroximate any nonlinear function. This way, the ro- osed aroach can work well for a broad class of generative models. Towards this end, we carefully design the DNN structure and the associated joint otimization criterion, and roose an effective and scalable algorithm to handle the formulated otimization roblem. Exeriments using different real datasets are emloyed to showcase the effectiveness of the roosed aroach.
"
199,2017,Adaptive Neural Networks for Efficient Inference,Oral/Poster,"We resent an aroach to adatively utilize dee neural networks in order to reduce the evaluation time on new examles without loss of accuracy. Rather than attemting to redesign or aroximate existing networks, we roose two schemes that adatively utilize networks. We first ose an adative network evaluation scheme, where we learn a system to adatively choose the comonents of a dee network to be evaluated for each examle. By allowing examles correctly classified using early layers of the system to exit, we avoid the comutational time associated with full evaluation of the network. We extend this to learn a network selection system that adatively selects the network to be evaluated for each examle. We show that comutational time can be dramatically reduced by exloiting the fact that many examles can be correctly classified using relatively efficient networks and that comlex, comutationally costly networks are only necessary for a small fraction of examles. We ose a global objective for learning an adative early exit or network selection olicy and solve it by reducing the olicy learning roblem to a layer-by-layer weighted binary classification roblem.  Emirically, these aroaches yield dramatic reductions in comutational cost, with u to a 2.8x seedu on state-of-the-art networks from the ImageNet image recognition challenge with minimal ($1\%$) loss of to5 accuracy. "
200,2017,The Statistical Recurrent Unit,Oral/Poster,"Sohisticated gated recurrent neural network architectures like LSTMs and GRUs have been shown to be highly effective in a myriad of alications. We develo an un-gated unit, the statistical recurrent unit (SRU), that is able to learn long term deendencies in data by only keeing moving averages of statistics. The SRU's architecture is simle, un-gated, and contains a comarable number of arameters to LSTMs; yet, SRUs erform favorably to more sohisticated LSTM and GRU alternatives, often outerforming one or both in various tasks. We show the efficacy of SRUs as comared to LSTMs and GRUs in an unbiased manner by otimizing resective architectures' hyerarameters for both synthetic and real-world tasks.
"
201,2017,Approximate Steepest Coordinate Descent,Oral/Poster,"We roose a new selection rule for the coordinate selection in coordinate descent methods for huge-scale otimization. The efficiency of this novel scheme is rovably better than the efficiency of uniformly random selection, and can reach the efficiency of steeest coordinate descent (SCD), enabling an acceleration of a factor of u to $n$, the number of coordinates. In many ractical alications, our scheme can be imlemented at no extra cost and comutational efficiency very close to the faster uniform selection.
Numerical exeriments with Lasso and Ridge regression show romising imrovements, in line with our theoretical guarantees."
202,2017,Consistent On-Line Off-Policy Evaluation,Oral/Poster,"The roblem of on-line off-olicy evaluation (OPE) has been actively studied in the last decade due to its imortance both as a stand-alone roblem and as a module in a olicy imrovement scheme. However, most Temoral Difference (TD) based solutions ignore the discreancy between the stationary distribution of the behavior and target olicies and its effect on the convergence limit when function aroximation is alied. In this aer we roose the Consistent Off-Policy Temoral Difference (COP-TD($\lambda$, $\beta$)) algorithm that addresses this issue and reduces this bias at some comutational exense. We show that COP-TD($\lambda$, $\beta$) can be designed to converge to the same value that would have been obtained by using on-olicy TD($\lambda$) with the target olicy. Subsequently, the roosed scheme leads to a related and romising heuristic we call log-COP-TD($\lambda$, $\beta$). Both algorithms have favorable emirical results to the current state of the art on-line OPE algorithms. Finally, our formulation sheds some new light on the recently roosed Emhatic TD learning."
203,2017,Variational Inference for Sparse and Undirected Models,Oral/Poster,"Undirected grahical models are alied in genomics, rotein structure rediction, and neuroscience to identify sarse interactions that underlie discrete data. Although Bayesian methods for inference would be favorable in these contexts, they are rarely used because they require doubly intractable Monte Carlo samling. Here, we develo a framework for scalable Bayesian inference of discrete undirected models based on two new methods. The first is Persistent VI, an algorithm for variational inference of discrete undirected models that avoids doubly intractable MCMC and aroximations of the artition function. The second is Fadeout, a rearameterization aroach for variational inference under sarsity-inducing riors that catures a osteriori correlations between arameters and hyerarameters with noncentered arameterizations. We find that, together, these methods for variational inference substantially imrove learning of sarse undirected grahical models in simulated and real roblems from hysics and biology.
"
204,2017,Know-Evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs,Oral/Poster,"The availability of large scale event data with time stams has given rise to dynamically evolving knowledge grahs that contain temoral information for each edge. Reasoning over time in such dynamic knowledge grahs is not yet well understood.
To this end, we resent Know-Evolve, a novel dee evolutionary knowledge network that learns non-linearly evolving entity reresentations over time. The occurrence of a fact (edge) is modeled as a multivariate oint rocess whose intensity function is modulated by the score for that fact comuted based on the learned entity embeddings. We demonstrate significantly imroved erformance over various relational learning aroaches on two large scale real-world datasets. Further, our method effectively redicts occurrence or recurrence time of a fact which is novel comared to rior reasoning aroaches in multi-relational setting.
"
205,2017,Capacity Releasing Diffusion for Speed and Locality.,Oral/Poster,"Diffusions and related random walk rocedures are of central imortance in many areas of machine learning, data analysis, and alied mathematics. Because they sread mass agnostically at each ste in an iterative manner, they can sometimes sread mass ``too aggressively,'' thereby failing to find the ``right'' clusters. We introduce a novel Caacity Releasing Diffusion (CRD) Process, which is both faster and stays more local than the classical sectral diffusion rocess. As an alication, we use our CRD Process to develo an imroved local algorithm for grah clustering. Our local grah clustering method can find local clusters in a model of clustering where one begins the CRD Process in a cluster whose vertices are connected better internally than externally by an $O(\log^2 n)$ factor, where $n$ is the number of nodes in the cluster. Thus, our CRD Process is the first local grah clustering algorithm that is not subject to the well-known quadratic Cheeger barrier. Our result requires a certain smoothness condition, which we exect to be an artifact of our analysis. Our emirical evaluation demonstrates imroved results, in articular for realistic social grahs where there are moderately good---but not very good---clusters."
206,2017,Hyperplane Clustering Via Dual Principal Component Pursuit,Oral/Poster,"State-of-the-art methods for clustering data drawn from a union of subsaces are based on sarse and low-rank reresentation theory and convex otimization algorithms. Existing results guaranteeing the correctness of such methods require the dimension of the subsaces to be small relative to the dimension of the ambient sace. When this assumtion is violated, as is, e.g., in the case of hyerlanes, existing methods are either comutationally too intensive (e.g., algebraic methods) or lack sufficient theoretical suort (e.g., K-Hyerlanes or RANSAC). 
In this aer we rovide theoretical and algorithmic contributions to the roblem of clustering data from a union of hyerlanes, by extending a recent subsace learning method called Dual Princial Comonent Pursuit (DPCP) to the multi-hyerlane case. We give theoretical guarantees under which, the non-convex $\ell_1$ roblem associated with DPCP admits a unique global minimizer equal to the normal vector of the most dominant hyerlane. Insired by this insight, we roose sequential (RANSAC-style) and iterative (K-Hyerlanes-style) hyerlane learning DPCP algorithms, which, via exeriments on synthetic and real data, are shown to outerform or be cometitive to the state-of-the-art."
207,2017,Combined Group and Exclusive Sparsity for Deep Neural Networks,Oral/Poster,"The number of arameters in a dee neural network is usually very large, which hels with its learning caacity but also hinders its scalability and racticality due to memorytime inefficiency and overfitting. To resolve this issue, we roose a sarsity regularization method that exloits both ositive and negative correlations among the features to enforce the network to be sarse, and at the same time remove any redundancies among the features to fully utilize the caacity of the network. Secifically, we roose to use an exclusive sarsity regularization based on (1,2)-norm, which romotes cometition for features between different weights, thus enforcing them to fit to disjoint sets of features. We further combine the exclusive sarsity with the grou sarsity based on (2,1)-norm, to romote both sharing and cometition for features in training of a dee neural network. We validate our method on multile ublic datasets, and the results show that our method can obtain more comact and efficient networks while also imroving the erformance over the base networks with full weights, as oosed to existing sarsity regularizations that often obtain efficiency at the exense of rediction accuracy.
"
208,2017,Input Switched Affine Networks: An RNN Architecture Designed for Interpretability,Oral/Poster,"There exist many roblem domains where the interretability of neural network models is essential for deloyment. Here we introduce a recurrent architecture comosed of inut-switched affine transformations - in other words an RNN without any exlicit nonlinearities, but with inut-deendent recurrent weights. This simle form allows the RNN to be analyzed via straightforward linear methods: we can exactly characterize the linear contribution of each inut to the model redictions; we can use a change-of-basis to disentangle inut, outut, and comutational hidden unit subsaces; we can fully reverse-engineer the architecture's solution to a simle task. Desite this ease of interretation, the inut switched affine network achieves reasonable erformance on a text modeling tasks, and allows greater comutational efficiency than networks with standard nonlinearities.
"
209,2017,StingyCD: Safely Avoiding Wasteful Updates in Coordinate Descent,Oral/Poster,"Coordinate descent (CD) is a scalable and simle algorithm for solving many otimization roblems in machine learning.
Desite this fact, CD can also be very comutationally wasteful. 
Due to sarsity in sarse regression roblems, for examle, the majority of CD udates often result in no rogress toward the solution.
To address this inefficiency, we roose a modified CD algorithm named ""StingyCD.""
By skiing over many udates that are guaranteed to not decrease the objective value, StingyCD significantly reduces convergence times.
Since StingyCD only skis udates with this guarantee, however, StingyCD does not fully exloit the roblem's sarsity.
For this reason, we also roose StingyCD+, an algorithm that achieves further seed-us by skiing udates more aggressively.
Since StingyCD and StingyCD+ rely on simle modifications to CD, it is also straightforward to use these algorithms with other aroaches to scaling otimization.
In emirical comarisons, StingyCD and StingyCD+ imrove convergence times considerably for several L1-regularized otimization roblems.
"
210,2017,Contextual Decision Processes with low Bellman rank are PAC-Learnable,Oral/Poster,"This aer studies systematic exloration for reinforcement learning (RL) with rich observations and function aroximation. We introduce contextual decision rocesses (CDPs), that unify most rior RL settings. Our first contribution is a comlexity measure, the Bellman rank, that we show enables tractable learning of near-otimal behavior in CDPs and is naturally small for many well-studied RL models. Our second contribution is a new RL algorithm that does systematic exloration to learn near-otimal behavior in CDPs with low Bellman rank. The algorithm requires a number of samles that is olynomial in all relevant arameters but indeendent of the number of unique contexts. Our aroach uses Bellman error minimization with otimistic exloration and rovides new insights into efficient exloration for RL with function aroximation.
"
211,2017,Tensor Belief Propagation,Oral/Poster,"We roose a new aroximate inference algorithm for grahical models, tensor belief roagation, based on aroximating the messages assed in the junction tree algorithm. Our algorithm reresents the otential functions of the grahical model and all messages on the junction tree comactly as mixtures of rank-1 tensors. Using this reresentation, we show how to erform the oerations required for inference on the junction tree efficiently: marginalisation can be comuted quickly due to the factored form of rank-1 tensors while multilication can be aroximated using samling. Our analysis gives sufficient conditions for the algorithm to erform well, including for the case of high-treewidth grahs, for which exact inference is intractable. We comare our algorithm exerimentally with several aroximate inference algorithms and show that it erforms well.
"
212,2017,Deep Generative Models for Relational Data with Side Information,Oral/Poster,"We resent a robabilistic framework for overlaing community discovery and link rediction for relational data, given as a grah. The roosed framework has: (1) a dee architecture which enables us to infer multile layers of latent featurescommunities for each node, roviding suerior link rediction erformance on more comlex networks and better interretability of the latent features; and (2) a regression model which allows directly conditioning the node latent features on the side information available in form of node attributes. Our framework handles both (1) and (2) via a clean, unified model, which enjoys full local conjugacy via data augmentation, and facilitates efficient inference via closed form Gibbs samling. Moreover, inference cost scales in the number of edges which is attractive for massive but sarse networks. Our framework is also easily extendable to model weighted networks with count-valued edges.  We comare with various state-of-the-art methods and reort results, both quantitative and qualitative, on several benchmark data sets.
"
213,2017,Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition,Oral/Poster,"We study k-GenEV, the roblem of finding the to k generalized eigenvectors, and k-CCA, the roblem of finding the to k vectors in canonical-correlation analysis. We roose algorithms LazyEV and LazyCCA to solve the two roblems with running times linearly deendent on the inut size and on k.

Furthermore, our algorithms are \emh{doubly-accelerated}: our running times deend only on the square root of the matrix condition number, and on the square root of the eigenga. This is the first such result for both k-GenEV or k-CCA. We also rovide the first ga-free results, which rovide running times that deend on 1\sqrt{\varesilon}$ rather than the eigenga.
"
214,2017,Multilevel Clustering via Wasserstein Means,Oral/Poster,"We roose a novel aroach to the roblem of multilevel clustering, which aims to simultaneously artition data in each grou and discover grouing atterns among grous in a otentially large hierarchically structured corus of data. Our method involves a joint otimization formulation over several saces of discrete robability measures, which are endowed with Wasserstein distance metrics. We roose a number of variants of this roblem, which admit fast otimization algorithms, by exloiting the connection to the roblem of finding Wasserstein barycenters.  Consistency roerties are established for the estimates of both local and global clusters. Finally, exeriment results with both synthetic and real data are resented to demonstrate the flexibility and scalability of the roosed aroach.
"
215,2017,Online and Linear-Time Attention by Enforcing Monotonic Alignments,Oral/Poster,"Recurrent neural network models with an attention mechanism have roven to be extremely effective on a wide variety of sequence-to-sequence roblems. However, the fact that soft attention mechanisms erform a ass over the entire inut sequence when roducing each element in the outut sequence recludes their use in online settings and results in a quadratic time comlexity. Based on the insight that the alignment between inut and outut sequence elements is monotonic in many roblems of interest, we roose an end-to-end differentiable method for learning monotonic alignments which, at test time, enables comuting attention online and in linear time. We validate our aroach on sentence summarization, machine translation, and online seech recognition roblems and achieve results cometitive with existing sequence-to-sequence models.
"
216,2017,Stochastic modified equations and adaptive stochastic gradient algorithms,Oral/Poster,"We develo the method of stochastic modified equations (SME), in which stochastic gradient algorithms are aroximated in the weak sense by continuous-time stochastic differential equations. We exloit the continuous formulation together with otimal control theory to derive novel adative hyer-arameter adjustment olicies. Our algorithms have cometitive erformance with the added benefit of being robust to varying models and datasets. This rovides a general methodology for the analysis and design of stochastic gradient algorithms. 
"
217,2017,A Simple Multi-Class Boosting Framework with Theoretical Guarantees and Empirical Proficiency,Oral/Poster,"There is a need for simle yet accurate white-box learning systems that train quickly and with little data. To this end, we showcase REBEL, a multi-class boosting method, and resent a novel family of weak learners called localized similarities. Our framework rovably minimizes the training error of any dataset at an exonential rate.
We carry out exeriments on a variety of synthetic and real datasets, demonstrating a consistent tendency to avoid overfitting. We evaluate our method on MNIST and standard UCI datasets against other state-of-the-art methods, showing the emirical roficiency of our method.
"
218,2017,Faster Greedy MAP Inference for Determinantal Point Processes,Oral/Poster,"Determinantal oint rocesses (DPPs) are oular robabilistic models that arise in many machine learning tasks, where distributions of diverse sets are characterized by determinants of their features. In this aer, we develo fast algorithms to find the most likely configuration (MAP) of large-scale DPPs, which is NP-hard in general. Due to the submodular nature of the MAP objective, greedy algorithms have been used with emirical success. Greedy imlementations require comutation of log-determinants, matrix inverses or solving linear systems at each iteration. We resent faster imlementations of the greedy algorithms by utilizing the orthogonal benefits of two log-determinant aroximation schemes: (a) first-order exansions to the matrix log-determinant function and (b) high-order exansions to the scalar log function with stochastic trace estimators. In our exeriments, our algorithms are orders of magnitude faster than their cometitors, while sacrificing marginal accuracy.
"
219,2017,ChoiceRank: Identifying Preferences from Node Traffic in Networks,Oral/Poster,"Understanding how users navigate in a network is of high interest in many alications. We consider a setting where only aggregate node-level traffic is observed and tackle the task of learning edge transition robabilities. We cast it as a reference learning roblem, and we study a model where choices follow Luce's axiom. In this case, the O(n) marginal counts of node visits are a sufficient statistic for the O(n^2) transition robabilities. We show how to make the inference roblem well-osed regardless of the network's structure, and we resent ChoiceRank, an iterative algorithm that scales to networks that contains billions of nodes and edges. We aly the model to two clickstream datasets and show that it successfully recovers the transition robabilities using only the network structure and marginal (node-level) traffic data. Finally, we also consider an alication to mobility networks and aly the model to one year of rides on New York City's bicycle-sharing system.
"
220,2017,On the Iteration Complexity of Support Recovery via Hard Thresholding Pursuit,Oral/Poster,"Recovering the suort of a sarse signal from its comressed samles has been one of the most imortant roblems in high dimensional statistics. In this aer, we resent a novel analysis for the hard thresholding ursuit (HTP) algorithm, showing that it exactly recovers the suort of an arbitrary s-sarse signal within O(sklogk) iterations via a roerly chosen roxy function, where k is the condition number of the roblem. In stark contrast to the theoretical results in the literature, the iteration comlexity we obtained holds without assuming the restricted isometry roerty, or relaxing the sarsity, or utilizing the otimality of the underlying signal. We further extend our result to a more challenging scenario, where the subroblem involved in HTP cannot be solved exactly. We rove that even in this setting, suort recovery is ossible and the comutational comlexity of HTP is established. Numerical study substantiates our theoretical results.
"
221,2017,Uniform Deviation Bounds for k-Means Clustering,Oral/Poster,"Uniform deviation bounds limit the difference between a model's exected loss and its loss on an emirical samle emuniformlyem for all models in a learning roblem. In this aer, we rovide a novel framework to obtain uniform deviation bounds for loss functions which are emunboundedem. As a result, we obtain cometitive uniform deviation bounds for k-Means clustering under weak assumtions on the underlying distribution. If the fourth moment is bounded, we rove a rate of O(m^(-12)) comared to the reviously known O(m^(-14)) rate. Furthermore, we show that the rate also deends on the kurtosis - the normalized fourth moment which measures the ""tailedness"" of a distribution. We also rovide imroved rates under rogressively stronger assumtions, namely, bounded higher moments, subgaussianity and bounded suort of the underlying distribution.
"
222,2017,Sequence Tutor: Conservative fine-tuning of sequence generation models with KL-control,Oral/Poster,"This aer rooses a general method for imroving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as samle diversity. An RNN is first re-trained on data using maximum likelihood estimation (MLE), and the robability distribution over the next token in the sequence learned by this model is treated as a rior olicy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality oututs that account for domain-secific incentives while retaining roximity to the rior olicy of the MLE RNN. To formalize this objective, we derive novel off-olicy RL methods for RNNs from KL-control. The effectiveness of the aroach is demonstrated on two alications; 1) generating novel musical melodies, and 2) comutational molecular generation. For both roblems, we show that the roosed method imroves the desired roerties and structure of the generated sequences, while maintaining information learned from data. 
"
223,2017,Dissipativity Theory for Nesterov's Accelerated Method,Oral/Poster,"In this aer, we adat the control theoretic concet of dissiativity theory to rovide a natural understanding of Nesterov's accelerated method. Our theory ties rigorous convergence rate analysis to the hysically intuitive notion of energy dissiation. Moreover, dissiativity allows one to efficiently construct Lyaunov functions (either numerically or analytically) by solving a small semidefinite rogram. Using novel suly rate functions, we show how to recover known rate bounds for Nesterov's method and we generalize the aroach to certify both linear and sublinear rates in a variety of settings. Finally, we link the continuous-time version of dissiativity to recent works on algorithm analysis that use discretizations of ordinary differential equations.
"
224,2017,Gradient Boosted Decision Trees for High Dimensional Sparse Output,Oral/Poster,"In this aer, we study the gradient boosted decision trees (GBDT) when the outut sace is high dimensional and sarse. For examle, in multilabel classification, the outut sace is a $L$-dimensional 01 vector, where $L$ is number of labels that can grow to millions and beyond in many modern alications. We show that vanilla GBDT can easily run out of memory or encounter near-forever running time in this regime, and roose a new GBDT variant, GBDT-SPARSE, to resolve this roblem by emloying $L_0$ regularization. We then discuss in detail how to utilize this sarsity to conduct GBDT training, including slitting the nodes, comuting the sarse residual, and redicting in sublinear time. Finally, we aly our algorithm to extreme multilabel classification roblems, and show that the roosed GBDT-SPARSE achieves an order of magnitude imrovements in model size and rediction time over existing methods, while yielding similar erformance."
225,2017,Zonotope hit-and-run for efficient sampling from projection DPPs,Oral/Poster,"Determinantal oint rocesses (DPPs) are distributions over sets of items that model diversity using kernels. Their alications in machine learning include summary extraction and recommendation systems.   Yet, the cost of samling from a DPP is rohibitive in large-scale alications, which has triggered an  effort towards efficient aroximate samlers.  We build a novel MCMC samler that combines ideas from combinatorial geometry, linear rogramming, and Monte Carlo methods to samle from DPPs with a fixed samle cardinality, also called rojection DPPs. Our samler leverages the ability of the hit-and-run MCMC kernel to efficiently move across convex bodies. Previous theoretical results yield a fast mixing time of our chain when targeting a distribution that is close to a rojection DPP, but not a DPP in general. Our emirical results demonstrate that this extends to samling rojection DPPs, i.e., our samler is more samle-efficient than revious aroaches which in turn translates to faster convergence when dealing with costly-to-evaluate functions, such as summary extraction in our exeriments. 
"
226,2017,Statistical Inference for Incomplete Ranking Data: The Case of Rank-Dependent Coarsening,Oral/Poster,"We consider the roblem of statistical inference for ranking data, secifically rank aggregation, under the assumtion that samles are incomlete in the sense of not comrising all choice alternatives. In contrast to most existing methods, we exlicitly model the rocess of turning a full ranking into an incomlete one, which we call the coarsening rocess. To this end, we roose the concet of rank-deendent coarsening, which assumes that incomlete rankings are roduced by rojecting a full ranking to a random subset of ranks. For a concrete instantiation of our model, in which full rankings are drawn from a Plackett-Luce distribution and observations take the form of airwise references, we study the erformance of various rank aggregation methods. In addition to redictive accuracy in the finite samle setting, we address the theoretical question of consistency, by which we mean the ability to recover a target ranking when the samle size goes to infinity, desite a otential bias in the observations caused by the (unknown) coarsening.
"
227,2017,Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization to Non-smooth Concave Maximization,Oral/Poster,"Iterative Hard Thresholding (IHT) is a class of rojected gradient descent methods for otimizing sarsity-constrained minimization models, with the best known efficiency and scalability in ractice. As far as we know, the existing IHT-style methods are designed for sarse minimization in rimal form. It remains oen to exlore duality theory and algorithms in such a non-convex and NP-hard setting. In this article, we bridge the ga by establishing a duality theory for sarsity-constrained minimization with $\ell_2$-regularized objective and roosing an IHT-style algorithm for dual maximization. Our sarse duality theory rovides a set of sufficient and necessary conditions under which the original NP-hardnon-convex roblem can be equivalently solved in a dual sace. The roosed dual IHT algorithm is a suer-gradient method for maximizing the non-smooth dual objective. An interesting finding is that the sarse recovery erformance of dual IHT is invariant to the Restricted Isometry Proerty (RIP), which is required by all the existing rimal IHT without sarsity relaxation. Moreover, a stochastic variant of dual IHT is roosed for large-scale stochastic otimization. Numerical results demonstrate that dual IHT algorithms can achieve more accurate model estimation given small number of training data and have higher comutational efficiency than the state-of-the-art rimal IHT-style algorithms."
228,2017,Uniform Convergence Rates for Kernel Density Estimation,Oral/Poster,"Kernel density estimation (KDE) is a oular nonarametric density estimation method. We (1) derive finite-samle high-robability density estimation bounds for multivariate KDE under mild density assumtions which hold uniformly in $x \in \mathbb{R}^d$ and bandwidth matrices. We aly these results to (2) mode, (3) density level set, and (4) class robability estimation and attain otimal rates u to logarithmic factors. We then (5) rovide an extension of our results under the manifold hyothesis. Finally, we (6) give uniform convergence results for local intrinsic dimension estimation."
229,2017,Deep Voice: Real-time Neural Text-to-Speech,Oral/Poster,"We resent Dee Voice, a roduction-quality text-to-seech system constructed entirely from dee neural networks. Dee Voice lays the groundwork for truly end-to-end neural seech synthesis. The system comrises five major building blocks: a segmentation model for locating honeme boundaries, a graheme-to-honeme conversion model, a honeme duration rediction model, a fundamental frequency rediction model, and an audio synthesis model. For the segmentation model, we roose a novel way of erforming honeme boundary detection with dee neural networks using connectionist temoral classification (CTC) loss. For the audio synthesis model, we imlement a variant of WaveNet that requires fewer arameters and trains faster than the original. By using a neural network for each comonent, our system is simler and more flexible than traditional text-to-seech systems, where each comonent requires laborious feature engineering and extensive domain exertise. Finally, we show that inference with our system can be erformed faster than real time and describe otimized WaveNet inference kernels on both CPU and GPU that achieve u to 400x seedus over existing imlementations.
"
230,2017,An Analytical Formula of Population Gradient for two-layered ReLU network and its Applications in Convergence and Critical Point Analysis,Oral/Poster,"In this aer, we exlore theoretical roerties of training a two-layered ReLU network $g(\vx; \vw) = \sum_{j=1}^K \sigma(\vw_j\trans\vx)$ with centered $d$-dimensional sherical Gaussian inut $\vx$ ($\sigma$=ReLU). We train our network with gradient descent on $\vw$ to mimic the outut of a teacher network with the same architecture and fixed arameters $\vw\ot$. We show that its oulation gradient has an analytical formula, leading to interesting theoretical analysis of critical oints and convergence behaviors. First, we rove that critical oints outside the hyerlane sanned by the teacher arameters (``out-of-lane``) are not isolated and form manifolds, and characterize in-lane critical-oint-free regions for two-ReLU case. On the other hand, convergence to $\vw\ot$ for one ReLU node is guaranteed with at least $(1-\esilon)2$ robability, if weights are initialized randomly with standard deviation uer-bounded by $O(\esilon\sqrt{d})$, in accordance with emirical ractice. For network with many ReLU nodes, we rove that an infinitesimal erturbation of weight initialization results in convergence towards $\vw\ot$ (or its ermutation), a henomenon known as sontaneous symmetric-breaking (SSB) in hysics. We assume no indeendence of ReLU activations. Simulation verifies our findings."
231,2017,Globally Induced Forest: A Prepruning Compression Scheme,Oral/Poster,"Tree-based ensemble models are heavy memory-wise. An undesired state of affairs
considering nowadays datasets, memory-constrained environment and
fittingrediction times.  In this aer, we roose the Globally Induced Forest
(GIF) to remedy this roblem. GIF is a fast reruning aroach to build
lightweight ensembles by iteratively deeening the current forest. It mixes
local and global otimizations to roduce accurate redictions under memory
constraints in reasonable time.  We show that the roosed method is more than
cometitive with standard tree-based ensembles under corresonding constraints,
and can sometimes even surass much larger models.
"
232,2017,A Divergence Bound for Hybrids of MCMC and Variational Inference and an Application to Langevin Dynamics and SGVI,Oral/Poster,"Two oular classes of methods for aroximate inference are Markov chain Monte Carlo (MCMC) and variational inference. MCMC tends to be accurate if run for a long enough time, while variational inference tends to give better aroximations at shorter time horizons. However, the amount of time needed for MCMC to exceed the erformance of variational methods can be quite high, motivating more fine-grained tradeoffs. This aer derives a distribution over variational arameters, designed to minimize a bound on the divergence between the resulting marginal distribution and the target, and gives an examle of how to samle from this distribution in a way that interolates between the behavior of existing methods based on Langevin dynamics and stochastic gradient variational inference (SGVI).
"
233,2017,Just Sort It! A Simple and Effective Approach to Active Preference Learning,Oral/Poster,"We address the roblem of learning a ranking by using adatively chosen airwise comarisons. Our goal is to recover the ranking accurately but to samle the comarisons saringly. If all comarison outcomes are consistent with the ranking, the otimal solution is to use an efficient sorting algorithm, such as Quicksort. But how do sorting algorithms behave if some comarison outcomes are inconsistent with the ranking? We give favorable guarantees for Quicksort for the oular Bradley-Terry model, under natural assumtions on the arameters. Furthermore, we emirically demonstrate that sorting algorithms lead to a very simle and effective active learning strategy: reeatedly sort the items. This strategy erforms as well as state-of-the-art methods (and much better than random samling) at a minuscule fraction of the comutational cost.
"
234,2017,On The Projection Operator to A Three-view Cardinality Constrained Set,Oral/Poster,"The cardinality constraint is an intrinsic way to restrict the solution structure in many domains, for examle, sarse learning, feature selection, and comressed sensing. To solve a cardinality constrained roblem, the key challenge is to solve the rojection onto the cardinality constraint set, which is NP-hard in general when there exist multile overlaed cardinality constraints. In this aer, we consider the scenario where the overlaed cardinality constraints satisfy a Three-view Cardinality Structure (TVCS), which reflects the natural restriction in many alications, such as identification of gene regulatory networks and task-worker assignment roblem. We cast the rojection into a linear rogramming, and show that for TVCS, the vertex solution of this linear rogramming is the solution for the original rojection roblem. We further rove that such solution can be found with the comlexity roortional to the number of variables and constraints. We finally use synthetic exeriments and two interesting alications in bioinformatics and crowdsourcing to validate the roosed TVCS model and method.
"
235,2017,Density Level Set Estimation on Manifolds with DBSCAN,Oral/Poster,"We show that DBSCAN can estimate the connected comonents of the $\lambda$-density level set $\{ x : f(x) \ge \lambda\}$ given $n$ i.i.d. samles from an unknown density $f$. We characterize the regularity of the level set boundaries using arameter $\beta  0$ and analyze the estimation error under the Hausdorff metric. When the data lies in $\mathbb{R}^D$ we obtain a rate of $\widetilde{O}(n^{-1(2\beta + D)})$, which matches known lower bounds u to logarithmic factors. When the data lies on an embedded unknown $d$-dimensional manifold in $\mathbb{R}^D$, then we obtain a rate of $\widetilde{O}(n^{-1(2\beta + d\cdot \max\{1, \beta \})})$. Finally, we rovide adative arameter tuning in order to attain these rates with no a riori knowledge of the intrinsic dimension, density, or $\beta$."
236,2017,DeepBach: a Steerable Model for Bach Chorales Generation,Oral/Poster,"This aer introduces DeeBach, a grahical model aimed at modeling olyhonic music and secifically hymn-like ieces. 
We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is caable of generating highly convincing chorales in the style of Bach.
DeeBach's strength comes from the use of seudo-Gibbs samling couled with an adated reresentation of musical data. This is in contrast with many automatic music comosition aroaches  which tend to comose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imosing ositional constraints such as notes, rhythms or cadences in the generated score. We also rovide a lugin on to of the MuseScore music editor making the interaction with DeeBach easy to use.
"
237,2017,Forward and Reverse Gradient-Based Hyperparameter Optimization,Oral/Poster,"We study two rocedures (reverse-mode and forward-mode) for comuting the gradient of the validation error with resect to the hyerarameters of any iterative learning algorithm such as stochastic gradient descent. These rocedures mirror two ways of comuting gradients for recurrent neural networks and have different trade-offs in terms  of running time and sace requirements. Our formulation of the reverse-mode rocedure is linked to revious work by Maclaurin et al (2015) but does not require reversible dynamics. Additionally, we exlore the use of constraints on the hyerarameters. The forward-mode rocedure is suitable for real-time hyerarameter udates, which may significantly seedu hyerarameter otimization on large datasets. We resent a series of exeriments on image and hone classification tasks. In the second task, revious gradient-based aroaches are rohibitive. We show that our real-time algorithm yields state-of-the-art results in affordable time.
"
238,2017,Forest-type Regression with General Losses and Robust Forest,Oral/Poster,"This aer introduces a new general framework for forest-tye regression which allows the develoment of robust forest regressors by selecting from a large family of robust loss functions. In articular, when lugged in the squared error and quantile losses, it will recover the classical random forest and quantile random forest. We then use robust loss functions to develo more robust forest-tye regression algorithms. In the exeriments, we show by simulation and real data that our robust forests are indeed much more insensitive to outliers, and choosing the right number of nearest neighbors can quickly imrove the generalization erformance of random forest.
"
239,2017,On the Sampling Problem for Kernel Quadrature,Oral/Poster,"The standard Kernel Quadrature method for numerical integration with random oint sets (also called Bayesian Monte Carlo) is known to converge in root mean square error at a rate determined by the ratio sd, where s and d encode the smoothness and dimension of the integrand. However, an emirical investigation reveals that the rate constant C is highly sensitive to the distribution of the random oints. In contrast to standard Monte Carlo integration, for which otimal imortance samling is well-understood, the samling distribution that minimises C for Kernel Quadrature does not admit a closed form. This aer argues that the ractical choice of samling distribution is an imortant oen roblem. One solution is considered; a novel automatic aroach based on adative temering and sequential Monte Carlo. Emirical results demonstrate a dramatic reduction in integration error of u to 4 orders of magnitude can be achieved with the roosed method.
"
240,2017,Maximum Selection and Ranking under Noisy Comparisons,Oral/Poster,"We consider $(\esilon,\delta)$-PAC maximum-selection and ranking
using airwise comarisons for \nobreak{general} robabilistic models whose
comarison robabilities satisfy strong stochastic 
transitivity and stochastic triangle inequality.
Modifying the oular knockout tournament, we roose a simle
maximum-selection algorithm that uses $\mathcal{O}\left(\frac{n}{\esilon^2}
\left(1+\log \frac1{\delta}\right)\right)$ comarisons, otimal u to a constant
factor.  We then derive a general framework that uses noisy binary
search to seed u many ranking algorithms, and
combine it with merge sort to obtain a ranking algorithm that uses
$\mathcal{O}\left(\frac n{\esilon^2}\log n(\log \log n)^3\right)$ comarisons
for  $\delta=\frac1n$, otimal u to a $(\log \log n)^3$ factor."
241,2017,Sparse + Group-Sparse Dirty Models: Statistical Guarantees without Unreasonable Conditions and a Case for Non-Convexity,Oral/Poster,"Imosing sarse + grou-sarse suerosition structures in high-dimensional arameter estimation is known to rovide flexible regularization  that is more realistic for many real-world roblems. For examle, such a suerosition enables artially-shared suort sets in multi-task learning, thereby striking the right balance between arameter overla across tasks and task secificity. Existing theoretical results on estimation consistency, however, are roblematic as they require too stringent an assumtion: the incoherence between sarse and grou-sarse suerosed comonents.  In this aer, we fill the ga between the ractical success and subotimal analysis of sarse + grou-sarse models, by roviding the first consistency results that do not require unrealistic assumtions. We also study non-convex counterarts of sarse + grou-sarse models. Interestingly, we show that these are guaranteed to recover the true suort set under much milder conditions and with smaller samle size than convex models, which might be critical in ractical alications as illustrated by our exeriments.
"
242,2017,Algorithmic Stability and Hypothesis Complexity,Oral/Poster,"We introduce a notion of algorithmic stability of learning algorithms---that we term \emh{hyothesis stability}---that catures stability of the hyothesis outut by the learning algorithm in the normed sace of functions from which hyotheses are selected. The main result of the aer bounds the generalization error of any learning algorithm in terms of its hyothesis stability. The bounds are based on martingale inequalities in the Banach sace to which the hyotheses belong. We aly the general bounds to bound the erformance of some learning algorithms based on emirical risk minimization and stochastic gradient descent.
"
243,2017,Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders,Oral/Poster,"Generative models in vision have seen raid rogress due to algorithmic imrovements and the availability of high-quality image datasets. In this aer, we offer contributions in both these areas to enable similar rogress in audio modeling. First, we detail a owerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temoral codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comarable ublic datasets. Using NSynth, we demonstrate imroved qualitative and quantitative erformance of the WaveNet autoencoder over a well-tuned sectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morhing between instruments, meaningfully interolating in timbre to create new tyes of sounds that are realistic and exressive.
"
244,2017,Adaptive Sampling Probabilities for Non-Smooth Optimization,Oral/Poster,"Standard forms of coordinate and stochastic gradient methods do not adat to structure in data; their good behavior under random samling is redicated on uniformity in data. When gradients in certain blocks of features (for coordinate descent) or examles (for SGD) are larger than others, there is a natural structure that can be exloited for quicker convergence.  Yet adative variants often suffer nontrivial comutational overhead. We resent a framework that discovers and leverages such structural roerties at a low comutational cost. We emloy a bandit otimization rocedure that ""learns"" robabilities for samling coordinates or examles in (non-smooth) otimization roblems, allowing us to guarantee erformance close to that of the otimal stationary samling distribution. When such structures exist, our algorithms achieve tighter convergence guarantees than their non-adative counterarts, and we comlement our analysis with exeriments on several datasets.
"
245,2017,Confident Multiple Choice Learning,Oral/Poster,"Ensemble methods are arguably the most trustworthy techniques for boosting the erformance of machine learning models. Poular indeendent ensembles (IE) relying on naive averagingvoting scheme have been of tyical choice for most alications involving dee neural networks, but they do not consider advanced collaboration among ensemble models. In this aer, we roose new ensemble methods secialized for dee neural networks, called confident multile choice learning (CMCL): it is a variant of multile choice learning (MCL) via addressing its overconfidence issue.In articular, the roosed major comonents of CMCL beyond the original MCL scheme are (i) new loss, i.e., confident oracle loss, (ii) new architecture, i.e., feature sharing and (iii) new training method, i.e., stochastic labeling. We demonstrate the effect of CMCL via exeriments on the image classification on CIFAR and SVHN, and the foreground-background segmentation on the iCoseg. In articular, CMCL using 5 residual networks rovides 14.05% and 6.60% relative reductions in the to-1 error rates from the corresonding IE scheme for the classification task on CIFAR and SVHN, resectively.
"
246,2017,Measuring Sample Quality with Kernels,Oral/Poster,"Aroximate Markov chain Monte Carlo (MCMC) offers the romise of more raid samling at the cost of more biased inference. Since standard MCMC diagnostics fail to detect these biases, researchers have develoed comutable Stein discreancy measures that rovably determine the convergence of a samle to its target distribution. This aroach was recently combined with the theory of reroducing kernels to define a closed-form kernel Stein discreancy (KSD) comutable by summing kernel evaluations across airs of samle oints. We develo a theory of weak convergence for KSDs based on Stein’s method, demonstrate that commonly used KSDs fail to detect non-convergence even for Gaussian targets, and show that kernels with slowly decaying tails rovably determine convergence for a large class of target distributions. The resulting convergence-determining KSDs are suitable for comaring biased, exact, and deterministic samle sequences and simler to comute and arallelize than alternative Stein discreancies. We use our tools to comare biased samlers, select samler hyerarameters, and imrove uon existing KSD aroaches to one-samle hyothesis testing and samle quality imrovement.
"
247,2017,Active Learning for Top-$K$ Rank Aggregation from Noisy Comparisons,Oral/Poster,"We exlore an active to-$K$ ranking roblem based on airwise comarisons that are collected ossibly in a sequential manner as er our design choice. We consider two settings: (1) \emh{to-$K$ sorting} in which the goal is to recover the to-$K$ items in order out of $n$ items; (2) \emh{to-$K$ artitioning} where only the set of  to-$K$ items is desired. Under a fairly general model which subsumes as secial cases various models (e.g., Strong Stochastic Transitivity model, BTL model and uniform noise model), we characterize uer bounds on the samle size required for to-$K$ sorting as well as for to-$K$ artitioning. As a consequence, we demonstrate that active ranking can offer significant multilicative gains in samle comlexity over assive ranking. Deending on the underlying stochastic noise model, such gain varies  from around $\frac{\log n}{\log \log n}$ to $\frac{ n^2 \log n }{\log \log n}$. 
We also resent an algorithm that is alicable to both settings. "
248,2017,Compressed Sensing using Generative Models,Oral/Poster,"The goal of comressed sensing is to estimate a vector from an underdetermined system of noisy linear measurements, by making use of rior knowledge on the structure of vectors in the relevant domain. For almost all results in this literature, the structure is reresented by sarsity in a well-chosen basis. We show how to achieve guarantees similar to standard comressed sensing but without emloying sarsity at all.  Instead, we suose that vectors lie near the range of a generative model $G: \R^k \to \R^n$.  Our main theorem is that, if $G$ is $L$-Lischitz, then roughly $\mathcal{O}(k \log L)$ random Gaussian measurements suffice for an $\ell_2\ell_2$ recovery guarantee. We demonstrate our results using generative models from ublished variational autoencoder and generative adversarial networks. Our method can use $5$-$10$x fewer measurements than Lasso for the same accuracy."
249,2017,Consistency Analysis for Binary Classification Revisited,Oral/Poster,"Statistical learning theory is at an inflection oint enabled by recent advances in understanding and otimizing a wide range of metrics. Of articular interest are non-decomosable metrics such as the F-measure and the Jaccard measure which cannot be reresented as a simle average over examles. Non-decomosability is the rimary source of difficulty in theoretical analysis, and interestingly has led to two distinct settings and notions of consistency. In this manuscrit we analyze both settings, from statistical and algorithmic oints of view, to exlore the connections and to highlight differences between them for a wide range of metrics. The analysis comlements revious results on this toic, clarifies common confusions around both settings, and rovides guidance to the theory and ractice of binary classification with comlex metrics. 
"
250,2017,A Closer Look at Memorization in Deep Networks,Oral/Poster,"We examine the role of memorization in dee learning, drawing connections to caacity, generalization, and adversarial robustness. While dee networks are caable of memorizing noise data, our results suggest that they tend to rioritize learning simle atterns first. In our exeriments, we exose qualitative differences in gradient-based otimization of dee neural networks (DNNs) on noise vs.~real data. We also demonstrate that for aroriately tuned exlicit regularization (e.g.,~droout) we can degrade DNN training erformance on noise datasets without comromising generalization on real data. Our analysis suggests that the notions of effective caacity which are dataset indeendent are unlikely to exlain the generalization erformance of dee networks when trained with gradient based methods because training data itself lays an imortant role in determining the degree of memorization.
"
251,2017,Learning to Generate Long-term Future via Hierarchical Prediction,Oral/Poster,"We roose a hierarchical aroach for making long-term redictions of future frames. To avoid inherent comounding errors in recursive ixel-level rediction, we roose to first estimate high-level structure in the inut frames, then redict how that structure evolves in the future, and finally by observing a single frame from the ast and the redicted high-level structure, we construct the future frames without having to observe any of the ixel-level redictions. Long-term video rediction is difficult to erform by recurrently observing the redicted frames because the small errors in ixel sace exonentially amlify as redictions are made deeer into the future. Our aroach revents ixel-level error roagation from haening by removing the need to observe the redicted frames. Our model is built with a combination of LSTM and analogy based encoder-decoder convolutional neural networks, which indeendently redict the video structure and generate the future frames, resectively. In exeriments, our model is evaluated on the Human3.6M and Penn Action datasets on the task of long-term ixel-level video rediction of humans erforming actions and demonstrate significantly better results than the state-of-the-art.
"
252,2017,Sub-sampled Cubic Regularization for Non-convex Optimization,Oral/Poster,"We consider the minimization of non-convex functions that tyically arise in machine learning. Secifically, we focus our attention on a variant of trust region methods known as cubic regularization. This aroach is articularly attractive because it escaes strict saddle oints and it rovides stronger convergence guarantees than first- and second-order as well as classical trust region methods. However, it suffers from a high comutational comlexity that makes it imractical for large-scale learning. Here, we roose a novel method that uses sub-samling to lower this comutational cost. By the use of concentration inequalities we rovide a samling scheme that gives sufficiently accurate gradient and Hessian aroximations to retain the strong global and local convergence guarantees of cubically regularized methods. To the best of our knowledge this is the first work that gives global convergence guarantees for a sub-samled variant of cubic regularization on non-convex functions. Furthermore, we rovide exerimental results suorting our theory.
"
253,2017,Regret Minimization in Behaviorally-Constrained Zero-Sum Games,Oral/Poster,"No-regret learning has emerged as a owerful tool for solving extensive-form games. This was facilitated by the counterfactual-regret minimization (CFR) framework, which relies on the instantiation of regret minimizers for simlexes at each information set of the game. We use an instantiation of the CFR framework to develo algorithms for solving behaviorally-constrained (and, as a secial case, erturbed in the Selten sense) extensive-form games, which allows us to comute aroximate Nash equilibrium refinements. Nash equilibrium refinements are motivated by a major deficiency in Nash equilibrium: it rovides virtually no guarantees on how it will lay in arts of the game tree that are reached with zero robability. Refinements can mend this issue, but have not been adoted in ractice, mostly due to a lack of scalable algorithms. We show that, comared to standard algorithms, our method finds solutions that have substantially better refinement roerties, while enjoying a convergence rate that is comarable to that of state-of-the-art algorithms for Nash equilibrium comutation both in theory and ractice.
"
254,2017,Variational Boosting: Iteratively Refining Posterior Approximations,Oral/Poster,"We roose a black-box variational inference method to aroximate intractable distributions with an increasingly rich aroximating class.  Our method, variational boosting, iteratively refines an existing variational aroximation by solving a sequence of otimization roblems, allowing a trade-off between comutation time and accuracy.  We exand the variational aroximating class by incororating additional covariance structure and by introducing new comonents to form a mixture.  We aly variational boosting to synthetic and real statistical models, and show that the resulting osterior inferences comare favorably to existing variational algorithms.
"
255,2017,Learning to Align the Source Code to the Compiled Object Code,Oral/Poster,"We roose a new neural network architecture and use it for the task of statement-by-statement alignment of source code and its comiled object code.
Our architecture learns the alignment between the two sequences -- one being the translation of the other -- by maing each statement to a context-deendent reresentation vector and aligning such vectors using a grid of the two sequence domains.
Our exeriments include short C functions, both artificial and human-written, and show that our neural network architecture is able to redict the alignment with high accuracy, outerforming known baselines. We also demonstrate that our model is general and can learn to solve grah roblems such as the Traveling Salesman Problem.
"
256,2017,Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction,Oral/Poster,"Sarse suort vector machine (SVM) is a oular classification technique that can simultaneously learn a small set of the most interretable features and identify the suort vectors. It has achieved great successes in many real-world alications. However, for large-scale roblems involving a huge number of samles and extremely high-dimensional features, solving sarse SVMs remains challenging. By noting that sarse SVMs induce sarsities in both feature and samle saces, we roose a novel aroach, which is based on accurate estimations of the rimal and dual otima of sarse SVMs, to simultaneously identify the features and samles that are guaranteed to be irrelevant to the oututs. Thus, we can remove the identified inactive samles and features from the training hase, leading to substantial savings in both the memory usage and comutational cost without sacrificing accuracy. To the best of our knowledge, the roosed method is the \emh{first} \emh{static} feature and samle reduction method for sarse SVMs. Exeriments on both synthetic and real datasets (e.g., the kddb dataset with about 20 million samles and 30 million features) demonstrate that our aroach significantly outerforms state-of-the-art methods and the seedu gained by our aroach can be orders of magnitude.
"
257,2017,Distributed Mean Estimation with Limited Communication,Oral/Poster,"Motivated by the need for distributed learning and otimization algorithms with low communication cost, we study communication efficient algorithms for distributed mean estimation. Unlike revious works, we make no robabilistic assumtions on the data. We first show that for $d$ dimensional data with $n$ clients, a naive stochastic rounding aroach yields a mean squared error (MSE) of $\Theta(dn)$ and uses a constant number of bits er dimension er client. We then extend this naive algorithm in two ways: we show that alying a structured random rotation before quantization reduces the error to $\mathcal{O}((\log d)n)$ and a better coding strategy further reduces the error to $\mathcal{O}(1n)$. We also show that the latter coding strategy  is otimal u to a constant in the minimax sense i.e., it achieves the best MSE for a given communication cost. We finally demonstrate the racticality of our algorithms by alying them to distributed Lloyd's algorithm for k-means and ower iteration for PCA."
258,2017,Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study,Oral/Poster,"Dee neural networks (DNNs) have advanced erformance on a wide range of comlex tasks, raidly outacing our understanding of the nature of their solutions. While ast work sought to advance our understanding of these models, none has made use of the rich history of roblem descritions, theories, and exerimental methods develoed by cognitive sychologists to study the human mind. To exlore the otential value of these tools, we chose a well-established analysis from develomental sychology that exlains how children learn word labels for objects, and alied that analysis to DNNs. Using datasets of stimuli insired by the original cognitive sychology exeriments, we find that state-of-the-art one shot learning models trained on ImageNet exhibit a similar bias to that observed in humans: they refer to categorize objects according to shae rather than color. The magnitude of this shae bias varies greatly among architecturally identical, but differently seeded models, and even fluctuates within seeds throughout training, desite nearly equivalent classification erformance. These results demonstrate the caability of tools from cognitive sychology for exosing hidden comutational roerties of DNNs, while concurrently roviding us with a comutational model for human word learning.
"
259,2017,Sequence to Better Sequence: Continuous Revision of Combinatorial Structures,Oral/Poster,"We resent a model that, after learning on observations of (sequence, outcome) airs, can be efficiently used to revise a new sequence in order to imrove its associated outcome.  Our framework requires neither examle imrovements, nor additional evaluation of outcomes for roosed revisions.  To avoid combinatorial-search over sequence elements, we secify a generative model with continuous latent factors, which is learned via joint aroximate inference using a recurrent variational autoencoder (VAE) and an outcome-redicting neural network module.  Under this model, gradient methods can be used to efficiently otimize the continuous latent factors with resect to inferred outcomes.  By aroriately constraining this otimization and using the VAE decoder to generate a revised sequence, we ensure the revision is fundamentally similar to the original sequence, is associated with better outcomes, and looks natural. These desiderata are roven to hold with high robability under our aroach, which is emirically demonstrated for revising natural language sentences.
"
260,2017,Natasha: Faster Non-Convex Stochastic Optimization Via Strongly Non-Convex Parameter,Oral/Poster,"Given a non-convex function $f(x)$ that is an average of $n$ smooth functions, we design stochastic first-order methods to find its aroximate stationary oints. The erformance of our new methods deend on the smallest (negative) eigenvalue $-\sigma$ of the Hessian. This arameter $\sigma$ catures how strongly non-convex $f(x)$ is, and is analogous to the strong convexity arameter for convex otimization.

At least in theory, our methods outerform known results for a range of arameter $\sigma$, and can also be used to find aroximate local minima. Our result imlies an interesting dichotomy: there exists a threshold $\sigma_0$ so that the (currently) fastest methods for $\sigma\sigma_0$ and for $\sigma\sigma_0$ have different behaviors: the former scales with $n^{23}$ and the latter scales with $n^{34}$.
"
261,2017,Reduced Space and Faster Convergence in Imperfect-Information Games via Pruning,Oral/Poster,"Iterative algorithms such as Counterfactual Regret Minimization (CFR) are the most oular way to solve large zero-sum imerfect-information games. In this aer we introduce Best-Resonse Pruning (BRP), an imrovement to iterative algorithms such as CFR that allows oorly-erforming actions to be temorarily runed. We rove that when using CFR in zero-sum games, adding BRP will asymtotically rune any action that is not art of a best resonse to some Nash equilibrium. This leads to rovably faster convergence and lower sace requirements. Exeriments show that BRP results in a factor of 7 reduction in sace, and the reduction factor increases with game size.
"
262,2017,Lost Relatives of the Gumbel Trick,Oral/Poster,"The Gumbel trick is a method to samle from a discrete robability distribution, or to estimate its normalizing artition function. The method relies on reeatedly alying a random erturbation to the distribution in a articular way, each time solving for the most likely configuration. We derive an entire family of related methods, of which the Gumbel trick is one member, and show that the new methods have suerior roerties in several settings with minimal additional comutational cost. In articular, for the Gumbel trick to yield comutational benefits for discrete grahical models, Gumbel erturbations on all configurations are tyically relaced with so-called low-rank erturbations. We show how a subfamily of our new methods adats to this setting, roving new uer and lower bounds on the log artition function and deriving a family of sequential samlers for the Gibbs distribution. Finally, we balance the discussion by showing how the simler analytical form of the Gumbel trick enables additional theoretical results.
"
263,2017,RobustFill: Neural Program Learning under Noisy I/O,Oral/Poster,"The roblem of automatically generating a comuter rogram from some secification has been studied since the early days of AI. Recently, two cometing aroaches for 'automatic rogram learning' have received significant attention: (1) 'neural rogram synthesis', where a neural network is conditioned on inutoutut (IO) examles and learns to generate a rogram, and (2) 'neural rogram induction', where a neural network generates new oututs directly using a latent rogram reresentation. Here, for the first time, we directly comare both aroaches on a large-scale, real-world learning task and we additionally contrast to rule-based rogram synthesis, which uses hand-crafted semantics to guide the rogram generation. 
Our neural models use a modified attention RNN to allow encoding of variable-sized sets of IO airs, which achieve 92% accuracy on a real-world test set, comared to the 34% accuracy of the revious best neural synthesis aroach. The synthesis model also outerforms a comarable induction model on this task, but we more imortantly demonstrate that the strength of each aroach is highly deendent on the evaluation metric and end-user alication. Finally, we show that we can train our neural models to remain very robust to the tye of noise exected in real-world data (e.g., tyos), while a highly-engineered rule-based system fails entirely.
"
264,2017,Efficient Distributed Learning with Sparsity,Oral/Poster,"We roose a novel, efficient aroach for distributed sarse learning with observations randomly artitioned across machines. In each round of the roosed method, worker machines comute the gradient of the loss on local data and the master machine solves a shifted $\ell_1$ regularized loss minimization roblem. After a number of communication rounds that scales only logarithmically with the number of machines, and indeendent of other arameters of the roblem, the roosed aroach rovably matches the estimation error bound of centralized methods."
265,2017,Nonparanormal Information Estimation,Oral/Poster,"We study the roblem of using i.i.d. samles from an unknown multivariate robability distribution  to estimate the mutual information of . This roblem has recently received attention in two settings: (1) where  is assumed to be Gaussian and (2) where  is assumed only to lie in a large nonarametric smoothness class. Estimators roosed for the Gaussian case converge in high dimensions when the Gaussian assumtion holds, but are brittle, failing dramatically when  is not Gaussian, while estimators roosed for the nonarametric case fail to converge with realistic samle sizes excet in very low dimension. Hence, there is a lack of robust mutual information estimators for many realistic data. To address this, we roose estimators for mutual information when  is assumed to be a nonaranormal (or Gaussian coula) model, a semiarametric comromise between Gaussian and nonarametric extremes. Using theoretical bounds and exeriments, we show these estimators strike a ractical balance between robustness and scalability.
"
266,2017,Visualizing and Understanding Multilayer Perceptron Models: A Case Study in Speech Processing,Oral/Poster,"Desite the recent success of dee learning, the nature of the transformations they aly to the inut features remains oorly understood. This study rovides an emirical framework to study the encoding roerties of node activations in various layers of the network, and to construct the exact function alied to each data oint in the form of a linear transform. These methods are used to discern and quantify roerties of feed-forward neural networks trained to ma acoustic features to honeme labels. We show a selective and nonlinear waring of the feature sace, achieved by forming rototyical functions to account for the ossible variation of each class. This study rovides a joint framework where the roerties of node activations and the functions imlemented by the network can be linked together. 
"
267,2017,Tensor-Train Recurrent Neural Networks for Video Classification,Oral/Poster,"The Recurrent Neural Networks and their variants have shown romising erformances in sequence modeling tasks such as Natural Language Processing. These models, however, turn out to be imractical and difficult to train when exosed to very high-dimensional inuts due to the large inut-to-hidden weight matrix. This may have revented RNNs' large-scale alication in tasks that involve very high inut dimensions such as video modeling; current aroaches reduce the inut dimensions using various feature extractors. To address this challenge, we roose a new, more general and efficient aroach by factorizing the inut-to-hidden weight matrix using Tensor-Train decomosition which is trained simultaneously with the weights themselves. We test our model on classification tasks using multile real-world video datasets and achieve cometitive erformances with state-of-the-art models, even though our model architecture is orders of magnitude less comlex. We believe that the roosed aroach rovides a novel and fundamental building block for modeling high-dimensional sequential data with RNN architectures and oens u many ossibilities to transfer the exressive and advanced architectures from other domains such as NLP to modeling high-dimensional sequential data. 
"
268,2017,“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions,Oral/Poster,"We develo and analyze a variant of Nesterov's accelerated gradient descent (AGD) for minimization of smooth non-convex functions. We rove that one of two cases occurs: either our AGD variant converges quickly, as if the function was convex, or we roduce a certificate that the function is “guilty” of being non-convex. This non-convexity certificate allows us to exloit negative curvature and obtain deterministic, dimension-free acceleration of convergence for non-convex functions. For a function $f$ with Lischitz continuous gradient and Hessian, we comute a oint $x$ with $\|\nabla f(x)\| \le \esilon$ in $O(\esilon^{-74} \log(1 \esilon) )$ gradient and function evaluations.  Assuming additionally that the third derivative is Lischitz, we require only $O(\esilon^{-53} \log(1 \esilon) )$ evaluations."
269,2017,Strongly-Typed Agents are Guaranteed to Interact Safely,Oral/Poster,"As artificial agents roliferate, it is becoming increasingly imortant to ensure that their interactions with one another are well-behaved. In this aer, we formalize a common-sense notion of when algorithms are well-behaved: an algorithm is safe if it does no harm. Motivated by recent rogress in dee learning, we focus on the secific case where agents udate their actions according to gradient descent. The aer shows that that gradient descent converges to a Nash equilibrium in safe games. The main contribution is to define strongly-tyed agents and show they are guaranteed to interact safely, thereby roviding sufficient conditions to guarantee safe interactions. A series of examles show that strong-tying generalizes certain key features of convexity, is closely related to blind source searation, and introduces a new ersective on classical multilinear games based on tensor decomosition. 
"
270,2017,Learning to Aggregate Ordinal Labels by Maximizing Separating Width,Oral/Poster,"While crowdsourcing has been a cost and time efficient method to label massive samles, one critical issue is quality control, for which the key challenge is to infer the ground truth from noisy or even adversarial data by various users. A large class of crowdsourcing roblems, such as those involving age, grade, level, or stage, have an ordinal structure in their labels. Based on a technique of samling estimated label from the osterior distribution, we define a novel searating width among the labeled observations to characterize the quality of samled labels, and develo an efficient algorithm to otimize it through solving multile linear decision boundaries and adjusting rior distributions. Our algorithm is emirically evaluated on several real world datasets, and demonstrates its suremacy over state-of-the-art methods.
"
271,2017,Programming with a Differentiable Forth Interpreter,Oral/Poster,"Given that in ractice training data is scarce for all but a small set of roblems, a core question is how to incororate rior knowledge into a model. In this aer, we consider the case of rior rocedural knowledge for neural networks, such as knowing how a rogram should traverse a sequence, but not what local actions should be erformed at each ste. To this end, we resent an end-to-end differentiable interreter for the rogramming language Forth which enables rogrammers to write rogram sketches with slots that can be filled with behaviour trained from rogram inut-outut data. We can otimise this behaviour directly through gradient descent techniques on user-secified objectives, and also integrate the rogram into any larger neural comutation grah. We show emirically that our interreter is able to effectively leverage different levels of rior rogram structure and learn comlex behaviours such as sequence sorting and addition. When connected to oututs of an LSTM and trained jointly, our interreter achieves state-of-the-art accuracy for end-to-end reasoning about quantities exressed in natural language stories.
"
272,2017,Innovation Pursuit: A New Approach to the Subspace Clustering Problem,Oral/Poster,"This aer resents a new scalable aroach, termed Innovation Pursuit (iPursuit), to the roblem of subsace clustering. iPursuit rests on a new geometrical idea whereby each subsace is identified based on its novelty with resect to the other subsaces. The subsaces are identified consecutively by solving a series of simle linear otimization roblems, each searching for a direction of innovation in the san of the data. A detailed mathematical analysis is rovided establishing sufficient conditions for the roosed aroach to correctly cluster the data oints. Moreover, the roosed direction search aroach can be integrated with sectral clustering to yield a new variant of sectral-clustering-based algorithms. Remarkably, the roosed aroach can rovably yield exact clustering even when the subsaces have significant intersections. The numerical simulations demonstrate that iPursuit can often outerform the state-of-the-art subsace clustering algorithms – more so for subsaces with significant intersections – along with substantial reductions in comutational comlexity.
"
273,2017,A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions,Oral/Poster,"Symmetric distribution roerties such as suort size, suort coverage, entroy, and roximity to uniformity, arise in many alications. Recently, researchers alied different estimators and analysis tools to derive asymtotically samle-otimal aroximations for each of these roerties. We show that a single, simle, lug-in
estimator---\emh{rofile maximum likelihood (PML)}--is samle cometitive for all symmetric roerties, and in articular is asymtotically samle-otimal for all the above roerties.
"
274,2017,Axiomatic Attribution for Deep Networks,Oral/Poster,"We study the roblem of attributing the rediction of a dee network to its inut features, a roblem reviously studied by several other
works. We identify two fundamental axioms—Sensitivity and Imlementation Invariance that
attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simle to imlement;
it just needs a few calls to the standard gradient oerator. We aly this method to a coule of image models, a coule of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network,
and to enable users to engage with models better.
"
275,2017,Sequence Modeling via Segmentations,Oral/Poster,"Segmental structure is a common attern in many tyes of sequences such as hrases in human languages. In this aer, we resent a robabilistic model for sequences via their segmentations. The robability of a segmented sequence is calculated as the roduct of the robabilities of all its segments,  where each segment is modeled using existing tools such as recurrent neural networks. Since the segmentation of a sequence is usually unknown in advance, we sum over all valid segmentations to obtain the final robability for the sequence. An efficient dynamic rogramming algorithm is develoed for  forward and backward comutations without resorting to any aroximation. We demonstrate our aroach on text segmentation and seech recognition tasks. In addition to quantitative results, we also show that our aroach can discover meaningful segments in their resective alication contexts.
"
276,2017,Convergence Analysis of Proximal Gradient with Momentum for Nonconvex Optimization,Oral/Poster,"In this work, we investigate the accelerated roximal gradient method for nonconvex rogramming (APGnc). The method comares between a usual roximal gradient ste and a linear extraolation ste, and accets the one that has a lower function value to achieve a monotonic decrease. In secific, under a general nonsmooth and nonconvex setting, we rovide a rigorous argument to show that the limit oints of the sequence generated by APGnc are critical oints of the objective function. Then, by exloiting the Kurdyka-Lojasiewicz (KL) roerty for a broad class of functions, we establish the linear and sub-linear convergence rates of the function value sequence generated by APGnc. We further roose a stochastic variance reduced APGnc (SVRG-APGnc), and establish its linear convergence under a secial case of the KL roerty. We also extend the analysis to the inexact version of these methods and develo an adative momentum strategy that imroves the numerical erformance.
"
277,2017,Coordinated Multi-Agent Imitation Learning,Oral/Poster,"We study the roblem of imitation learning from demonstrations of multile coordinating agents. One key challenge in this setting is that learning a good model of coordination can be difficult, since coordination is often imlicit in the demonstrations and must be inferred as a latent variable.  We roose a joint aroach that simultaneously learns a latent coordination model along with the individual olicies. In articular, our method integrates unsuervised structure learning with conventional imitation learning. We illustrate the ower of our aroach on a difficult roblem of learning multile olicies for fine-grained behavior modeling in team sorts, where different layers occuy different roles in the coordinated team strategy.  We show that having a coordination model to infer the roles of layers  yields substantially imroved imitation loss comared to conventional baselines.
"
278,2017,Uncorrelation and Evenness: a New Diversity-Promoting Regularizer,Oral/Poster,"Latent sace models (LSMs) rovide a rinciled and effective way to extract hidden atterns from observed data. To coe with two challenges in LSMs: (1) how to cature infrequent atterns when attern frequency is imbalanced and (2) how to reduce model size without sacrificing their exressiveness, several studies have been roosed
to ""diversify"" LSMs, which design regularizers to encourage the comonents therein to be ""diverse"". In light of the limitations of existing aroaches, we design a new diversity-romoting regularizer by considering two factors: uncorrelation and evenness, which encourage the comonents to be uncorrelated and to lay equally imortant roles in modeling data. Formally, this amounts to encouraging the covariance matrix of the comonents to have more uniform eigenvalues. We aly the regularizer to two LSMs and develo an efficient otimization algorithm. Exeriments on healthcare, image and text data demonstrate the effectiveness of the regularizer.
"
279,2017,Differentiable Programs with Neural Libraries,Oral/Poster,"We develo a framework for combining differentiable rogramming languages with neural networks. Using this framework we create end-to-end trainable systems that learn to write interretable algorithms with ercetual comonents. We exlore the benefits of inductive biases for strong generalization and modularity that come from the rogram-like structure of our models. In articular, modularity allows us to learn a library of (neural) functions which grows and imroves as more tasks are solved. Emirically, we show that this leads to lifelong learning systems that transfer knowledge to new tasks more effectively than baselines.
"
280,2017,Selective Inference for Sparse High-Order Interaction Models,Oral/Poster,"Finding statistically significant high-order interactions in redictive modeling is imortant but challenging task because the ossible number of high-order interactions is extremely large (e.g., $ 10^{17}$). In this aer we study feature selection and statistical inference for sarse high-order interaction models. Our main contribution is to extend recently develoed selective inference framework for linear models to high-order interaction models by develoing a novel algorithm for efficiently characterizing the selection event for the selective inference of high-order interactions.  We demonstrate the effectiveness of the roosed algorithm by alying it to an HIV drug resonse rediction roblem."
281,2017,Gradient Coding: Avoiding Stragglers in Distributed Learning,Oral/Poster,"We roose a novel coding theoretic framework for mitigating stragglers in distributed learning. We show how carefully relicating data blocks and coding across gradients can rovide tolerance to failures and stragglers for synchronous Gradient Descent. We imlement our schemes in ython (using MPI) to run on Amazon EC2, and show how we comare against baseline aroaches in running time and generalization error.
"
282,2017,On Calibration of Modern Neural Networks,Oral/Poster,"Confidence calibration -- the roblem of redicting robability estimates reresentative of the true correctness likelihood -- is imortant for classification models in many alications. We discover that modern neural networks, unlike those from a decade ago, are oorly calibrated. Through extensive exeriments, we observe that deth, width, weight decay, and Batch Normalization are imortant factors influencing calibration. We evaluate the erformance of various ost-rocessing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and exeriments not only offer insights into neural network learning, but also rovide a simle and straightforward recie for ractical settings: on most datasets, temerature scaling -- a single-arameter variant of Platt Scaling -- is surrisingly effective at calibrating redictions.
"
283,2017,Latent LSTM Allocation: Joint clustering and non-linear dynamic modeling of sequence data,Oral/Poster,"Recurrent neural networks, such as long-short term memory (LSTM) networks, are owerful tools for modeling sequential data like user browsing history (Tan et al., 2016; Korusik et al., 2016) or natural language text (Mikolov et al., 2010). However, to generalize across different user tyes, LSTMs require a large number of arameters, notwithstanding the simlicity of the underlying dynamics, rendering it uninterretable, which is highly undesirable in user modeling. The increase in comlexity and arameters arises due to a large action sace in which many of the actions have similar intent or toic. In this aer, we introduce Latent LSTM Allocation (LLA) for user modeling combining hierarchical Bayesian models with LSTMs. In LLA, each user is modeled as a sequence of actions, and the model jointly grous actions into toics and learns the temoral dynamics over the toic sequence, instead of action sace directly. This leads to a model that is highly interretable, concise, and can cature intricate dynamics. We resent an efficient Stochastic EM inference algorithm for our model that scales to millions of usersdocuments. Our exerimental evaluations show that the roosed model comares favorably with several state-of-the-art baselines.
"
284,2017,How to Escape Saddle Points Efficiently,Oral/Poster,"This aer shows that a erturbed form of gradient descent converges to a second-order stationary oint in a number iterations which deends only oly-logarithmically on dimension (i.e., it is almost ``dimension-free''). The convergence rate of this rocedure matches the well-known convergence rate of gradient descent to first-order stationary oints, u to log factors. When all saddle oints are non-degenerate, all second-order stationary oints are local minima, and our result thus shows that erturbed gradient descent can escae saddle oints almost for free. Our results can be directly alied to many machine learning alications, including dee learning. As a articular concrete examle of such an alication, we show that our results can be used directly to establish shar global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle oints, which may be of indeendent interest to the non-convex otimization community.
"
285,2017,Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability,Oral/Poster,"Many real-world tasks involve multile agents with artial observability and limited communication. Learning is challenging in these settings due to local viewoints of agents, which erceive the world as non-stationary due to concurrently-exloring teammates. Aroaches that learn secialized olicies for individual tasks face roblems when alied to the real world: not only do agents have to learn and store distinct olicies for each task, but in ractice identities of tasks are often non-observable, making these aroaches inalicable. This aer formalizes and addresses the roblem of multi-task multi-agent reinforcement learning under artial observability. We introduce a decentralized single-task learning aroach that is robust to concurrent interactions of teammates, and resent an aroach for distilling single-task olicies into a unified olicy that erforms well across multile related tasks, without exlicit rovision of task identity.
"
286,2017,Learning Latent Space Models with Angular Constraints,Oral/Poster,"The large model caacity of latent sace models (LSMs) enables them to achieve great erformance on various alications, but meanwhile renders LSMs to be rone to overfitting. Several recent studies investigate a new tye of regularization aroach, which encourages comonents in LSMs to be diverse, for the sake of alleviating overfitting. While they have shown romising emirical effectiveness, in theory why larger ""diversity"" results in less overfitting is still unclear. To bridge this ga, we roose a new diversity-romoting aroach that is both theoretically analyzable and emirically effective. Secifically, we use near-orthogonality to characterize ""diversity"" and imose angular constraints (ACs) on the comonents of LSMs to romote diversity. A generalization error analysis shows that larger diversity results in smaller estimation error and larger aroximation error. An efficient ADMM algorithm is develoed to solve the constrained LSM roblems. Exeriments demonstrate that ACs imrove generalization erformance of LSMs and outerform other diversity-romoting aroaches.
"
287,2017,Developing Bug-Free Machine Learning Systems With Formal Mathematics,Oral/Poster,"Noisy data, non-convex objectives, model missecification, and numerical instability can all cause undesired behaviors in machine learning systems.  As a result, detecting actual imlementation errors can be extremely difficult.  We demonstrate a methodology in which develoers use an interactive roof assistant to both imlement their system and to state a formal theorem defining what it means for their system to be correct.  The rocess of roving this theorem interactively in the roof assistant exoses all imlementation errors since any error in the rogram would cause the roof to fail.  As a case study, we imlement a new system, Certigrad, for otimizing over stochastic comutation grahs, and we generate a formal (i.e. machine-checkable) roof that the gradients samled by the system are unbiased estimates of the true mathematical gradients.  We train a variational autoencoder using Certigrad and find the erformance comarable to training the same model in TensorFlow.
"
288,2017,Dictionary Learning Based on Sparse Distribution Tomography,Oral/Poster,"We roose a new statistical dictionary learning algorithm for sarse signals that is based on an $\alha$-stable innovation model. The arameters of the underlying model---that is, the atoms of the dictionary, the sarsity index $\alha$ and the disersion of the transform-domain coefficients---are recovered
using a new tye of robability distribution tomograhy. Secifically, we drive our estimator with a series of random rojections of the data, which results in an efficient algorithm.
Moreover, since the rojections are achieved using linear combinations, we can invoke the generalized central limit theorem to justify the use of our method for sarse signals that are not necessarily $\alha$-stable. 
We evaluate our algorithm by erforming two tyes of exeriments: image in-ainting and image denoising. In both cases, we find that our aroach is cometitive with state-of-the-art dictionary learning techniques. 
Beyond the algorithm itself, two asects of this study are interesting in their own right. The first is our statistical formulation of the roblem, which unifies the toics of dictionary learning and indeendent comonent analysis. The second is a generalization of a classical theorem about isometries of $\ell_$-norms that constitutes the foundation of our aroach.
"
289,2017,Learning Discrete Representations via Information Maximizing Self-Augmented Training,Oral/Poster,"Learning discrete reresentations of data is a central machine learning task because of the comactness of the reresentations and ease of interretation. The task includes clustering and hash learning as secial cases. Dee neural networks are romising to be used because they can model the non-linearity of data and scale to large datasets. However, their model comlexity is huge, and therefore, we need to carefully regularize the networks in order to learn useful reresentations that exhibit intended invariance for alications of interest. To this end, we roose a method called Information Maximizing Self-Augmented Training (IMSAT). In IMSAT, we use data augmentation to imose the invariance on discrete reresentations. More secifically, we encourage the redicted reresentations of augmented data oints to be close to those of the original data oints in an end-to-end fashion. At the same time, we maximize the information-theoretic deendency between data and their redicted discrete reresentations. Extensive exeriments on benchmark datasets show that IMSAT roduces state-of-the-art results for both clustering and unsuervised hash learning.
"
290,2017,"Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging",Oral/Poster,"We address the statistical and otimization imacts of using classical sketch versus Hessian sketch to solve aroximately the Matrix Ridge Regression (MRR) roblem. Prior research has considered the effects of classical sketch on least squares regression (LSR), a strictly simler roblem. We establish that classical sketch has a similar effect uon the otimization roerties of MRR as it does on those of LSR---namely, it recovers nearly otimal solutions. In contrast, Hessian sketch does not have this guarantee; instead, the aroximation error is governed by a subtle interlay between the ``mass'' in the resonses and the otimal objective value. For both tyes of aroximations, the regularization in the sketched MRR roblem gives it significantly different statistical roerties from the sketched LSR roblem. In articular, there is a bias-variance trade-off in sketched MRR that is not resent in sketched LSR. We rovide uer and lower bounds on the biases and variances of sketched MRR; these establish that the variance is significantly increased when classical sketches are used, while the bias is significantly increased when using Hessian sketches. Emirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the otimal MRR solutions. We establish theoretically and emirically that model averaging greatly decreases this ga. Thus, in the distributed setting, sketching combined with model averaging is a owerful technique that quickly obtains near-otimal solutions to the MRR roblem while greatly mitigating the statistical risks incurred by sketching.
"
291,2017,Estimating the unseen from multiple populations,Oral/Poster,"Given samles from a distribution, how many new elements should we exect to find if we kee on samling this distribution? This is an imortant and actively studied roblem, with many alications ranging from secies estimation to genomics. We generalize this extraolation and related unseen estimation roblems to the multile oulation setting, where oulation $j$ has an unknown distribution $D_j$ from which we observe $n_j$ samles. We derive an otimal estimator for the total number of elements we exect to find among new samles across the oulations. Surrisingly, we rove that our estimator's accuracy is indeendent of the number of oulations. We also develo an efficient otimization algorithm to solve the more general roblem of estimating multi-oulation frequency distributions. We validate our methods and theory through extensive exeriments. Finally, on a real dataset of human genomes across multile ancestries, we demonstrate how our aroach for unseen estimation can enable cohort designs that can discover interesting mutations with greater efficiency."
292,2017,Meritocratic Fairness for Cross-Population Selection,Oral/Poster,"We consider the roblem of selecting a strong ool of individuals from several oulations with incomarable skills (e.g. soccer layers, mathematicians, and singers) in a fair manner.  The quality of an individual is defined to be their relative rank (by cumulative distribution value) within their own oulation, which ermits cross-oulation comarisons.  We study algorithms which attemt to select the highest quality subset desite the fact that true CDF values are not known, and can only be estimated from the finite ool of candidates.  Secifically, we quantify the regret in quality imosed by ""meritocratic"" notions of fairness, which require that individuals are selected with robability that is monotonically increasing in their true quality. We give algorithms with rovable fairness and regret guarantees, as well as lower bounds, and rovide emirical results which suggest that our algorithms erform better than the theory suggests.  We extend our results to a sequential batch setting, in which an algorithm must reeatedly select subsets of individuals from new ools of alicants, but has the benefit of being able to comare them to the accumulated data from revious rounds.
"
293,2017,Neural networks and rational functions,Oral/Poster,"Neural networks and rational functions efficiently aroximate each other.  In more detail, it is shown here that for any ReLU network, there exists a rational function of degree $O(olylog(1\esilon))$ which is $\esilon$-close, and similarly for any rational function there exists a ReLU network of size $O(olylog(1\esilon))$ which is $\esilon$-close.  By contrast, olynomials need degree $\Omega(oly(1\esilon))$ to aroximate even a single ReLU.  When converting a ReLU network to a rational function as above, the hidden constants deend exonentially on the number of layers, which is shown to be tight;  in other words, a comositional reresentation can be beneficial even for rational functions.
"
294,2017,Input Convex Neural Networks,Oral/Poster,"This aer resents the inut convex neural network
architecture. These are scalar-valued (otentially dee) neural
networks with constraints on the network arameters such that the
outut of the network is a convex function of (some of) the inuts.
The networks allow for efficient inference via otimization over some
inuts to the network given others, and can be alied to settings
including structured rediction, data imutation, reinforcement
learning, and others.  In this aer we lay the basic groundwork for
these models, roosing methods for inference, otimization and
learning, and analyze their reresentational ower.  We show that many
existing neural network architectures can be made inut-convex with
a minor modification, and develo secialized otimization
algorithms tailored to this setting. Finally, we highlight the
erformance of the methods on multi-label rediction, image
comletion, and reinforcement learning roblems, where we show
imrovement over the existing state of the art in many cases.
"
295,2017,Co-clustering through Optimal Transport,Oral/Poster,"In this aer, we resent a novel method for co-clustering, an unsuervised learning aroach that aims at discovering homogeneous grous of data instances and features by grouing them simultaneously. The roosed method uses the entroy regularized otimal transort between emirical measures defined on data instances and features in order to obtain an estimated joint robability density function reresented by the otimal couling matrix. This matrix is further factorized to obtain the induced row and columns artitions using multiscale reresentations aroach. To justify our method theoretically, we show how the solution of the regularized otimal transort can be seen from the variational inference ersective thus motivating its use for co-clustering. The algorithm derived for the roosed method and its kernelized version based on the notion of Gromov-Wasserstein distance are fast, accurate and can determine automatically the number of both row and column clusters. These features are vividly demonstrated through extensive exerimental evaluations. 
"
296,2017,OptNet: Differentiable Optimization as a Layer in Neural Networks,Oral/Poster,"This aer resents OtNet, a network architecture that integrates
  otimization roblems (here, secifically in the form of quadratic rograms)
  as individual layers in larger end-to-end trainable dee networks.
  These layers encode constraints and comlex deendencies
  between the hidden states that traditional convolutional and
  fully-connected layers often cannot cature.
  In this aer, we exlore the foundations for such an architecture:
  we show how techniques from sensitivity analysis, bilevel
  otimization, and imlicit differentiation can be used to
  exactly differentiate through these layers and with resect
  to layer arameters;
  we develo a highly efficient solver for these layers that exloits fast
  GPU-based batch solves within a rimal-dual interior oint method, and which
  rovides backroagation gradients with virtually no additional cost on to of
  the solve;
  and we highlight the alication of these aroaches in several roblems.
  In one notable examle, we show that the method is
  caable of learning to lay mini-Sudoku (4x4) given just inut and outut games,
  with no a riori information about the rules of the game;
  this highlights the ability of our architecture to learn hard
  constraints better than other neural architectures.
"
297,2017,Multiple Clustering Views from Multiple Uncertain Experts,Oral/Poster,"Exert inut can imrove clustering erformance. In today's collaborative 
environment, the availability of crowdsourced multile exert inut is 
becoming common. Given multile exerts' inuts, most existing aroaches can 
only discover one clustering structure. However, data is multi-faced by nature 
and can be clustered in different ways (also known as views). 
In an exloratory analysis roblem where ground truth is not known, different 
exerts may have diverse views on how to cluster data.
In this aer, we address the roblem on how to automatically discover 
multile ways to cluster data given otentially diverse inuts from multile 
uncertain exerts. We roose a novel Bayesian robabilistic model that 
automatically learns the multile exert views and the clustering structure 
associated with each view. The benefits of learning the exerts' views include 
1) enabling the discovery of multile diverse clustering structures, and
2) imroving the quality of clustering solution in each view by assigning 
higher weights to exerts with higher confidence.
In our aroach, the exert views, multile clustering structures and 
exert confidences are jointly learned via variational inference.
Exerimental results on synthetic datasets, benchmark datasets and a
real-world disease subtying roblem show that our roosed aroach outerforms
cometing baselines, including meta clustering, semi-suervised clustering,
semi-crowdsourced clustering and consensus clustering.
"
298,2017,Parseval Networks: Improving Robustness to Adversarial Examples,Oral/Poster,"We introduce Parseval networks, a form of dee neural networks in which the Lischitz constant of linear, convolutional and aggregation layers is constrained to be smaller than $1$. Parseval networks are emirically and theoretically motivated by an analysis of the robustness of the redictions made by dee neural networks when their inut is subject to an adversarial erturbation. The most imortant feature of Parseval networks is to maintain weight matrices of linear and convolutional layers to be (aroximately) Parseval tight frames, which are extensions of orthogonal matrices to non-square matrices. We describe how these constraints can be maintained efficiently during SGD. We show that Parseval networks match the state-of-the-art regarding accuracy on CIFAR-10100 and Street View House Numbers (SVHN), while being more robust than their vanilla counterart against adversarial examles. Incidentally, Parseval networks also tend to train faster and make a better usage of the full caacity of the networks."
299,2017,"Clustering by Sum of Norms: Stochastic Incremental Algorithm, Convergence and Cluster Recovery",Oral/Poster,"Standard clustering methods such as K-means,  Gaussian mixture models, and hierarchical clustering are beset by local minima, which are sometimes drastically subotimal. Moreover the number of clusters K must be known in advance. The recently introduced the sum-of-norms (SON) or Clusterath convex relaxation of k-means and hierarchical clustering shrinks cluster centroids toward one another and ensure a unique global minimizer. We give a scalable stochastic incremental algorithm based on roximal iterations to solve the SON roblem with convergence guarantees. We also show that the algorithm recovers clusters under quite general conditions which have a similar form to the unifying roximity condition introduced in the aroximation algorithms community (that covers aradigm cases such as Gaussian mixtures and lanted artition models). We give exerimental results to confirm that our algorithm scales much better than revious methods while roducing clusters of comarable quality.
"
300,2017,Regularising Non-linear Models Using Feature Side-information,Oral/Poster,"Very often features come with their own vectorial descritions which rovide detailed information about their roerties. We refer to these vectorial descritions as feature side-information. In the standard learning scenario, inut is reresented as a vector of features and the feature side-information is most often ignored or used only for feature selection rior to model fitting. We believe that feature side-information which carries information about features intrinsic roerty will hel imrove model rediction if used in a roer way during learning rocess. In this aer, we roose a framework that allows for the incororation of the feature side-information during the learning of very general model families to imrove the rediction erformance. We control the structures of the learned models so that they reflect features' similarities as these are defined on the basis of the side-information. We erform exeriments on a number of benchmark datasets which show significant redictive erformance gains, over a number of baselines, as a result of the exloitation of the side-information.
"
301,2017,Clustering High Dimensional Dynamic Data Streams,Oral/Poster,"We resent data streaming algorithms for the $k$-median roblem in high-dimensional dynamic geometric data streams, i.e. streams allowing both insertions and deletions of oints from a discrete Euclidean sace $\{1, 2, \ldots \Delta\}^d$.
Our algorithms use $k \esilon^{-2} \oly(d \log \Delta)$ sacetime and maintain with high robability a small weighted set 
of oints (a coreset) such that for every set of $k$ centers the cost of the coreset $(1+\esilon)$-aroximates the cost of the streamed oint set.
We also rovide algorithms that guarantee only ositive weights in the coreset with additional logarithmic factors in the sace and time comlexities.
We can use this ositively-weighted coreset to comute a $(1+\esilon)$-aroximation for the $k$-median roblem 
by any efficient offline $k$-median algorithm.  
All revious algorithms for comuting a $(1+\esilon)$-aroximation for the $k$-median roblem over dynamic data streams required sace and time exonential in $d$.
Our algorithms can be generalized to metric saces of bounded doubling dimension."
302,2017,Fast k-Nearest Neighbour Search via Prioritized DCI,Oral/Poster,"Most exact methods for k-nearest neighbour search suffer from the curse of dimensionality; that is, their query times exhibit exonential deendence on either the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing (DCI) offers a romising way of circumventing the curse by avoiding sace artitioning and achieves a query time that grows sublinearly in the intrinsic dimensionality. In this aer, we roose a variant of DCI, which we call Prioritized DCI, and show a remarkable imrovement in the deendence of query time on intrinsic dimensionality. In articular, a linear increase in intrinsic dimensionality, which could mean an exonential increase in the number of oints near a query, can be mostly counteracted with just a linear increase in sace. We also demonstrate emirically that Prioritized DCI significantly outerforms rior methods. In articular, relative to Locality-Sensitive Hashing (LSH), Prioritized DCI reduces the number of distance evaluations by a factor of 14 to 116 and the memory consumtion by a factor of 21. 
"
303,2017,Deep Spectral Clustering Learning,Oral/Poster,"Clustering is the task of grouing a set of examles so that similar examles are groued into the same cluster while dissimilar examles are in different clusters. The quality of a clustering deends on two roblem-deendent factors which are i) the chosen similarity metric and ii) the data reresentation. Suervised clustering aroaches, which exloit labeled artitioned datasets have thus been roosed, for instance to learn a metric otimized to erform clustering. However, most of these aroaches assume that the reresentation of the data is fixed and then learn an aroriate linear transformation. Some dee suervised clustering learning aroaches have also been roosed. However, they rely on iterative methods to comute gradients resulting in high algorithmic comlexity. In this aer, we roose a dee suervised clustering metric learning method that formulates a novel loss function. We derive a closed-form exression for the gradient that is efficient to comute: the comlexity to comute the gradient is linear in the size of the training mini-batch and quadratic in the reresentation dimensionality. We further reveal how our aroach can be seen as learning sectral clustering. Exeriments on standard real-world datasets confirm state-of-the-art Recall@K erformance.
"
304,2017,Joint Dimensionality Reduction and Metric Learning: A Geometric Take,Oral/Poster,"To be tractable and robust to data noise, existing metric learning algorithms commonly rely on PCA as a re-rocessing ste. How can we know, however, that PCA, or any other secific dimensionality reduction technique, is the method of choice for the roblem at hand? The answer is simle: We cannot! To address this issue, in this aer, 
we develo a Riemannian framework to jointly learn a maing erforming dimensionality reduction and a metric in the induced sace.
Our exeriments evidence that, while we directly work on high-dimensional features, our aroach yields cometitive runtimes with and higher accuracy than state-of-the-art metric learning algorithms.
"
305,2017,ProtoNN: Compressed and Accurate kNN for Resource-scarce Devices,Oral/Poster,"Several real-world alications require real-time rediction on resource-scarce devices such as an Internet of Things (IoT) sensor. Such alications demand rediction models with small storage and comutational comlexity that do not comromise significantly on accuracy.  In this work, we roose ProtoNN, a novel algorithm that addresses the roblem of real-time and accurate rediction on resource-scarce devices. ProtoNN is insired by k-Nearest Neighbor (KNN) but has several orders lower storage and rediction comlexity. ProtoNN models can be deloyed even on devices with uny storage and comutational ower (e.g. an Arduino UNO with 2kB RAM) to get excellent rediction accuracy. 
ProtoNN derives its strength from three key ideas: a) learning a small number of rototyes to reresent the entire training set, b) sarse low dimensional rojection of data, c) joint discriminative learning of the rojection and rototyes with exlicit model size constraint. We conduct systematic emirical evaluation of ProtoNN on a variety of suervised learning tasks (binary, multi-class, multi-label classification) and show that it gives nearly state-of-the-art rediction accuracy on resource-scarce devices while consuming several orders lower storage, and using minimal working memory. 
"
306,2017,Device Placement Optimization with Reinforcement Learning,Oral/Poster,"The ast few years have witnessed a growth in size and comutational requirements for training and inference with neural networks. Currently, a common aroach to address these requirements is to use a heterogeneous distributed environment with a mixture of hardware devices such as CPUs and GPUs. Imortantly, the decision of lacing arts of the neural models on devices is often made by human exerts based on simle heuristics and intuitions. In this aer, we ro- ose a method which learns to otimize device lacement for TensorFlow comutational grahs. Key to our method is the use of a sequence-to-sequence model to redict which subsets of oerations in a TensorFlow grah should run on which of the available devices. The execution time of the redicted lacements is then used as the reward signal to otimize the arameters of the sequence-to-sequence model. Our main result is that on Incetion-V3 for ImageNet classification, and on RNN LSTM, for language modeling and neural machine translation, our model finds non-trivial device lacements that outerform hand-crafted heuristics and traditional algo-rithmic methods.
"
307,2017,Dynamic Word Embeddings,Oral/Poster,"We resent a robabilistic language model for time-stamed text data which tracks the semantic evolution of individual words over time. The model reresents words and contexts by latent trajectories in an embedding sace. At each moment in time, the embedding vectors are inferred from a robabilistic version of word2vec [Mikolov et al., 2013]. These embedding vectors are connected in time through a latent  diffusion rocess. We describe two scalable variational inference algorithms--ski-gram smoothing and ski-gram filtering--that allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift. Exerimental results on three different corora demonstrate that our dynamic model infers word embedding trajectories that are more interretable and lead to higher redictive likelihoods than cometing methods that are based on static models trained searately on time slices.
"
308,2017,Asynchronous Stochastic Gradient Descent with Delay Compensation,Oral/Poster,"With the fast develoment of dee learning, it has become common to learn big neural networks using massive training data. Asynchronous Stochastic Gradient Descent (ASGD) is widely adoted to fulfill this task for its efficiency, which is, however, known to suffer from the roblem of delayed gradients. That is, when a local worker adds its gradient to the global model, the global model may have been udated by other workers and this gradient becomes ``delayed''. We roose a novel technology to comensate this delay, so as to make the otimization behavior of ASGD closer to that of sequential SGD. This is achieved by leveraging Taylor exansion of the gradient function and efficient aroximators to the Hessian matrix of the loss function. We call the new algorithm Delay Comensated ASGD (DC-ASGD). We evaluated the roosed algorithm on CIFAR-10 and ImageNet datasets, and the exerimental results demonstrate that DC-ASGD outerforms both synchronous SGD and asynchronous SGD, and nearly aroaches the erformance of sequential SGD.
"
309,2017,Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution,Oral/Poster,"Recently, reinforcement learning with dee neural networks has achieved great success in challenging continuous control roblems such as 3D locomotion and robotic maniulation. However, in real-world control roblems, the actions one can take are bounded by hysical constraints, which introduces a bias when the standard Gaussian distribution is used as the stochastic olicy. In this work, we roose to use the Beta distribution as an alternative and analyze the bias and variance of the olicy gradients of both olicies. We show that the Beta olicy is bias-free and rovides significantly faster convergence and higher scores over the Gaussian olicy when both are used with trust region olicy otimization (TRPO) and actor critic with exerience relay (ACER), the state-of-the-art on- and off-olicy stochastic methods resectively, on OenAI Gym's and MuJoCo's continuous control environments.
"
310,2017,Fractional Langevin Monte Carlo: Exploring Levy Driven Stochastic Differential Equations for MCMC,Oral/Poster,"Along with the recent advances in scalable Markov Chain Monte Carlo methods, samling techniques that are based on Langevin diffusions have started receiving increasing attention. These so called Langevin Monte Carlo (LMC) methods are based on diffusions driven by a Brownian motion, which gives rise to Gaussian roosal distributions in the resulting algorithms. Even though these aroaches have roven successful in many alications, their erformance can be limited by the light-tailed nature of the Gaussian roosals. In this study, we extend classical LMC and develo a novel Fractional LMC (FLMC) framework that is based on a family of heavy-tailed distributions, called alha-stable Levy distributions. As oosed to classical aroaches, the roosed aroach can ossess large jums while targeting the correct distribution, which would be beneficial for efficient exloration of the state sace. We develo novel comutational methods that can scale u to large-scale roblems and we rovide formal convergence analysis of the roosed scheme. Our exeriments suort our theory: FLMC can rovide suerior erformance in multi-modal settings, imroved convergence rates, and robustness to algorithm arameters. 
"
311,2017,Preferential Bayesian Optmization,Oral/Poster,"Bayesian otimization (BO) has emerged during the last few years as an effective aroach to otimize black-box functions where direct queries of the objective are exensive. We consider the case where direct access to the function is not ossible, but information about user references is. Such scenarios arise in roblems where human references are modeled, such as  AB tests or recommender systems. 
We resent a new framework for this scenario that we call Preferential Bayesian Otimization (PBO) and that allows to find the otimum of a latent function that can only be queried through airwise comarisons, so-called duels. PBO extend the alicability of standard BO ideas and generalizes revious discrete dueling aroaches by modeling the robability of the the winner of each duel  by means of Gaussian rocess model with a Bernoulli likelihood. The latent reference function is used to define a family of acquisition functions that extend usual olicies used in BO. We illustrate the benefits of PBO in a variety of exeriments in which we show how the way correlations are modeled is the key ingredient to drastically reduce the number of comarisons to find the otimum of the latent function of interest.
"
312,2017,Being Robust (in High Dimensions) Can Be Practical,Oral/Poster,"Robust estimation is much more challenging in high-dimensions than it is in one-dimension: Most techniques either lead to intractable otimization roblems or estimators that can tolerate only a tiny fraction of errors. Recent work in theoretical comuter science has shown that, in aroriate distributional models, it is ossible to robustly estimate the mean and covariance with olynomial time algorithms that can tolerate a constant fraction of corrutions, indeendent of the dimension. However, the samle and time comlexity of these algorithms is rohibitively large for high-dimensional alications. In this work, we address both of these issues by establishing samle comlexity bounds that are otimal, u to logarithmic factors, as well as giving various refinements that allow the algorithms to tolerate a much larger fraction of corrutions. 
Finally, we show on both synthetic and real data that our algorithms have state-of-the-art erformance and suddenly make high-dimensional robust estimation a realistic ossibility.  
"
313,2017,Differentially Private Ordinary Least Squares,Oral/Poster,"Linear regression is one of the most revalent techniques in machine learning;
however, it is also common to use linear regression for its exlanatory caabilities rather than label rediction. 
Ordinary Least Squares (OLS) is often used in statistics to establish a correlation between an attribute (e.g. gender) and a label (e.g. income)  in the resence of other (otentially correlated) features. 
OLS assumes a articular model that randomly generates the data, and derives t-values --- reresenting the likelihood of each real value to be the true correlation. 
Using t-values, OLS can release a confidence interval, which is an interval on the reals that is likely to contain the true correlation; and when this interval does not intersect the origin, we can reject the null hyothesis as it is likely that the true correlation is non-zero.
Our work aims at achieving similar guarantees on data under differentially rivate estimators.
First, we show that for well-sread data, the  Gaussian Johnson-Lindenstrauss Transform (JLT) gives a very good aroximation of t-values; secondly, when JLT aroximates Ridge regression (linear regression with l_2-regularization) we derive, under certain conditions, confidence intervals using the rojected data; lastly, we derive, under different conditions, confidence intervals for the ``Analyze Gauss'' algorithm (Dwork et al 2014).
"
314,2017,"When can Multi-Site Datasets be Pooled for Regression? Hypothesis Tests, $\ell_2$-consistency and Neuroscience Applications",Oral/Poster,"Many studies in biomedical and health sciences involve small samle sizes due to logistic or financial constraints. Often, identifying weak (but scientifically interesting) associations between a set of redictors and a resonse necessitates ooling datasets from multile diverse labs or grous. While there is a rich literature in statistical machine learning to address distributional shifts and inference in multi-site datasets, it is less clear when such ooling is guaranteed to hel (and when it does not) – indeendent of the inference algorithms we use. In this aer, we resent a hyothesis test to answer this question, both for classical and high dimensional linear regression. We recisely identify regimes where ooling datasets across multile sites is sensible, and how such olicy decisions can be made via simle checks executable on each site before any data transfer ever haens. With a focus on Alzheimer’s disease studies, we resent emirical results showing that in regimes suggested by our analysis, ooling a local dataset with data from an international study imroves ower.
"
315,2017,Deep Tensor Convolution on Multicores,Oral/Poster,"Dee convolutional neural networks (ConvNets) of 3-dimensional kernels allow joint modeling of satiotemoral features. These networks have imroved erformance of video and volumetric image analysis, but have been limited in size due to the low memory ceiling of GPU hardware. Existing CPU imlementations overcome this constraint but are imractically slow. Here we extend and otimize the faster Winograd-class of  convolutional algorithms to the $N$-dimensional case and secifically for CPU hardware. First, we remove the need to manually hand-craft algorithms by exloiting the relaxed constraints and chea sarse access of CPU memory. Second, we maximize CPU utilization and multicore scalability by transforming data matrices to be cache-aware, integer multiles of AVX vector widths. Treating 2-dimensional ConvNets as a secial (and the least beneficial) case of our aroach, we demonstrate a 5 to 25-fold imrovement in throughut comared to revious state-of-the-art."
316,2017,Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling,Oral/Poster,"Most existing sequence labelling models rely on a fixed decomosition of a target sequence into a sequence of basic units. These methods suffer from two major drawbacks: $1$) the set of basic units is fixed, such as the set of words, characters or honemes in seech recognition, and $2$) the decomosition of target sequences is fixed. These drawbacks usually result in sub-otimal erformance of modeling sequences. In this aer, we extend the oular CTC loss criterion to alleviate these limitations, and roose a new loss function called \textit{Gram-CTC}. While reserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomosition of target sequences. 
Unlike CTC, Gram-CTC allows the model to outut variable number of characters at each time ste, which enables the model to cature longer term deendency and imroves the comutational efficiency.
We demonstrate that the roosed Gram-CTC imroves CTC in terms of both erformance and efficiency on the large vocabulary seech recognition task at multile scales of data, and that with Gram-CTC we can outerform the state-of-the-art on a standard seech benchmark."
317,2017,Adaptive Consensus ADMM for Distributed Optimization,Oral/Poster,"The alternating direction method of multiliers (ADMM) is commonly used for distributed model fitting roblems, but its erformance and reliability deend strongly on user-defined enalty arameters.   We study distributed ADMM methods that boost erformance by using different fine-tuned algorithm arameters on each worker node. We resent a O(1k) convergence rate for adative ADMM methods with node-secific arameters, and roose adative consensus ADMM (ACADMM), which automatically tunes arameters without user oversight.
"
318,2017,Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning,Oral/Poster,"Reinforcement learning algorithms for real-world robotic alications must be able to handle comlex, unknown dynamical systems while maintaining data-efficient learning. These requirements are handled well by model-free and model-based RL aroaches, resectively. In this work, we aim to combine the advantages of these aroaches. By focusing on time-varying linear-Gaussian olicies, we enable a model-based algorithm based on the linear-quadratic regulator that can be integrated into the model-free framework of ath integral olicy imrovement. We can further combine our method with guided olicy search to train arbitrary arameterized olicies such as dee neural networks. Our simulation and real-world exeriments demonstrate that this method can solve challenging maniulation tasks with comarable or better erformance than model-free methods while maintaining the samle efficiency of model-based methods.
"
319,2017,Stochastic  Bouncy  Particle Sampler,Oral/Poster,"We introduce a stochastic version of the non-reversible, rejection-free Bouncy Particle Samler (BPS), a Markov rocess whose samle trajectories are iecewise linear, to efficiently samle  Bayesian osteriors in big datasets. We rove that in the BPS no bias is introduced by noisy evaluations of the log-likelihood gradient. On the other hand, we argue that efficiency considerations favor a small, controllable bias, in exchange for faster mixing. We introduce a simle method that controls this trade-off. We illustrate these ideas in several examles  which outerform revious aroaches.
"
320,2017,Max-value Entropy Search for Efficient Bayesian Optimization,Oral/Poster,"Entroy Search (ES) and Predictive Entroy Search (PES) are oular and emirically successful Bayesian Otimization techniques. Both rely on a comelling information-theoretic motivation, and maximize the information gained about the $\arg\max$ of the unknown function; yet, both are lagued by the exensive comutation  for estimating entroies. We roose a new criterion, Max-value Entroy Search (MES), that instead uses the information about the maximum function value. We show relations of MES to other Bayesian otimization methods, and establish a regret bound. We observe that MES maintains or imroves the good emirical erformance of ESPES, while tremendously lightening the comutational burden. In articular, MES is much more robust to the number of samles used for comuting the entroy, and hence more efficient for higher dimensional roblems."
321,2017,Multilabel Classification with Group Testing and Codes,Oral/Poster,"In recent years, the multiclass and mutlilabel classification roblems we encounter in many alications have very large (10^3-10^6) number of classes. However, each instance belongs to only one or few classes, i.e., the label vectors are sarse. In this work, we roose a novel aroach based on grou testing to solve such large multilabel classification roblems with sarse label vectors. We describe various grou testing constructions, and advocate the use of concatenated Reed Solomon codes and unbalanced biartite exander grahs for extreme classification roblems. The roosed aroach has several advantages theoretically and ractically over existing oular methods. Our method oerates on the binary alhabet and can utilize the well-established binary classifiers for learning. The error correction caabilities of the codes are leveraged for the first time in the learning roblem to correct rediction errors. Even if a linearly growing number of classifiers mis-classify, these errors are fully corrected. We establish Hamming loss error bounds for the aroach. More imortantly, our method utilizes a simle rediction algorithm and does not require matrix inversion or solving otimization roblems making the algorithm very inexensive. Numerical exeriments with various datasets illustrate the suerior erformance of our method.
"
322,2017,Priv’IT: Private and Sample Efficient Identity Testing,Oral/Poster,"We develo differentially rivate hyothesis testing methods for the small samle regime. Given a samle D from a categorical distribution  over some domain Sigma, an exlicitly described distribution q over Sigma, some rivacy arameter esilon, accuracy arameter alha, and requirements betaemI$ and  betaemII for the tye I and tye II errors of our test, the goal is to distinguish between =q and dtv(,q) &gt; alha. We rovide theoretical bounds for the samle size |D| so that our method both satisfies (esilon,0)-differential rivacy, and guarantees betaemI and betaemII tye I and tye II errors. We show that differential rivacy may come for free in some regimes of arameters, and we always beat the samle comlexity resulting from running the chi^2-test with noisy counts, or standard aroaches such as reetition for endowing non-rivate chi^2-style statistics with differential rivacy guarantees. We exerimentally comare  the samle comlexity of our method to that of recently roosed methods for rivate hyothesis testing.
"
323,2017,Learning from Clinical Judgments: Semi-Markov-Modulated Marked Hawkes Processes for Risk Prognosis,Oral/Poster,"Critically ill atients in regular wards are vulnerable to unanticiated adverse events which require romt transfer to the intensive care unit (ICU). To allow for accurate rognosis of deteriorating atients, we develo a novel continuous-time robabilistic model for a monitored atient's temoral sequence of hysiological data. Our model catures ""informatively samled"" atient eisodes: the clinicians' decisions on when to observe a hositalized atient's vital signs and lab tests over time are reresented by a marked Hawkes rocess, with intensity arameters that are modulated by the atient's latent clinical states, and with observable hysiological data (mark rocess) modeled as a switching multi-task Gaussian rocess. In addition, our model catures ""informatively censored"" atient eisodes by reresenting the atient's latent clinical states as an absorbing semi-Markov jum rocess. The model arameters are learned from offline atient eisodes in the electronic health records via an EM-based algorithm. Exeriments conducted on a cohort of atients admitted to a major medical center over a 3-year eriod show that risk rognosis based on our model significantly outerforms the currently deloyed medical risk scores and other baseline machine learning algorithms.
"
324,2017,MEC: Memory-efficient Convolution for Deep Neural Network,Oral/Poster,"Convolution  is a critical comonent in modern dee neural networks, thus several algorithms for convolution have been develoed.
Direct convolution is simle but suffers from oor erformance. As an alternative, multile indirect
methods have been roosed including im2col-based convolution,
FFT-based convolution, or Winograd-based algorithm.
However, all these indirect methods have high memory overhead,
which  creates erformance degradation and offers a oor trade-off between erformance and memory consumtion.
In this work, we roose a memory-efficient convolution or MEC with comact lowering,
which  reduces memory overhead substantially and accelerates convolution rocess.
MEC lowers the inut matrix in a simle yet efficientcomact way (i.e., much less memory overhead), and then
executes  multile small matrix multilications in arallel to get convolution comleted.
Additionally, the reduced memory footrint imroves memory sub-system efficiency, imroving erformance.
Our exerimental results show that MEC reduces memory consumtion significantly with
good seedu on both mobile and server latforms, comared with other indirect convolution algorithms.
"
325,2017,Coupling Distributed and Symbolic Execution for Natural Language Queries,Oral/Poster,"Building neural networks to query a knowledge base (a table) with natural language is an emerging research toic in dee learning. An executor for table querying tyically requires multile stes of execution because queries may have comlicated structures. In revious studies, researchers have develoed either fully distributed executors or symbolic executors for table querying. A distributed executor can be trained in an end-to-end fashion, but is weak in terms of execution efficiency and exlicit interretability. A symbolic executor is efficient in execution, but is very difficult to train esecially at initial stages. In this aer, we roose to coule distributed and symbolic execution for natural language queries, where the symbolic executor is retrained with the distributed executor's intermediate execution results in a ste-by-ste fashion. Exeriments show that our aroach significantly outerforms both distributed and symbolic executors, exhibiting high accuracy, high learning efficiency, high execution efficiency, and high interretability.
"
326,2017,Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks,Oral/Poster,"In this aer, we determine the otimal convergence rates for strongly convex and smooth distributed otimization in two settings: centralized and decentralized communications over a network. For centralized (i.e. masterslave) algorithms, we show that distributing Nesterov's accelerated gradient descent is otimal and achieves a recision $\varesilon  0$ in time $O(\sqrt{\kaa_g}(1+\Delta\tau)\ln(1\varesilon))$, where $\kaa_g$ is the condition number of the (global) function to otimize, $\Delta$ is the diameter of the network, and $\tau$ (res. $1$) is the time needed to communicate values between two neighbors (res. erform local comutations). For decentralized algorithms based on gossi, we rovide the first otimal algorithm, called the multi-ste dual accelerated (MSDA) method, that achieves a recision $\varesilon  0$ in time $O(\sqrt{\kaa_l}(1+\frac{\tau}{\sqrt{\gamma}})\ln(1\varesilon))$, where $\kaa_l$ is the condition number of the local functions and $\gamma$ is the (normalized) eigenga of the gossi matrix used for communication between nodes. We then verify the efficiency of MSDA against state-of-the-art methods for two roblems: least-squares regression and classification by logistic regression."
327,2017,Prediction under Uncertainty in Sparse Spectrum Gaussian Processes with Applications to Filtering and Control,Oral/Poster,"Sarse Sectrum Gaussian Processes (SSGPs) are a owerful tool for scaling Gaussian rocesses (GPs) to large datasets. Existing SSGP algorithms for regression assume deterministic inuts, recluding their use in many real-world robotics and engineering alications where accounting for inut uncertainty is crucial. We address this roblem by roosing two analytic moment-based aroaches with closed-form exressions for SSGP regression with uncertain inuts. Our methods are more general and scalable than their standard GP counterarts, and are naturally alicable to multi-ste rediction or uncertainty roagation. We show that efficient algorithms for Bayesian filtering and stochastic model redictive control can use these methods, and we evaluate our algorithms with comarative analyses and both real-world and simulated exeriments.
"
328,2017,Canopy --- Fast Sampling with Cover Trees,Oral/Poster,"Hierarchical Bayesian models often cature distributions over a very large number of distinct atoms.
The need for these models arises when organizing huge amount of unsuervised data, for instance, features extracted using dee convnets that can be exloited to organize abundant unlabeled images.
Inference for hierarchical Bayesian models in such cases can be rather nontrivial, leading to aroximate aroaches.
In this work, we roose \emh{Canoy}, a samler based on Cover Trees that is exact, has guaranteed runtime logarithmic in the number of atoms, and is rovably olynomial in the inherent dimensionality of the underlying arameter sace.
In other words, the algorithm is as fast as search over a hierarchical data structure.
We rovide theory for Canoy and demonstrate its effectiveness on both synthetic and real datasets, consisting of over 100 million images.
"
329,2017,Bayesian Optimization with Tree-structured Dependencies,Oral/Poster,"Bayesian otimization has been successfully used to otimize comlex black-box functions whose evaluations are exensive. In many alications, like in dee learning and redictive analytics, the otimization domain is itself comlex and structured. In this work, we focus on use cases where this domain exhibits a known deendency structure. The benefit of leveraging this structure is twofold: we exlore the search sace more efficiently and osterior inference scales more favorably with the number of observations than Gaussian Process-based aroaches ublished in the literature. We introduce a novel surrogate model for Bayesian otimization which combines indeendent Gaussian Processes with a linear model that encodes a tree-based deendency structure and can transfer information between overlaing decision sequences. We also design a secialized  two-ste acquisition function that exlores the search sace more effectively. Our exeriments on synthetic tree-structured functions and the tuning of feedforward neural networks trained on a range of binary classification datasets show that our method comares favorably with cometing aroaches.
"
330,2017,High-Dimensional Structured Quantile Regression,Oral/Poster,"Quantile regression aims at modeling the conditional median and quantiles of a resonse variable given certain redictor variables. In this work we consider the roblem of linear quantile regression in high dimensions where the number of redictor variables is much higher than the number of samles available for arameter estimation. We assume the true arameter to have some structure characterized as having a small value according to some atomic norm R(.) and consider the norm regularized quantile regression estimator. We characterize the samle comlexity for consistent recovery and give non-asymtotic bounds on the estimation error. While this roblem has been reviously considered, our analysis reveals geometric and statistical characteristics of the roblem not available in rior literature. We erform exeriments on synthetic data which suort the theoretical results.
"
331,2017,Differentially Private Submodular Maximization: Data Summarization in Disguise,Oral/Poster,"Many data summarization alications are catured by the general framework of submodular maximization. As a consequence,  a wide range of efficient aroximation algorithms have been develoed. However, when such alications involve sensitive data about individuals, their rivacy concerns are not automatically addressed. To remedy this roblem, we roose a general and systematic study of differentially rivate submodular maximization. We resent rivacy-reserving algorithms for both monotone and non-monotone submodular maximization under cardinality, matroid, and -extendible system constraints, with guarantees that are cometitive with otimal. Along the way, we analyze a new algorithm for non-monotone submodular maximization, which is the first (even non-rivately) to achieve a constant aroximation ratio while running in linear time. We additionally rovide two concrete exeriments to validate the efficacy of these algorithms. 
"
332,2017,Learning to Detect Sepsis with a Multitask Gaussian Process RNN Classifier,Oral/Poster,"We resent a scalable end-to-end classifier that uses streaming hysiological and medication data to accurately redict the onset of sesis, a life-threatening comlication from infections that has high mortality and morbidity.  Our roosed framework models the multivariate trajectories of continuous-valued hysiological time series using multitask Gaussian rocesses, seamlessly accounting for the high uncertainty, frequent missingness, and irregular samling rates tyically associated with real clinical data.  The Gaussian rocess is directly connected to a black-box classifier that redicts whether a atient will become setic, chosen in our case to be a recurrent neural network to account for the extreme variability in the length of atient encounters.  We show how to scale the comutations associated with the Gaussian rocess in a manner so that the entire system can be discriminatively trained end-to-end using backroagation.   In a large cohort of heterogeneous inatient encounters at our university health system we find that it outerforms several baselines at redicting sesis, and yields 19.4\% and 55.5\% imroved areas under the Receiver Oerating Characteristic and Precision Recall curves as comared to the NEWS score currently used by our hosital.
"
333,2017,Beyond Filters: Compact Feature Map for Portable Deep Model,Oral/Poster,"Convolutional neural networks (CNNs) have shown extraordinary erformance in a number of alications, but they are usually of heavy design for the accuracy reason. Beyond comressing the filters in CNNs, this aer focuses on the redundancy in the feature mas derived from the large number of filters in a layer. We roose to extract intrinsic reresentation of the feature mas and reserve the discriminability of the features. Circulant matrix is emloyed to formulate the feature ma transformation, which only requires O(dlog d) comutation comlexity to embed a d-dimensional feature ma. The filter is then re-configured to establish the maing from original inut to the new comact feature ma, and the resulting network can reserve intrinsic information of the original network with significantly fewer arameters, which not only decreases the online memory for launching CNN but also accelerates the comutation seed. Exeriments on benchmark image datasets demonstrate the sueriority of the roosed algorithm over state-of-the-art methods.
"
334,2017,Image-to-Markup Generation with Coarse-to-Fine Attention,Oral/Poster,"We resent a neural encoder-decoder model to convert images into resentational marku based on a scalable coarse-to-fine attention mechanism. Our method is evaluated in the context of image-to-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical exressions aired with LaTeX marku. We show that unlike neural OCR techniques using CTC-based models, attention-based aroaches can tackle this non-standard OCR task. Our aroach outerforms classical mathematical OCR systems by a large margin on in-domain rendered data, and, with retraining, also erforms well on out-of-domain handwritten data. To reduce the inference comlexity associated with the attention-based aroaches, we introduce a new coarse-to-fine attention layer that selects a suort region before alying attention.
"
335,2017,Projection-free Distributed Online Learning in Networks,Oral/Poster,"The conditional gradient algorithm has regained a surge of research interest in recent years due to its high efficiency in handling large-scale machine learning roblems. However, none of existing studies has exlored it in the distributed online learning setting, where locally light comutation is assumed. In this aer, we fill this ga by roosing the distributed online conditional gradient algorithm, which eschews the exensive rojection oeration needed in its counterart algorithms by exloiting much simler linear otimization stes. We give a regret bound for the roosed algorithm as a function of the network size and toology, which will be smaller on smaller grahs or ""well-connected"" grahs. Exeriments on two large-scale real-world datasets for a multiclass classification task confirm the comutational benefit of the roosed algorithm and also verify the theoretical regret bound.
"
336,2017,Learning Stable Stochastic Nonlinear Dynamical Systems,Oral/Poster,"A data-driven identification of dynamical systems requiring only minimal rior knowledge is romising whenever no analytically derived model structure is available, e.g., from first rinciles in hysics. However,    meta-knowledge on the system's behavior is often given and should be exloited: Stability as fundamental roerty is essential when the model is used for controller design or movement generation. Therefore, this aer  rooses a framework for learning stable stochastic systems from data. We focus on identifying a state-deendent coefficient form of the nonlinear stochastic model which is globally asymtotically stable according to robabilistic Lyaunov methods. We comare our aroach to other state of the art methods on real-world datasets in terms of flexibility and   stability.
"
337,2017,A Simulated Annealing Based Inexact Oracle for Wasserstein Loss Minimization,Oral/Poster,"Learning under a Wasserstein loss, a.k.a. Wasserstein loss minimization (WLM), is an emerging research toic for gaining insights from a large set of structured objects. Desite being concetually simle, WLM roblems are comutationally challenging because they involve minimizing over functions of quantities (i.e. Wasserstein distances) that themselves require numerical algorithms to comute. In this aer, we introduce a stochastic aroach based on simulated annealing for solving WLMs. Particularly, we have develoed a Gibbs samler to aroximate effectively and efficiently the artial gradients of a sequence of Wasserstein losses. Our new aroach has the advantages of numerical stability and readiness for warm starts. These characteristics are valuable for WLM roblems that often require multile levels of iterations in which the oracle for comuting the value and gradient of a loss function is embedded. We alied the method to otimal transort with Coulomb cost and the Wasserstein non-negative matrix factorization roblem, and made comarisons with the existing method of entroy regularization. 
"
338,2017,Multi-fidelity Bayesian Optimisation with Continuous Approximations,Oral/Poster,"Bandit methods for black-box otimisation, such as Bayesian otimisation,
are used in a variety of alications including hyer-arameter tuning and
exeriment design.
Recently, \emh{multi-fidelity} methods have garnered
considerable attention since function evaluations have become increasingly exensive in
such alications.
Multi-fidelity methods use chea aroximations to the function of
interest to seed u the overall otimisation rocess.
However, most multi-fidelity methods assume only a finite number of aroximations.
On the other hand, in many ractical alications, a continuous sectrum of aroximations might be
available.
For instance, when tuning an exensive neural network, one might choose to aroximate the
cross validation erformance using less data $N$ andor few training iterations $T$.
Here, the aroximations are best viewed as arising out of a continuous two dimensional
sace $(N,T)$.
In this work, we develo a Bayesian otimisation method, \boca, for this setting.
We characterise its theoretical roerties and show that it achieves better regret than
than strategies which ignore the aroximations.
\bocas outerforms several other baselines in synthetic and real exeriments."
339,2017,High-dimensional Non-Gaussian Single Index Models via Thresholded Score Function Estimation,Oral/Poster,"We consider estimating the arametric comonent​ ​of single index models in high dimensions.​ ​Comared with existing work,​ ​we do not require the covariate to be normally​ ​distributed. Utilizing Stein’s Lemma,​ ​we roose estimators based on the score​ ​function of the covariate. Moreover, to handle​ ​​score function and resonse variables​ ​that are heavy-tailed, our estimators are constructed​ ​via carefully thresholding their emirical​ ​counterarts. Under a bounded fourth​ ​moment condition, we establish otimal statistical​ ​rates of convergence for the roosed​ ​estimators. Extensive numerical exeriments​ ​are rovided to back u our theory.
"
340,2017,Differentially Private Learning of Graphical Models using CGMs,Oral/Poster,"We investigate the roblem of learning discrete grahical models in a differentially rivate way. Aroaches to this roblem range from rivileged algorithms that conduct learning comletely behind the rivacy barrier to schemes that release rivate summary statistics aired with algorithms to learn arameters from those statistics. We show that the aroach of releasing noisy sufficient statistics using the Lalace mechanism achieves a good trade-off between rivacy, utility, and racticality. A naive learning algorithm that uses the noisy sufficient statistics ``as is'' outerforms general-urose differentially rivate learning algorithms. However, it has three limitations: it ignores knowledge about the data generating rocess, rests on uncertain theoretical foundations, and exhibits certain athologies. We develo a more rinciled aroach that alies the formalism of collective grahical models to erform inference over the true sufficient statistics within an exectation-maximization framework. We show that this learns better models than cometing aroaches on both synthetic data and on real human mobility data used as a case study.
"
341,2017,"iSurvive: An Interpretable, Event-time Prediction Model for mHealth",Oral/Poster,"An imortant mobile health (mHealth) task is the use of multimodal data, such as sensor streams and self-reort, to construct interretable time-to-event redictions of, for examle, lase to alcohol or illicit drug use.  Interretability of the rediction model is imortant for accetance and adotion by domain scientists, enabling model oututs and arameters to inform theory and guide intervention design.  Temoral latent state models are therefore attractive, and so we adot the continuous time hidden Markov model (CT-HMM) due to its ability to describe irregular arrival times of event data.  Standard CT-HMMs, however, are not secialized for redicting the time to a future event, the key variable for mHealth interventions.  Also, standard emission models lack a sufficiently rich structure to describe multimodal data and incororate domain knowledge.  We resent iSurvive, an extension of classical survival analysis to a CT-HMM.  We resent a arameter learning method for GLM emissions and survival model fitting, and resent romising results on both synthetic data and an mHealth drug use dataset.
"
342,2017,Efficient softmax approximation for GPUs,Oral/Poster,"We roose an aroximate strategy to efficiently train neural network based language models over very large vocabularies. Our aroach, called adative softmax, circumvents the linear deendency on the vocabulary size by exloiting the unbalanced word distribution to form clusters that exlicitly minimize the exectation of comutation time. Our aroach further
reduces the comutational cost by exloiting the secificities of modern architectures and matrix-matrix vector oerations, making it articularly suited for grahical rocessing units. Our exeriments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our aroach brings a large gain in efficiency over standard aroximations while achieving an accuracy close to that of the full softmax. The code of our method is available at htts:github.comfacebookresearchadative-softmax.
"
343,2017,Multichannel End-to-end Speech Recognition,Oral/Poster,"The field of seech recognition is in the midst of a aradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology. Using an attention mechanism in a recurrent encoder-decoder architecture solves the dynamic time alignment roblem, allowing joint end-to-end training of the acoustic and language modeling comonents. In this aer we extend the end-to-end framework to encomass microhone array signal rocessing for noise suression and seech enhancement within the acoustic encoding network. This allows the beamforming comonents to be otimized jointly within the recognition architecture to imrove the end-to-end seech recognition objective. Exeriments on the noisy seech benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system outerformed the attention-based baseline with inut from a conventional adative beamformer.
"
344,2017,Local Bayesian Optimization of Motor Skills,Oral/Poster,"Bayesian otimization is renowned for its samle efficiency but its alication to higher dimensional tasks is imeded by its focus on global otimization. To scale to higher dimensional roblems, we leverage the samle efficiency of Bayesian otimization in a local context. The otimization of the acquisition function is restricted to the vicinity of a Gaussian search distribution which is moved towards high value areas of the objective. The roosed information-theoretic udate of the search distribution results in a Bayesian interretation of local stochastic search: the search distribution encodes rior
knowledge on the otimum’s location and is weighted at each iteration by the likelihood of this location’s otimality. We demonstrate the
effectiveness of our algorithm on several benchmark objective functions as well as a continuous robotic task in which an informative rior is obtained by imitation learning.
"
345,2017,Improving Gibbs Sampler Scan Quality with DoGS,Oral/Poster,"The airwise influence matrix of Dobrushin has long been used as an analytical tool to bound the rate of convergence of Gibbs samling. In this work, we use Dobrushin influence as the basis of a ractical tool to certify and efficiently imrove the quality of a Gibbs samler. Our Dobrushin-otimized Gibbs samlers (DoGS) offer customized variable selection orders for a given samling budget and variable subset of interest, exlicit bounds on total variation distance to stationarity, and certifiable imrovements over the standard systematic and uniform random scan Gibbs samlers. In our exeriments with image segmentation, Markov chain Monte Carlo maximum likelihood estimation, and Ising model inference, DoGS consistently deliver higher-quality inferences with significantly smaller samling budgets than standard Gibbs samlers.
"
346,2017,Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space,Oral/Poster,"Chemical sace is so large that brute force searches for new interesting
molecules are infeasible. High-throughut virtual screening via comuter cluster simulations can seed u the discovery rocess by collecting very large amounts of data in arallel, e.g., u to hundreds or thousands of arallel measurements. Bayesian otimization (BO) can roduce additional acceleration by sequentially identifying the most useful simulations or exeriments to be erformed next. However, current BO methods cannot scale to the large numbers of arallel measurements and the massive libraries of molecules currently used in high-throughut screening. Here, we roose a scalable solution based on a arallel and distributed imlementation of Thomson samling (PDTS). We show that, in small scale roblems, PDTS erforms similarly as arallel exected imrovement (EI), a batch version of the most widely used BO heuristic. Additionally, in settings where arallel EI does not scale, PDTS outerforms other scalable baselines such as a greedy search, esilon-greedy aroaches and a random search method. These results show that PDTS is a successful solution for large-scale arallel BO.
"
347,2017,Robust Structured Estimation with Single-Index Models,Oral/Poster,"In this aer, we investigate general single-index models (SIMs) in high dimensions. Based on U-statistics, we roose two tyes of robust estimators for the recovery of model arameters, which can be viewed as generalizations of several existing algorithms for one-bit comressed sensing (1-bit CS). With minimal assumtion on noise, the statistical guarantees are established for the generalized estimators under suitable conditions, which allow general structures of underlying arameter. Moreover, the roosed estimator is novelly instantiated for SIMs with monotone transfer function, and the obtained estimator can better leverage the monotonicity. Exerimental results are rovided to suort our theoretical analyses.
"
348,2017,Minimizing Trust Leaks for Robust Sybil Detection,Oral/Poster,"Sybil detection is a crucial task to rotect online social networks (OSNs) against 
intruders who try to maniulate automatic services rovided by OSNs to their customers.
In this aer, 
we first discuss the robustness of grah-based Sybil detectors SybilRank and Integro and refine theoretically their security guarantees 
towards more realistic assumtions. 
After that, we formally introduce adversarial settings for the grah-based Sybil detection roblem 
and derive a corresonding otimal attacking strategy by exloitation of trust leaks.
Based on our analysis, we roose transductive Sybil ranking (TSR), a robust extension to SybilRank and Integro that directly minimizes trust leaks. 
Our emirical evaluation shows significant advantages of TSR 
over state-of-the-art cometitors on a variety of attacking scenarios on artificially generated data and real-world datasets.
"
349,2017,Learning Sleep Stages from Radio Signals: A Conditional Adversarial Architecture,Oral/Poster,"We focus on redicting slee stages from radio measurements without any attached sensors on subjects. 
We introduce a new redictive model that combines convolutional and recurrent neural networks to extract slee-secific subject-invariant features from RF signals and cature the temoral rogression of slee.  A key innovation underlying our aroach is a modified adversarial training regime that discards extraneous information secific to individuals or measurement conditions, while retaining all information relevant to the redictive task.
We analyze our game theoretic setu and emirically demonstrate that our model achieves significant imrovements over state-of-the-art solutions.
"
350,2017,Dropout Inference in Bayesian Neural Networks with Alpha-divergences,Oral/Poster,"To obtain uncertainty estimates with real-world Bayesian dee learning models, ractical inference aroximations are needed. Droout variational inference (VI) for examle has been used for machine vision and medical alications, but VI can severely underestimates model uncertainty. Alha-divergences are alternative divergences to VI's KL objective, which are able to avoid VI's uncertainty underestimation. But these are hard to use in ractice: existing techniques can only use Gaussian aroximating distributions, and require existing models to be changed radically, thus are of limited use for ractitioners. We roose a re-arametrisation of the alha-divergence objectives, deriving a simle inference technique which, together with droout, can be easily imlemented with existing models by simly changing the loss of the model. We demonstrate imroved uncertainty estimates and accuracy comared to VI in droout networks. We study our model's eistemic uncertainty far away from the data using adversarial images, showing that these can be distinguished from non-adversarial images by examining our model's uncertainty.
"
351,2017,Latent Intention Dialogue Models,Oral/Poster,"Develoing a dialogue agent that is caable of making autonomous decisions and communicating by natural language is one of the long-term goals of machine learning research. 
The traditional aroaches either rely on hand-crafting a small state-action set for alying reinforcement learning that is not scalable or constructing deterministic models for learning dialogue sentences that fail to cature the conversational stochasticity.
In this aer, however, we roose a Latent Intention Dialogue Model that emloys a discrete latent variable to learn underlying dialogue intentions in the framework of neural variational inference.
Additionally, in a goal-oriented dialogue scenario, the latent intentions can be interreted as actions guiding the generation of machine resonses, which can be further refined autonomously by reinforcement learning.
The exeriments demonstrate the effectiveness of discrete latent variable models on learning goal-oriented dialogues, and the results outerform the ublished benchmarks on both corus-based evaluation and human evaluation.
"
352,2017,Robust Guarantees of Stochastic Greedy Algorithms,Oral/Poster,"In this aer we analyze the robustness of stochastic variants of the greedy algorithm for submodular maximization. Our main result shows that for maximizing a monotone submodular
function under a cardinality constraint, iteratively selecting an element whose marginal contribution is aroximately maximal in exectation is a sufficient condition to obtain the otimal aroximation guarantee with exonentially high robability, assuming the cardinality is sufficiently large. One consequence of our result is that the linear-time STOCHASTIC-GREEDY algorithm recently roosed in (Mirzasoleiman et al.,2015) achieves the otimal running time while maintaining an otimal aroximation guarantee. We also show that high robability guarantees cannot be obtained for stochastic greedy algorithms under matroid constraints, and rove an aroximation guarantee which holds in exectation.  In contrast to the guarantees of the greedy algorithm, we show that the aroximation ratio of stochastic local search is arbitrarily bad, with high robability, as well as in exectation.
"
353,2017,Count-Based Exploration with Neural Density Models,Oral/Poster,"Bellemare et al. (2016) introduced the notion of a seudo-count, derived from a density model, to generalize count-based exloration to non-tabular reinforcement learning. 
This seudo-count was used to generate an exloration bonus for a DQN agent and combined with a mixed Monte Carlo udate was sufficient to achieve state of the art on the Atari 2600 game Montezuma's Revenge.
We consider two questions left oen by their work: First, how imortant is the quality of the density model for exloration? Second, what role does the Monte Carlo udate lay in exloration? We answer the first question by demonstrating the use of PixelCNN, an advanced neural density model for images, to suly a seudo-count. In articular, we examine the intrinsic difficulties in adating Bellemare et al.'s aroach when assumtions about the model are violated. The result is a more ractical and general algorithm requiring no secial aaratus. We combine PixelCNN seudo-counts with different agent architectures to dramatically imrove the state of the art on several hard Atari games. One surrising finding is that the mixed Monte Carlo udate is a owerful facilitator of exloration in the sarsest of settings, including Montezuma's Revenge.
"
354,2017,Magnetic Hamiltonian Monte Carlo,Oral/Poster,"Hamiltonian Monte Carlo (HMC) exloits Hamiltonian dynamics to construct efficient roosals for Markov chain Monte Carlo (MCMC). In this aer, we resent a generalization of HMC which exloits non-canonical Hamiltonian dynamics. 
We refer to this algorithm as magnetic HMC, since in 3 dimensions a subset of the dynamics ma onto the mechanics of a charged article couled to a magnetic field. 
We establish a theoretical basis for the use of non-canonical Hamiltonian dynamics in MCMC, and construct a symlectic, leafrog-like integrator allowing for the imlementation of magnetic HMC. 
Finally, we exhibit several examles where these non-canonical dynamics can lead to imroved mixing of magnetic HMC relative to ordinary HMC.
"
355,2017,Uncertainty Assessment and False Discovery Rate Control in High-Dimensional Granger Causal Inference,Oral/Poster,"Causal inference among high-dimensional time series data roves an imortant research roblem in many fields. While in the classical regime one often establishes causality among time series via a concet known as “Granger causality,” exist- ing aroaches for Granger causal inference in high-dimensional data lack the means to char- acterize the uncertainty associated with Granger causality estimates (e.g., -values and confidence intervals). We make two contributions in this work. First, we introduce a novel asymtotically unbiased Granger causality estimator with corre- sonding test statistics and confidence intervals to allow, for the first time, uncertainty characteriza- tion in high-dimensional Granger causal inference. Second, we introduce a novel method for false dis- covery rate control that achieves higher ower in multile testing than existing techniques and that can coe with deendent test statistics and deen- dent observations. We corroborate our theoretical results with exeriments on both synthetic data and real-world climatological data.
"
356,2017,Toward Efficient and Accurate Covariance Matrix Estimation on Compressed Data,Oral/Poster,"Estimating covariance matrices is a fundamental technique in various domains, most notably in machine learning and signal rocessing. To tackle the challenges of extensive communication costs, large storage caacity requirements, and high rocessing time comlexity when handling massive high-dimensional and distributed data, we roose an efficient and accurate covariance matrix estimation method via data comression. In contrast to revious data-oblivious comression schemes, we leverage a data-aware weighted samling method to construct low-dimensional data for such estimation. We rigorously rove that our roosed estimator is unbiased and requires smaller data to achieve the same accuracy with secially designed samling distributions. Besides, we deict that the comutational rocedures in our algorithm are efficient. All achievements imly an imroved tradeoff between the estimation accuracy and comutational costs. Finally, the extensive exeriments on synthetic and real-world datasets validate the suerior roerty of our method and illustrate that it significantly outerforms the state-of-the-art algorithms.
"
357,2017,The Price of Differential Privacy For Online Learning,Oral/Poster,"We design differentially rivate algorithms for the roblem of online linear otimization in the full information and bandit settings with otimal O(T^0.5) regret bounds. In the full-information setting, our results demonstrate that ε-differential rivacy may be ensured for free -- in articular, the regret bounds scale as O(T^0.5+1ε). For bandit linear otimization, and as a secial case, for non-stochastic multi-armed bandits, the roosed algorithm achieves a regret of O(T^0.5ε), while the reviously best known regret bound was O(T^{23}ε).
"
358,2017,Bidirectional learning for time-series models with hidden units,Oral/Poster,"Hidden units can lay essential roles in modeling time-series having long-term deendency or on-linearity but make it difficult to learn associated arameters.  Here we roose a way to learn such a time-series model by training a backward model for the time-reversed time-series, where the backward model has a common set of arameters as the original (forward) model.  Our key observation is that only a subset of the arameters is hard to learn, and that subset is comlementary between the forward model and the backward model.  By training both of the two models, we can effectively learn the values of the arameters that are hard to learn if only either of the two models is trained.  We aly bidirectional learning to a dynamic Boltzmann machine extended with hidden units.  Numerical exeriments with synthetic and real datasets clearly demonstrate advantages of bidirectional learning.
"
359,2017,Multiplicative Normalizing Flows for Variational Bayesian Neural Networks,Oral/Poster,"We reinterret multilicative noise in neural networks as auxiliary random variables that augment the aroximate osterior in a variational setting for Bayesian neural networks. We show that through this interretation it is both efficient and straightforward to imrove the aroximation by emloying normalizing flows while still allowing for local rearametrizations and a tractable lower bound. In exeriments we show that with this new aroximation we can significantly imrove uon classical mean field for Bayesian neural networks on both redictive accuracy as well as redictive uncertainty.
"
360,2017,Discovering Discrete Latent Topics with Neural Variational Inference,Oral/Poster,"Toic models have been widely exlored as robabilistic generative models of documents. Traditional inference methods have sought closed-form derivations for udating the models, however as the exressiveness of these models grows, so does the difficulty of erforming fast and accurate inference over their arameters. This aer resents alternative neural aroaches to toic modelling by roviding arameterisable distributions over toics which ermit training by backroagation in the framework of neural variational inference. In addition, with the hel of a stick-breaking construction, we roose a recurrent network that is able to discover a notionally unbounded number of toics, analogous to Bayesian non-arametric toic models. Exerimental results on the MXM Song Lyrics, 20NewsGrous and Reuters News datasets demonstrate the effectiveness and efficiency of these neural toic models.
"
361,2017,Guarantees for Greedy Maximization of Non-submodular Functions with Applications,Oral/Poster,"We investigate the erformance of the standard Greedy algorithm for cardinality constrained maximization of non-submodular nondecreasing set functions. While there are strong theoretical guarantees on the erformance of Greedy for maximizing submodular functions, there are few guarantees for non-submodular ones. However, Greedy enjoys strong emirical erformance for many imortant non-submodular functions, e.g., the Bayesian A-otimality objective in exerimental design. We rove theoretical guarantees suorting the emirical erformance. Our guarantees are characterized by a combination of the (generalized) curvature $\alha$ and the submodularity ratio $\gamma$. In articular, we rove that Greedy enjoys a tight aroximation guarantee of $\frac{1}{\alha}(1- e^{-\gamma\alha})$ for cardinality constrained maximization. In addition, we bound the submodularity ratio and curvature for several imortant real-world objectives, including the Bayesian A-otimality objective, the determinantal function of a square submatrix and certain linear rograms with combinatorial constraints. We exerimentally validate our theoretical findings for both synthetic and  real-world alications."
362,2017,Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning,Oral/Poster,"As a ste towards develoing zero-shot task generalization caabilities in reinforcement learning (RL), we introduce a new RL roblem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this roblem, we consider two tyes of generalizations: to reviously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we roose a new objective which encourages learning corresondences between similar subtasks by making analogies. For generalization over sequential instructions, we resent a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we roose a new neural architecture in the meta controller that learns when to udate the subtask, which makes learning more efficient. Exerimental results on a stochastic 3D domain show that the roosed ideas are crucial for generalization to longer instructions as well as unseen instructions.
"
363,2017,Probabilistic Path Hamiltonian Monte Carlo,Oral/Poster,"Hamiltonian Monte Carlo (HMC) is an efficient and effective means of samling osterior distributions on Euclidean sace, which has been extended to manifolds with boundary. However, some alications require an extension to more general saces. For examle, hylogenetic (evolutionary) trees are defined in terms of both a discrete grah and associated continuous arameters; although one can reresent these asects using a single connected sace, this rather comlex sace is not suitable for existing HMC algorithms. In this aer, we develo Probabilistic Path HMC (PPHMC) as a first ste to samling distributions on saces with intricate combinatorial structure. We define PPHMC on orthant comlexes, show that the resulting Markov chain is ergodic, and rovide a romising imlementation for the case of hylogenetic trees in oen-source software. We also show that a surrogate function to ease the transition across a boundary on which the log-osterior has discontinuous derivatives can greatly imrove efficiency.
"
364,2017,Uncovering Causality from Multivariate Hawkes Integrated Cumulants,Oral/Poster,"We design a new nonarametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes rocess.
This matrix not only encodes the mutual influences of each
node of the rocess, but also disentangles the causality relationshis between them.
Our aroach is the first that leads to an estimation of this matrix \emh{without any arametric modeling and estimation of the kernels themselves}.
A consequence is that it can give an estimation of causality relationshis between nodes (or users), based on their activity timestams (on a social network for instance), without knowing or estimating the shae of the activities lifetime.
For that urose, we introduce a moment matching method that fits the second-order and the third-order integrated cumulants of the rocess.
A theoretical analysis allows to rove that this new estimation technique is consistent.
Moreover, we show on numerical exeriments that our aroach is indeed very robust to the shae of the kernels, and
gives aealing results on the MemeTracker database and on financial order book data.
"
365,2017,Robust Gaussian Graphical Model Estimation with Arbitrary Corruption,Oral/Poster,"We study the roblem of estimating the high-dimensional Gaussian grahical model where the data are arbitrarily corruted. We roose a robust estimator for the sarse recision matrix in the high-dimensional regime.  At the core of our method is a robust covariance matrix estimator, which is based on truncated inner roduct. We establish the statistical guarantee of our estimator on both estimation error and model selection consistency. In articular, we show that rovided that the number of corruted samles $n_2$ for each variable satisfies $n_2 \lesssim \sqrt{n}\sqrt{\log d}$, where $n$ is the samle size and $d$ is the number of variables, the roosed robust recision matrix estimator attains the same statistical rate as the standard estimator for Gaussian grahical models. In addition, we roose a hyothesis testing rocedure to assess the uncertainty of our robust estimator. We demonstrate the effectiveness of our method through extensive exeriments on both synthetic data and real-world genomic data."
366,2017,Pain-Free Random Differential Privacy with Sensitivity Sampling,Oral/Poster,"Poular aroaches to differential rivacy, such as the Lalace and exonential mechanisms, calibrate randomised smoothing through global sensitivity of the target non-rivate function. Bounding such sensitivity is often a rohibitively comlex analytic calculation. As an alternative, we roose a straightforward samler for estimating sensitivity of non-rivate mechanisms. Since our sensitivity estimates hold with high robability, any mechanism that would be (esilon,delta)-differentially rivate under bounded global sensitivity automatically achieves (esilon,delta,gamma)-random differential rivacy (Hall et al. 2012), without any target-secific calculations required. We demonstrate on worked examle learners how our usable aroach adots a naturally-relaxed rivacy guarantee, while achieving more accurate releases even for non-rivate functions that are black-box comuter rograms.
"
367,2017,Learning Hawkes Processes from Short Doubly-Censored Event Sequences,Oral/Poster,"Many real-world alications require robust algorithms to learn oint rocess models based on a tye of incomlete data --- the so-called short doubly-censored (SDC) event sequences. 
In this aer, we study this critical roblem of quantitative asynchronous event sequence analysis under the framework of Hawkes rocesses by leveraging the general idea of data synthesis. 
In articular, given SDC event sequences observed in a variety of time intervals, we roose a samling-stitching data synthesis method --- samling redecessor and successor for each SDC event sequence from otential candidates and stitching them together to synthesize long training sequences. 
The rationality and the feasibility of our method are discussed in terms of arguments based on likelihood. 
Exeriments on both synthetic and real-world data demonstrate that the roosed data synthesis method imroves learning results indeed for both time-invariant and time-varying Hawkes rocesses.
"
368,2017,Variational Dropout Sparsifies Deep Neural Networks,Oral/Poster,"We exlore a recently roosed Variational Droout technique that rovided an elegant Bayesian interretation to Gaussian Droout. We extend Variational Droout to the case when droout rates are unbounded, roose a way to reduce the variance of the gradient estimator and reort first exerimental results with individual droout rates er weight. Interestingly, it leads to extremely sarse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in emirical Bayes but has a number of advantages. We reduce the number of arameters u to 280 times on LeNet architectures and u to 68 times on VGG-like networks with a negligible decrease of accuracy.
"
369,2017,Toward Controlled Generation of Text,Oral/Poster,"Generic generation and maniulation of text is challenging and has limited success comared to recent dee generative modeling in visual domain. This aer aims at generating lausible text sentences, whose attributes are controlled by learning disentangled latent reresentations with designated semantics. We roose a new neural generative model which combines variational auto-encoders (VAEs) and holistic attribute discriminators for effective imosition of semantic structures. The model can alternatively be seen as enhancing VAEs with the wake-slee algorithm for leveraging fake samles as extra training data. With differentiable aroximation to discrete text samles, exlicit constraints on indeendent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns interretable reresentations from even only word annotations, and roduces short sentences with desired attributes of sentiment and tenses. Quantitative exeriments using trained classifiers as evaluators validate the accuracy of sentence and attribute generation.
"
370,2017,Robust Submodular Maximization: A Non-Uniform Partitioning Approach,Oral/Poster,"We study the roblem of maximizing a monotone submodular function subject to a cardinality constraint $k$, with the added twist that a number of items $\tau$ from the returned set may be removed. We focus on the worst-case setting considered in \cite{orlin2016robust}, in which a constant-factor aroximation guarantee was given for $\tau = o(\sqrt{k})$.  In this aer, we solve a key oen roblem raised therein, resenting a new Partitioned Robust (PRo) submodular maximization algorithm  that achieves the same guarantee for more general $\tau = o(k)$.  Our algorithm constructs artitions consisting of buckets with exonentially increasing sizes, and alies standard submodular otimization subroutines on the buckets in order to construct the robust solution. We numerically demonstrate the erformance of PRo in data summarization and influence maximization, demonstrating gains over both the greedy algorithm and the algorithm of \cite{orlin2016robust}."
371,2017,Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning,Oral/Poster,"Many real-world roblems, such as network acket routing and urban traffic control, are naturally modeled as multi-agent reinforcement learning (RL) roblems. However, existing multi-agent RL methods tyically scale oorly in the roblem size. Therefore, a key challenge is to translate the success of dee learning on single-agent RL to the multi-agent setting. A major stumbling block is that indeendent Q-learning, the most oular multi-agent RL method, introduces nonstationarity that makes it incomatible with the exerience relay memory on which dee Q-learning relies. This aer rooses two methods that address this roblem: 1) using a multi-agent variant of imortance samling to naturally decay obsolete data and 2) conditioning each agent's value function on a fingerrint that disambiguates the age of the data samled from the relay memory. Results on a challenging decentralised variant of StarCraft unit micromanagement confirm that these methods enable the successful combination of exerience relay with multi-agent RL.
"
372,2017,Stochastic Gradient Monomial Gamma Sampler,Oral/Poster,"Scaling Markov Chain Monte Carlo (MCMC) to estimate osterior distributions from large datasets has been made ossible as a result of advances in stochastic gradient techniques.
Desite their success, mixing erformance of existing methods when samling from multimodal distributions can be less efficient with insufficient Monte Carlo samles; this is evidenced by slow convergence and insufficient exloration of osterior distributions.
We roose a generalized framework to imrove the samling efficiency of stochastic gradient MCMC, by leveraging a generalized kinetics that delivers suerior stationary mixing, esecially in multimodal distributions, and roose several techniques to overcome the ractical issues.
We show that the roosed aroach is better at exloring a comlicated multimodal osterior distribution, and demonstrate imrovements over other stochastic gradient MCMC methods on various alications.
"
373,2017,Cost-Optimal Learning of Causal Graphs,Oral/Poster,"We consider the roblem of learning a causal grah over a set of variables with interventions. We study the cost-otimal causal grah learning roblem: For a given skeleton (undirected version of the causal grah), design the set of interventions with minimum total cost, that can uniquely identify any causal grah with the given skeleton. We show that this roblem is solvable in olynomial time. Later, we consider the case when the number of interventions is limited. For this case, we rovide olynomial time algorithms when the skeleton is a tree or a clique tree. For a general chordal skeleton, we develo an efficient greedy algorithm, which can be imroved when the causal grah skeleton is an interval grah.
"
374,2017,Algebraic Variety Models for High-Rank Matrix Completion,Oral/Poster,"We consider a non-linear generalization of low-rank matrix comletion to the case where the data belongs to an algebraic variety, i.e., each data oint is a solution to a system of olynomial equations. In this case the original matrix is ossibly high-rank, but it becomes low-rank after maing each column to a higher dimensional sace of monomial features. Algebraic varieties cature a range of well-studied linear models, including affine subsaces and their union, but also quadratic and higher degree curves and surfaces. We study the samling requirements for a general variety model with a focus on the union of affine subsaces. We roose an efficient matrix comletion algorithm that minimizes a convex or non-convex surrogate of the rank of the lifted matrix. Our algorithm uses the well-known ""kernel trick'' to avoid working directly with the high-dimensional lifted data matrix and scales efficiently with data size. We show the roosed algorithm is able to recover synthetically generated data u to the redicted samling comlexity bounds. The algorithm also outerforms standard techniques in exeriments with real data.
"
375,2017,Differentially Private Clustering in High-Dimensional Euclidean Spaces,Oral/Poster,"We study the roblem of clustering sensitive data while reserving the rivacy of individuals reresented in the dataset, which has broad alications in ractical machine learning and data analysis tasks. Although the roblem has been widely studied in the context of low-dimensional, discrete saces, much remains unknown concerning rivate clustering in high-dimensional Euclidean saces $\R^d$. In this work, we give differentially rivate and efficient algorithms achieving strong guarantees for $k$-means and $k$-median clustering when $d=\Omega(\olylog(n))$. Our algorithm achieves clustering loss at most $\log^3(n)\OPT+\oly(\log n,d,k)$, advancing the state-of-the-art result of $\sqrt{d}\OPT+\oly(\log n,d^d,k^d)$. We also study the case where the data oints are $s$-sarse and show that the clustering loss can scale logarithmically with $d$, i.e., $\log^3(n)\OPT+\oly(\log n,\log d,k,s)$. Exeriments on both synthetic and real datasets verify the effectiveness of the roosed method."
376,2017,Coherent probabilistic forecasts for hierarchical time series,Oral/Poster,"Many alications require forecasts for a hierarchy comrising a set of time series along with aggregates of subsets of these series. Hierarchical forecasting require not only good rediction accuracy at each level of the hierarchy, but also the coherency between different levels --- the roerty that forecasts add u aroriately across the hierarchy. A fundamental limitation of rior research is the focus on forecasting the mean of each time series. We consider the situation where robabilistic forecasts are needed for each series in the hierarchy, and roose an algorithm to comute redictive distributions rather than mean forecasts only. Our algorithm has the advantage of synthesizing information from different levels in the hierarchy through a sarse forecast combination and a robabilistic hierarchical aggregation. We evaluate the accuracy of our forecasting algorithm on both simulated data and large-scale electricity smart meter data. The results show consistent erformance gains comared to state-of-the art methods.
"
377,2017,Unimodal Probability Distributions for Deep Ordinal Classification,Oral/Poster,"Probability distributions roduced by the cross- entroy loss for ordinal classification roblems can ossess undesired roerties. We roose a straightforward technique to constrain discrete ordinal robability distributions to be unimodal via the use of the Poisson and binomial robability distributions. We evaluate this aroach in the context of dee learning on two large ordinal image datasets, obtaining romising results.
"
378,2017,Adversarial Feature Matching for Text Generation,Oral/Poster,"The Generative Adversarial Network (GAN) has achieved great success in generating realistic (real-valued) synthetic data. However, convergence issues and difficulties dealing with discrete data hinder the alicability of GAN to text. We roose a framework for generating realistic text via adversarial training. We emloy a long short-term memory network as generator, and a convolutional network as discriminator. Instead of using the standard objective of GAN, we roose matching the high-dimensional latent feature distributions of real and synthetic sentences, via a kernelized discreancy metric. This eases adversarial training by alleviating the mode-collasing roblem. Our exeriments show suerior erformance in quantitative evaluation, and demonstrate that our model can generate realistic-looking sentences.
"
379,2017,Probabilistic Submodular Maximization in Sub-Linear Time,Oral/Poster,"In this aer, we consider otimizing submodular functions that are drawn from some unknown distribution.  This setting arises, e.g., in recommender systems, where the utility of a subset of items may deend on a user-secific submodular utility function.  In modern alications, the ground set of items is often so large that even the widely used (lazy) greedy algorithm is not efficient enough.  As a remedy, we introduce the roblem of {\em sublinear time robabilistic submodular maximization}: Given training examles of functions (e.g., via user feature vectors), we seek to reduce the ground set so that otimizing new functions drawn from the same distribution will rovide almost as much value when restricted to the reduced ground set as when using the full set.  

We cast this roblem as a two-stage submodular maximization and develo  a novel efficient algorithm for this roblem which offers 12(1 - 1e^2) aroximation ratio 
for general monotone submodular functions and general matroid constraints. We demonstrate the effectiveness of our aroach on several real-world roblem instances where running the maximization roblem on the reduced ground set leads to two folds seed-u while incurring almost no loss. 
"
380,2017,The Predictron:  End-To-End Learning and Planning,Oral/Poster,"One of the key challenges of artificial intelligence is to learn models that are effective in the context of lanning. In this document we introduce the redictron architecture. The redictron consists of a fully abstract model, reresented by a Markov reward rocess, that can be rolled forward multile ""imagined"" lanning stes. Each forward ass of the redictron accumulates internal rewards and values over multile lanning deths. The redictron is trained end-to-end so as to make these accumulated values accurately aroximate the true value function. We alied the redictron to rocedurally generated random mazes and a simulator for the game of ool. The redictron yielded significantly more accurate redictions than conventional dee neural network architectures.
"
381,2017,Stochastic Gradient MCMC Methods for Hidden Markov Models,Oral/Poster,"Stochastic gradient MCMC (SG-MCMC) algorithms have roven useful in scaling Bayesian inference to large datasets under an assumtion of i.i.d data. We instead develo an SG-MCMC algorithm to learn the arameters of hidden Markov models (HMMs) for time-deendent data. There are two challenges to alying SG-MCMC in this setting: The latent discrete states, and needing to break deendencies when considering minibatches. We consider a marginal likelihood reresentation of the HMM and roose an algorithm that harnesses the inherent memory decay of the rocess. We demonstrate the effectiveness of our algorithm on synthetic exeriments and an ion channel recording data, with runtimes significantly outerforming batch MCMC. 
"
382,2017,Identification and Model Testing in Linear Structural Equation Models using Auxiliary Variables,Oral/Poster,"We develoed a novel aroach to identification and model testing in linear structural equation models (SEMs) based on auxiliary variables (AVs), which generalizes a widely-used family of methods known as instrumental variables. The identification roblem is concerned with the conditions under which causal arameters can be uniquely estimated from an observational, non-causal covariance matrix. In this aer, we rovide an algorithm for the identification of causal arameters in linear structural models that subsumes revious state-of-the-art methods. In other words, our algorithm identifies strictly more coefficients and models than methods reviously known in the literature. Our algorithm builds on a grah-theoretic characterization of conditional indeendence relations between auxiliary and model variables, which is develoed in this aer. Further, we leverage this new characterization for allowing identification when limited exerimental data or new substantive knowledge about the domain is available. Lastly, we develo a new rocedure for model testing using AVs.
"
383,2017,High-Dimensional Variance-Reduced Stochastic Gradient Expectation-Maximization Algorithm,Oral/Poster,"We roose a generic
stochastic exectation-maximization (EM) algorithm for the estimation of high-dimensional latent variable models. At the core of our algorithm is a novel semi-stochastic variance-reduced gradient designed for the $Q$-function in the EM algorithm. Under a mild condition on the initialization, our algorithm is guaranteed to attain a linear convergence rate to the unknown arameter of the latent variable model, and achieve an otimal statistical rate u to a logarithmic factor for arameter estimation. Comared with existing high-dimensional EM algorithms, our algorithm enjoys a better comutational comlexity and is therefore more efficient. We aly our generic algorithm to two illustrative latent variable models: Gaussian mixture model and mixture of linear regression, and demonstrate the advantages of our algorithm by both theoretical analysis and numerical exeriments. 
We believe that the roosed semi-stochastic gradient is of indeendent interest for general nonconvex otimization roblems with bivariate structures."
384,2017,Differentially Private Chi-squared Test by Unit Circle Mechanism,Oral/Poster,"This aer develos differentially rivate mechanisms for $\chi^2$ test of indeendence. While existing works ut their effort into roerly controlling the tye-I error, in addition to that, we investigate the tye-II error of differentially rivate mechanisms. Based on the analysis, we resent unit circle mechanism: a novel differentially rivate mechanism based on the geometrical roerty of the test statistics. Comared to existing outut erturbation mechanisms, our mechanism imroves the dominated term of the tye-II error from $O(1)$ to $O(\ex(-\sqrt{\numSamle}))$ where $\numSamle$ is the samle size. Furthermore, we introduce novel rocedures for multile $\chi^2$ tests by incororating the unit circle mechanism into the sarse vector technique and the exonential mechanism. These rocedures can control the family-wise error rate (FWER) roerly, which has never been attained by existing mechanisms.

"
385,2017,Soft-DTW: a Differentiable Loss Function for Time-Series,Oral/Poster,"We roose in this aer a differentiable learning loss between time series, building uon the celebrated dynamic time waring (DTW) discreancy. Unlike the Euclidean distance, DTW can comare time series of variable size and is robust to shifts or dilatations across the time dimension. To comute DTW, one tyically solves a minimal-cost alignment roblem between two time series using dynamic rogramming. Our work takes advantage of a smoothed formulation
of DTW, called soft-DTW, that comutes the soft-minimum of all alignment costs. We show in this aer that soft-DTW is a \emh{differentiable} loss function, and that both its value and gradient can be comuted with quadratic timesace comlexity (DTW has quadratic time but linear sace comlexity). We show that this regularization is articularly well suited to average and cluster time series under the DTW geometry, a task for which our roosal
significantly outerforms existing baselines~\cite{etitjean2011global}. Next, we roose to tune the arameters of a machine that oututs time series by minimizing its fit with ground-truth labels in a soft-DTW sense. Source code is available at \url{htts:github.commblondelsoft-dtw}.
"
386,2017,Learning Continuous Semantic Representations of Symbolic Expressions,Oral/Poster,"Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of reresentation learning. As a ste in this direction, we roose a new architecture, called neural equivalence network, for the roblem of learning continuous semantic reresentations of algebraic and logical exressions. These networks are trained to reresent semantic equivalence, even of exressions that are syntactically very different. The challenge is that  semantic reresentations must be comuted in a syntax-directed manner,  because semantics is comositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We erform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean exression tyes, showing that our model significantly outerforms existing architectures.
"
387,2017,On Approximation Guarantees for Greedy Low Rank Optimization,Oral/Poster,"We rovide new aroximation guarantees for greedy low rank matrix estimation under standard assumtions of restricted strong convexity and smoothness. Our novel analysis  also uncovers reviously unknown connections between the low rank estimation and combinatorial otimization, so much so that our bounds are reminiscent of corresonding aroximation bounds in submodular maximization. Additionally, we rovide also rovide statistical recovery guarantees. Finally, we resent emirical comarison of greedy estimation with established baselines on two imortant real-world roblems. 
"
388,2017,Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning,Oral/Poster,"Instability and variability of Dee Reinforcement
Learning (DRL) algorithms tend to adversely affect
their erformance. Averaged-DQN is a simle
extension to the DQN algorithm, based on
averaging reviously learned Q-values estimates,
which leads to a more stable training rocedure
and imroved erformance by reducing aroximation
error variance in the target values. To understand
the effect of the algorithm, we examine
the source of value function estimation errors and
rovide an analytical comarison within a simlified
model. We further resent exeriments
on the Arcade Learning Environment benchmark
that demonstrate significantly imroved stability
and erformance due to the roosed extension.
"
389,2017,Deep Latent Dirichlet Allocation with Topic-Layer-Adaptive Stochastic Gradient Riemannian MCMC,Oral/Poster,"It is challenging to develo stochastic gradient based scalable inference for dee discrete latent variable models (LVMs), due to the difficulties in not only comuting the gradients, but also adating the ste sizes to different latent factors and hidden layers. For the Poisson gamma belief network (PGBN), a recently roosed dee discrete LVM, we derive an alternative reresentation that is referred to as dee latent Dirichlet allocation (DLDA). Exloiting data augmentation and marginalization techniques, we derive a block-diagonal Fisher information matrix and its inverse for the simlex-constrained global model arameters of DLDA. Exloiting that Fisher information matrix with stochastic gradient MCMC, we resent toic-layer-adative stochastic gradient Riemannian (TLASGR) MCMC that jointly learns simlex-constrained global arameters across all layers and toics, with toic and layer secific learning rates. State-of-the-art results are demonstrated on big data sets.
"
390,2017,Estimating individual treatment effect: generalization bounds and algorithms,Oral/Poster,"There is intense interest in alying machine learning to roblems of causal inference in fields such as healthcare, economics and education. In articular, individual-level causal inference has imortant alications such as recision medicine. We give a new theoretical analysis and family of algorithms for redicting individual treatment effect (ITE) from observational data, under the assumtion known as strong ignorability. The algorithms learn a ``balanced'' reresentation such that the induced treated and control distributions look similar, and we give a novel and intuitive generalization-error bound showing the exected ITE estimation error of a reresentation is bounded by a sum of the standard generalization-error of that reresentation and the distance between the treated and control distributions induced by the reresentation. We use Integral Probability Metrics to measure distances between distributions, deriving exlicit bounds for the Wasserstein and Maximum Mean Discreancy (MMD) distances. Exeriments on real and simulated data show the new algorithms match or outerform the state-of-the-art.
"
391,2017,"Collect at Once, Use Effectively: Making Non-interactive Locally Private Learning Possible",Oral/Poster,"Non-interactive Local Differential Privacy (LDP) requires data analysts to collect data from users through noisy channel at once. In this aer, we extend the frontiers of Non-interactive LDP learning and estimation from several asects. For learning with smooth generalized linear losses, we roose an aroximate stochastic gradient oracle estimated from non-interactive LDP channel using Chebyshev exansion, which is combined with inexact gradient methods to obtain an efficient algorithm with quasi-olynomial samle comlexity bound. For the high-dimensional world, we discover that under $\ell_2$-norm assumtion on data oints, high-dimensional sarse linear regression and mean estimation can be achieved with logarithmic deendence on dimension, using random rojection and aroximate recovery.  We also extend our methods to Kernel Ridge Regression. Our work is the first one that makes learning and estimation ossible for a broad range of learning tasks under non-interactive LDP model."
392,2017,Variational Policy for Guiding Point Processes,Oral/Poster,"Temoral oint rocesses have been widely alied to model event sequence data generated by online users. In this aer, we consider the roblem of how to design the otimal control olicy for oint rocesses, such that the stochastic system driven by the oint rocess is steered to a target state. In articular, we exloit the key insight to view the stochastic otimal control roblem from the ersective of otimal measure and variational inference. We further roose a convex otimization framework and an efficient algorithm to udate the olicy adatively to the current system state. Exeriments on synthetic and real-world data show that our algorithm can steer the user activities much more accurately and efficiently than other stochastic control methods. 
"
393,2017,Dance Dance Convolution,Oral/Poster,"Dance Dance Revolution (DDR) is a oular rhythm-based video game. Players erform stes on a dance latform in synchronization with music as directed by on-screen ste charts. While many ste charts are available in standardized acks, layers may grow tired of existing charts, or wish to dance to a song for which no chart exists. We introduce the task of learning to choreograh. Given a raw audio track, the goal is to roduce a new ste chart. This task decomoses naturally into two subtasks: deciding when to lace stes and deciding which stes to select. For the ste lacement task, we combine recurrent and convolutional neural networks to ingest sectrograms of low-level audio features to redict stes, conditioned on chart difficulty. For ste selection, we resent a conditional LSTM generative model that substantially outerforms n-gram and fixed-window aroaches.
"
394,2017,Language Modeling with Gated Convolutional Networks,Oral/Poster,"The re-dominant aroach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to cature unbounded context. In this aer we develo a finite context aroach through stacked convolutions, which can be more efficient since they allow arallelization over sequential tokens. We roose a novel simlified gating mechanism that outerforms Oord et al. (2016) and investigate the imact of key architectural decisions. The roosed aroach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term deendencies, as well as cometitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude comared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent aroach is cometitive with strong recurrent models on these large scale language tasks.
"
395,2017,"Deletion-Robust Submodular Maximization: Data Summarization with ""the Right to be Forgotten""",Oral/Poster,"How can we summarize a dynamic data stream when elements selected for the summary can be deleted at any time? This is an imortant challenge in online services, where the users generating the data may decide to exercise their right to restrict the service rovider from using (art of) their data due to rivacy concerns. Motivated by this challenge, we introduce the dynamic deletion-robust submodular maximization roblem. We develo the first resilient streaming algorithm, called ROBUST-STREAMING, with a constant factor aroximation guarantee to the otimum solution. We evaluate the effectiveness of our aroach on several real-world alica tions, including summarizing (1) streams of geo-coordinates (2); streams of images; and (3) click-stream log data, consisting of 45 million feature vectors from a news recommendation task.
"
396,2017,FeUdal Networks for Hierarchical Reinforcement Learning,Oral/Poster,"We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our aroach is insired by the feudal reinforcement learning roosal of Dayan and Hinton, and gains ower and efficacy by decouling end-to-end learning across multile levels -- allowing it to utilise different resolutions of time. Our framework emloys a Manager module and a Worker module. The Manager oerates at a slower time scale and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates rimitive actions at every tick of the environment. The decouled structure of FuN conveys several benefits -- in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-olicies associated with different goals set by the Manager. These roerties allow FuN to dramatically outerform a strong baseline agent on tasks that involve long-term credit assignment or memorisation.
"
397,2017,Distributed Batch Gaussian Process Optimization,Oral/Poster,"This aer resents a novel distributed batch Gaussian rocess uer confidence bound (DB-GP-UCB) algorithm for erforming batch Bayesian otimization (BO) of highly comlex, costly-to-evaluate black-box objective functions. In contrast to existing batch BO algorithms, DB-GP-UCB can jointly otimize a batch of inuts (as oosed to selecting the inuts of a batch one at a time) while still reserving scalability in the batch size. To realize this, we generalize GP-UCB to a new batch variant amenable to a Markov aroximation, which can then be naturally formulated as a multi-agent distributed constraint otimization roblem in order to fully exloit the efficiency of its state-of-the-art solvers for achieving linear time in the batch size. Our DB-GP-UCB algorithm offers ractitioners the flexibility to trade off between the aroximation quality and time efficiency by varying the Markov order. We rovide a theoretical guarantee for the convergence rate of DB-GP-UCB via bounds on its cumulative regret. Emirical evaluation on synthetic benchmark objective functions and a real-world otimization roblem shows that DB-GP-UCB outerforms the state-of-the-art batch BO algorithms.
"
398,2017,Recursive Partitioning for Personalization using Observational Data,Oral/Poster,"We study the roblem of learning to choose from $m$ discrete treatment otions (e.g., news item or medical drug) the one with best causal effect for a articular instance (e.g., user or atient) where the training data consists of assive observations of covariates, treatment, and the outcome of the treatment. The standard aroach to this roblem is regress and comare: slit the training data by treatment, fit a regression model in each slit, and, for a new instance, redict all $m$ outcomes and ick the best. By reformulating the roblem as a single learning task rather than $m$ searate ones, we roose a new aroach based on recursively artitioning the data into regimes where different treatments are otimal. We extend this aroach to an otimal artitioning aroach that finds a globally otimal artition, achieving a comact, interretable, and imactful ersonalization model. We develo new tools for validating and evaluating ersonalization models on observational data and use these to demonstrate the ower of our novel aroaches in a ersonalized medicine and a job training alication."
399,2017,Optimal Densification for Fast and Accurate Minwise Hashing,Oral/Poster,"Minwise hashing is a fundamental and one of the most successful hashing algorithm in the literature. Recent advances based on the idea of densification~\cite{Proc:OneHashLSH_ICML14,Proc:Shrivastava_UAI14} have shown that it is ossible to comute $k$ minwise hashes, of a vector with $d$ nonzeros, in mere $(d + k)$ comutations, a significant imrovement over the classical $O(dk)$. These advances have led to an algorithmic imrovement in the query comlexity of traditional indexing algorithms based on minwise hashing.  Unfortunately, the variance of the current densification techniques is unnecessarily high, which leads to significantly oor accuracy comared to vanilla minwise hashing, esecially when the data is sarse. In this aer, we rovide a novel densification scheme which relies on carefully tailored 2-universal hashes. We show that the roosed scheme is variance-otimal, and without losing the runtime efficiency, it is significantly more accurate than existing densification techniques.  As a result, we obtain a significantly efficient hashing scheme which has the same variance and collision robability as minwise hashing. Exerimental evaluations on real sarse and high-dimensional datasets validate our claims. We believe that given the significant advantages, our method will relace minwise hashing imlementations in ractice."
400,2017,An Adaptive Test of Independence with Analytic Kernel Embeddings,Oral/Poster,"A new comutationally efficient deendence measure, and an adative statistical test of indeendence, are roosed. The deendence measure is the difference between analytic embeddings of the joint distribution and the roduct of the marginals, evaluated at a finite set of locations (features). These features are chosen so as to maximize a lower bound on the test ower, resulting in a test that is data-efficient, and that runs in linear time (with resect to the samle size n). The otimized features can be interreted as evidence to reject the null hyothesis, indicating regions in the joint domain where the joint distribution and the roduct of the marginals differ most. Consistency of the indeendence test is established, for an aroriate choice of features. In real-world benchmarks, indeendence tests using the otimized features erform comarably to the state-of-the-art quadratic-time HSIC test, and outerform cometing O(n) and O(n log n) tests.
"
401,2017,Deep Value Networks Learn to Evaluate and Iteratively Refine Structured Outputs,Oral/Poster,"We aroach structured outut rediction by otimizing a dee value network (DVN) to recisely estimate the task loss on different outut configurations for a given inut. Once the model is trained, we erform inference by gradient descent on the continuous relaxations of the outut variables to find oututs with romising scores from the value network. When alied to image segmentation, the value network takes an image and a segmentation mask as inuts and redicts a scalar estimating the intersection over union between the inut and ground truth masks. For multi-label classification, the DVN's objective is to correctly redict the F1 score for any otential label configuration. The
DVN framework achieves the state-of-the-art results on multi-label rediction and image segmentation benchmarks.
"
402,2017,World of Bits: An Open-Domain Platform for Web-Based Agents,Oral/Poster,"While simulated game environments have greatly accelerated research in reinforcement learning, existing environments lack the oen-domain realism of tasks in comuter vision or natural language rocessing, which oerate on artifacts created by humans in natural, organic settings. To foster reinforcement learning research in such settings, we introduce the World of Bits (WoB), a latform in which agents comlete tasks on the Internet by erforming low-level keyboard and mouse actions. The two main challenges are: (i) to curate a large, diverse set of interesting web-based tasks, and (ii) to ensure that these tasks have a well-defined reward structure and are reroducible desite the transience of the web. To do this, we develo a methodology in which crowdworkers create tasks defined by natural language questions and rovide demonstrations of how to answer the question on real websites using keyboard and mouse; HTTP traffic is cached to create a reroducible offline aroximation of the web site. Finally, we show that agents trained via behavioral cloning and reinforcement learning can successfully comlete a range of our web-based tasks.
"
403,2017,Convolutional Sequence to Sequence Learning,Oral/Poster,"The revalent aroach to sequence to sequence learning mas an inut sequence to a variable length outut sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Comared to recurrent models, comutations over all elements can be fully arallelized during training to better exloit the GPU hardware and otimization is easier since the number of non-linearities is fixed and indeendent of the inut length. Our use of gated linear units eases gradient roagation and we equi each decoder layer with a searate attention module. We outerform the accuracy of the dee LSTM setu of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster seed, both on GPU and CPU.
"
404,2017,Analysis and Optimization of Graph Decompositions by Lifted Multicuts,Oral/Poster,"We study the set of all decomositions (clusterings) of a grah through its characterization as a set of lifted multicuts. This leads us to ractically relevant insights related to the definition of classes of decomositions by must-join and must-cut constraints and related to the comarison of clusterings by metrics. To find otimal decomositions defined by minimum cost lifted multicuts, we establish some roerties of some facets of lifted multicut olytoes, define efficient searation rocedures and aly these in a branch-and-cut algorithm.
"
405,2017,Deciding How to Decide: Dynamic Routing in Artificial Neural Networks,Oral/Poster,"We roose and systematically evaluate three strategies for training dynamically-routed artificial neural networks: grahs of learned transformations through which different inut signals may take different aths. Though some aroaches have advantages over others, the resulting networks are often qualitatively similar. We find that, in dynamically-routed networks trained to classify images, layers and branches become secialized to rocess distinct categories of images. Additionally, given a fixed comutational budget, dynamically-routed networks tend to erform better than comarable statically-routed networks.
"
406,2017,Scalable Multi-Class Gaussian Process Classification using Expectation Propagation,Oral/Poster,"This aer describes an exectation roagation (EP) method for multi-class classification with Gaussian rocesses that scales well to very large datasets. In such a method the estimate of the log-marginal-likelihood involves a sum across the data instances. This enables efficient training using stochastic gradients and mini-batches. When this tye of training is used, the comutational cost does not deend on the number of data instances N. Furthermore, extra assumtions in the aroximate inference rocess make the memory cost indeendent of N. The  consequence is that the roosed EP method can be used on datasets with millions of instances. We comare emirically this method with alternative aroaches that aroximate the required comutations using variational inference. The results show that it erforms similar or even better than these techniques, which sometimes give significantly worse redictive distributions in terms of the test log-likelihood. Besides this, the training rocess of the roosed aroach also seems to converge in a smaller number of iterations.
"
407,2017,Identifying Best Interventions through Online Importance Sampling,Oral/Poster,"Motivated by alications in comutational advertising and systems biology, we consider the roblem of identifying the best out of several ossible soft interventions at a source node $V$ in an acyclic causal directed grah, to maximize the exected value of a target node $Y$ (located downstream of $V$). Our setting imoses a fixed total budget for samling under various interventions, along with cost constraints on different tyes of interventions. We ose this as a best arm identification bandit roblem with $K$ arms, where each arm is a soft intervention at $V$ and leverage the information leakage among the arms to rovide the first ga deendent error and simle regret bounds for this roblem. Our results are a significant imrovement over the traditional best arm identification results. We emirically show that our algorithms outerform the state of the art in the Flow Cytometry data-set, and also aly our algorithm for model interretation of the Incetion-v3 dee net that classifies images."
408,2017,Stochastic Generative Hashing,Oral/Poster,"Learning-based binary hashing has become a owerful aradigm for fast search and retrieval in massive databases. However, due to the requirement of discrete oututs for the hash functions, learning such functions is known to be very challenging. In addition, the objective functions adoted by existing hashing techniques are mostly chosen heuristically. In this aer, we roose a novel generative aroach to learn hash functions through Minimum Descrition Length rincile such that the learned hash codes maximally comress the dataset and can also be used to regenerate the inuts. We also develo an efficient learning algorithm based on the stochastic distributional gradient, which avoids the notorious difficulty caused by binary outut constraints, to jointly otimize the arameters of the hash function and the associated generative model. Extensive exeriments on a variety of large-scale datasets show that the roosed method achieves better retrieval results than the existing state-of-the-art methods. 
"
409,2017,Sliced Wasserstein Kernel for Persistence Diagrams,Oral/Poster,"Persistence diagrams (PDs) lay a key role in toological data analysis (TDA), in which they are routinely used to describe succinctly comlex toological roerties of comlicated shaes. PDs enjoy strong stability roerties and have roven their utility in various learning contexts.  They do not, however, live in a sace naturally endowed with a Hilbert structure and are usually comared with secific distances, such as the bottleneck distance. To incororate PDs in a learning ieline, several kernels have been roosed for PDs with a strong emhasis on the stability of the RKHS distance w.r.t. erturbations of the PDs.  In this article, we use the Sliced Wasserstein aroximation $\SW$ of the Wasserstein distance to define a new kernel for PDs, which is not only rovably stable but also rovably discriminative w.r.t. the Wasserstein distance $W^1_\infty$ between PDs. We also demonstrate its racticality, by develoing an aroximation technique to reduce kernel comutation time, and show that our roosal comares favorably to existing kernels for PDs on several benchmarks.
"
410,2017,Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction,Oral/Poster,"Recently, researchers have demonstrated state-of-the-art erformance on sequential rediction roblems using dee neural networks and Reinforcement Learning (RL). For some of these roblems, oracles that can demonstrate good erformance may be available during training, but are not used by lain RL methods. To take advantage of this extra information, we roose AggreVaTeD, an extension of the Imitation Learning (IL) aroach of Ross &am; Bagnell (2014). AggreVaTeD allows us to use exressive differentiable olicy reresentations such as dee networks, while leveraging training-time oracles to achieve faster and more accurate solutions with less training data. Secifically, we resent two gradient rocedures that can learn neural network olicies for several roblems, including a sequential rediction task and several high-dimensional robotics control roblems. We also rovide a comrehensive theoretical study of IL that demonstrates that we can exect u to exonentially-lower samle comlexity for learning with AggreVaTeD than with lain RL algorithms. Our results and theory indicate that IL (and AggreVaTeD in articular) can be a more effective strategy for sequential rediction than lain RL. 
"
411,2017,Real-Time Adaptive Image Compression,Oral/Poster,"We resent a machine learning-based aroach to lossy image comression which outerforms all existing codecs, while running in real-time.

Our algorithm tyically roduces file sizes 3 times smaller than JPEG, 2.5 times smaller than JPEG 2000, and 2.3 times smaller than WebP on datasets of generic images across a sectrum of quality levels. At the same time, our codec is designed to be lightweight and deloyable: for examle, it can encode or decode the Kodak dataset in less than 10ms er image on GPU. 

Our architecture is an autoencoder featuring yramidal analysis, an adative coding module, and regularization of the exected codelength. We also sulement our aroach with adversarial training secialized towards use in a comression setting: this enables us to roduce visually leasing reconstructions for very low bitrates.
"
412,2017,Improved Variational Autoencoders for Text Modeling using Dilated Convolutions,Oral/Poster,"Recent work on generative text modeling has found that variational autoencoders (VAE) with LSTM decoders erform worse than simler LSTM language models (Bowman et al., 2015). This negative result is so far oorly understood, but has been attributed to the roensity of LSTM decoders to ignore conditioning informa- tion from the encoder. In this aer, we ex- eriment with a new tye of decoder for VAE: a dilated CNN. By changing the decoder’s di- lation architecture, we control the size of con- text from reviously generated words. In ex- eriments, we find that there is a trade-off be- tween contextual caacity of the decoder and ef- fective use of encoding information. We show that when carefully managed, VAEs can outer- form LSTM language models. We demonstrate erlexity gains on two datasets, reresenting the first ositive language modeling result with VAE. Further, we conduct an in-deth investigation of the use of VAE (with our new decoding archi- tecture) for semi-suervised and unsuervised la- beling tasks, demonstrating gains over several strong baselines.
"
413,2017,Near-Optimal Design of Experiments via Regret Minimization,Oral/Poster,"We consider comutationally tractable methods for the exerimental design roblem, where k out of n design oints of dimension  are selected so that certain otimality criteria are aroximately satisfied. Our algorithm finds a (1+es)-aroximate otimal design when k is a linear function of ; in contrast, existing results require k to be suer-linear in . Our algorithm also handles all oular otimality criteria, while existing ones only handle one or two such criteria. Numerical results on synthetic and real-world design roblems verify the ractical effectiveness of the roosed algorithm.
"
414,2017,Neural Episodic Control,Oral/Poster,"Dee reinforcement learning methods attain suer-human erformance in a wide range of environments.
Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable erformance.
We roose Neural Eisodic Control: a dee reinforcement learning agent that is able to raidly assimilate new exeriences and act uon them. 
Our agent uses a semi-tabular reresentation of the value function: a buffer of ast exerience containing slowly changing state reresentations and raidly udated estimates of the value function.
We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general urose dee reinforcement learning agents.
"
415,2017,Random Feature Expansions for Deep Gaussian Processes,Oral/Poster,"The comosition of multile Gaussian Processes as a Dee Gaussian Process DGP enables a dee robabilistic nonarametric aroach to flexibly tackle comlex machine learning roblems with sound quantification of uncertainty. Existing inference aroaches for DGP models have limited scalability and are notoriously cumbersome to construct. In this work we introduce a novel formulation of DGPs based on random feature exansions that we train using stochastic variational inference. This yields a ractical learning framework which significantly advances the state-of-the-art in inference for DGPs, and enables accurate quantification of uncertainty. We extensively showcase the scalability and erformance of our roosal on several datasets with u to 8 million observations, and various DGP architectures with u to 30 hidden layers. 
"
416,2017,Deep IV: A Flexible Approach for Counterfactual Prediction,Oral/Poster,"Counterfactual rediction requires understanding causal relationshis between so-called treatment and outcome variables. This aer rovides a recie for augmenting dee learning methods to accurately characterize such relationshis in the resence of instrument variables (IVs) -- sources of treatment randomization that are conditionally indeendent from the outcomes.  Our IV secification resolves into two rediction tasks that can be solved with dee neural nets: a first-stage network for treatment rediction and a second-stage network whose  loss function involves integration over the conditional treatment distribution.  This Dee IV framework allows us to take advantage of off-the-shelf suervised learning techniques to estimate causal effects by adating the loss function. Exeriments show that it outerforms existing machine learning aroaches. 
"
417,2017,"ZipML: Training Linear Models with End-to-End Low Precision, and a Little Bit of Deep Learning",Oral/Poster,"Recently there has been significant interest in training machine-learning models at low recision: by reducing recision, one can reduce comutation and communication by one order of magnitude. We examine training at reduced recision, both from a theoretical and ractical ersective, and ask: is it ossible to train models at end-to-end low recision with rovable guarantees? Can this lead to consistent order-of-magnitude seedus? We mainly focus on linear models, and the answer is yes for linear models. We develo a simle framework called ZiML based on one simle but novel strategy called double samling. Our ZiML framework is able to execute training at low recision with no bias, guaranteeing convergence, whereas naive quantization would introduce significant bias. We validate our framework across a range of alications, and show that it enables an FPGA rototye that is u to 6.5× faster than an imlementation using full 32-bit recision. We further develo a variance-otimal stochastic quantization strategy and show that it can make a significant difference in a variety of settings. When alied to linear models together with double samling, we save u to another 1.7× in data movement comared with uniform quantization. When training dee networks with quantized models, we achieve higher accuracy than the state-of-the-art XNOR-Net.
"
418,2017,Adapting Kernel Representations Online Using Submodular Maximization,Oral/Poster,"Kernel reresentations rovide a nonlinear reresentation, through similarities to rototyes, but require only simle linear learning algorithms given those rototyes. In a continual learning setting, with a constant stream of observations, it is critical to have an efficient mechanism for sub-selecting rototyes amongst observations. In this work, we develo an aroximately submodular criterion for this setting, and an efficient online greedy submodular maximization algorithm for otimizing the criterion. We extend streaming submodular maximization algorithms to continual learning, by removing the need for multile asses---which is infeasible---and instead introducing the idea of coverage time. We roose a general block-diagonal aroximation for the greedy udate with our criterion, that enables udates linear in the number of rototyes. We emirically demonstrate the effectiveness of this aroximation, in terms of aroximation quality, significant runtime imrovements, and effective rediction erformance.
"
419,2017,End-to-End Learning for Structured Prediction Energy Networks,Oral/Poster,"Structured Prediction Energy Networks (SPENs) are a simle, yet exressive family of structured rediction models (Belanger and McCallum, 2016).  An energy function over candidate structured oututs is given by a dee network, and redictions are formed by gradient-based otimization. This aer resents end-to-end learning for SPENs, where the energy function is discriminatively trained by back-roagating through gradient-based rediction. In our exerience, the aroach is substantially more accurate than the structured SVM method of Belanger and McCallum (2016), as it allows us to use more sohisticated non-convex energies. We rovide a collection of techniques for imroving the seed, accuracy, and memory requirements of end-to-end SPENs, and demonstrate the ower of our method on 7-Scenes image denoising and CoNLL-2005 semantic role labeling tasks. In both, inexact minimization of non-convex SPEN energies is suerior to baseline methods that use simlistic energy functions that can be minimized exactly.
"
420,2017,Neural Message Passing for Quantum Chemistry,Oral/Poster,"Suervised learning on molecules has incredible otential to be useful in chemistry, drug discovery, and materials science. Luckily, several romising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message assing algorithm and aggregation rocedure to comute a function of their entire inut grah. At this oint, the next ste is to find a articularly effective variant of this general aroach and aly it to chemical rediction benchmarks until we either solve them or reach the limits of the aroach. In this aer, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and exlore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an imortant molecular roerty rediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.
"
421,2017,Grammar Variational Autoencoder,Oral/Poster,"Dee generative models have been wildly successful at learning coherent latent reresentations for continuous data such as natural images, artwork, and audio. However, generative modeling of discrete data such as arithmetic exressions and molecular structures still oses significant challenges. Crucially, state-of-the-art methods often roduce oututs that are not valid. We make the key observation that frequently, discrete data can be reresented as a arse tree from a context-free grammar. We roose a variational autoencoder which directly encodes from and decodes to these arse trees, ensuring the generated oututs are always syntactically valid. Surrisingly, we show that not only does our model more often generate valid oututs, it also learns a more coherent latent sace in which nearby oints decode to similar discrete oututs. We demonstrate the effectiveness of our learned models by showing their imroved erformance in Bayesian otimization for symbolic regression and molecule generation.
"
422,2017,Robust Budget Allocation via Continuous Submodular Functions,Oral/Poster,"The otimal allocation of resources for maximizing influence, sread of information or coverage, has gained attention in the ast years, in articular in machine learning and data mining. But in alications, the arameters of the roblem are rarely known exactly, and using wrong arameters can lead to undesirable outcomes. We hence revisit a continuous version of the Budget Allocation or Biartite Influence Maximization roblem introduced by Alon et al. (2012) from a robust otimization ersective, where an adversary may choose the least favorable arameters within a confidence set. The resulting roblem is a nonconvex-concave saddle oint roblem (or game). We show that this nonconvex roblem can be solved exactly by leveraging connections to continuous submodular functions, and by solving a constrained submodular minimization roblem. Although constrained submodular minimization is hard in general, here, we establish conditions under which such a roblem can be solved to arbitrary recision esilon.
"
423,2017,Neural Optimizer Search using Reinforcement Learning,Oral/Poster,"We resent an aroach to automate the rocess of discovering otimization methods, with a focus on dee learning architectures. We train a Recurrent Neural Network controller to generate a string in a secific domain language that describes a mathematical udate equation based on a list of rimitive functions, such as the gradient, running average of the gradient, etc. The controller is trained with Reinforcement Learning to maximize the erformance of a model after a few eochs. On CIFAR-10, our method discovers several udate rules that are better than many commonly used otimizers, such as Adam, RMSPro, or SGD with and without Momentum on a ConvNet model. These otimizers can also be transferred to erform well on different neural network architectures, including Google’s neural machine translation system.
"
424,2017,Asynchronous Distributed Variational Gaussian Processes for Regression,Oral/Poster,"Gaussian rocesses (GPs) are owerful non-arametric function estimators. However, their alications are largely limited by the exensive comutational cost of the inference rocedures. Existing stochastic or distributed synchronous variational inferences, although have alleviated this issue by scaling u GPs to millions of samles, are still far from satisfactory for real-world large alications, where the data sizes are often orders of magnitudes larger, say, billions. To solve this roblem, we roose ADVGP, the first Asynchronous Distributed Variational Gaussian Process inference for regression, on the recent large-scale machine learning latform, PARAMETER SERVER. ADVGP uses a novel, flexible variational framework based on a weight sace augmentation, and imlements the highly efficient, asynchronous roximal gradient otimization. While maintaining comarable or better redictive erformance, ADVGP greatly imroves uon the efficiency of the existing variational methods. With ADVGP, we effortlessly scale u GP regression to a real-world alication with billions of samles and demonstrate an excellent, suerior rediction accuracy to the oular linear models.
"
425,2017,Counterfactual Data-Fusion for Online Reinforcement Learners,Oral/Poster,"The Multi-Armed Bandit roblem with Unobserved Confounders (MABUC) considers decision-making settings where unmeasured variables can influence both the agent's decisions and received rewards (Bareinboim et al., 2015). Recent findings showed that unobserved confounders (UCs) ose a unique challenge to algorithms based on standard randomization (i.e., exerimental data); if UCs are naively averaged out, these algorithms behave sub-otimally, ossibly incurring infinite regret. In this aer, we show how counterfactual-based decision-making circumvents these roblems and leads to a coherent fusion of observational and exerimental data. We then demonstrate this new strategy in an enhanced Thomson Samling bandit layer, and suort our findings' efficacy with extensive simulations.
"
426,2017,Large-Scale Evolution of Image Classifiers,Oral/Poster,"Neural networks have roven effective at solving difficult roblems but designing their architectures can be challenging, even for image classification roblems alone. Our goal is to minimize human articiation, so we emloy evolutionary algorithms to discover such networks automatically. Desite significant comutational requirements, we show that it is now ossible to evolve models with accuracies within the range of those ublished in the last year. Secifically, we emloy simle evolutionary techniques at unrecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6% (95.6% for ensemble) and 77.0%, resectively. To do this, we use novel and intuitive mutation oerators that navigate large search saces; we stress that no human articiation is required once evolution starts and that the outut is a fully-trained model. Throughout this work, we lace secial emhasis on the reeatability of results, the variability in the outcomes and the comutational requirements.
"
427,2017,Spherical Structured Feature Maps for Kernel Approximation,Oral/Poster,"We roose Sherical Structured Feature (SSF) mas to aroximate shift and rotation invariant kernels as well as $b^{th}$-order arc-cosine kernels~\cite{cho2009kerneldeelearning}.  We construct SSF mas based on the oint set on $d-1$ dimensional shere $\mathbb{S}^{d-1}$. We rove that the inner roduct of SSF mas are unbiased estimates for above kernels if asymtotically uniformly distributed oint set  on $\mathbb{S}^{d-1}$ is given. According to ~\cite{brauchart2015distributing}, otimizing
the discrete Riesz s-energy can generate asymtotically uniformly distributed oint set on $\mathbb{S}^{d-1}$. Thus, we roose 
an efficient coordinate decent method to find a local otimum of the discrete Riesz s-energy for SSF mas construction. Theoretically, SSF mas construction achieves linear sace comlexity and loglinear time comlexity.  Emirically, SSF mas achieve suerior erformance comared with  other methods. "
428,2017,A Unified View of Multi-Label Performance Measures,Oral/Poster,"Multi-label classification deals with the roblem where each instance is associated with multile class labels. Because evaluation in multi-label classification is more comlicated than single-label setting, a number of erformance measures have been roosed. It is noticed that an algorithm usually erforms differently on different measures. Therefore, it is imortant to understand which algorithms erform well on which measure(s) and why. In this aer, we roose a unified margin view to revisit eleven erformance measures in multi-label classification. In articular, we define label-wise margin and instance-wise margin, and rove that through maximizing these margins, different corresonding erformance measures are to be otimized. Based on the defined margins, a max-margin aroach called LIMO is designed and emirical results validate our theoretical findings.
"
429,2017,Accelerating Eulerian Fluid Simulation With Convolutional Networks,Oral/Poster,"Efficient simulation of the Navier-Stokes equations for fluid flow is a long standing roblem in alied mathematics, for which state-of-the-art methods require large comute resources. In this work, we roose a data-driven aroach that leverages the aroximation ower of dee-learning with the recision of standard solvers to obtain fast and highly realistic simulations. Our method solves the incomressible Euler equations using the standard oerator slitting method, in which a large sarse linear system with many free arameters must be solved. We use a Convolutional Network with a highly tailored architecture, trained using a novel unsuervised learning framework to solve the linear system. We resent real-time 2D and 3D simulations that outerform recently roosed data-driven methods; the obtained results are realistic and show good generalization roerties.
"
430,2017,Rule-Enhanced Penalized Regression by Column Generation using Rectangular Maximum Agreement,Oral/Poster,"We describe a learning rocedure enhancing L1-enalized regression by adding dynamically generated rules describing multidimensional “box” sets. Our rule-adding rocedure is based on the classical column generation method for high-dimensional linear rogramming. The ricing roblem for our column generation rocedure reduces to the NP-hard rectangular maximum agreement (RMA) roblem of finding a box that best discriminates between two weighted datasets. We solve this roblem exactly using a arallel branch-and-bound rocedure. The resulting rule-enhanced regression rocedure is comutation-intensive, but has romising rediction erformance.
"
431,2017,High Dimensional Bayesian Optimization with Elastic Gaussian Process,Oral/Poster,"Bayesian otimization is an efficient way to otimize exensive black-box functions such as designing a new roduct with highest quality or hyerarameter tuning of a machine learning algorithm. However, it has a serious limitation when the arameter sace is high-dimensional as Bayesian otimization crucially deends on solving a global otimization of a surrogate utility function in the same sized dimensions. The surrogate utility function, known commonly as acquisition function is a continuous function but can be extremely shar at high dimension - having only a few eaks marooned in a large terrain of almost flat surface. Global otimization algorithms such as DIRECT are infeasible at higher dimensions and gradient-deendent methods cannot move if initialized in the flat terrain. We roose an algorithm that enables local gradient-deendent algorithms to move through the flat terrain by using a sequence of gross-to-finer Gaussian rocess riors on the objective function as we leverage two underlying facts - a) there exists a large enough length-scales for which the acquisition function can be made to have a significant gradient at any location in the arameter sace, and b) the extrema of the consecutive acquisition functions are close although they are different only due to a small difference in the length-scales. Theoretical guarantees are rovided and exeriments clearly demonstrate the utility of the roosed method at high dimension using both benchmark test functions and real-world case studies. 
"
432,2017,Nyström Method with Kernel K-means++ Samples as Landmarks,Oral/Poster,"We investigate, theoretically and emirically, the effectiveness of kernel K-means++ samles as landmarks in the Nyström method for low-rank aroximation of kernel matrices. Previous emirical studies (Zhang et al., 2008; Kumar et al.,2012) observe that the landmarks obtained using (kernel) K-means clustering define a good low-rank aroximation of kernel matrices. However, the existing work does not rovide a theoretical guarantee on the aroximation error for this aroach to landmark selection. We close this ga and rovide the first bound on the aroximation error of the Nystrom method with kernel K-means++ samles as landmarks. Moreover, for the frequently used Gaussian kernel we rovide a theoretically sound motivation for erforming Lloyd refinements of kernel K-means++ landmarks in the instance sace. We substantiate our theoretical results emirically by comaring the aroach to several state-of-the-art algorithms.
"
433,2017,Scalable Generative Models for Multi-label Learning with Missing Labels,Oral/Poster,"We resent a scalable, generative framework for multi-label learning with missing labels. Our framework consists of a latent factor model for the binary label matrix, which is couled with an exosure model to account for label missingness (i.e., whether a zero in the label matrix is indeed a zero or denotes a missing observation). The underlying latent factor model also assumes that the low-dimensional embeddings of each label vector are directly conditioned on the resective feature vector of that examle. Our generative framework admits a simle inference rocedure, such that the arameter estimation reduces to a sequence of simle weighted least-square regression roblems, each of which can be solved easily, efficiently, and in arallel. Moreover, inference can also be erformed in an online fashion using mini-batches of training examles, which makes our framework scalable for large data sets, even when using moderate comutational resources. We reort both quantitative and qualitative results for our framework on several benchmark data sets, comaring it with a number of state-of-the-art methods.
"
