,Year,Title,Decision,Abstract
0,2018,Spline Filters For End-to-End Deep Learning,Oral/Poster,"We roose to tackle the roblem of end-to-end learning for raw waveforms signals by introducing learnable continuous time-frequency atoms. The derivation of these filters is achieved by first, defining a functional sace with a given smoothness order and boundary conditions. From this sace, we derive the arametric analytical filters. Their differentiability roerty allows gradient-based otimization. As such, one can equi any Dee Neural Networks (DNNs) with these filters. This enables us to tackle in a front-end fashion a large scale bird detection task based on the freefield1010 dataset known to contain key challenges, such as high dimensional inuts ($100000$) and the resence of multile sources and soundscaes."
1,2018,Non-linear motor control by local learning in spiking neural networks,Oral/Poster,"Learning weights in a siking neural network with hidden neurons, using local, stable and online rules, to control non-linear body dynamics is an oen roblem. Here, we emloy a suervised scheme, Feedback-based Online Local Learning Of Weights (FOLLOW), to train a heterogeneous network of siking neurons with hidden layers, to control a two-link arm so as to reroduce a desired state trajectory. We show that the network learns an inverse model of the non-linear dynamics, i.e. it infers from state trajectory as inut to the network, the continuous-time command that roduced the trajectory. Connection weights are adjusted via a local lasticity rule that involves re-synatic firing and ost-synatic feedback of the error in the inferred command. We roose a network architecture, termed differential feedforward, and show that it gives a lower test error than other feedforward and recurrent architectures. We demonstrate the erformance of the inverse model to control a two-link arm along a desired trajectory.
"
2,2018,Implicit Quantile Networks for Distributional Reinforcement Learning,Oral/Poster,"In this work, we build on recent advances in distributional reinforcement learning to give a generally alicable, flexible, and state-of-the-art distributional variant of DQN. We achieve this by using quantile regression to aroximate the full quantile function for the state-action return distribution. By rearameterizing a distribution over the samle sace, this yields an imlicitly defined return distribution and gives rise to a large class of risk-sensitive olicies. We demonstrate imroved erformance on the 57 Atari 2600 games in the ALE, and use our algorithm's imlicitly defined distributions to study the effects of risk-sensitive olicies in Atari games.
"
3,2018,An Inference-Based Policy Gradient Method for Learning Options,Oral/Poster,"In the ursuit of increasingly intelligent learning systems, abstraction lays a vital role in enabling sohisticated decisions to be made in comlex environments. The otions framework rovides formalism for such abstraction over sequences of decisions.  However most models require that otions be given a riori, resumably secified by hand, which is neither efficient, nor scalable. Indeed, it is referable to learn otions directly from interaction with the environment.  Desite several efforts, this remains a difficult roblem. In this work we develo a novel olicy gradient method for the automatic learning of olicies with otions. This algorithm uses inference methods to simultaneously imrove all of the otions available to an agent, and thus can be emloyed in an off-olicy manner, without observing otion labels. The differentiable inference rocedure emloyed yields otions that can be easily interreted. Emirical results confirm these attributes, and indicate that our algorithm has an imroved samle efficiency relative to state-of-the-art in learning otions end-to-end.
"
4,2018,Predict and Constrain: Modeling Cardinality in Deep Structured Prediction,Oral/Poster,"Many machine learning roblems require the rediction of multi-dimensional labels. Such structured rediction models can benefit from modeling deendencies between labels. Recently, several dee learning aroaches to structured rediction have been roosed. Here we focus on caturing cardinality constraints in such models. Namely, constraining the number of non-zero labels that the model oututs. Such constraints have roven very useful in revious structured rediction methods, but it is a challenge to introduce them into a dee learning aroach. Here we show how to do this via a novel dee architecture. Our aroach outerforms strong baselines, achieving state-of-the-art results on multi-label classification benchmarks.
"
5,2018,Differentially Private Matrix Completion Revisited,Oral/Poster,"We rovide the first rovably joint differentially rivate algorithm with formal utility guarantees for the roblem of user-level rivacy-reserving collaborative filtering. Our algorithm is based on the Frank-Wolfe method, and it consistently estimates the underlying reference matrix as long as the number of users $m$ is $\omega(n^{54})$, where $n$ is the number of items, and each user rovides her reference for at least $\sqrt{n}$ randomly selected items. Along the way, we rovide an otimal differentially rivate algorithm for singular vector comutation, based on the celebrated Oja's method, that rovides significant savings in terms of sace and time while oerating on sarse matrices. We also emirically evaluate our algorithm on a suite of datasets, and show that it consistently outerforms the state-of-the-art rivate algorithms."
6,2018,Differentiable plasticity: training plastic neural networks with backpropagation,Oral/Poster,"How can we build agents that kee learning from exerience, quickly and efficiently, after their initial training? Here we take insiration from the main mechanism of learning in biological brains: synatic lasticity, carefully tuned by evolution to roduce efficient lifelong learning. We show that lasticity, just like connection weights, can be otimized by gradient descent in large (millions of arameters) recurrent networks with Hebbian lastic connections. First, recurrent lastic networks with more than two million arameters can be trained to memorize and reconstruct sets of novel, high-dimensional (1000+ ixels) natural images not seen during training. Crucially,   traditional non-lastic recurrent networks fail to solve this task. Furthermore, trained lastic networks can also solve generic meta-learning tasks such as the Omniglot task, with cometitive results and little arameter overhead. Finally, in reinforcement learning settings, lastic networks outerform non-lastic equivalent in a maze exloration task. We conclude that differentiable lasticity may rovide a owerful novel aroach to the learning-to-learn roblem.
"
7,2018,Model-Level Dual Learning,Oral/Poster,"Many artificial intelligence tasks aear in dual forms like English$\leftrightarrow$French translation and seech$\leftrightarrow$text transformation. Existing dual learning schemes, which are roosed to solve a air of such dual tasks, exlore how to leverage such dualities from data level. In this work, we roose a new learning framework, model-level dual learning, which takes duality of tasks into consideration while designing the architectures for the rimaldual models, and ties the model arameters that laying similar roles in the two tasks. We study both symmetric and asymmetric model-level dual learning. Our algorithms achieve significant imrovements on neural machine translation and sentiment analysis."
8,2018,CoVeR: Learning Covariate-Specific Vector Representations with Tensor Decompositions,Oral/Poster,"Word embedding is a useful aroach to cature co-occurrence structures in large text corora. However, in addition to the text data itself, we often have additional covariates associated with individual corus documents---e.g. the demograhic of the author, time and venue of ublication---and we would like the embedding to naturally cature this information. We roose CoVeR, a new tensor decomosition model for vector embeddings with covariates. CoVeR jointly learns a \emh{base} embedding for all the words as well as a weighted diagonal matrix to model how each covariate affects the base embedding. To obtain author or venue-secific embedding, for examle, we can then simly multily the base embedding by the associated transformation matrix. The main advantages of our aroach are data efficiency and interretability of the covariate transformation. Our exeriments demonstrate that our joint model learns substantially better covariate-secific embeddings comared to the standard aroach of learning a searate embedding for each covariate using only the relevant subset of data, as well as other related methods. Furthermore, CoVeR encourages the embeddings to be ``toic-aligned'' in that the dimensions have secific indeendent meanings. This allows our covariate-secific embeddings to be comared by toic, enabling downstream differential analysis. We emirically evaluate the benefits of our algorithm on datasets, and demonstrate how it can be used to address many natural questions about covariate effects.
"
9,2018,Tree Edit Distance Learning via Adaptive Symbol Embeddings,Oral/Poster,"Metric learning has the aim to imrove classification accuracy by learning a distance measure which brings data oints from the same class closer together and ushes data oints from different classes further aart. Recent research has demonstrated that metric learning aroaches can also be alied to trees, such as molecular structures, abstract syntax trees of comuter rograms, or syntax trees of natural language, by learning the cost function of an edit distance, i.e. the costs of relacing, deleting, or inserting nodes in a tree. However, learning such costs directly may yield an edit distance which violates metric axioms, is challenging to interret, and may not generalize well. In this contribution, we roose a novel metric learning aroach for trees which learns an edit distance indirectly by embedding the tree nodes as vectors, such that the Euclidean distance between those vectors suorts class discrimination. We learn such embeddings by reducing the distance to rototyical trees from the same class and increasing the distance to rototyical trees from different classes. In our exeriments, we show that our roosed metric learning aroach imroves uon the state-of-the-art in metric learning for trees on six benchmark data sets, ranging from comuter science over biomedical data to a natural-language rocessing data set containing over 300,000 nodes.
"
10,2018,Gradually Updated Neural Networks for Large-Scale Image Recognition,Oral/Poster,"Deth is one of the keys that make neural networks succeed in the task of large-scale image recognition. The state-of-the-art network architectures usually increase the deths by cascading convolutional layers or building blocks. In this aer, we resent an alternative method to increase the deth. Our method is by introducing comutation orderings to the channels within convolutional layers or blocks, based on which we gradually comute the oututs in a channel-wise manner. The added orderings not only increase the deths and the learning caacities of the networks without any additional comutation costs, but also eliminate the overla singularities so that the networks are able to converge faster and erform better. Exeriments show that the networks based on our method achieve the state-of-the-art erformances on CIFAR and ImageNet datasets.
"
11,2018,One-Shot Segmentation in Clutter,Oral/Poster,"We tackle the roblem of one-shot segmentation: finding and segmenting a reviously unseen object in a cluttered scene based on a single instruction examle. We roose a novel dataset, which we call {\it cluttered Omniglot}. Using a baseline architecture combining a Siamese embedding for detection with a U-net for segmentation we show that increasing levels of clutter make the task rogressively harder. Using oracle models with access to various amounts of ground-truth information, we evaluate different asects of the roblem and show that in this kind of visual search task, detection and segmentation are two intertwined roblems, the solution to each of which hels solving the other. We therefore introduce {\it MaskNet}, an imroved model that attends to multile candidate locations, generates segmentation roosals to mask out background clutter and selects among the segmented objects. Our findings suggest that such image recognition models based on an iterative refinement of object detection and foreground segmentation may rovide a way to deal with highly cluttered scenes.
"
12,2018,Active Testing: An Efficient and Robust Framework for Estimating Accuracy,Oral/Poster,"Much recent work on large-scale visual recogni- tion aims to scale u learning to massive, noisily- annotated datasets. We address the roblem of scaling-u the evaluation of such models to large- scale datasets with noisy labels. Current rotocols for doing so require a human user to either vet (re-annotate) a small fraction of the testset and ignore the rest, or else correct errors in annotation as they are found through manual insection of results. In this work, we re-formulate the roblem as one of active testing, and examine strategies for efficiently querying a user so as to obtain an accurate erformance estimate with minimal vet- ting. We demonstrate the effectiveness of our roosed active testing framework on estimating two erformance metrics, Precision@K and mean Average Precisions, for two oular Comuter Vi- sion tasks, multilabel classification and instance segmentation, resectively. We further show that our aroach is able to siginificantly save human annotation effort and more robust than alterna- tive evaluation rotocols.
"
13,2018,Learning Deep ResNet Blocks Sequentially using Boosting Theory,Oral/Poster,"We rove a \emh{multi-channel telescoing sum boosting} theory for the ResNet architectures which simultaneously creates a new technique for boosting over features (in contrast with labels) and rovides a new algorithm for ResNet-style architectures.  Our roosed training algorithm, \emh{BoostResNet}, is articularly suitable in non-differentiable architectures.  Our method only requires the relatively inexensive sequential training of $T$ ``shallow ResNets''. We rove that the training error decays exonentially with the deth $T$ if the weak module classifiers that we train erform slightly better than some weak baseline.  In other words, we roose a weak learning condition and rove a boosting theory for ResNet under the weak learning condition.  A generalization error bound based on margin theory is roved and suggests that ResNet could be resistant to overfitting using a network with $l_1$ norm bounded weights."
14,2018,Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings,Oral/Poster,"In this work, we take a reresentation learning ersective on hierarchical reinforcement learning, where the roblem of learning lower layers in a hierarchy is transformed into the roblem of learning trajectory-level generative models. We show that we can learn continuous latent reresentations of trajectories, which are effective in solving temorally extended and multi-stage roblems. Our roosed model, SeCTAR, draws insiration from variational autoencoders, and learns latent reresentations of trajectories. A key comonent of this method is to learn both a latent-conditioned olicy and a latent-conditioned model which are consistent with each other. Given the same latent, the olicy generates a trajectory which should match the trajectory redicted by the model. This model rovides a built-in rediction mechanism, by redicting the outcome of closed loo olicy behavior. We roose a novel algorithm for erforming hierarchical RL with this model, combining model-based lanning in the learned latent sace with an unsuervised exloration objective. We show that our model is effective at reasoning over long horizons with sarse rewards for several simulated tasks, outerforming standard reinforcement learning methods and rior methods for hierarchical reasoning, model-based lanning, and exloration. This model rovides a built-in rediction mechanism, by redicting the outcome of closed loo olicy behavior. We roose a novel algorithm for erforming hierarchical RL with this model, combining model-based lanning in the learned latent sace with an unsuervised exloration objective. We show that our model is effective at reasoning over long horizons with sarse rewards for several simulated tasks, outerforming standard reinforcement learning methods and rior methods for hierarchical reasoning, model-based lanning, and exloration.
"
15,2018,Problem Dependent Reinforcement Learning Bounds Which Can Identify Bandit Structure in MDPs,Oral/Poster,"In order to make good decision under uncertainty an agent must learn from observations. To do so, two of the most common frameworks are Contextual Bandits and Markov Decision Processes (MDPs).  In this aer, we study whether there exist algorithms for the more general framework (MDP) which automatically rovide the best erformance bounds for the secific roblem at hand without user intervention and without modifying the algorithm. In articular, it is found that a very minor variant of a recently roosed reinforcement learning algorithm for MDPs already matches the best ossible regret bound $\tilde O (\sqrt{SAT})$ in the dominant term if deloyed on a tabular Contextual Bandit roblem desite the agent being agnostic to such setting."
16,2018,Stochastic PCA with $\ell_2$ and $\ell_1$ Regularization,Oral/Poster,"We revisit convex relaxation based methods for stochastic otimization of rincial comonent analysis (PCA). While methods that directly solve the nonconvex roblem have been shown to be otimal in terms of statistical and comutational efficiency, the methods based on convex relaxation have been shown to enjoy comarable, or even suerior, emirical erformance -- this motivates the need for a  deeer formal understanding of the latter. Therefore, in this aer, we study variants of stochastic gradient descent for a convex relaxation of PCA with (a) $\ell_2$, (b) $\ell_1$, and (c) elastic net ($\ell_1+\ell_2)$ regularization in the hoe that these variants yield (a) better iteration comlexity, (b) better control on the rank of the intermediate iterates, and (c) both, resectively. We show, theoretically and emirically, that comared to revious work on convex relaxation based methods, the roosed variants yield faster convergence and imrove overall runtime  to achieve a certain user-secified $\esilon$-subotimality on the PCA objective. Furthermore, the roosed methods are shown to converge both in terms of the PCA objective as well as the distance between subsaces. However, there still remains a ga in comutational requirements for the roosed methods when comared with existing nonconvex aroaches."
17,2018,Subspace Embedding and Linear Regression with Orlicz Norm,Oral/Poster,"We consider a generalization of the classic linear regression roblem to the case when the loss is an Orlicz norm. An Orlicz norm is arameterized by a non-negative convex function G: Rem+ - &gt; Rem+ with G(0) = 0: the Orlicz norm of a n-dimensional vector x is defined as |x|emG = inf{ alha &gt; 0 | sumem{i = 1}^n G( |xemi|  alha ) &lt; = 1 }. We consider the cases where the function G grows subquadratically. Our main result is based on a new oblivious embedding which embeds the column sace of a given nxd matrix A with Orlicz norm into a lower dimensional sace with L2 norm. Secifically, we show how to efficiently find an mxn embedding matrix S (m &lt; n), such that for every d-dimensional vector x, we have Omega(1(d log n)) |Ax|emG &lt; = |SAx|em2 &lt; = O(d^2 log n) |Ax|emG. By alying this subsace embedding technique, we show an aroximation algorithm for the regression roblem minemx |Ax-b|emG, u to a O( d log^2 n ) factor. As a further alication of our techniques, we show how to also use them to imrove on the algorithm for the L low rank matrix aroximation roblem for 1 &lt; =  &lt; 2.
"
18,2018,Signal and  Noise Statistics Oblivious Orthogonal Matching Pursuit,Oral/Poster,"Orthogonal matching ursuit (OMP) is a widely used algorithm for recovering sarse high dimensional vectors in linear regression models. The otimal erformance of OMP requires a riori knowledge of either the sarsity of regression vector or noise statistics. Both these statistics are rarely known a riori and are very difficult to estimate.  In this aer, we resent a novel technique called residual ratio thresholding (RRT) to oerate OMP without any a riori knowledge of sarsity and noise statistics and establish  finite samle and large samle suort recovery guarantees for the same. Both analytical results and numerical simulations in real and synthetic data sets indicate that RRT has a erformance comarable to OMP with a riori knowledge of sarsity and noise statistics.
"
19,2018,Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope,Oral/Poster,"We roose a method to learn dee ReLU-based classifiers that are rovably robust against norm-bounded adversarial erturbations on the training data. For reviously unseen examles, the aroach is guaranteed to detect all adversarial examles, though it may flag some non-adversarial examles as well. The basic idea is to consider a convex outer aroximation of the set of activations reachable through a norm-bounded erturbation, and we develo a robust otimization rocedure that minimizes the worst case loss over this outer region (via a linear rogram). Crucially, we show that the dual roblem to this linear rogram can be reresented itself as a dee network similar to the backroagation network, leading to very efficient otimization aroaches that roduce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward asses through a slightly modified version of the original network (though ossibly with much larger batch sizes), we can learn a classifier that is rovably robust to any norm-bounded adversarial attack. We illustrate the aroach on a number of tasks to train classifiers with robust adversarial guarantees (e.g. for MNIST, we roduce a convolutional classifier that rovably has less than 5.8% test error for any adversarial attack with bounded $\ell_\infty$ norm less than $\esilon = 0.1$)."
20,2018,Learning the Reward Function for a Misspecified Model,Oral/Poster,"In model-based reinforcement learning it is tyical to decoule the roblems of learning the dynamics model and learning the reward function. However, when the dynamics model is flawed, it may generate erroneous states that would never occur in the true environment. It is not clear a riori what value the reward function should assign to such states. This aer resents a novel error bound that accounts for the reward model's behavior in states samled from the model. This bound is used to extend the existing Hallucinated DAgger-MC algorithm, which offers theoretical erformance guarantees in deterministic MDPs that do not assume a erfect model can be learned. Emirically, this aroach to reward learning can yield dramatic imrovements in control erformance when the dynamics model is flawed.
"
21,2018,Deep Reinforcement Learning in Continuous Action Spaces: a Case Study in the Game of Simulated Curling,Oral/Poster,"Many real-world alications of reinforcement learning require an agent to select otimal actions from continuous action saces. Recently, dee neural networks have successfully been alied to games with discrete actions saces. However, dee neural networks for discrete actions are not suitable for devising strategies for games in which a very small change in an action can dramatically affect the outcome. In this aer, we resent a new framework which incororates a dee neural network that can be used to learn game strategies based on a kernel-based Monte Carlo tree search that finds actions within a continuous sace. To avoid hand-crafted features, we train our network using suervised learning followed by reinforcement learning with a high-fidelity simulator for the Olymic sort of curling. The rogram trained under our framework outerforms existing rograms equied with several hand-crafted features and won an international digital curling cometition.
"
22,2018,Do Outliers Ruin Collaboration?,Oral/Poster,"We consider the roblem of learning a binary classifier from $n$ different data sources, among which at most an $\eta$ fraction are adversarial. The overhead is defined as the ratio between the samle comlexity of learning in this setting and that of learning the same hyothesis class on a single data distribution. We resent an algorithm that achieves an $O(\eta n + \ln n)$ overhead, which is roved to be worst-case otimal. We also discuss the otential challenges to the design of a comutationally efficient learning algorithm with a small overhead."
23,2018,"Dropout Training, Data-dependent Regularization, and Generalization Bounds",Oral/Poster,"We study the roblem of generalization guarantees for droout training. A general framework is first roosed for learning rocedures with random erturbation on model arameters. The generalization error is bounded by sum of two offset Rademacher comlexities: the main term is Rademacher comlexity of the hyothesis class with minus offset induced by the erturbation variance, which characterizes data-deendent regularization by the random erturbation; the auxiliary term is offset Rademacher comlexity for the variance class, controlling the degree to which this regularization effect can be weakened. For neural networks, we estimate uer and lower bounds for the variance induced by truthful droout, a variant of droout that we roose to ensure unbiased outut and fit into our framework, and the variance bounds exhibits connection to adative regularization methods. By alying our framework to ReLU networks with one hidden layer, a generalization uer bound is derived with no assumtions on the arameter norms or data distribution, with $O(1n)$ fast rate and adativity to geometry of data oints being achieved at the same time."
24,2018,Competitive Multi-agent Inverse Reinforcement Learning with Sub-optimal Demonstrations,Oral/Poster,"This aer considers the roblem of inverse reinforcement learning in zero-sum stochastic games when exert demonstrations are known to be subotimal. Comared to revious works that decoule agents in the game by assuming otimality in exert olicies, we introduce a new objective function that directly its exerts against Nash Equilibrium olicies, and we design an algorithm to solve for the reward function in the context of inverse reinforcement learning with dee neural networks as model aroximations. To ?nd Nash Equilibrium in large-scale games, we also roose an adversarial training algorithm for zero-sum stochastic games, and show the theoretical aeal of non-existence of local otima in its objective function. In numerical exeriments, we demonstrate that our Nash Equilibrium and inverse reinforcement learning algorithms address games that are not amenable to existing benchmark algorithms. Moreover, our algorithm successfully recovers reward and olicy functions regardless of the quality of the sub-otimal exert demonstration set.
"
25,2018,Continual Reinforcement Learning with Complex Synapses,Oral/Poster,"Unlike humans, who are caable of continual learning over their lifetimes, artificial neural networks have long been known to suffer from a henomenon known as catastrohic forgetting, whereby new learning can lead to abrut erasure of reviously acquired knowledge. Whereas in a neural network the arameters are tyically modelled as scalar values, an individual synase in the brain comrises a comlex network of interacting biochemical comonents that evolve at different timescales. In this aer, we show that by equiing tabular and dee reinforcement learning agents with a synatic model that incororates this biological comlexity (Benna &am; Fusi, 2016), catastrohic forgetting can be mitigated at multile timescales. In articular, we find that as well as enabling continual learning across sequential training of two simle tasks, it can also be used to overcome within-task forgetting by reducing the need for an exerience relay database.
"
26,2018,Equivalence of Multicategory SVM and Simplex Cone SVM: Fast Computations and Statistical Theory,Oral/Poster,"The multicategory SVM (MSVM) of Lee et al. (2004) is a natural generalization of the classical, binary suort vector machines (SVM).  However, its use has been limited by comutational difficulties.  The simlex-cone SVM (SCSVM) of Mroueh et al. (2012) is a comutationally efficient multicategory classifier, but its use has been limited by a seemingly oaque interretation.  We show that MSVM and SCSVM are in fact exactly equivalent, and rovide a bijection between their tuning arameters.  MSVM may then be entertained as both a natural and comutationally efficient multicategory extension of SVM.  We further rovide a Donsker theorem for finite-dimensional kernel MSVM and artially answer the oen question ertaining to the very cometitive erformance of One-vs-Rest methods against MSVM.  Furthermore, we use the derived asymtotic covariance formula to develo an inverse-variance weighted classification rule which imroves on the One-vs-Rest aroach.
"
27,2018,Quickshift++: Provably Good Initializations for Sample-Based Mean Shift,Oral/Poster,"We rovide initial seedings to the Quick Shift clustering algorithm, which aroximate the locally high-density regions of the data. Such seedings act as more stable and exressive cluster-cores than the singleton modes found by Quick Shift. We establish statistical consistency guarantees for this modification. We then show strong clustering erformance on real datasets as well as romising alications to image segmentation.
"
28,2018,Learning Diffusion using Hyperparameters,Oral/Poster,"In this aer we advocate for a hyerarametric aroach to learn diffusion in the indeendent cascade (IC) model. The samle comlexity of this model is a function of the number of edges in the network and consequently learning becomes infeasible when the network is large. We study a natural restriction of the hyothesis class using additional information available in order to dramatically reduce the samle comlexity of the learning rocess. In articular we assume that diffusion robabilities can be described as a function of a global hyerarameter and features of the individuals in the network. One of the main challenges with this aroach is that training a model reduces to otimizing a non-convex objective. Desite this obstacle, we can shrink the best-known samle comlexity bound for learning IC by a factor of |E|d where |E| is the number of edges in the grah and d is the dimension of the hyerarameter. We show that under mild assumtions about the distribution generating the samles one can rovably train a model with low generalization error. Finally, we use large-scale diffusion data from Facebook to show that a hyerarametric model using aroximately 20 features er node achieves remarkably high accuracy.
"
29,2018,Learning a Mixture of Two Multinomial Logits,Oral/Poster,"The classical Multinomial Logit (MNL) is a behavioral model for user   choice.  In this model, a user is offered a slate of choices (a   subset of a finite universe of $n$ items), and selects exactly one   item from the slate, each with robability roortional to its   (ositive) weight.  Given a set of observed slates and choices, the   likelihood-maximizing item weights are easy to learn at scale, and   easy to interret.  However, the model fails to reresent common   real-world behavior.   As a result, researchers in user choice often turn to mixtures of   MNLs, which are known to aroximate a large class of models of   rational user behavior.  Unfortunately, the only known algorithms   for this roblem have been heuristic in nature.  In this aer we   give the first olynomial-time algorithms for exact learning of   uniform mixtures of two MNLs.  Interestingly, the arameters of the   model can be learned for any $n$ by samling the behavior of random   users only on slates  of sizes 2 and 3; in contrast, we show that slates   of size 2 are insufficient by themselves."
30,2018,Crowdsourcing with Arbitrary Adversaries,Oral/Poster,"Most existing works on crowdsourcing assume that the workers follow the Dawid-Skene model, or the one-coin model as its secial case, where every worker makes mistakes indeendently of other workers and with the same error robability for every task. We study a significant extension of this restricted model. We allow almost half of the workers to deviate from the one-coin model and for those workers, their robabilities of making an error to be task-deendent and to be arbitrarily correlated. In other words, we allow for arbitrary adversaries, for which not only error robabilities can be high, but which can also erfectly collude. In this adversarial scenario, we design an efficient algorithm to consistently estimate the workers’ error robabilities.
"
31,2018,Deep Density Destructors,Oral/Poster,"We roose a unified framework for dee density models by formally defining density destructors. A density destructor is an invertible function that transforms a given density to the uniform density---essentially destroying any structure in the original density. This destructive transformation generalizes Gaussianization via ICA and more recent autoregressive models such as MAF and Real NVP. Informally, this transformation can be seen as a generalized whitening rocedure or a multivariate generalization of the univariate CDF function. Unlike Gaussianization, our destructive transformation has the elegant roerty that the density function is equal to the absolute value of the Jacobian determinant. Thus, each layer of a dee density can be seen as a shallow density---uncovering a fundamental connection between shallow and dee densities. In addition, our framework rovides a common interface for all revious methods enabling them to be systematically combined, evaluated and imroved. Leveraging the connection to shallow densities, we also roose a novel tree destructor based on tree densities and an image-secific destructor based on ixel locality. We illustrate our framework on a 2D dataset, MNIST, and CIFAR-10. Code is available on first author's website.
"
32,2018,Programmatically Interpretable Reinforcement Learning,Oral/Poster,"We resent a reinforcement learning framework, called Programmatically Interretable Reinforcement Learning (PIRL), that is designed to generate interretable and verifiable agent olicies. Unlike the oular Dee Reinforcement Learning (DRL) aradigm, which reresents olicies by neural networks, PIRL reresents olicies using a high-level, domain-secific rogramming language. Such rogrammatic olicies have the benefits of being more easily interreted than neural networks, and being amenable to verification by symbolic methods. We roose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth otimization roblem of finding a rogrammatic olicy with maximal reward. NDPS works by first learning a neural olicy network using DRL, and then erforming a local search over rogrammatic olicies that seeks to minimize a distance from this neural “oracle”. We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable olicies that ass some significant erformance bars. We also show that PIRL olicies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresonding olicies discovered by DRL.
"
33,2018,Structured Evolution with Compact Architectures for Scalable Policy Optimization,Oral/Poster,"We resent a new method of blackbox otimization via gradient aroximation with the use of structured random orthogonal matrices, roviding more accurate estimators than baselines and with rovable theoretical guarantees. We show that this algorithm can be successfully alied to learn better quality comact olicies than those using standard gradient estimation techniques. The comact olicies we learn have several advantages over unstructured ones, including faster training algorithms and faster inference. These benefits are imortant when the olicy is deloyed on real hardware with limited resources. Further, comact olicies rovide more scalable architectures for derivative-free otimization (DFO) in high-dimensional saces. We show that most robotics tasks from the OenAI Gym can be solved using neural networks with less than 300 arameters, with almost linear time comlexity of the inference hase, with u to 13x fewer arameters relative to the Evolution Strategies (ES) algorithm introduced by Salimans et al. (2017). We do not need heuristics such as fitness shaing to learn good quality olicies, resulting in a simle and theoretically motivated training mechanism.
"
34,2018,The Weighted Kendall and High-order Kernels for Permutations,Oral/Poster,"We roose new ositive definite kernels for ermutations. First we introduce a weighted version of the Kendall kernel, which allows to weight unequally the contributions of different item airs in the ermutations deending on their ranks. Like the Kendall kernel, we show that the weighted version is invariant to relabeling of items and can be comuted efficiently in O(n ln(n)) oerations, where n is the number of items in the ermutation. Second, we roose a suervised aroach to learn the weights by jointly otimizing them with the function estimated by a kernel machine. Third, while the Kendall kernel considers airwise comarison between items, we extend it by considering higher-order comarisons among tules of items and show that the suervised aroach of learning the weights can be systematically generalized to higher-order ermutation kernels.
"
35,2018,"The Limits of Maxing, Ranking, and Preference Learning",Oral/Poster,"We resent a comrehensive understanding of three imortant roblems in PAC reference learning: maximum selection (maxing), ranking, and estimating \emh{all} airwise reference robabilities, in the adative setting. With just Weak Stochastic Transitivity, we show that maxing requires $\Omega(n^2)$ comarisons and with slightly more restrictive Medium Stochastic Transitivity, we resent a linear comlexity maxing algorithm. With Strong Stochastic Transitivity and Stochastic Triangle Inequality, we derive a ranking algorithm with otimal $\mathcal{O}(n\log n)$ comlexity and an otimal algorithm that estimates all airwise reference robabilities."
36,2018,Black Box FDR,Oral/Poster,"Analyzing large-scale, multi-exeriment studies requires scientists to test each exerimental outcome for statistical significance and then assess the results as a whole. We resent Black Box FDR (BB-FDR), an emirical-Bayes method for analyzing multi-exeriment studies when many covariates are gathered er exeriment. BB-FDR learns a series of black box redictive models to boost ower and control the false discovery rate (FDR) at two stages of study analysis. In Stage 1, it uses a dee neural network rior to reort which exeriments yielded significant outcomes. In Stage 2, a searate black box model of each covariate is used to select features that have significant redictive ower across all exeriments. In benchmarks, BB-FDR outerforms cometing state-of-the-art methods in both stages of analysis. We aly BB-FDR to two real studies on cancer drug efficacy. For both studies, BB-FDR increases the roortion of significant outcomes discovered and selects variables that reveal key genomic drivers of drug sensitivity and resistance in cancer.
"
37,2018,Variable Selection via Penalized Neural Network: a Drop-Out-One Loss Approach,Oral/Poster,"We roose a variable selection method for high dimensional regression models, which allows for comlex, nonlinear, and high-order interactions among variables. The roosed method aroximates this comlex system using a enalized neural network and selects exlanatory variables by measuring their utility in exlaining the variance of the resonse variable. This measurement is based on a novel statistic called Dro-Out-One Loss. The roosed method also allows (overlaing) grou variable selection. We rove that the roosed method can select relevant variables and exclude irrelevant variables with robability one as the samle size goes to infinity, which is referred to as the Oracle Proerty. Exerimental results on simulated and real world datasets show the efficiency of our method in terms of variable selection and rediction accuracy.
"
38,2018,Clustering Semi-Random Mixtures of Gaussians,Oral/Poster,"Gaussian mixture models (GMM) are the most widely used statistical model for the k-means clustering roblem and form a oular framework for clustering in machine learning and data analysis. In this aer, we roose a natural robust model for k-means clustering that generalizes the Gaussian mixture model, and that we believe will be useful in identifying robust algorithms. Our first contribution is a olynomial time algorithm that rovably recovers the ground-truth u to small classification error w.h.., assuming certain searation between the comonents. Perhas surrisingly, the algorithm we analyze is the oular Lloyd's algorithm for k-means clustering that is the method-of-choice in ractice. Our second result comlements the uer bound by giving a nearly matching lower bound on the number of misclassified oints incurred by any k-means clustering algorithm on the semi-random model.
"
39,2018,Leveraging Well-Conditioned Bases: Streaming and Distributed Summaries in Minkowski $p$-Norms,Oral/Poster,"Work on aroximate linear algebra   has led to efficient distributed and streaming   algorithms for    roblems such as aroximate matrix multilication, low rank aroximation,   and regression, rimarily for the Euclidean norm $\ell_2$.   We study other   $\ell_$ norms, which are more robust for $  2$, and can be used   to find outliers for $  2$.    Unlike revious algorithms for such norms,  we give algorithms that are (1) deterministic, (2) work simultaneously for every $ \geq 1$, including $ = \infty$, and (3) can be   imlemented in both    distributed and streaming environments. We study $\ell_$-regression,    entrywise $\ell_$-low rank aroximation,    and versions of aroximate matrix multilication."
40,2018,Learning by Playing - Solving Sparse Reward Tasks from Scratch,Oral/Poster,"We roose Scheduled Auxiliary Control (SAC-X), a new learning aradigm in the context of Reinforcement Learning (RL). SAC-X enables learning of comlex behaviors - from scratch - in the resence of multile sarse reward signals. To this end, the agent is equied with a set of general auxiliary tasks, that it attemts to learn simultaneously via off-olicy RL. The key idea behind our method is that active (learned) scheduling and execution of auxiliary olicies allows the agent to efficiently exlore its environment - enabling it to excel at sarse reward RL. Our exeriments in several challenging robotic maniulation settings demonstrate the ower of our aroach.
"
41,2018,Structured Control Nets for Deep Reinforcement Learning,Oral/Poster,"In recent years, Dee Reinforcement Learning has made imressive advances in solving several imortant benchmark roblems for sequential decision making. Many control alications use a generic multilayer ercetron (MLP) for non-vision arts of the olicy network. In this work, we roose a new neural network architecture for the olicy network reresentation that is simle yet effective. The roosed Structured Control Net (SCN) slits the generic MLP into two searate sub-modules: a nonlinear control module and a linear control module. Intuitively, the nonlinear control is for forward-looking and global control, while the linear control stabilizes the local dynamics around the residual of global control. We hyothesize that this will bring together the benefits of both linear and nonlinear olicies: imrove training samle efficiency, final eisodic reward, and generalization of learned olicy, while requiring a smaller network and being generally alicable to different training methods. We validated our hyothesis with cometitive results on simulations from OenAI MuJoCo, Roboschool, Atari, and a custom urban driving environment, with various ablation and generalization tests, trained with multile black-box and olicy gradient training methods. The roosed architecture has the otential to imrove uon broader control tasks by incororating roblem secific riors into the architecture. As a case study, we demonstrate much imroved erformance for locomotion tasks by emulating the biological central attern generators (CPGs) as the nonlinear art of the architecture.
"
42,2018,Stagewise Safe Bayesian Optimization with Gaussian Processes,Oral/Poster,"Enforcing safety is a key asect of many roblems ertaining to sequential decision making under uncertainty, which require the decisions made at every ste to be both informative of the otimal decision and also safe. For examle, we value both efficacy and comfort in medical theray, and efficiency and safety in robotic control. We consider this roblem of otimizing an unknown utility function with absolute feedback or reference feedback subject to unknown safety constraints. We develo an efficient safe Bayesian otimization algorithm, StageOt, that searates safe region exansion and utility function maximization into two distinct stages. Comared to existing aroaches which interleave between exansion and otimization, we show that StageOt is more efficient and naturally alicable to a broader class of roblems. We rovide theoretical guarantees for both the satisfaction of safety constraints as well as convergence to the otimal utility value. We evaluate StageOt on both a variety of synthetic exeriments, as well as in clinical ractice.  We demonstrate that StageOt is more effective than existing safe otimization aroaches, and is able to safely and effectively otimize sinal cord stimulation theray in our clinical exeriments.
"
43,2018,Bayesian Optimization of Combinatorial Structures,Oral/Poster,"The otimization of exensive-to-evaluate black-box functions over combinatorial structures is an ubiquitous task in machine learning, engineering and the natural sciences. The combinatorial exlosion of the search sace and costly evalu- ations ose challenges for current techniques in discrete otimization and machine learning, and critically require new algorithmic ideas (NIPS BayesOt 2017).   This article rooses, to the best of our knowledge, the first algorithm to overcome these challenges, based on an adative, scal able model that identifies useful combinatorial structure even when data is scarce. Our acquisition function ioneers the use of semidefinite rogramming to achieve efficiency and scalability.  Exerimental evaluations demonstrate that this algorithm consistently outerforms other methods from combinatorial and Bayesian otimization.
"
44,2018,GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models,Oral/Poster,"Modeling and generating grahs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling comlex distributions over grahs and then efficiently samling from these distributions is challenging due to the non-unique, high-dimensional nature of grahs and the comlex, non-local deendencies that exist between edges in a given grah. Here we roose GrahRNN, a dee autoregressive model that addresses the above challenges and aroximates any distribution of grahs with minimal assumtions about their structure.  GrahRNN learns to generate grahs by training on a reresentative set of grahs and decomoses the grah generation rocess into a sequence of node and edge formations, conditioned on the grah structure generated so far.  In order to quantitatively evaluate the erformance of GrahRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discreancy, which measure distances between sets of grahs. Our exeriments show that GrahRNN significantly outerforms all baselines, learning to generate diverse grahs that match the structural characteristics of a target set, while also scaling to grahs 50 times larger than revious dee models.
"
45,2018,Dependent Relational Gamma Process Models for Longitudinal Networks,Oral/Poster,"A robabilistic framework based on the covariate-deendent relational gamma rocess is develoed to analyze relational data arising from longitudinal networks. The roosed framework characterizes networked nodes by nonnegative node-grou membershis, which allow each node to belong to multile latent grous simultaneously, and encodes edge robabilities between each air of nodes using a Bernoulli Poisson link to the embedded latent sace. Within the latent sace, our framework models the birth and death dynamics of individual grous via a thinning function. Our framework also catures the evolution of individual node-grou membershis over time using gamma Markov rocesses. Exloiting the recent advances in data augmentation and marginalization techniques, a simle and efficient Gibbs samler is roosed for osterior comutation. Exerimental results on a simulation study and three real-world temoral network data sets demonstrate the model’s caability, cometitive erformance and scalability comared to state-of-the-art methods.
"
46,2018,K-means clustering using random matrix sparsification,Oral/Poster,"K-means clustering algorithm using Lloyd's heuristic  is one of the most commonly used tools in data mining and machine learning that shows romising erformance. However, it suffers from a high comutational cost resulting from airwise Euclidean distance comutations between data oints and cluster centers in each iteration of Lloyd's heuristic. Main contributing factor of this comutational bottle neck is a matrix-vector multilication ste, where the matrix contains all the data oints and the vector is a cluster center. In this aer we show that we can randomly sarsify the original data matrix resulting in a sarse data matrix which can significantly seed u the above mentioned matrix vector multilication ste without significantly affecting cluster quality. In articular, we show that otimal k-means clustering solution of the sarse data matrix, obtained by alying random matrix sarsification, results in an aroximately otimal k-means clustering objective of the original data matrix. Our emirical studies on three real world datasets corroborate our theoretical findings and demonstrate that our roosed sarsification method can  indeed achieve satisfactory clustering erformance.
"
47,2018,Hierarchical Clustering with Structural Constraints,Oral/Poster,"Hierarchical clustering is a oular unsuervised data analysis method. For many real-world alications, we would like to exloit rior information about the data that imoses constraints on the clustering hierarchy, and is not catured by the set of features available to the algorithm. This gives rise to the roblem of hierarchical clustering with structural constraints. Structural constraints ose major challenges for bottom-u aroaches like averagesingle linkage and even though they can be naturally incororated into to-down divisive algorithms, no formal guarantees exist on the quality of their outut. In this aer, we rovide rovable aroximation guarantees for two simle to-down algorithms, using a recently introduced otimization viewoint of hierarchical clustering with airwise similarity information (Dasguta, 2016). We show how to find good solutions even in the resence of conflicting rior information, by formulating a constraint-based regularization of the objective. Furthemore, we exlore a variation of this objective for dissimilarity information (Cohen-Addad et al., 2018) and imrove uon current techniques. Finally, we demonstrate our aroach on a real dataset for the taxonomy alication.
"
48,2018,Kronecker Recurrent Units,Oral/Poster,"Our work addresses two imortant issues with recurrent neural networks: (1) they are over-arametrized, and (2) the recurrent weight matrix is ill-conditioned. The former increases the samle comlexity of learning and the training time. The latter causes the vanishing and exloding gradient roblem.  We resent a flexible recurrent neural network model called Kronecker Recurrent Units (KRU). KRU achieves arameter efficiency in RNNs through a Kronecker factored recurrent matrix. It overcomes the ill-conditioning of the recurrent matrix by enforcing soft unitary constraints on the factors. Thanks to the small dimensionality of the factors, maintaining these constraints is comutationally efficient. Our exerimental results on seven standard data-sets reveal that KRU can reduce the number of arameters by three orders of magnitude in the recurrent weight matrix comared to the existing recurrent models, without trading the statistical erformance.  These results in articular show that while there are advantages in having a high dimensional recurrent sace, the caacity of the recurrent art of the model can be dramatically reduced.
"
49,2018,Semi-Supervised Learning via Compact Latent Space Clustering,Oral/Poster,"We resent a novel cost function for semi-suervised learning of neural networks that encourages comact clustering of the latent sace to facilitate searation. The key idea is to dynamically create a grah over embeddings of labeled and unlabeled samles of a training batch to cature underlying structure in feature sace, and use label roagation to estimate its high and low density regions. We then devise a cost function based on Markov chains on the grah that regularizes the latent sace to form a single comact cluster er class, while avoiding to disturb existing clusters during otimization. We evaluate our aroach on three benchmarks and comare to state-of-the art with romising results. Our aroach combines the benefits of grah-based regularization  with efficient, inductive inference, does not require modifications to a network architecture, and can thus be easily alied to existing networks to enable an effective use of unlabeled data.
"
50,2018,Dynamic Evaluation of Neural Sequence Models,Oral/Poster,"We exlore dynamic evaluation, where sequence models are adated to the recent sequence history using gradient descent, assigning higher robabilities to re-occurring sequential atterns. We develo a dynamic evaluation aroach that outerforms existing adatation aroaches in our comarisons. We aly dynamic evaluation to outerform all revious word-level erlexities on the Penn Treebank and WikiText-2 datasets (achieving 51.1 and 44.3 resectively) and all revious character-level cross-entroies on the text8 and Hutter Prize datasets (achieving 1.19 bitschar and 1.08 bitschar resectively).
"
51,2018,TACO: Learning Task Decomposition via Temporal Alignment for Control,Oral/Poster,"Many advanced Learning from Demonstration (LfD) methods consider the decomosition of comlex, real-world tasks into simler sub-tasks. By reusing the corresonding sub-olicies within and between tasks, we can rovide training data for each olicy from different high-level tasks and comose them to erform novel ones. Existing aroaches to modular LfD focus either on learning a single high-level task or deend on domain knowledge and temoral segmentation. In contrast, we roose a weakly suervised, domain-agnostic aroach based on task sketches, which include only the sequence of sub-tasks erformed in each demonstration. Our aroach simultaneously aligns the sketches with the observed demonstrations and learns the required sub-olicies. This imroves generalisation in comarison to searate otimisation rocedures. We evaluate the aroach on multile domains, including a simulated 3D robot arm control task using urely image-based observations.  The results show that our aroach erforms commensurately with fully suervised aroaches, while requiring significantly less annotation effort.
"
52,2018,A Spectral Approach to Gradient Estimation for Implicit Distributions,Oral/Poster,"Recently there have been increasing interests in learning and inference with imlicit distributions (i.e., distributions without tractable densities). To this end, we develo a gradient estimator for imlicit distributions based on Stein's identity and a sectral decomosition of kernel oerators, where the eigenfunctions are aroximated by the Nystr{\""o}m method. Unlike the revious works that only rovide estimates at the samle oints, our aroach directly estimates the gradient function, thus allows for a simle and rinciled out-of-samle extension. We rovide theoretical results on the error bound of the estimator and discuss the bias-variance tradeoff in ractice. The effectiveness of our method is demonstrated by alications to gradient-free Hamiltonian Monte Carlo and variational inference with imlicit distributions. Finally, we discuss the intuition behind the estimator by drawing connections between the Nystr{\""o}m method and kernel PCA, which indicates that the estimator can automatically adat to the geometry of the underlying distribution.
"
53,2018,Quasi-Monte Carlo Variational Inference,Oral/Poster,"Many machine learning roblems involve Monte Carlo gradient estimators. As a rominent examle,  we focus on Monte Carlo variational inference (MCVI)  in this aer. The erformance of MCVI crucially deends on the variance of its stochastic gradients. We roose variance  reduction by means of Quasi-Monte Carlo (QMC) samling.  QMC relaces N i.i.d. samles from a uniform robability  distribution by a deterministic sequence of samles of length N. This sequence covers the underlying random variable sace more evenly than i.i.d. draws, reducing the variance of the gradient estimator. With our novel aroach, both the score function and the rearameterization  gradient estimators lead to much faster convergence.  We also roose a new algorithm for Monte Carlo objectives, where we oerate with a constant learning rate and increase the number of QMC samles er iteration. We rove that this way, our algorithm can converge asymtotically at a faster rate than SGD . We furthermore rovide theoretical guarantees on qmc for Monte Carlo objectives that go beyond MCVI , and suort our findings by several exeriments on large-scale data sets from various domains.
"
54,2018,Learning to Optimize Combinatorial Functions,Oral/Poster,"Submodular functions have become a ubiquitous tool in machine learning. They are learnable from data, and can be otimized efficiently and with guarantees. Nonetheless, recent negative results show that otimizing learned surrogates of submodular functions can result in arbitrarily bad aroximations of the true otimum. Our goal in this aer is to highlight the source of this hardness, and roose an alternative criterion for otimizing general combinatorial functions from samled data. We rove a tight equivalence showing that a class of functions is otimizable if and only if it can be learned. We rovide efficient and scalable otimization algorithms for several function classes of interest, and demonstrate their utility on the task of otimally choosing trending social media items.
"
55,2018,"Proportional Allocation: Simple, Distributed, and Diverse Matching with High Entropy",Oral/Poster,"Insired by many alications of biartite matching in online advertising and machine learning, we study a simle and natural iterative roortional allocation algorithm: Maintain a riority score $\riority_a$ for each node $a\in\advertisers$ on one side of the biartition, initialized as $\riority_a=1$. Iteratively allocate the nodes $i\in \imressions$ on the other side to eligible nodes in $\advertisers$ in roortion of their riority scores. After each round, for each node $a\in \advertisers$, decrease or increase the score $\riority_a$ based on whether it is over- or under- allocated.  Our first result is that this simle, distributed algorithm converges to a $(1-\esilon)$-aroximate fractional $b$-matching solution in $O({\log n\over \esilon^2} )$ rounds. We also extend the roortional allocation algorithm and convergence results to the maximum weighted matching roblem, and show that the algorithm can be naturally tuned to roduce maximum matching with {\em high entroy}. High entroy, in turn, imlies additional desirable roerties of this matching, e.g., it satisfies certain diversity and fairness (aka anonymity) roerties that are desirable in a variety of alications in online advertising and machine learning."
56,2018,Representation Learning on Graphs with Jumping Knowledge Networks,Oral/Poster,"Recent dee learning aroaches for reresentation learning on grahs follow a neighborhood aggregation rocedure. We analyze some imortant roerties of these models, and roose a strategy to overcome those. In articular, the range of ""neighboring"" nodes that a node's reresentation draws from strongly deends on the grah structure, analogous to the sread of a random walk. To adat to local neighborhood roerties and tasks, we exlore an architecture -- juming knowledge (JK) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware reresentation. In a number of exeriments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art erformance. Furthermore, combining the JK framework with models like Grah Convolutional Networks, GrahSAGE and Grah Attention Networks consistently imroves those models' erformance.
"
57,2018,NetGAN: Generating Graphs via Random Walks,Oral/Poster,"We roose NetGAN - the first imlicit generative model for grahs able to mimic real-world networks. We ose the roblem of grah generation as learning the distribution of biased random walks over the inut grah. The roosed model is based on a stochastic neural network that generates discrete outut samles and is trained using the Wasserstein GAN objective. NetGAN is able to roduce grahs that exhibit well-known network atterns without exlicitly secifying them in the model definition. At the same time, our model exhibits strong generalization roerties, as highlighted by its cometitive link rediction erformance, desite not being trained secifically for this task. Being the first aroach to combine both of these desirable roerties, NetGAN oens exciting avenues for further research.
"
58,2018,INSPECTRE: Privately Estimating the Unseen,Oral/Poster,"We develo differentially rivate methods for estimating various distributional roerties. Given a samle from a discrete distribution , some functional f, and accuracy and rivacy arameters alha and esilon, the goal is to estimate f() u to accuracy alha, while maintaining esilon-differential rivacy of the samle. We rove almost-tight bounds on the samle size required for this roblem for several functionals of interest, including suort size, suort coverage, and entroy. We show that the cost of rivacy is negligible in a variety of settings, both theoretically and exerimentally. Our methods are based on a sensitivity analysis of several state-of-the-art methods for estimating these roerties with sublinear samle comlexities
"
59,2018,Locally Private Hypothesis Testing,Oral/Poster,"We initiate the study of differentially rivate hyothesis testing in the local-model, under both the standard (symmetric) randomized-resonse mechanism (Warner 1965, Kasiviswanathan et al, 2008} and the newer (non-symmetric) mechanisms (Bassily & Smith, 2015, Bassily et al, 2017). First, we study the general framework of maing each user's tye into a signal and show that the roblem of finding the maximum-likelihood distribution over the signals is feasible. Then we discuss the randomized-resonse mechanism and show that, in essence, it mas the null- and alternative-hyotheses onto new sets, an affine translation of the original sets. We then give samle comlexity bounds for identity and indeendence testing under randomized-resonse. We then move to the newer non-symmetric mechanisms and show that there too the roblem of finding the maximum-likelihood distribution is feasible. Under the mechanism of Bassily et al we give identity and indeendence testers with better samle comlexity than the testers in the symmetric case, and we also roose a $\chi^2$-based identity tester which we investigate emirically."
60,2018,Latent Space Policies for Hierarchical Reinforcement Learning,Oral/Poster,"We address the roblem of learning hierarchical dee neural network olicies for reinforcement learning. In contrast to methods that exlicitly restrict or crile lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse strategies via a maximum entroy reinforcement learning objective. Each layer is also augmented with latent random variables, which are samled from a rior distribution during the training of that layer. The maximum entroy objective causes these latent variables to be incororated into the layer's olicy, and the higher level layer can directly control the behavior of the lower layer through this latent sace. Furthermore, by constraining the maing from latent variables to actions to be invertible, higher layers retain full exressivity: neither the higher layers nor the lower layers are constrained in their behavior. Our exerimental evaluation demonstrates that we can imrove on the erformance of single-layer olicies on standard benchmark tasks simly by adding additional layers, and that our method can solve more comlex sarse-reward tasks by learning higher-level olicies on to of high-entroy skills otimized for simle low-level objectives.
"
61,2018,More Robust Doubly Robust Off-policy Evaluation,Oral/Poster,"We study the roblem of off-olicy evaluation (OPE) in reinforcement learning (RL), where the goal is to estimate the erformance of a olicy from the data generated by another olicy(ies). In articular, we focus on the doubly robust (DR) estimators that consist of an imortance samling (IS) comonent and a erformance model, and utilize the low (or zero) bias of IS and low variance of the model at the same time. Although the accuracy of the model has a huge imact on the overall erformance of DR, most of the work on using the DR estimators in OPE has been focused on imroving the IS art, and not much on how to learn the model. In this aer, we roose alternative DR estimators, called more robust doubly robust (MRDR), that learn the model arameter by minimizing the variance of the DR estimator. We first resent a formulation for learning the DR model in RL. We then derive formulas for the variance of the DR estimator in both contextual bandits and RL, such that their gradients w.r.t. the model arameters can be estimated from the samles, and roose methods to efficiently minimize the variance. We rove that the MRDR estimators are strongly consistent and asymtotically otimal. Finally, we evaluate MRDR in bandits and RL benchmark roblems, and comare its erformance with the existing methods.
"
62,2018,Learning to Explain: An Information-Theoretic Perspective on Model Interpretation,Oral/Poster,"We introduce instancewise feature selection as a methodology for model interretation.  Our method is based on learning a function to extract a subset of features that are most informative for each given examle.  This feature selector is trained to maximize the mutual information between selected features and the resonse variable, where the conditional distribution of the resonse variable given the inut is the model to be exlained. We develo an efficient variational aroximation to the mutual information, and show the effectiveness of our method on a variety of synthetic and real data sets using both quantitative metrics and human evaluation.
"
63,2018,End-to-end Active Object Tracking via Reinforcement Learning,Oral/Poster,"We study active object tracking, where a tracker takes as inut the visual observation (\ie, frame sequence) and roduces the camera control signal (\eg, move forward, turn left, \etc). Conventional methods tackle the tracking and the camera control searately, which is challenging to tune jointly. It also incurs many human efforts for labeling and many exensive trial-and-errors in real-world. To address these issues, we roose, in this aer, an end-to-end solution via dee reinforcement learning, where a ConvNet-LSTM function aroximator is adoted for the direct frame-to-action rediction. We further roose an environment augmentation technique and a customized reward function, which are crucial for a successful training. The tracker trained in simulators (ViZDoom, Unreal Engine) shows good generalization in the case of unseen object moving ath, unseen object aearance, unseen background, and distracting object. It can restore tracking when occasionally losing the target. With the exeriments over the VOT dataset, we also find that the tracking ability, obtained solely from simulators, can otentially transfer to real-world scenarios.
"
64,2018,Efficient and Consistent Adversarial Bipartite Matching,Oral/Poster,"Many imortant structured rediction roblems, including learning to rank items, corresondence-based natural language rocessing, and multi-object tracking, can be formulated as weighted biartite matching otimizations. Existing structured rediction aroaches have significant drawbacks when alied under the constraints of erfect biartite matchings. Exonential family robabilistic models, such as the conditional random field (CRF), rovide statistical consistency guarantees, but suffer comutationally from the need to comute the normalization term of its distribution over matchings, which is a #P-hard matrix ermanent comutation. In contrast, the structured suort vector machine (SSVM) rovides comutational efficiency, but lacks Fisher consistency, meaning that there are distributions of data for which it cannot learn the otimal matching even under ideal learning conditions (i.e., given the true distribution and selecting from all measurable otential functions). We roose adversarial biartite matching to avoid both of these limitations. We develo this aroach algorithmically, establish its comutational efficiency and Fisher consistency roerties, and aly it to matching roblems that demonstrate its emirical benefits.
"
65,2018,SparseMAP: Differentiable Sparse Structured Inference,Oral/Poster,"Structured rediction requires searching over a combinatorial number of structures. To tackle it, we introduce SarseMAP, a new method for sarse structured inference, together with corresonding loss functions. SarseMAP inference is able to automatically select only a few global structures: it is situated between MAP inference, which icks a single structure, and marginal inference, which assigns robability mass to all structures, including imlausible ones. Imortantly, SarseMAP can be comuted using only calls to a MAP oracle, hence it is alicable even to roblems where marginal inference is intractable, such as linear assignment. Moreover, thanks to the solution sarsity, gradient backroagation is efficient regardless of the structure. SarseMAP thus enables us to augment dee neural networks with generic and sarse structured hidden layers. Exeriments in deendency arsing and natural language inference reveal cometitive accuracy, imroved interretability, and the ability to cature natural language ambiguities, which is attractive for ieline systems.
"
66,2018,Bilevel Programming for Hyperparameter Optimization and Meta-Learning,Oral/Poster,"We introduce a framework based on bilevel rogramming that unifies gradient-based hyerarameter otimization and meta-learning. We show that an aroximate version of the bilevel roblem can be solved by taking into exlicit account the otimization dynamics for the inner objective.  Deending on the secific setting, the outer variables take either the meaning of hyerarameters in a suervised learning roblem or arameters of a meta-learner. We rovide sufficient conditions under which solutions of the aroximate roblem converge to those of the exact roblem. We instantiate our aroach for meta-learning in the case of  dee learning where reresentation layers are treated as hyerarameters shared across a set of training eisodes.  In exeriments, we confirm our theoretical findings, resent encouraging results for few-shot learning and contrast the bilevel aroach against classical aroaches for learning-to-learn.
"
67,2018,Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes Theory,Oral/Poster,"In meta-learning an agent extracts knowledge from observed tasks, aiming to facilitate learning of novel future tasks. Under the assumtion that future tasks are `related’ to revious tasks,  reresentations should be learned in such a way that they cature the common structure across learned tasks, while allowing the learner sufficient flexibility to adat to novel asects of a new task. We resent a framework for meta-learning that is based on generalization error bounds, allowing us to extend various PAC-Bayes bounds to meta-learning. Learning takes lace through the construction of a distribution over hyotheses based on the observed tasks, and its utilization for learning a new task. Thus, rior knowledge is incororated through setting an exerience-deendent rior for novel tasks. We develo a gradient-based algorithm, and imlement it for dee neural networks, based on minimizing an objective function derived from the bounds, and demonstrate its effectiveness numerically. In addition to establishing the imroved erformance available through meta-learning, we demonstrate the intuitive way by which rior information is manifested at different levels of the network.
"
68,2018,Parameterized Algorithms for the Matrix Completion Problem,Oral/Poster,"We consider two matrix comletion roblems, in which we are given a matrix with missing entries and the task is to comlete the matrix in a way that (1) minimizes the rank, or (2) minimizes the number of distinct rows. We study the arameterized comlexity of the two aforementioned roblems with resect to several arameters of interest, including the minimum number of matrix rows, columns, and rows lus columns needed to cover all missing entries. We obtain new algorithmic results showing that, for the bounded domain case, both roblems are fixed-arameter tractable with resect to all aforementioned arameters. We comlement these results with a lower-bound result for the unbounded domain case that rules out fixed-arameter tractability w.r.t. some of the arameters under consideration.
"
69,2018,Nearly Optimal Robust Subspace Tracking,Oral/Poster,"Robust subsace tracking (RST) can  be simly understood as a dynamic (time-varying) extension of robust PCA.  More recisely, it is the roblem of tracking data lying in a fixed or slowly-changing low-dimensional subsace while being robust to sarse outliers. This work develos a recursive rojected comressive sensing algorithm called  ``Nearly Otimal RST (NORST)'', and obtains one of the first guarantees for it. We show that NORST rovably solves RST under weakened standard RPCA assumtions, slow subsace change, and a lower bound on (most) outlier magnitudes.  Our guarantee shows that (i) NORST is online (after initialization) and enjoys near-otimal values of tracking delay, lower bound on  required delay between subsace change times, and of memory comlexity; and (ii) it has a significantly imroved worst-case outlier tolerance comared with all revious robust PCA or RST methods without requiring any model on how the outlier suort is generated.
"
70,2018,Katyusha X: Simple Momentum Method for Stochastic Sum-of-Nonconvex Optimization,Oral/Poster,"The roblem of minimizing sum-of-nonconvex functions (i.e., convex functions that are average of non-convex ones) is becoming increasing imortant in machine learning, and is the core machinery for PCA, SVD, regularized Newton's method, accelerated non-convex otimization, and more. We show how to rovably obtain an accelerated stochastic algorithm for minimizing sum-of-nonconvex functions, by adding one additional line to the well-known SVRG method. This line corresonds to momentum, and shows how to directly aly momentum to the finite-sum stochastic minimization of sum-of-nonconvex functions. As a side result, our method enjoys linear arallel seed-u using mini-batch.
"
71,2018,signSGD: Compressed Optimisation for Non-Convex Problems,Oral/Poster,"Training large neural networks requires distributing learning across multile workers, where the cost of communicating gradients can be a significant bottleneck.  signSGD alleviates this roblem by  transmitting just the sign of each minibatch stochastic gradient. We rove that it can get the best of both worlds: comressed gradients and SGD-level convergence rate. The relative $\ell_1\ell_2$ geometry of gradients, noise and curvature informs whether signSGD or SGD is theoretically better suited to a articular roblem. On the ractical side we find that the momentum counterart of signSGD is able to match the accuracy and convergence seed of Adam on dee Imagenet models. We extend our theory to the distributed setting, where the arameter server uses majority vote to aggregate gradient signs from each worker enabling 1-bit comression of worker-server communication in both directions. Using a theorem by Gauss we rove that majority vote can achieve the same reduction in variance as full recision distributed SGD. Thus, there is great romise for sign-based otimisation schemes to achieve fast communication and fast convergence. Code to reroduce exeriments is to be found at htts:github.comjxbzsignSGD."
72,2018,Synthesizing Robust Adversarial Examples,Oral/Poster,"Standard methods for generating adversarial examles for neural networks do not consistently fool neural network classifiers in the hysical world due to a combination of viewoint shifts, camera noise, and other natural transformations, limiting their relevance to real-world systems. We demonstrate the existence of robust 3D adversarial objects, and we resent the first algorithm for synthesizing examles that are adversarial over a chosen distribution of transformations. We synthesize two-dimensional adversarial images that are robust to noise, distortion, and affine transformation. We aly our algorithm to comlex three-dimensional objects, using 3D-rinting to manufacture the first hysical adversarial objects. Our results demonstrate the existence of 3D adversarial objects in the hysical world.
"
73,2018,Differentiable Abstract Interpretation for Provably Robust Neural Networks,Oral/Poster,"We introduce a scalable method for training robust neural networks based on abstract interretation. We resent several abstract transformers which balance efficiency with recision and show these can be used to train large neural networks that are certifiably robust to adversarial erturbations.
"
74,2018,Stochastic Training of Graph Convolutional Networks with Variance Reduction,Oral/Poster,"Grah convolutional networks (GCNs) are owerful dee neural networks for grah-structured data. However, GCN comutes the reresentation of a node recursively from its neighbors, making the recetive field size grow exonentially with the number of layers. Previous attemts on reducing the recetive field size by subsamling neighbors do not have convergence guarantee, and their recetive field size er node is still in the order of hundreds. In this aer, we develo control variate based algorithms with new theoretical guarantee to converge to a local otimum of GCN regardless of the neighbor samling size. Emirical results show that our algorithms enjoy similar convergence rate and model quality with the exact algorithm using only two neighbors er node. The running time of our algorithms on a large Reddit dataset is only one seventh of revious neighbor samling algorithms.
"
75,2018,Neural Relational Inference for Interacting Systems,Oral/Poster,"Interacting systems are revalent in nature, from dynamical systems in hysics to comlex societal dynamics. The interlay of comonents can give rise to comlex behavior, which can often be exlained using a simle model of the system's constituent arts. In this work, we introduce the neural relational inference (NRI) model: an unsuervised model that learns to infer interactions while simultaneously learning the dynamics urely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code reresents the underlying interaction grah and the reconstruction is based on grah neural networks. In exeriments on simulated hysical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsuervised manner. We further demonstrate that we can find an interretable structure and redict comlex dynamics in real motion cature and sorts tracking data.
"
76,2018,Which Training Methods for GANs do actually Converge?,Oral/Poster,"Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this aer, we show that the requirement of absolute continuity is necessary: we describe a simle yet rototyical counterexamle showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently roosed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient enalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator udates er generator udate do not always converge to the equilibrium oint. We discuss these results, leading us to a new exlanation for the stability roblems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and rove local convergence for simlified gradient enalties even if the generator and data distributions lie on lower dimensional manifolds. We find these enalties to work well in ractice and use them to learn high-resolution generative image models for a variety of datasets with little hyerarameter tuning.
"
77,2018,Learning Independent Causal Mechanisms,Oral/Poster,"Statistical learning relies uon data samled from a distribution, and we usually do not care what actually generated it in the first lace. From the oint of view of causal modeling, the structure of each distribution is induced by hysical mechanisms that give rise to deendences between observables.  Mechanisms, however, can be meaningful autonomous modules of generative models that make sense beyond a articular entailed data distribution, lending themselves to transfer between roblems. We develo an algorithm to recover a set of indeendent (inverse) mechanisms from a set of transformed data oints. The aroach is unsuervised and based on a set of exerts that comete for data generated by the mechanisms, driving secialization. We analyze the roosed method in a series of exeriments on image data. Each exert learns to ma a subset of the transformed data back to a reference distribution. The learned mechanisms generalize to novel domains. We discuss imlications for transfer learning and links to recent trends in generative modeling.
"
78,2018,Nonconvex Optimization for Regression with Fairness Constraints,Oral/Poster,"The unfairness of a regressor is evaluated by measuring the correlation between the estimator and the sensitive attribute (e.g., race, gender, age), and the coefficient of determination (CoD) is a natural extension of the correlation coefficient when more than one sensitive attribute exists. As is well known, there is a trade-off between fairness and accuracy of a regressor, which imlies a erfectly fair otimizer does not always yield a useful rediction. Taking this into consideration, we otimize the accuracy of the estimation subject to a user-defined level of fairness. However, a fairness level as a constraint induces a nonconvexity of the feasible region, which disables the use of an off-the-shelf convex otimizer.  Desite such nonconvexity, we show an exact solution is available by using tools of global otimization theory. Furthermore, we roose a nonlinear extension of the method by kernel reresentation. Unlike most of existing fairness-aware machine learning methods, our method allows us to deal with numeric and multile sensitive attributes.
"
79,2018,Fairness Without Demographics in Repeated Loss Minimization,Oral/Poster,"Machine learning models (e.g., seech recognizers) trained on average loss suffer from reresentation disarity---minority grous (e.g., non-native seakers) carry less weight in the training objective, and thus tend to suffer higher loss. Worse, as model accuracy affects user retention, a minority grou can shrink over time. In this aer, we first show that the status quo of emirical risk minimization (ERM) amlifies reresentation disarity over time, which can even turn initially fair models unfair. To mitigate this, we develo an aroach based on distributionally robust otimization (DRO), which minimizes the worst case risk over all distributions close to the emirical distribution. We rove that this aroach controls the risk of the minority grou at each time ste, in the sirit of Rawlsian distributive justice, while remaining oblivious to the identity of the grous. We demonstrate that DRO revents disarity amlification on examles where ERM fails, and show imrovements in minority grou user satisfaction in a real-world text autocomlete task.
"
80,2018,MSplit LBI: Realizing Feature Selection and Dense Estimation Simultaneously in Few-shot and Zero-shot Learning,Oral/Poster,"It is one tyical and general toic of learning a good embedding model to efficiently learn the reresentation coefficients between two sacessubsaces. To solve this task, $L_{1}$ regularization is widely used for the ursuit of feature selection and avoiding overfitting, and yet the sarse estimation of features in $L_{1}$ regularization may cause the underfitting of training data. $L_{2}$ regularization is also frequently used, but it is a biased estimator. In this aer, we roose the idea that the features consist of three orthogonal arts, \emh{namely} sarse strong signals, dense weak signals and random noise, in which both strong and weak signals contribute to the fitting of data. To facilitate such novel decomosition, \emh{MSlit} LBI is for the first time roosed to realize feature selection and dense estimation simultaneously. We rovide theoretical and simulational verification that our method exceeds $L_{1}$ and $L_{2}$ regularization, and extensive exerimental results show that our method achieves state-of-the-art erformance in the few-shot and zero-shot learning."
81,2018,Nonoverlap-Promoting Variable Selection,Oral/Poster,"Variable selection is a classic roblem in machine learning (ML), widely used to find imortant exlanatory factors, and imrove generalization erformance and interretability of ML models. In this aer, we consider variable selection for models where multile resonses are to be redicted based on the same set of covariates. Since each resonse is relevant to a unique subset of covariates, we desire the selected variables for different resonses have small overla. We roose a regularizer that simultaneously encourage orthogonality and sarsity, which jointly brings in an effect of reducing overla. We aly this regularizer to four model instances and develo efficient algorithms to solve the regularized roblems. We rovide a formal analysis on why the roosed regularizer can reduce generalization error. Exeriments on both simulation studies and real-world datasets demonstrate the effectiveness of the roosed regularizer in selecting less-overlaed variables and imroving generalization erformance.
"
82,2018,Towards More Efficient Stochastic Decentralized Learning: Faster Convergence and Sparse Communication,Oral/Poster,"Recently, the decentralized otimization roblem is attracting growing attention. Most existing methods are deterministic with high er-iteration cost and have a convergence rate quadratically deending on the roblem condition number. Besides, the dense communication is necessary to ensure the convergence even if the dataset is sarse. In this aer, we generalize the decentralized otimization roblem to a monotone oerator root finding roblem, and roose a stochastic algorithm named DSBA that (1) converges geometrically with a rate linearly deending on the roblem condition number, and (2) can be imlemented using sarse communication only. Additionally, DSBA handles imortant learning roblems like AUC-maximization which can not be tackled efficiently in the revious roblem setting. Exeriments on convex minimization and AUC-maximization validate the efficiency of our method.
"
83,2018,Graph Networks as Learnable Physics Engines for Inference and Control,Oral/Poster,"Understanding and interacting with everyday hysical scenes requires rich knowledge about the structure of the world, reresented either imlicitly in a value or olicy function, or exlicitly in a transition model. Here we introduce a new class of learnable models--based on grah networks--which imlement an inductive bias for object- and relation-centric reresentations of comlex, dynamical systems. Our results show that as a forward model, our aroach suorts accurate redictions from real and simulated data, and surrisingly strong and efficient generalization, across eight distinct hysical systems which we varied arametrically and structurally. We also found that our inference model can erform system identification. Our models are also differentiable, and suort online lanning via gradient-based trajectory otimization, as well as offline olicy otimization. Our framework offers new oortunities for harnessing and exloiting rich knowledge about the world, and takes a key ste toward building machines with more human-like reresentations of the world.
"
84,2018,An Alternative View: When Does SGD Escape Local Minima?,Oral/Poster,"Stochastic gradient descent (SGD) is widely used in machine learning. Although being commonly viewed as a fast but not accurate version of gradient descent (GD), it always finds better solutions than GD for modern neural networks. In order to understand this henomenon, we take an alternative view that SGD is working on the convolved (thus smoothed) version of the loss function. We show that, even if the function $f$ has many bad local minima or saddle oints, as long as for every oint $x$, the weighted average of the gradients of its neighborhoods is one oint convex with resect to the desired solution $x^*$, SGD will get close to, and then stay around $x^*$ with constant robability. Our result identifies a set of functions that SGD rovably works, which is much larger than the set of convex functions. Emirically, we observe that the loss surface of neural networks enjoys nice one oint convexity roerties locally, therefore our theorem hels exlain why SGD works so well for neural networks."
85,2018,Asynchronous Decentralized Parallel Stochastic Gradient Descent,Oral/Poster,"Most commonly used distributed machine learning systems are either synchronous or centralized asynchronous. Synchronous algorithms like AllReduce-SGD erform oorly in a heterogeneous environment, while asynchronous algorithms using a arameter server suffer from 1) communication bottleneck at arameter servers when workers are many, and 2) significantly worse convergence when the traffic to arameter server is congested. Can we design an algorithm that is robust in a heterogeneous environment, while being communication efficient and maintaining the best-ossible convergence rate? In this aer, we roose an asynchronous decentralized stochastic gradient decent algorithm (AD-PSGD) satisfying all above exectations. Our theoretical analysis shows AD-PSGD converges at the otimal $O(1\sqrt{K})$ rate as SGD and has linear seedu w.r.t. number of workers. Emirically, AD-PSGD outerforms the best of decentralized arallel SGD (D-PSGD), asynchronous arallel SGD (A-PSGD), and standard data arallel SGD (AllReduce-SGD), often by orders of magnitude in a heterogeneous environment. When training ResNet-50 on ImageNet with u to 128 GPUs, AD-PSGD converges (w.r.t eochs) similarly to the AllReduce-SGD, but each eoch can be u to 4-8x faster than its synchronous counterarts in a network-sharing HPC environment. To the best of our knowledge, AD-PSGD is the first asynchronous algorithm that achieves a similar eoch-wise convergence rate as AllReduce-SGD, at an over 100-GPU scale."
86,2018,An Estimation and Analysis Framework for the Rasch Model,Oral/Poster,"The Rasch model is widely used for item resonse analysis in alications ranging from recommender systems to sychology, education, and finance. While a number of estimators have been roosed for the Rasch model over the last decades, the associated analytical erformance guarantees are mostly asymtotic. This aer rovides a framework that relies on a novel linear minimum mean-squared error (L-MMSE) estimator which enables an exact, nonasymtotic, and closed-form analysis of the arameter estimation error under the Rasch model. The roosed framework rovides guidelines on the number of items and resonses required to attain low estimation errors in tests or surveys. We furthermore demonstrate its efficacy on a number of real-world collaborative filtering datasets, which reveals that the roosed L-MMSE estimator erforms on ar with state-of-the-art nonlinear estimators in terms of redictive erformance.
"
87,2018,Mitigating Bias in Adaptive Data Gathering via Differential Privacy,Oral/Poster,"Data that is gathered adatively --- via bandit algorithms, for examle --- exhibits bias. This is true both when gathering simle numeric valued data --- the emirical means ket track of by stochastic bandit algorithms are biased downwards --- and when gathering more comlicated data --- running hyothesis tests on comlex data gathered via contextual bandit algorithms leads to false discovery. In this aer, we show that this roblem is mitigated if the data collection rocedure is differentially rivate. This lets us both bound the bias of simle numeric valued quantities (like the emirical means of stochastic bandit algorithms), and correct the -values of hyothesis tests run on the adatively gathered data. Moreover, there exist differentially rivate bandit algorithms with near otimal regret bounds: we aly existing theorems in the simle stochastic case, and give a new analysis for linear contextual bandits. We comlement our theoretical results with exeriments validating our theory.
"
88,2018,Local Private Hypothesis Testing: Chi-Square Tests,Oral/Poster,"The local model for differential rivacy is emerging as the reference model for ractical alications of collecting  and sharing sensitive information while satisfying strong rivacy guarantees.  In the local model, there is no trusted entity which is allowed to have each individual's raw data as is assumed in the traditional curator model. Individuals' data are usually erturbed before sharing them. We exlore the design of rivate hyothesis tests in the local model, where each data entry is erturbed to ensure the rivacy of each articiant.  Secifically, we analyze locally rivate chi-square tests for goodness of fit and indeendence testing.
"
89,2018,Disentangling by Factorising,Oral/Poster,"We define and address the roblem of unsuervised learning of disentangled reresentations on data generated from indeendent factors of variation. We roose FactorVAE, a method that disentangles by encouraging the distribution of reresentations to be factorial and hence indeendent across the dimensions. We show that it imroves uon beta-VAE by roviding a better trade-off between disentanglement and reconstruction quality and being more robust to the number of training iterations. Moreover, we highlight the roblems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.
"
90,2018,Efficient Bias-Span-Constrained Exploration-Exploitation in Reinforcement Learning,Oral/Poster,"We introduce SCAL, an algorithm designed to erform efficient exloration-exloration in any unknown weakly-communicating Markov Decision Process (MDP) for which an uer bound c on the san of the otimal bias function is known. For an MDP with $S$ states, $A$ actions and $\Gamma \leq S$ ossible next states, we rove a regret bound of $O(c\sqrt{\Gamma SAT})$, which significantly imroves over existing algorithms (e.g., UCRL and PSRL), whose regret scales linearly with the MDP diameter $D$. In fact, the otimal bias san is finite and often much smaller than $D$ (e.g., $D=+\infty$ in non-communicating MDPs). A similar result was originally derived by Bartlett and Tewari (2009) for REGAL.C, for which no tractable algorithm is available. In this aer, we relax the otimization roblem at the core of REGAL.C, we carefully analyze its roerties, and we rovide the first comutationally efficient algorithm to solve it. Finally, we reort numerical simulations suorting our theoretical findings and showing how SCAL significantly outerforms UCRL in MDPs with large diameter and small san."
91,2018,Learning to search with MCTSnets,Oral/Poster,"Planning roblems are among the most imortant and well-studied roblems in artificial intelligence. They are most tyically solved by tree search algorithms that simulate ahead into the future, evaluate future states, and back-u those evaluations to the root of a search tree. Among these algorithms, Monte-Carlo tree search (MCTS) is one of the most general, owerful and widely used. A tyical imlementation of MCTS uses cleverly designed rules, otimised to the articular characteristics of the domain. These rules control where the simulation traverses, what to evaluate in the states that are reached, and how to back-u those evaluations. In this aer we instead learn where, what and how to search. Our architecture, which we call an MCTSnet, incororates simulation-based search inside a neural network, by exanding, evaluating and backing-u a vector embedding. The arameters of the network are trained end-to-end using gradient-based otimisation. When alied to small searches in the well-known lanning roblem Sokoban, the learned search algorithm significantly outerformed MCTS baselines.
"
92,2018,Decoupled Parallel Backpropagation with Convergence Guarantee,Oral/Poster,"Backroagation algorithm is indisensable for the training of feedforward neural networks. It requires roagating error gradients sequentially from the outut layer all the way back to the inut layer. The backward locking in backroagation algorithm constrains us from udating network layers in arallel and fully leveraging the comuting resources. Recently, several algorithms have been roosed for breaking the backward locking. However, their erformances degrade seriously when networks are dee. In this aer, we roose decouled arallel backroagation algorithm for dee learning otimization with convergence guarantee. Firstly, we decoule the backroagation algorithm using delayed gradients, and show that the backward locking is removed when we slit the networks into multile modules. Then, we utilize decouled arallel backroagation in two stochastic methods and rove that our method guarantees convergence to critical oints for the non-convex roblem. Finally, we erform exeriments for training dee convolutional neural networks on benchmark datasets. The exerimental results not only confirm our theoretical analysis, but also demonstrate that the roosed method can achieve significant seedu without loss of accuracy.
"
93,2018,On Learning Sparsely Used Dictionaries from Incomplete Samples,Oral/Poster,"Existing algorithms for dictionary learning assume that the entries of the (high-dimensional) inut data are fully observed. However, in several ractical alications, only an incomlete fraction of the data entries may be available. For incomlete settings, no rovably correct and olynomial-time algorithm has been reorted in the dictionary learning literature. In this aer, we rovide rovable aroaches for learning -- from incomlete samles -- a family of dictionaries whose atoms have sufficiently ``sread-out'' mass. First, we roose a descent-style iterative algorithm that linearly converges to the true dictionary when rovided a sufficiently coarse initial estimate. Second, we roose an initialization algorithm that utilizes a small number of extra fully observed samles to roduce such a coarse initial estimate. Finally, we theoretically analyze their erformance and rovide asymtotic statistical and comutational guarantees.
"
94,2018,Variational Network Inference: Strong and Stable with Concrete Support,Oral/Poster,"Traditional methods for the discovery of latent network structures are limited in two ways: they either assume that all the signal comes from the network (i.e. there is no source of signal outside the network) or they lace constraints on the network arameters to ensure model or algorithmic stability. We address these limitations by roosing a  model that incororates a Gaussian rocess rior on a network-indeendent comonent and formally roving that we get algorithmic stability for free while roviding a novel ersective on model stability as well as robustness results and recise intervals for key inference arameters. We show that, on three alications, our aroach outerforms revious methods consistently.
"
95,2018,Weakly Submodular Maximization Beyond Cardinality Constraints: Does Randomization Help Greedy?,Oral/Poster,"Submodular functions are a broad class of set functions that naturally arise in many machine learning alications. Due to their combinatorial structures, there has been a myriad of algorithms for maximizing such functions under various constraints. Unfortunately, once a function deviates from submodularity (even slightly), the known algorithms may erform arbitrarily oorly. Amending this issue, by obtaining aroximation results for functions obeying roerties that generalize submodularity, has been the focus of several recent works. One such class, known as weakly submodular functions, has received a lot of recent attention from the machine learning community due to its strong connections to restricted strong convexity and sarse reconstruction. In this aer, we rove that a randomized version of the greedy algorithm achieves an aroximation ratio of $(1 + 1\gamma )^{-2}$ for weakly submodular maximization subject to a general matroid constraint, where $\gamma$ is a arameter measuring the distance from submodularity. To the best of our knowledge, this is the first algorithm with a non-trivial aroximation guarantee for this constrained otimization roblem. Moreover, our exerimental results show that our roosed algorithm erforms well in a variety of real-world roblems, including regression, video summarization, slice site detection, and black-box interretation."
96,2018,Data Summarization at Scale: A Two-Stage Submodular Approach,Oral/Poster,"The sheer scale of modern datasets has resulted in a dire need for summarization techniques that can identify reresentative elements in a dataset. Fortunately, the vast majority of data summarization tasks satisfy an intuitive diminishing returns condition known as submodularity, which allows us to find nearly-otimal solutions in linear time. We focus on a two-stage submodular framework where the goal is to use some given training functions to reduce the ground set so that otimizing new functions (drawn from the same distribution) over the reduced set rovides almost as much value as otimizing them over the entire ground set. In this aer, we develo the first streaming and distributed solutions to this roblem. In addition to roviding strong theoretical guarantees, we demonstrate both the utility and efficiency of our algorithms on real-world tasks including image summarization and ride-share otimization.
"
97,2018,Best Arm Identification in Linear Bandits with Linear Dimension Dependency,Oral/Poster,"We study the best arm identification roblem in linear bandits, where the mean reward of each arm deends linearly on an unknown $d$-dimensional arameter vector $\theta$, and the goal is to identify the arm with the largest exected reward. We first design and analyze a novel randomized $\theta$ estimator based on the solution to the convex relaxation of an otimal $G$-allocation exeriment design roblem. Using this estimator, we describe an algorithm whose samle comlexity deends linearly on the dimension $d$, as well as an algorithm with samle comlexity deendent on the reward gas of the best $d$ arms, matching the lower bound arising from the ordinary to-arm identification roblem. We finally comare the emirical erformance of our algorithms with other state-of-the-art algorithms in terms of both samle comlexity and comutational time."
98,2018,Learning with Abandonment,Oral/Poster,"Consider a latform that wants to learn a ersonalized olicy for each user, but the latform faces the risk of a user abandoning the latform if they are dissatisfied with the actions of the latform.  For examle, a latform is interested in ersonalizing the number of newsletters it sends, but faces the risk that the user unsubscribes forever.  We roose a general thresholded learning model for scenarios like this, and discuss the structure of otimal olicies.  We describe salient features of otimal ersonalization algorithms and how feedback the latform receives imacts the results.  Furthermore, we investigate how the latform can efficiently learn the heterogeneity across users by interacting with a oulation and rovide erformance guarantees.
"
99,2018,Hyperbolic Entailment Cones for Learning Hierarchical Embeddings,Oral/Poster,"Learning grah reresentations via low-dimensional embeddings that reserve relevant network roerties is an imortant class of roblems in machine learning. We here resent a novel method to embed directed acyclic grahs. Following rior work, we first advocate for using hyerbolic saces which rovably model tree-like structures better than Euclidean geometry. Second, we view hierarchical relations as artial orders defined using a family of nested geodesically convex cones. We rove that these entailment cones admit an otimal shae with a closed form exression both in the Euclidean and hyerbolic saces, and they canonically define the embedding learning rocess. Exeriments show significant imrovements of our method over strong recent baselines both in terms of reresentational caacity and generalization.
"
100,2018,Generative Temporal Models with Spatial Memory for Partially Observed Environments,Oral/Poster,"In model-based reinforcement learning, generative and temoral models of environments can be leveraged to boost agent erformance, either by tuning the agent's reresentations during training or via use as art of an exlicit lanning mechanism. However, their alication in ractice has been limited to simlistic environments, due to the difficulty of training such models in larger, otentially artially-observed and 3D environments. In this work we introduce a novel action-conditioned generative model of such challenging environments. The model features a non-arametric satial memory system in which we store learned, disentangled reresentations of the environment. Low-dimensional satial udates are comuted using a state-sace model that makes use of knowledge on the rior dynamics of the moving agent, and high-dimensional visual observations are modelled with a Variational Auto-Encoder. The result is a scalable architecture caable of erforming coherent redictions over hundreds of time stes across a range of artially observed 2D and 3D environments.
"
101,2018,DiCE: The Infinitely Differentiable Monte Carlo Estimator,Oral/Poster,"The score function estimator is widely used for estimating gradients of stochastic objectives in stochastic comutation grahs (SCG), eg., in reinforcement learning and meta-learning. While deriving the first-order gradient estimators by differentiating a surrogate loss (SL) objective is comutationally and concetually simle, using the same aroach for higher-order derivatives is more challenging. Firstly, analytically deriving and imlementing such estimators is laborious and not comliant with automatic differentiation. Secondly, reeatedly alying SL to construct new objectives for each order derivative involves increasingly cumbersome grah maniulations. Lastly, to match the first-order gradient under differentiation, SL treats art of the cost as a fixed samle, which we show leads to missing and wrong terms for estimators of higher-order derivatives. To address all these shortcomings in a unified way, we introduce DiCE, which rovides a single objective that can be differentiated reeatedly, generating correct estimators of derivatives of any order in SCGs. Unlike SL, DiCE relies on automatic differentiation for erforming the requisite grah maniulations. We verify the correctness of DiCE both through a roof and numerical evaluation of the DiCE derivative estimates. We also use DiCE to roose and evaluate a novel aroach for multi-agent learning. Our code is available at htts:goo.glxkkGxN.
"
102,2018,Orthogonal Recurrent Neural Networks with Scaled Cayley Transform,Oral/Poster,"Recurrent Neural Networks (RNNs) are designed to handle sequential data but suffer from vanishing or exloding gradients.  Recent work on Unitary Recurrent Neural Networks (uRNNs) have been used to address this issue and in some cases, exceed the caabilities of Long Short-Term Memory networks (LSTMs).  We roose a simler and novel udate scheme to maintain orthogonal recurrent weight matrices without using comlex valued matrices. This is done by arametrizing with a skew-symmetric matrix using the Cayley transform; such a arametrization is unable to reresent matrices with negative one eigenvalues, but this limitation is overcome by scaling the recurrent weight matrix by a diagonal matrix consisting of ones and negative ones.  The roosed training scheme involves a straightforward gradient calculation and udate ste. In several exeriments, the roosed scaled Cayley orthogonal recurrent neural network (scoRNN) achieves suerior results with fewer trainable arameters than other unitary RNNs.
"
103,2018,Least-Squares Temporal Difference Learning for the Linear Quadratic Regulator,Oral/Poster,"Reinforcement learning (RL) has been successfully used to solve many continuous control tasks. Desite its imressive results however, fundamental questions regarding the samle comlexity of RL on continuous roblems remain oen.  We study the erformance of RL in this setting by considering the behavior of the Least-Squares Temoral Difference (LSTD) estimator on the classic Linear Quadratic Regulator (LQR) roblem from otimal control.  We give the first finite-time analysis of the number of samles needed to estimate the value function for a fixed static state-feedback olicy to within esilon-relative error.  In the rocess of deriving our result, we give a general characterization for when the minimum eigenvalue of the emirical covariance matrix formed along the samle ath of a fast-mixing stochastic rocess concentrates above zero, extending a result by Koltchinskii and Mendelson in the indeendent covariates setting.  Finally, we rovide exerimental evidence indicating that our analysis correctly catures the qualitative behavior of LSTD on several LQR instances.
"
104,2018,Spotlight: Optimizing Device Placement for Training Deep Neural Networks,Oral/Poster,"Training dee neural networks (DNNs) requires an increasing amount of comutation resources, and it becomes tyical to use a mixture of GPU and CPU devices. Due to the heterogeneity of these devices, a recent challenge is how each oeration in a neural network can be otimally laced on these devices, so that the training rocess can take the shortest amount of time ossible. The current state-of-the-art solution uses reinforcement learning based on the olicy gradient method, and it suffers from subotimal training times.  In this aer, we roose Sotlight, a new reinforcement learning algorithm based on roximal olicy otimization, designed secifically for finding an otimal device lacement for training DNNs. The design of our new algorithm relies uon a new model of the device lacement roblem: by modeling it as a Markov decision rocess with multile stages, we are able to rove that Sotlight achieves a theoretical guarantee on erformance imrovements.  We have imlemented Sotlight in the CIFAR-10 benchmark and deloyed it on the Google Cloud latform. Extensive exeriments have demonstrated that the training time with lacements recommended by Sotlight is 60.9% of that recommended by the olicy gradient method.
"
105,2018,Universal Planning Networks: Learning Generalizable Representations for Visuomotor Control,Oral/Poster,"A key challenge in comlex visuomotor control is learning abstract reresentations that are effective for secifying goals, lanning, and generalization. To this end, we introduce universal lanning networks (UPN). UPNs embed differentiable lanning within a goal-directed olicy. This lanning comutation unrolls a forward model in a latent sace and infers an otimal action lan through gradient descent trajectory otimization. The lan-by-gradient-descent rocess and its underlying reresentations are learned end-to-end to directly otimize a suervised imitation learning objective. We find that the reresentations learned are not only effective for goal-directed visual imitation via gradient-based trajectory otimization, but can also rovide a metric for secifying goals using images. The learned reresentations can be leveraged to secify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image based goals. We were able to achieve successful transfer of visuomotor lanning strategies across robots with significantly different morhologies and actuation caabilities. Visit htts:sites.google. comviewun-ublichome for video highlights.
"
106,2018,Coordinated Exploration in Concurrent Reinforcement Learning,Oral/Poster,"We consider a team of reinforcement learning agents that concurrently learn to oerate in a common environment. We identify three roerties - adativity, commitment, and diversity - which are necessary for efficient coordinated exloration and demonstrate that straightforward extensions to single-agent otimistic and osterior samling aroaches fail to satisfy them. As an alternative, we roose seed samling, which extends osterior samling in a manner that meets these requirements. Simulation results investigate how er-agent regret decreases as the number of agents grows, establishing substantial advantages of seed samling over alternative exloration schemes.
"
107,2018,A probabilistic framework for multi-view feature learning with many-to-many associations via neural networks,Oral/Poster,"A simle framework Probabilistic Multi-view Grah Embedding (PMvGE) is roosed for multi-view feature learning with many-to-many associations so that it generalizes various existing multi-view methods. PMvGE is a robabilistic model for redicting new associations via grah embedding of the nodes of data vectors with links of their associations. Multi-view data vectors with many-to-many associations are transformed by neural networks to feature vectors in a shared sace, and the robability of new association between two data vectors is modeled by the inner roduct of their feature vectors. While existing multi-view feature learning techniques can treat only either of many-to-many association or non-linear transformation, PMvGE can treat both simultaneously. By combining Mercer's theorem and the universal aroximation theorem, we rove that PMvGE learns a wide class of similarity measures across views. Our likelihood-based estimator enables efficient comutation of non-linear transformations of data vectors in large-scale datasets by minibatch SGD, and numerical exeriments illustrate that PMvGE outerforms existing multi-view methods.
"
108,2018,Learning Steady-States of Iterative Algorithms over Graphs,Oral/Poster,"Many grah analytics roblems can be solved via iterative algorithms where the solutions are often characterized by a set of steady-state conditions. Different algorithms resect to different set of fixed oint constraints, so instead of using these traditional algorithms, can we learn an algorithm which can obtain the same steady-state solutions automatically from examles, in an effective and scalable way? How to reresent the meta learner for such algorithm and how to carry out the learning? In this aer, we roose an embedding reresentation for iterative algorithms over grahs, and design a learning method which alternates between udating the embeddings and rojecting them onto the steady-state constraints. We demonstrate the effectiveness of our framework using a few commonly used grah algorithms, and show that in some cases, the learned algorithm can handle grahs with more than 100,000,000 nodes in a single machine.
"
109,2018,Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples,Oral/Poster,"We identify obfuscated gradients, a kind of gradient masking, as a henomenon that leads to a false sense of security in defenses against adversarial examles. While defenses that cause obfuscated gradients aear to defeat iterative otimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three tyes of obfuscated gradients we discover, we develo attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 comletely, and 1 artially, in the original threat model each aer considers.
"
110,2018,Fair and Diverse DPP-Based Data Summarization,Oral/Poster,"Samling methods that choose a subset of the data roortional to its diversity in the feature sace are oular for data summarization. However, recent studies have noted the occurrence of bias – e.g., under or over reresentation of a articular gender or ethnicity – in such data summarization methods. In this aer we initiate a study of the roblem of oututting a diverse and fair summary of a given dataset. We work with a well-studied determinantal measure of diversity and corresonding distributions (DPPs) and resent a framework that allows us to incororate a general class of fairness constraints into such distributions. Designing efficient algorithms to samle from these constrained determinantal distributions, however, suffers from a comlexity barrier; we resent a fast samler that is rovably good when the inut vectors satisfy a natural roerty. Our emirical results on both real-world and synthetic datasets show that the diversity of the samles roduced by adding fairness constraints is not too far from the unconstrained case.
"
111,2018,Learning Implicit Generative Models with the Method of Learned Moments,Oral/Poster,"We roose a method of moments (MoM) algorithm for training large-scale imlicit generative models. Moment estimation in this setting encounters two roblems: it is often difficult to define the millions of moments needed to learn the model arameters, and it is hard to determine which roerties are useful when secifying moments. To address the first issue, we introduce a moment network, and define the moments as the network's hidden units and the gradient of the network's outut with resect to its arameters. To tackle the second roblem, we use asymtotic theory to highlight desiderata for moments -- namely they should minimize the asymtotic variance of estimated model arameters -- and introduce an objective to learn better moments. The sequence of objectives created by this Method of Learned Moments (MoLM) can train high-quality neural image samlers. On CIFAR-10, we demonstrate that MoLM-trained generators achieve significantly higher Incetion Scores and lower Frechet Incetion Distances than those trained with gradient enalty-regularized and sectrally-normalized adversarial objectives. These generators also achieve nearly erfect Multi-Scale Structural Similarity Scores on CelebA, and can create high-quality samles of 128x128 images.
"
112,2018,Chi-square Generative Adversarial Network,Oral/Poster,"To assess the difference between real and synthetic data, Generative Adversarial Networks (GANs) are trained using a distribution discreancy measure. Three widely emloyed measures are information-theoretic divergences, integral robability metrics, and Hilbert sace discreancy metrics. We elucidate the theoretical connections between these three oular GAN training criteria and roose a novel rocedure, called $\chi^2$ (Chi-square) GAN, that is concetually simle, stable at training and resistant to mode collase. Our rocedure naturally generalizes to address the roblem of simultaneous matching of multile distributions. Further, we roose a resamling strategy that significantly imroves samle quality, by reurosing the trained critic function via an imortance weighting mechanism. Exeriments show that the roosed rocedure imroves stability and convergence, and yields state-of-art results on a wide range of generative modeling tasks."
113,2018,Streaming Principal Component Analysis in Noisy Setting,Oral/Poster,"We study streaming algorithms for rincial comonent analysis (PCA) in noisy settings. We resent comutationally efficient algorithms with sub-linear regret bounds for PCA in the resence of noise, missing data, and gross outliers.
"
114,2018,Partial Optimality and Fast Lower Bounds for Weighted Correlation Clustering,Oral/Poster,"Weighted correlation clustering is hard to solve and hard to aroximate for general grahs. Its alications in network analysis and comuter vision call for efficient algorithms. To this end, we make three contributions: We establish artial otimality conditions that can be checked efficiently, and doing so recursively solves the roblem for series-arallel grahs to otimality, in linear time. We exloit the acking dual of the roblem to comute a heuristic, but non-trivial lower bound faster than that of a canonical linear rogram relaxation. We introduce a re-weighting with the dual solution by which efficient local search algorithms converge to better feasible solutions. The effectiveness of our methods is demonstrated emirically on a number of benchmark instances.
"
115,2018,SGD and Hogwild! Convergence Without the Bounded Gradients Assumption,Oral/Poster,"Stochastic gradient descent (SGD) is the otimization algorithm of choice in many machine learning alications such as regularized emirical risk minimization and training dee neural networks. The classical convergence analysis of SGD is carried out under the assumtion  that the norm of the stochastic gradient is uniformly bounded. While this might hold for some loss functions, it is always violated for  cases where the objective  function is strongly convex. In (Bottou et al.,2016), a new analysis of convergence of SGD is erformed under the assumtion that stochastic gradients are bounded with resect to the true gradient norm. Here we show that for stochastic roblems arising in machine learning such bound always holds; and we also roose an alternative convergence analysis of SGD with diminishing learning rate regime, which results in more relaxed conditions than those in (Bottou et al.,2016). We then move on the asynchronous arallel setting, and rove convergence of Hogwild! algorithm in the same regime, obtaining the first convergence results for this method in the case of diminished learning rate.
"
116,2018,Computational Optimal Transport: Complexity by Accelerated Gradient Descent Is Better Than by Sinkhorn's Algorithm,Oral/Poster,"We analyze two algorithms for aroximating the general otimal transort (OT) distance between two discrete distributions of size $n$, u to accuracy $\varesilon$.  For the first algorithm, which is based on the celebrated Sinkhorn's algorithm, we rove the comlexity bound $\widetilde{O}\left(\frac{n^2}{\varesilon^2}\right)$ arithmetic oerations ($\widetilde{O}$ hides olylogarithmic factors $(\ln n)^c$, $c0$).  For the second one, which is based on our novel Adative Primal-Dual Accelerated Gradient Descent (APDAGD) algorithm, we rove the comlexity bound $\widetilde{O}\left(\min\left\{\frac{n^{94}}{\varesilon}, \frac{n^{2}}{\varesilon^2} \right\}\right)$ arithmetic oerations.  Both bounds have better deendence on $\varesilon$ than the state-of-the-art result given by $\widetilde{O}\left(\frac{n^2}{\varesilon^3}\right)$.  Our second algorithm not only has better deendence on $\varesilon$ in the comlexity bound, but also is not secific to entroic regularization and can solve the OT roblem with different regularizers."
117,2018,Stability and Generalization of Learning Algorithms that Converge to Global Optima,Oral/Poster,"We establish novel generalization bounds for learning algorithms that converge to global minima. We derive black-box stability results that only deend on the convergence of a learning algorithm and the geometry around the minimizers of the emirical risk function. The results are shown for non-convex loss functions satisfying the Polyak-Lojasiewicz (PL) and the quadratic growth (QG) conditions, which we show arise for 1-layer neural networks with leaky ReLU activations and dee neural networks with linear activations. We use our results to establish the stability of first-order methods such as  stochastic gradient descent (SGD), gradient descent (GD), randomized coordinate descent (RCD), and the stochastic variance reduced gradient method (SVRG), in both the PL and the strongly convex setting. Our results match or imrove state-of-the-art generalization bounds and can easily extend to similar otimization algorithms. Finally, although our results imly comarable stability for SGD and GD in the PL setting, we show that there exist simle quadratic models with multile local minima where SGD is stable but GD is not.
"
118,2018,Optimal Rates of Sketched-regularized Algorithms for Least-Squares Regression over Hilbert Spaces,Oral/Poster,"We investigate regularized algorithms combining with rojection for least-squares regression roblem over a Hilbert sace, covering nonarametric regression over a reroducing kernel Hilbert sace. We rove convergence results with resect to variants of norms, under a caacity assumtion on the hyothesis sace and a regularity condition on the target function.  As a result, we obtain otimal rates for regularized algorithms with randomized sketches, rovided that the sketch dimension is roortional to the effective dimension u to a logarithmic factor. As a byroduct, we obtain similar results for Nystr\""{o}m regularized algorithms. Our results rovide otimal, distribution-deendent rates for sketchedNystr\""{o}m regularized algorithms, considering both the attainable and non-attainable cases.
"
119,2018,Adafactor: Adaptive Learning Rates with Sublinear Memory Cost,Oral/Poster,"In several recently roosed stochastic otimization methods (e.g. RMSPro, Adam, Adadelta), arameter udates are scaled by the inverse square roots of exonential moving averages of squared ast gradients. Maintaining these er-arameter second-moment estimators requires memory equal to the number of arameters.  For the case of neural network weight matrices, we roose maintaining only the er-row and er-column sums of these moving averages, and estimating the er-arameter second moments based on these sums. We demonstrate emirically that this method roduces similar results to the baseline. Secondly, we show that adative methods can roduce larger-than-desired udates when the decay rate of the second moment accumulator is too slow. We roose udate cliing and a gradually increasing decay rate scheme as remedies. Combining these methods and droing momentum, we achieve comarable results to the ublished Adam regime in training the Transformer model on the WMT 2014 English-German machine translation task, while using very little auxiliary storage in the otimizer.  Finally, we roose scaling the arameter udates based on the scale of the arameters themselves.
"
120,2018,Fast Parametric Learning with Activation Memorization,Oral/Poster,"Neural networks trained with backroagation often struggle to identify classes that have been observed a small number of times. In alications where most class labels are rare, such as language modelling, this can become a erformance bottleneck. One otential remedy is to augment the network with a fast-learning non-arametric model which stores recent activations and class labels into an external memory. We exlore a simlified architecture where we treat a subset of the model arameters as fast memory stores. This can hel retain information over longer time intervals than a traditional memory, and does not require additional sace or comute. In the case of image classification, we dislay faster binding of novel classes on an Omniglot image curriculum task. We also show imroved erformance for word-based language models on news reorts (GigaWord), books (Project Gutenberg) and Wikiedia articles (WikiText-103) - the latter achieving a state-of-the-art erlexity of 29.2.
"
121,2018,Essentially No Barriers in Neural Network Energy Landscape,Oral/Poster,"Training neural networks involves finding minima of a high-dimensional non-convex loss function. Relaxing from linear interolations, we construct continuous aths between minima of recent neural network architectures on CIFAR10 and CIFAR100. Surrisingly, the aths are essentially flat in both the training and test landscaes. This imlies that minima are erhas best seen as oints on a single connected manifold of low loss, rather than as the bottoms of distinct valleys.
"
122,2018,Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global,Oral/Poster,"We consider dee linear networks with arbitrary convex differentiable  loss. We rovide a short and elementary roof of the fact that all local minima are global minima if the hidden layers are either 1) at least as wide as the inut layer, or 2) at least as wide as the outut layer. This result is the strongest ossible in the following sense: If the loss is convex and Lischitz but not differentiable then dee linear networks can have sub-otimal local minima.
"
123,2018,Generalized Robust Bayesian Committee Machine for Large-scale Gaussian Process Regression,Oral/Poster,"In order to scale standard Gaussian rocess (GP) regression to large-scale datasets, aggregation models emloy factorized training rocess and then combine redictions from distributed exerts. The state-of-the-art aggregation models, however, either rovide inconsistent redictions or require time-consuming aggregation rocess. We first rove the inconsistency of tyical aggregations using disjoint or random data artition, and then resent a consistent yet efficient aggregation model for large-scale GP. The roosed model inherits the advantages of aggregations, e.g., closed-form inference and aggregation, arallelization and distributed comuting. Furthermore, theoretical and emirical analyses reveal that the new aggregation model erforms better due to the consistent redictions that converge to the true underlying function when the training size aroaches infinity.
"
124,2018,Bayesian Quadrature for Multiple Related Integrals,Oral/Poster,"Bayesian robabilistic numerical methods are a set of tools roviding osterior distributions on the outut of numerical methods. The use of these methods is usually motivated by the fact that they can reresent our uncertainty due to incomletefinite information about the continuous mathematical roblem being aroximated. In this aer, we demonstrate that this aradigm can rovide additional advantages, such as the ossibility of transferring information between several numerical methods. This allows users to reresent uncertainty in a more faithful manner and, as a by-roduct, rovide increased numerical efficiency. We roose the first such numerical method by extending the well-known Bayesian quadrature algorithm to the case where we are interested in comuting the integral of several related functions. We then rove convergence rates for the method in the well-secified and missecified cases, and demonstrate its efficiency in the context of multi-fidelity models for comlex engineering systems and a roblem of global illumination in comuter grahics.
"
125,2018,Deep Predictive Coding Network for Object Recognition,Oral/Poster,"Based on the redictive coding theory in neuro- science, we designed a bi-directional and recur- rent neural net, namely dee redictive coding networks (PCN), that has feedforward, feedback, and recurrent connections. Feedback connections from a higher layer carry the rediction of its lower-layer reresentation; feedforward connec- tions carry the rediction errors to its higher-layer. Given image inut, PCN runs recursive cycles of bottom-u and to-down comutation to udate its internal reresentations and reduce the differ- ence between bottom-u inut and to-down re- diction at every layer. After multile cycles of recursive udating, the reresentation is used for image classification. With benchmark datasets (CIFAR-10100, SVHN, and MNIST), PCN was found to always outerform its feedforward-only counterart: a model without any mechanism for recurrent dynamics, and its erformance tended to imrove given more cycles of comutation over time. In short, PCN reuses a single architecture to recursively run bottom-u and to-down ro- cesses to refine its reresentation towards more accurate and definitive object recognition.
"
126,2018,Neural Inverse Rendering for General Reflectance Photometric Stereo,Oral/Poster,"We resent a novel convolutional neural network architecture for hotometric stereo (Woodham, 1980), a roblem of recovering 3D object surface normals from multile images observed under varying illuminations. Desite its long history in comuter vision, the roblem still shows fundamental challenges for surfaces with unknown general reflectance roerties (BRDFs). Leveraging dee neural networks to learn comlicated reflectance models is romising, but studies in this direction are very limited due to difficulties in acquiring accurate ground truth for training and also in designing networks invariant to ermutation of inut images. In order to address these challenges, we roose a hysics based unsuervised learning framework where surface normals and BRDFs are redicted by the network and fed into the rendering equation to synthesize observed images. The network weights are otimized during testing by minimizing reconstruction loss between observed and synthesized images. Thus, our learning rocess does not require ground truth normals or even re-training on external images. Our method is shown to achieve the state-of-the-art erformance on a challenging real-world scene benchmark.
"
127,2018,On the Relationship between Data Efficiency and Error for Uncertainty Sampling,Oral/Poster,"While active learning offers otential cost savings, the actual data efficiency---the reduction in amount of labeled data needed to obtain the same error rate---observed in ractice is mixed. This aer oses a basic question: when is active learning actually helful? We rovide an answer for logistic regression with the oular active learning algorithm, uncertainty samling. Emirically, on 21 datasets from OenML, we find a strong inverse correlation between data efficiency and the error rate of the final classifier. Theoretically, we show that for a variant of uncertainty samling, the asymtotic data efficiency is within a constant factor of the inverse error rate of the limiting classifier.
"
128,2018,Selecting Representative Examples for Program Synthesis,Oral/Poster,"Program synthesis is a class of regression roblems where one seeks a solution, in the form of a source-code rogram, maing the inuts to their corresonding oututs exactly. Due to its recise and combinatorial nature, rogram synthesis is commonly formulated as a constraint satisfaction roblem, where inut-outut examles are encoded as constraints and solved with a constraint solver. A key challenge of this formulation is scalability: while constraint solvers work well with a few well-chosen examles, a large set of examles can incur significant overhead in both time and memory. We describe a method to discover a subset of examles that is both small and reresentative: the subset is constructed iteratively, using a neural network to redict the robability of unchosen examles conditioned on the chosen examles in the subset, and greedily adding the least robable examle. We emirically evaluate the reresentativeness of the subsets constructed by our method, and demonstrate such subsets can significantly imrove synthesis time and stability.
"
129,2018,Conditional Neural Processes,Oral/Poster,"Dee neural networks excel at function aroximation, yet they are tyically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), exloit rior knowledge to quickly infer the shae of a new function at test time. Yet, GPs are comutationally exensive, and it can be hard to design aroriate riors. In this aer we roose a family of neural models, Conditional Neural Processes (CNPs), that combine the benefits of both. CNPs are insired by the flexibility of stochastic rocesses such as GPs, but are structured as neural networks and trained via gradient descent. CNPs make accurate redictions after observing only a handful of training data oints, yet scale to comlex functions and large datasets. We demonstrate the erformance and versatility of the aroach on a range of canonical machine learning tasks, including regression, classification and image comletion.
"
130,2018,Hierarchical Long-term Video Prediction without Supervision,Oral/Poster,"Much of recent research has been devoted to video rediction and generation, yet most of the revious works have demonstrated only limited success in generating videos on short-term horizons. The hierarchical video rediction method by Villegas et al. (2017) is an examle of a state-of-the-art method for long-term video rediction, but their method is limited because it requires ground truth annotation of high-level structures (e.g., human joint landmarks) at training time. Our network encodes the inut frame, redicts a high-level encoding into the future, and then a decoder with access to the first frame roduces the redicted image from the redicted encoding. The decoder also roduces a mask that outlines the redicted foreground object (e.g., erson) as a by-roduct. Unlike Villegas et al. (2017), we develo a novel training method that jointly trains the encoder, the redictor, and the decoder together without highlevel suervision; we further imrove uon this by using an adversarial loss in the feature sace to train the redictor. Our method can redict about 20 seconds into the future and rovides better results comared to Denton and Fergus (2018) and Finn et al. (2016) on the Human 3.6M dataset.
"
131,2018,Adversarial Risk and the Dangers of Evaluating Against Weak Attacks,Oral/Poster,"This aer investigates recently roosed aroaches for defending against adversarial examles and evaluating adversarial robustness. We motivate \emh{adversarial risk} as an objective for achieving models robust to worst-case inuts. We then frame commonly used attacks and evaluation metrics as defining a tractable surrogate objective to the true adversarial risk. This suggests that models may otimize this surrogate rather than the true adversarial risk. We formalize this notion as \textit{obscurity to an adversary}, and develo tools and heuristics for identifying obscured models and designing transarent models. We demonstrate that this is a significant roblem in ractice by reurosing gradient-free otimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently roosed defenses to near zero. Our hoe is that our formulations and results will hel researchers to develo more owerful defenses.
"
132,2018,A Classification-Based Study of Covariate Shift in GAN Distributions,Oral/Poster,"A basic, and still largely unanswered, question in the context of Generative Adversarial Networks (GANs) is whether they are truly able to cature all the fundamental characteristics of the distributions they are trained on. In articular, evaluating the diversity of GAN distributions is challenging and existing methods rovide only a artial understanding of this issue. In this aer, we develo quantitative and scalable tools for assessing the diversity of GAN distributions. Secifically, we take a classification-based ersective and view loss of diversity as a form of covariate shift introduced by GANs. We examine two secific forms of such shift:  mode collase and boundary distortion. In contrast to rior work, our methods need only minimal human suervision and can be readily alied  to state-of-the-art GANs on large, canonical datasets. Examining oular GANs using our tools indicates that these GANs have significant roblems in reroducing the more distributional roerties of their training dataset.
"
133,2018,Gated Path Planning Networks,Oral/Poster,"Value Iteration Networks (VINs) are effective differentiable ath lanning modules that can be used by agents to erform navigation while still maintaining end-to-end differentiability of the entire architecture. Desite their effectiveness, they suffer from several disadvantages including training instability, random seed sensitivity, and other otimization roblems. In this work, we reframe VINs as recurrent-convolutional networks which demonstrates that VINs coule recurrent convolutions with an unconventional max-ooling activation. From this ersective, we argue that standard gated recurrent udate equations could otentially alleviate the otimization issues laguing VIN. The resulting architecture, which we call the Gated Path Planning Network, is shown to emirically outerform VIN on a variety of metrics such as learning seed, hyerarameter sensitivity, iteration count, and even generalization. Furthermore, we show that this erformance ga is consistent across different maze transition tyes, maze sizes and even show success on a challenging 3D environment, where the lanner is only rovided with first-erson RGB images.
"
134,2018,Automatic Goal Generation for Reinforcement Learning Agents,Oral/Poster,"Reinforcement learning (RL) is a owerful technique to train an agent to erform a task; however, an agent that is trained using RL is only caable of achieving the single task that is secified via its reward function.   Such an aroach does not scale well to settings in which an agent needs to erform a diverse set of tasks, such as navigating to varying ositions in a room or moving objects to varying locations.  Instead, we roose a method that allows an agent to automatically discover the range of tasks that it is caable of erforming in its environment.  We use a generator network to roose tasks for the agent to try to accomlish, each task being secified as reaching a certain arametrized subset of the state-sace.  The generator network is otimized using adversarial training to roduce tasks that are always at the aroriate level of difficulty for the agent, thus automatically roducing a curriculum.  We show that, by using this framework, an agent can efficiently and automatically learn to erform a wide set of tasks without requiring any rior knowledge of its environment, even when only sarse rewards are available. Videos and code available at htts:sites.google.comviewgoalgeneration4rl.
"
135,2018,ADMM and Accelerated ADMM as Continuous Dynamical Systems,Oral/Poster,"Recently, there has been an increasing interest in using tools from dynamical systems to analyze the behavior of simle otimization algorithms such as gradient descent and accelerated variants. This aer strengthens such connections by deriving the differential equations that model the continuous limit of the sequence of iterates generated by the alternating direction method of multiliers, as well as an accelerated variant. We emloy the direct method of Lyaunov to analyze the stability of critical oints of the dynamical systems and to obtain associated convergence rates.
"
136,2018,Dissipativity Theory for Accelerating Stochastic Variance Reduction: A Unified Analysis of SVRG and Katyusha Using Semidefinite Programs,Oral/Poster,"Techniques for reducing the variance of gradient estimates used in stochastic rogramming algorithms for convex finite-sum roblems have received a great deal of attention in recent years. By leveraging dissiativity theory from control, we rovide a new ersective on two imortant variance-reduction algorithms: SVRG and its direct accelerated variant Katyusha. Our ersective rovides a hysically intuitive understanding of the behavior of SVRG-like methods via a rincile of energy conservation. The tools discussed here allow us to automate the convergence analysis of SVRG-like methods by caturing their essential roerties in small semidefinite rograms amenable to standard analysis and comutational techniques. Our aroach recovers existing convergence results for SVRG and Katyusha and generalizes the theory to alternative arameter choices. We also discuss how our aroach comlements the linear couling technique. Our combination of ersectives leads to a better understanding of accelerated variance-reduced stochastic methods for finite-sum roblems.
"
137,2018,Contextual Graph Markov Model: A Deep and Generative Approach to Graph Processing,Oral/Poster,"We introduce the Contextual Grah Markov Model, an aroach combining ideas from generative models and neural networks for the rocessing of grah data. It founds on a constructive methodology to build a dee architecture comrising layers of robabilistic models that learn to encode the structured information in an incremental fashion. Context is diffused in an efficient and scalable way across the grah vertexes and edges. The resulting grah encoding is used in combination with discriminative models to address structure classification benchmarks.
"
138,2018,Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic Geometry,Oral/Poster,"We are concerned with the discovery of hierarchical relationshis from large-scale unstructured similarity scores. For this urose, we study different models of hyerbolic sace and find that learning embeddings in the Lorentz model is substantially more efficient than in the Poincaré-ball model. We show that the roosed aroach allows us to learn high-quality embeddings of large taxonomies which yield imrovements over Poincaré embeddings, esecially in low dimensions. Lastly, we aly our model to discover hierarchies in two real-world datasets: we show that an embedding in hyerbolic sace can reveal imortant asects of a comany's organizational structure as well as reveal historical relationshis between language families.
"
139,2018,Fast Variance Reduction Method with Stochastic Batch Size,Oral/Poster,"In this aer we study a family of variance reduction methods with randomized batch size---at each ste, the algorithm first randomly chooses the batch size and then selects a batch of samles to conduct a variance-reduced stochastic udate. We give the linear converge rate for this framework for comosite functions, and show that the otimal strategy to achieve the best converge rate er data access is to always choose batch size equalling to 1, which is equivalent to the SAGA algorithm. However, due to the resence of cachedisk IO effect in comuter architecture, number of data access cannot reflect the running time because of 1) random memory access is much slower than sequential access, 2) when data is too big to fit into memory, disk seeking takes even longer time. After taking these into account, choosing batch size equals to 1 is no longer otimal, so we roose a new algorithm called SAGA++ and theoretically show how to calculate the otimal average batch size. Our algorithm outerforms SAGA and other existing batch and stochastic solvers on real datasets. In addition, we also conduct a recise analysis to comare different udate rules for variance reduction methods, showing that SAGA++ converges faster than SVRG in theory.
"
140,2018,Lyapunov Functions for First-Order Methods: Tight Automated Convergence Guarantees,Oral/Poster,"We resent a novel way of generating Lyaunov functions for roving linear convergence rates of first-order otimization methods. Our aroach rovably obtains the fastest linear convergence rate that can be verified by a quadratic Lyaunov function (with given states), and only relies on solving a small-sized semidefinite rogram. Our aroach combines the advantages of erformance estimation roblems (PEP, due to Drori and Teboulle (2014)) and integral quadratic constraints (IQC, due to Lessard et al. (2016)), and relies on convex interolation (due to Taylor et al. (2017c;b)).
"
141,2018,Nonparametric Regression with Comparisons: Escaping the Curse of Dimensionality with Ordinal Information,Oral/Poster,"In suervised learning, we leverage a labeled dataset to design methods for function estimation. In many ractical situations, we are able to obtain alternative feedback, ossibly at a low cost. A broad goal is to understand the usefulness of, and to design algorithms to exloit, this alternative feedback. We focus on a semi-suervised setting where we obtain additional ordinal (or comarison) information for otentially unlabeled samles. We consider ordinal feedback of varying qualities where we have either a erfect ordering of the samles, a noisy ordering of the samles or noisy airwise comarisons between the samles. We rovide a recise quantification of the usefulness of these tyes of ordinal feedback in non-arametric regression, showing that in many cases it is ossible to accurately estimate an underlying function with a very small labeled set, effectively escaing the curse of dimensionality. We develo an algorithm called Ranking-Regression (RR) and analyze its accuracy as a function of size of the labeled and unlabeled datasets and various noise arameters. We also resent lower bounds, that establish fundamental limits for the task and show that RR is otimal in a variety of settings. Finally, we resent exeriments that show the efficacy of RR and investigate its robustness to various sources of noise and model-missecification.
"
142,2018,The Well-Tempered Lasso,Oral/Poster,"We study the comlexity of the entire regularization ath for least squares regression with 1-norm enalty, known as the Lasso. Every regression arameter in the Lasso changes linearly as a function of the regularization value. The number of changes is regarded as the Lasso's comlexity. Exerimental results using exact ath following exhibit olynomial comlexity of the Lasso in the roblem size. Alas, the ath comlexity of the Lasso on artificially designed regression roblems is exonential  We use smoothed analysis as a mechanism for bridging the ga between worst case settings and the de facto low comlexity. Our analysis assumes that the observed data has a tiny amount of intrinsic noise. We then rove that the Lasso's comlexity is olynomial in the roblem size.
"
143,2018,Transfer Learning via Learning to Transfer,Oral/Poster,"In transfer learning, what and how to transfer are two rimary issues to be addressed, as different transfer learning algorithms alied between a source and a target domain result in different knowledge transferred and thereby the erformance imrovement in the target domain. Determining the otimal one that maximizes the erformance imrovement requires either exhaustive exloration or considerable exertise. Meanwhile, it is widely acceted in educational sychology that human beings imrove transfer learning skills of deciding what to transfer through meta-cognitive reflection on inductive transfer learning ractices. Motivated by this, we roose a novel transfer learning framework known as Learning to Transfer (L2T) to automatically determine what and how to transfer are the best by leveraging revious transfer learning exeriences. We establish the L2T framework in two stages: 1) we learn a reflection function encryting transfer learning skills from exeriences; and 2) we infer what and how to transfer are the best for a future air of domains by otimizing the reflection function. We also theoretically analyse the algorithmic stability and generalization bound of L2T, and emirically demonstrate its sueriority over several state-of-the-art transfer learning algorithms.
"
144,2018,Pseudo-task Augmentation: From Deep Multitask Learning to Intratask Sharing—and Back,Oral/Poster,"Dee multitask learning boosts erformance by sharing learned structure across related tasks. This aer adats ideas from dee multitask learning to the setting where only a single task is available. The method is formalized as seudo-task augmentation, in which models are trained with multile decoders for each task. Pseudo-tasks simulate the effect of training towards closely-related tasks drawn from the same universe. In a suite of exeriments, seudo-task augmentation is shown to imrove erformance on single-task learning roblems. When combined with multitask learning, further imrovements are achieved, including state-of-the-art erformance on the CelebA dataset, showing that seudo-task augmentation and multitask learning have comlementary value. All in all, seudo-task augmentation is a broadly alicable and efficient way to boost erformance in dee learning systems.
"
145,2018,Analysis of Minimax Error Rate for Crowdsourcing and Its Application to Worker Clustering Model,Oral/Poster,"While crowdsourcing has become an imortant means to label data, there is great interest in estimating the ground truth from unreliable labels roduced by crowdworkers.  The Dawid and Skene (DS) model is one of the most well-known models in the study of crowdsourcing. Desite its ractical oularity, theoretical error analysis for the DS model has been conducted only under restrictive assumtions on class riors, confusion matrices, or the number of labels each worker rovides. In this aer, we derive a minimax error rate under more ractical setting for a broader class of crowdsourcing models including the DS model as a secial case. We further roose the worker clustering model, which is more ractical than the DS model under real crowdsourcing settings. The wide alicability of our theoretical analysis allows us to immediately investigate the behavior of this roosed model, which can not be analyzed by existing studies. Exerimental results showed that there is a strong similarity between the lower bound of the minimax error rate derived by our theoretical analysis and the emirical error of the estimated value.
"
146,2018,Deep One-Class Classification,Oral/Poster,"Desite the great advances made by dee learning in many machine learning roblems, there is a  relative dearth of dee learning aroaches for anomaly detection. Those aroaches which do exist involve networks trained to erform a task other than anomaly detection, namely generative models or comression, which are in turn adated for use in anomaly detection; they are not trained on an anomaly detection based objective. In this aer we introduce a new anomaly detection method---Dee Suort Vector Data Descrition---, which is trained on an anomaly detection based objective. The adatation to the dee regime necessitates that our neural network and training rocedure satisfy certain roerties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examles of GTSRB sto signs.
"
147,2018,Binary Partitions with Approximate  Minimum Impurity,Oral/Poster,"The roblem of slitting attributes is one of the main stes in the construction of decision trees. In order to decide the best slit,  imurity measures such as Entroy and Gini are widely used.  In ractice, decision-tree inducers use heuristics for finding slits with small imurity when they consider nominal attributes with a large number of distinct values. However, there are no known guarantees for the quality of the slits obtained by these heuristics. To fill this ga, we roose two new slitting rocedures that rovably achieve near-otimal imurity. We also reort exeriments that rovide  evidence that the roosed methods are interesting candidates  to be emloyed in slitting nominal attributes with many values during decision treerandom forest induction.
"
148,2018,Beyond 1/2-Approximation for Submodular Maximization on Massive Data Streams,Oral/Poster,"Many tasks in machine learning and data mining, such as data diversification, non-arametric learning, kernel machines, clustering etc., require extracting a small but reresentative summary from a massive dataset. Often, such roblems can be osed as maximizing a submodular set function subject to a cardinality constraint. We consider this question in the streaming setting, where elements arrive over time at a fast ace and thus we need to design an efficient, low-memory algorithm. One such method, roosed by Badanidiyuru et al. (2014), always finds a 0.5-aroximate solution. Can this aroximation factor be imroved? We answer this question affirmatively by designing a new algorithm Salsa for streaming submodular maximization. It is the first low-memory, singleass algorithm that imroves the factor 0.5, under the natural assumtion that elements arrive in a random order. We also show that this assumtion is necessary, i.e., that there is no such algorithm with better than 0.5-aroximation when elements arrive in arbitrary order. Our exeriments demonstrate that Salsa significantly outerforms the state of the art in alications related to exemlar-based clustering, social grah analysis, and recommender systems.
"
149,2018,"Yes, but Did It Work?: Evaluating Variational Inference",Oral/Poster,"While it's always ossible to comute a variational aroximation to a osterior distribution, it can be difficult to discover roblems with this aroximation"". We roose two  diagnostic algorithms to alleviate this roblem. The  Pareto-smoothed imortance samling (PSIS) diagnostic  gives a goodness of fit measurement for joint distributions, while simultaneously imroving the error in the estimate. The variational   simulation-based calibration (VSBC) assesses the average erformance  of oint estimates.
"
150,2018,Black-Box Variational Inference for Stochastic Differential Equations,Oral/Poster,"Parameter inference for stochastic differential equations is challenging due to the resence of a latent diffusion rocess. Working with an Euler-Maruyama discretisation for the diffusion, we use variational inference to jointly learn the arameters and the diffusion aths. We use a standard mean-field variational aroximation of the arameter osterior, and introduce a recurrent neural network to aroximate the osterior for the diffusion aths conditional on the arameters. This neural network learns how to rovide Gaussian state transitions which bridge between observations in a very similar way to the conditioned diffusion rocess. The resulting black-box inference method can be alied to any SDE system with light tuning requirements. We illustrate the method on a Lotka-Volterra system and an eidemic model, roducing accurate arameter estimates in a few hours.
"
151,2018,Online Convolutional Sparse Coding with Sample-Dependent Dictionary,Oral/Poster,"Convolutional sarse coding (CSC) has been oularly used for the learning of shift-invariant dictionaries in image and signal rocessing. However, existing methods have limited scalability. In this aer, instead of convolving with a dictionary shared by all samles, we roose the use of a samle-deendent dictionary in which each filter is a linear combination of a small set of base filters learned from data. This added flexibility allows a large number of samle-deendent atterns to be catured, which is esecially useful in the handling of large or high-dimensional data sets. Comutationally, the resultant model can be efficiently learned by online learning. Extensive exerimental results on a number of data sets show that the roosed method outerforms existing CSC algorithms with significantly reduced time and sace comlexities.
"
152,2018,Learning to Speed Up Structured Output Prediction,Oral/Poster,"Predicting structured oututs can be comutationally onerous due to the combinatorially large outut saces. In this aer, we focus on reducing the rediction time of a trained black-box structured classifier without losing accuracy. To do so, we train a seedu classifier that learns to mimic a black-box classifier under the learning-to-search aroach. As the structured classifier redicts more examles, the seedu classifier will oerate as a learned heuristic to guide search to favorable regions of the outut sace. We resent a mistake bound for the seedu classifier and identify inference situations where it can indeendently make correct judgments without inut features. We evaluate our method on the task of entity and relation extraction and show that the seedu classifier outerforms even greedy search in terms of seed without loss of accuracy.
"
153,2018,Differentially Private Identity and Equivalence Testing of Discrete Distributions,Oral/Poster,"We study the fundamental roblems of identity and equivalence testing over a discrete oulation from random samles.  Our goal is to develo efficient testers while guaranteeing differential rivacy to the individuals of the oulation.  We rovide  samle-efficient differentially rivate testers for these roblems. Our theoretical results significantly imrove over the best known  algorithms for identity testing, and are the first results  for rivate equivalence testing.  The concetual message of our work is that there exist rivate hyothesis testers that are nearly as samle-efficient as their non-rivate counterarts.  We erform an exerimental evaluation of our algorithms  on synthetic data. Our exeriments illustrate that our rivate testers achieve small tye \rom{1}  and tye \rom{2} errors with samle size {\em sublinear} in the domain size of the underlying distributions.
"
154,2018,Information Theoretic Guarantees for Empirical Risk Minimization with Applications to Model Selection and Large-Scale Optimization,Oral/Poster,"In this aer, we derive bounds on the mutual information of the emirical risk minimization (ERM) rocedure for both 0-1 and strongly-convex loss classes. We rove that under the Axiom of Choice, the existence of an ERM learning rule with a vanishing mutual information is equivalent to the assertion that the loss class has a finite VC dimension, thus bridging  information theory with statistical learning theory. Similarly, an asymtotic bound on the mutual information is established for strongly-convex loss classes in terms of the number of model arameters. The latter result rests on a central limit theorem (CLT) that we derive in this aer. In addition, we use our results to analyze the excess risk in stochastic convex otimization and unify revious works. Finally, we resent two imortant alications. First, we show that the ERM  of strongly-convex loss classes can be trivially scaled to big data using a naive arallelization algorithm with rovable guarantees. Second, we roose a simle information criterion for model selection and demonstrate exerimentally that it outerforms the oular Akaike's information criterion (AIC) and Schwarz's Bayesian information criterion (BIC).
"
155,2018,BOCK : Bayesian Optimization with Cylindrical Kernels,Oral/Poster,"A major challenge in Bayesian Otimization is the boundary issue where an algorithm sends too many evaluations near the boundary of its search sace. In this aer, we roose BOCK, Bayesian Otimization with Cylindrical Kernels, whose basic idea is to transform the ball geometry of the search sace using a cylindrical transformation. Because of the transformed geometry, the Gaussian Process-based surrogate model sends less budget searching near the boundary, while concentrating its efforts relatively more near the center of the search region, where we exect the solution to be located. We evaluate BOCK extensively, showing that it is not only more accurate and efficient, but it also scales successfully to roblems with a dimensionality as high as 500. We show that the better accuracy and scalability of BOCK even allows otimizing modestly sized neural network layers, as well as neural network hyerarameters.
"
156,2018,BOHB: Robust and Efficient Hyperparameter Optimization at Scale,Oral/Poster,"Modern dee learning methods are very sensitive to many hyerarameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyerarameter otimization is tyically comutationally infeasible. On the other hand, bandit-based configuration evaluation aroaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we roose to combine the benefits of both Bayesian otimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime erformance and fast convergence to otimal configurations. We roose a new ractical state-of-the-art hyerarameter otimization method, which consistently outerforms both Bayesian otimization and Hyerband on a wide range of roblem tyes, including high-dimensional toy functions, suort vector machines, feed-forward neural networks, Bayesian neural networks, dee reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being concetually simle and easy to imlement.
"
157,2018,Distributed Nonparametric Regression under Communication Constraints,Oral/Poster,"This aer studies the roblem of nonarametric estimation of a smooth function with data distributed across multile machines.  We assume an indeendent samle from a white noise model is collected at each machine, and an estimator of the underlying true function needs to be constructed at a central machine.  We lace limits on the number of bits that each machine can use to transmit information to the central machine.  Our results give both asymtotic lower bounds and matching uer bounds on the statistical risk under various settings. We identify three regimes, deending on the relationshi among the number of machines, the size of data available at each machine, and the communication budget.  When the communication budget is small, the statistical risk deends solely on this communication bottleneck, regardless of the samle size. In the regime where the communication budget is large, the classic minimax risk in the non-distributed estimation setting is recovered.  In an intermediate regime, the statistical risk deends on both the samle size and the communication budget.
"
158,2018,Optimal Tuning for Divide-and-conquer Kernel Ridge Regression with Massive Data,Oral/Poster,"Divide-and-conquer is a owerful aroach for large and massive data analysis.  In the nonarameteric regression setting, although various theoretical frameworks have been established to achieve otimality in estimation or hyothesis testing, how to choose the tuning arameter in a ractically effective way is still an oen roblem. In this aer, we roose a data-driven rocedure based on divide-and-conquer for selecting the tuning arameters in kernel ridge regression by modifying the oular Generalized Cross-validation (GCV, Wahba, 1990). While the roosed criterion is comutationally scalable for massive data sets, it is also shown under mild conditions to be asymtotically otimal in the sense that minimizing the roosed distributed-GCV (dGCV) criterion is equivalent to minimizing the true global conditional emirical loss of the averaged function estimator, extending the existing otimality results of GCV to the divide-and-conquer framework.
"
159,2018,WHInter: A Working set algorithm for High-dimensional sparse second order Interaction models,Oral/Poster,"Learning sarse linear models with two-way interactions is desirable in many alication domains such as genomics. $\ell_1$-regularised linear models are oular to estimate sarse models, yet standard imlementations fail to address secifically the quadratic exlosion of candidate two-way interactions in high dimensions, and tyically do not scale to genetic data with hundreds of thousands of features. Here we resent WHInter, a working set algorithm to solve large $\ell_1$-regularised roblems with two-way interactions for binary design matrices. The novelty of WHInter stems from a new bound to efficiently identify working sets while avoiding to scan all features, and on fast comutations insired from solutions to the maximum inner roduct search roblem. We aly WHInter to simulated and real genetic data and show that it is more scalable and two orders of magnitude faster than the state of the art."
160,2018,Safe Element Screening for Submodular Function Minimization,Oral/Poster,"Submodular functions are discrete analogs of convex functions, which have alications in various fields, including machine learning and comuter vision. However, in large-scale alications, solving Submodular Function Minimization (SFM) roblems remains challenging. In this aer, we make the first attemt to extend the emerging technique named screening in large-scale sarse learning to SFM for accelerating its otimization rocess. We first conduct a careful studying of the relationshis between SFM and the corresonding convex roximal roblems, as well as the accurate rimal otimum estimation of the roximal roblems. Relying on this study, we subsequently roose a novel safe screening method to quickly identify the elements guaranteed to be included (we refer to them as active) or excluded (inactive) in the final otimal solution of SFM during the otimization rocess. By removing the inactive elements and fixing the active ones, the roblem size can be dramatically reduced, leading to great savings in the comutational cost without sacrificing any accuracy. To the best of our knowledge, the roosed method is the first screening method in the fields of SFM and even combinatorial otimization, thus ointing out a new direction for accelerating SFM algorithms. Exeriment results on both synthetic and real datasets demonstrate the significant seedus gained by our aroach.
"
161,2018,Feedback-Based Tree Search for Reinforcement Learning,Oral/Poster,"Insired by recent successes of Monte-Carlo tree search (MCTS) in a number of artificial intelligence (AI) alication domains, we roose a reinforcement learning (RL) technique that iteratively alies MCTS on batches of small, finite-horizon versions of the original infinite-horizon Markov decision rocess. The terminal condition of the finite-horizon roblems, or the leaf-node evaluator of the decision tree generated by MCTS, is secified using a combination of an estimated value function and an estimated olicy function. The recommendations generated by the MCTS rocedure are then rovided as feedback in order to refine, through classification and regression, the leaf-node evaluator for the next iteration. We rovide the first samle comlexity bounds for a tree search-based RL algorithm. In addition, we show that a dee neural network imlementation of the technique can create a cometitive AI agent for the oular multi-layer online battle arena (MOBA) game King of Glory.
"
162,2018,Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement,Oral/Poster,"The ability to transfer skills across tasks has the otential to scale u reinforcement learning (RL) agents to environments currently out of reach. Recently, a framework based on two ideas, successor features (SFs) and generalised olicy imrovement (GPI), has been introduced as a rinciled way of transferring skills. In this aer we extend the SF&am;GPI framework in two ways. One of the basic assumtions underlying the original formulation of SF&am;GPI is that rewards for all tasks of interest can be comuted as linear combinations of a fixed set of features. We relax this constraint and show that the theoretical guarantees suorting the framework can be extended to any set of tasks that only differ in the reward function. Our second contribution is to show that one can use the reward functions themselves as features for future tasks, without any loss of exressiveness, thus removing the need to secify a set of features beforehand. This makes it ossible to combine SF&am;GPI with dee learning in a more stable way. We emirically verify this claim on a comlex 3D environment where observations are images from a first-erson ersective. We show that the transfer romoted by SF&am;GPI leads to very good olicies on unseen tasks almost instantaneously. We also describe how to learn olicies secialised to the new tasks in a way that allows them to be added to the agent's set of skills, and thus be reused in the future.
"
163,2018,Data-Dependent Stability of Stochastic Gradient Descent,Oral/Poster,"We establish a data-deendent notion of algorithmic stability for Stochastic Gradient Descent (SGD), and emloy it to develo novel generalization bounds. This is in contrast to revious distribution-free algorithmic stability results for SGD which deend on the worst-case constants. By virtue of the data-deendent argument, our bounds rovide new insights into learning with SGD on convex and non-convex roblems. In the convex case, we show that the bound on the generalization error deends on the risk at the initialization oint. In the non-convex case, we rove that the exected curvature of the objective function around the initialization oint has crucial influence on the generalization error. In both cases, our results suggest a simle data-driven strategy to stabilize SGD by re-screening its initialization. As a corollary, our results allow us to show otimistic generalization bounds that exhibit fast convergence rates for SGD subject to a vanishing emirical risk and low noise of stochastic gradient.
"
164,2018,LeapsAndBounds: A Method for Approximately Optimal Algorithm Configuration,Oral/Poster,"We consider the roblem of configuring general-urose solvers to run efficiently on roblem instances drawn from an unknown distribution. The goal of the configurator is to find a configuration that runs fast on average on most instances, and do so with the least amount of total work. It can run a chosen solver on a random instance until the solver finishes or a timeout is reached. We roose LeasAndBounds, an algorithm that tests configurations on randomly selected roblem instances for longer and longer time. We rove that the caed exected runtime of the configuration returned by LeasAndBounds is close to the otimal exected runtime, while our algorithm’s running time is near-otimal. Our results show that LeasAndBounds is more efficient than the recent algorithm of Kleinberg et al. (2017), which, to our knowledge, is the only other algorithm configuration method with non-trivial theoretical guarantees. Exerimental results on configuring a ublic SAT solver on a new benchmark dataset also stand witness to the sueriority of our method.
"
165,2018,Scalable Deletion-Robust Submodular Maximization: Data Summarization with Privacy and Fairness Constraints,Oral/Poster,"Can we efficiently extract useful information from a large user-generated dataset while rotecting the rivacy of the users andor ensuring fairness in reresentation? We cast this roblem as an instance of a deletion-robust submodular maximization where art of the data may be deleted or masked due to rivacy concerns or fairness criteria. We roose the first memory-efficient centralized, streaming, and distributed methods with constant-factor aroximation guarantees against \textit{any} number of adversarial deletions. We extensively evaluate the erformance of our algorithms on real-world alications, including (i) Uber-ick u locations with location rivacy constraints;  (ii) feature selection with fairness constraints for income rediction and crime rate rediction; and (iii) robust to deletion summarization of census data, consisting of 2,458,285 feature vectors. Our exeriments show that our solution is robust against even $80\%$ of data deletion."
166,2018,Covariate Adjusted Precision Matrix Estimation via Nonconvex Optimization,Oral/Poster,"We roose a nonconvex estimator for the covariate adjusted recision matrix estimation roblem in the high dimensional regime, under sarsity constraints. To solve this estimator, we roose an alternating gradient descent algorithm with hard thresholding. Comared with existing methods along this line of research, which lack theoretical guarantees in otimization error andor statistical error, the roosed algorithm not only is comutationally much more efficient with a linear rate of convergence, but also attains the otimal statistical rate u to a logarithmic factor. Thorough exeriments on both synthetic and real data suort our theory.
"
167,2018,Comparing Dynamics: Deep Neural Networks versus Glassy Systems,Oral/Poster,"We analyze numerically the training dynamics of dee neural networks (DNN) by using methods develoed in statistical hysics of glassy systems. The two main issues we address are the comlexity of the loss-landscae and of the dynamics within it, and to what extent DNNs share similarities with glassy systems. Our findings, obtained for different architectures and data-sets, suggest that during the training rocess the dynamics slows down because of an increasingly large number of flat directions. At large times, when the loss is aroaching zero, the system diffuses at the bottom of the landscae. Desite some similarities with the dynamics of mean-field glassy systems, in articular, the absence of barrier crossing, we find distinctive dynamical behaviors in the two cases, thus showing that the statistical roerties of the corresonding loss and energy landscaes are different. In contrast, when the network is under-arametrized we observe a tyical glassy behavior, thus suggesting the existence of different hases deending on whether the network is under-arametrized or over-arametrized.
"
168,2018,An Optimal Control Approach to Deep Learning and Applications to Discrete-Weight Neural Networks,Oral/Poster,"Dee learning is formulated as a discrete-time otimal control roblem. This allows one to characterize necessary conditions for otimality and develo training algorithms that do not rely on gradients with resect to the trainable arameters. In articular, we introduce the discrete-time method of successive aroximations (MSA), which is based on the Pontryagin's maximum rincile, for training neural networks. A rigorous error estimate for the discrete MSA is obtained, which sheds light on its dynamics and the means to stabilize the algorithm. The develoed methods are alied to train, in a rather rinciled way, neural networks with weights that are constrained to take values in a discrete set. We obtain cometitive erformance and interestingly, very sarse weights in the case of ternary networks, which may be useful in model deloyment in low-memory devices.
"
169,2018,Not All Samples Are Created Equal: Deep Learning with Importance Sampling,Oral/Poster,"Dee Neural Network training sends most of the comutation on examles that are roerly handled, and could be ignored. We roose to mitigate this henomenon with a rinciled imortance samling scheme that focuses comutation on ""informative"" examles, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: first, we derive a tractable uer bound to the er-samle gradient norm, and second we derive an estimator of the variance reduction achieved with imortance samling, which enables us to switch it on when it will result in an actual seedu. The resulting scheme can be used by changing a few lines of code in a standard SGD rocedure, and we demonstrate exerimentally on image classification, CNN fine-tuning, and RNN training, that for a fixed wall-clock time budget, it rovides a reduction of the train losses of u to an order of magnitude and a relative imrovement of test errors between 5% and 17%.
"
170,2018,"Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks",Oral/Poster,"In recent years, state-of-the-art methods in comuter vision have utilized increasingly dee convolutional neural network architectures (CNNs), with some of the most successful models emloying hundreds or even thousands of layers. A variety of athologies such as vanishingexloding gradients make training such dee networks challenging. While residual connections and batch normalization do enable training at these deths, it has remained unclear whether such secialized architecture designs are truly necessary to train dee CNNs. In this work, we demonstrate that it is ossible to train vanilla CNNs with ten thousand layers or more simly by using an aroriate initialization scheme. We derive this initialization scheme theoretically by develoing a mean field theory for signal roagation and by characterizing the conditions for dynamical isometry, the equilibration of singular values of the inut-outut Jacobian matrix. These conditions require that the convolution oerator be an orthogonal transformation in the sense that it is norm-reserving. We resent an algorithm for generating such random initial orthogonal convolution kernels and demonstrate emirically that they enable efficient training of extremely dee architectures.
"
171,2018,Path Consistency Learning in Tsallis Entropy Regularized MDPs,Oral/Poster,"We study the sarse entroy-regularized reinforcement learning (ERL) roblem in which the entroy term is a secial form of the Tsallis entroy. The otimal olicy of this formulation is sarse, i.e., at each state, it has non-zero robability for only a small number of actions. This addresses the main drawback of the standard Shannon entroy-regularized RL (soft ERL) formulation, in which the otimal olicy is softmax, and thus, may assign a non-negligible robability mass to non-otimal actions. This roblem is aggravated as the number of actions is increased. In this aer, we follow the work of Nachum et al. (2017) in the soft ERL setting, and roose a class of novel ath consistency learning (PCL) algorithms, called sarse PCL, for the sarse ERL roblem that can work with both on-olicy and off-olicy data. We first derive a sarse consistency equation that secifies a relationshi between the otimal value function and olicy of the sarse ERL along any system trajectory. Crucially, a weak form of the converse is also true, and we quantify the sub-otimality of a olicy which satisfies sarse consistency, and show that as we increase the number of actions, this sub-otimality is better than that of the soft ERL otimal olicy. We then use this result to derive the sarse PCL algorithms. We emirically comare sarse PCL with its soft counterart, and show its advantage, esecially in roblems with a large number of actions.
"
172,2018,Lipschitz Continuity in Model-based Reinforcement Learning,Oral/Poster,"We examine the imact of learning Lischitz continuous models in the context of model-based reinforcement learning. We rovide a novel bound on multi-ste rediction error of Lischitz models where we quantify the error using the Wasserstein metric. We go on to rove an error bound for the value-function estimate arising from Lischitz models and show that the estimated value function is itself Lischitz. We conclude with emirical results that show the benefits of controlling the Lischitz constant of neural-network models.
"
173,2018,Bounds on the Approximation Power of Feedforward Neural Networks,Oral/Poster,"The aroximation ower of general feedforward neural networks with iecewise linear activation functions is investigated. First, lower bounds on the size of a network are established in terms of the aroximation error and network deth and width. These bounds imrove uon state-of-the-art bounds for certain classes of functions, such as strongly convex functions. Second, an uer bound is established on the difference of two neural networks with identical weights but different activation functions.
"
174,2018,Linear Spectral Estimators and an Application to Phase Retrieval,Oral/Poster,"Phase retrieval refers to the roblem of recovering real-  or comlex-valued vectors from magnitude measurements. The best-known algorithms for this roblem are iterative in nature and rely on so-called sectral initializers that rovide accurate initialization vectors. We roose a novel class of estimators suitable for general nonlinear measurement systems, called linear sectral estimators (LSPEs), which can be used to comute accurate initialization vectors for hase retrieval roblems. The roosed LSPEs not only rovide accurate initialization vectors for noisy hase retrieval systems with structured or random measurement matrices, but also enable the derivation of shar and nonasymtotic mean-squared error bounds. We demonstrate the efficacy of LSPEs on synthetic and real-world hase retrieval roblems, and we show that our estimators  significantly outerform existing methods for  structured measurement systems that arise in ractice.
"
175,2018,Testing Sparsity over Known and Unknown Bases,Oral/Poster,"Sarsity is a basic roerty of real vectors that is exloited in a wide variety of ma- chine learning alications. In this work, we describe roerty testing algorithms for sar- sity that observe a low-dimensional rojec- tion of the inut. We consider two settings. In the first setting, we test sarsity with re- sect to an unknown basis: given inut vec- tors $y_1 ,...,y_ \in R^d$ whose concatenation as columns forms $Y \in R^{d \times }$ , does $Y = AX$ for matrices $A \in R^{d\times m}$ and $X \in R^{m \times }$ such that each column of $X$ is $k$-sarse, or is $Y$ “far” from having such a decomosition? In the second setting, we test sarsity with re- sect to a known basis: for a fixed design ma- trix $A \in R^{d \times m}$ , given inut vector $y \in R^d$ , is $y = Ax$ for some $k$-sarse vector $x$ or is $y$ “far” from having such a decomosition? We analyze our algorithms using tools from high-dimensional geometry and robability."
176,2018,Inference Suboptimality in Variational Autoencoders,Oral/Poster,"Amortized inference allows latent-variable models trained via variational learning to scale to large datasets. The quality of aroximate inference is determined by two factors: a) the caacity of the variational distribution to match the true osterior and b) the ability of the recognition network to roduce good variational arameters for each dataoint. We examine aroximate inference in variational autoencoders in terms of these factors. We find that divergence from the true osterior is often due to imerfect recognition networks, rather than the limited comlexity of the aroximating distribution. We show that this is due artly to the generator learning to accommodate the choice of aroximation. Furthermore, we show that the arameters used to increase the exressiveness of the aroximation lay a role in generalizing inference rather than simly imroving the comlexity of the aroximation.
"
177,2018,Semi-Implicit Variational Inference,Oral/Poster,"Semi-imlicit variational inference (SIVI) is introduced to exand the commonly used analytic variational distribution family, by mixing the variational arameter with a flexible distribution. This mixing distribution can assume any density function, exlicit or not, as long as indeendent random samles can be generated via rearameterization. Not only does SIVI exand the variational family to incororate highly flexible variational distributions, including imlicit ones that have no analytic density functions, but also sandwiches the evidence lower bound (ELBO) between a lower bound and an uer bound, and further derives an asymtotically exact surrogate ELBO that is amenable to otimization via stochastic gradient ascent. With a substantially exanded variational family and a novel otimization algorithm, SIVI is shown to closely match the accuracy of MCMC in inferring the osterior in a variety of Bayesian inference tasks.
"
178,2018,Variance Regularized Counterfactual Risk Minimization via Variational Divergence Minimization,Oral/Poster,"Off-olicy learning, the task of evaluating and imroving olicies using historic data collected from a logging olicy, is imortant because on-olicy evaluation is usually exensive and has adverse imacts. One of the major challenge of off-olicy learning is to derive counterfactual estimators that also has low variance and thus low generalization error. In this work, insired by learning bounds for imortance samling roblems, we resent a new counterfactual learning rincile for off-olicy learning with bandit feedbacks. Our method regularizes the generalization error by minimizing the distribution divergence between the logging olicy and the new olicy, and removes the need for iterating through all training samles to comute samle variance regularization in rior work. With neural network olicies, our end-to-end training algorithms using variational divergence minimization showed significant imrovement over conventional baseline algorithms and is also consistent with our theoretical results.
"
179,2018,Limits of Estimating Heterogeneous Treatment Effects: Guidelines for Practical Algorithm Design,Oral/Poster,"Estimating heterogeneous treatment effects from observational data is a central roblem in many domains. Because counterfactual data is inaccessible, the roblem differs fundamentally from suervised learning, and entails a more comlex set of modeling choices. Desite a variety of recently roosed algorithmic solutions, a rinciled guideline for building estimators of treatment effects using machine learning algorithms is still lacking. In this aer, we rovide such a guideline by characterizing the fundamental limits of estimating heterogeneous treatment effects, and establishing conditions under which these limits can be achieved. Our analysis reveals that the relative imortance of the different asects of observational data vary with the samle size. For instance, we show that selection bias matters only in small-samle regimes, whereas with a large samle size, the way an algorithm models the control and treated outcomes is what bottlenecks its erformance. Guided by our analysis, we build a ractical algorithm for estimating treatment effects using a non-stationary Gaussian rocesses with doubly-robust hyerarameters. Using a standard semi-synthetic simulation setu, we show that our algorithm outerforms the state-of-the-art, and that the behavior of existing algorithms conforms with our analysis.
"
180,2018,A Semantic Loss Function for Deep Learning with Symbolic Knowledge,Oral/Poster,"This aer develos a novel methodology for using symbolic knowledge in dee learning. From first rinciles, we derive a semantic loss function that bridges between neural outut vectors and logical constraints. This loss function catures how close the neural network is to satisfying the constraints on its outut. An exerimental evaluation shows that it effectively guides the learner to achieve (near-)state-of-the-art results on semi-suervised multi-class classification. Moreover, it significantly increases the ability of the neural network to redict structured objects, such as rankings and aths. These discrete concets are tremendously difficult to learn, and benefit from a tight integration of dee learning and symbolic reasoning methods.
"
181,2018,Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization,Oral/Poster,"Vanishing and exloding gradients are two of the main obstacles in training  dee neural networks, esecially in caturing long range deendencies in  recurrent neural networks (RNNs). In this aer, we resent an efficient  arametrization of the transition matrix of an RNN that allows us to stabilize  the gradients that arise in its training. Secifically, we arameterize the  transition matrix by its singular value decomosition (SVD), which allows us  to exlicitly track and control its singular values. We attain efficiency by  using tools that are common in numerical linear algebra, namely Householder  reflectors for reresenting the orthogonal matrices that arise in the SVD. By  exlicitly controlling the singular values, our roosed Sectral-RNN method allows  us to easily solve the exloding gradient roblem and we observe that it  emirically solves the vanishing gradient issue to a large extent. We note  that the SVD arameterization can be used for any rectangular weight matrix,  hence it can be easily extended to any dee neural network, such as a  multi-layer ercetron. Theoretically, we demonstrate that our  arameterization does not lose any exressive ower, and show how it  otentially makes the otimization rocess easier. Our extensive  exerimental  results also demonstrate that the roosed framework converges faster, and has  good generalization, esecially in caturing long range deendencies, as shown  on the synthetic addition and coy tasks, as well as on MNIST and Penn Tree Bank data sets.
"
182,2018,An Efficient Semismooth Newton based Algorithm for Convex Clustering,Oral/Poster,"Clustering is a fundamental roblem in unsuervised learning. Poular methods like K-means, may suffer from instability as they are rone to get stuck in its local minima. Recently, the sumof-norms (SON) model (also known as clustering ath), which is a convex relaxation of hierarchical clustering model, has been roosed in (Lindsten et al., 2011) and (Hocking et al., 2011). Although numerical algorithms like alternating direction method of multiliers (ADMM) and alternating minimization algorithm (AMA) have been roosed to solve convex clustering model (Chi &am; Lange, 2015), it is known to be very challenging to solve large-scale roblems. In this aer, we roose a semismooth Newton based augmented Lagrangian method for large-scale convex clustering roblems. Extensive numerical exeriments on both simulated and real data demonstrate that our algorithm is highly efficient and robust for solving large-scale roblems. Moreover, the numerical results also show the suerior erformance and scalability of our algorithm comaring to existing first-order methods.
"
183,2018,Lightweight Stochastic Optimization for Minimizing Finite Sums with Infinite Data,Oral/Poster,"Variance reduction has been commonly used in stochastic otimization. It relies crucially on the assumtion that the data set is finite. However, when the data are imuted with random noise as in data augmentation, the erturbed data set becomes essentially infinite. Recently, the stochastic MISO (S-MISO) algorithm is introduced to address this exected risk minimization roblem. Though it converges faster than SGD, a significant amount of memory is required. In this aer, we roose two SGD-like algorithms for exected risk minimization with random erturbation, namely, stochastic samle average gradient (SSAG) and stochastic SAGA (S-SAGA). The memory cost of SSAG does not deend on the samle size, while that of S-SAGA is the same as those of variance reduction methods on unerturbed data. Theoretical analysis and exerimental results on logistic regression and AUC maximization show that SSAG has faster convergence rate than SGD with comarable sace requirement while S-SAGA outerforms S-MISO in terms of both iteration comlexity and storage.
"
184,2018,Exploiting the Potential of Standard Convolutional Autoencoders for Image Restoration by Evolutionary Search,Oral/Poster,"Researchers have alied dee neural networks to image restoration tasks, in which they roosed various network architectures, loss functions, and training methods. In articular, adversarial training, which is emloyed in recent studies, seems to be a key ingredient to success. In this aer, we show that simle convolutional autoencoders (CAEs) built uon only standard network comonents, i.e., convolutional layers and ski connections, can outerform the state-of-the-art methods which emloy adversarial training and sohisticated loss functions. The secret is to search for good architectures using an evolutionary algorithm. All we did was to train the otimized CAEs by minimizing the l2 loss between reconstructed images and their ground truths using the ADAM otimizer. Our exerimental results show that this aroach achieves 27.8 dB eak signal to noise ratio (PSNR) on the CelebA dataset and 33.3 dB on the SVHN dataset, comared to 22.8 dB and 19.0 dB rovided by the former state-of-the-art methods, resectively.
"
185,2018,Efficient Neural Architecture Search via Parameters Sharing,Oral/Poster,"We roose Efficient Neural Architecture Search (ENAS), a fast and inexensive aroach for automatic model design. ENAS constructs a large comutational grah, where each subgrah reresents a neural network architecture, hence forcing all architectures to share their arameters. A controller is trained with olicy gradient to search for a subgrah that maximizes the exected reward on a validation set. Meanwhile a model corresonding to the selected subgrah is trained to minimize a canonical cross entroy loss. Sharing arameters among child models allows ENAS to deliver strong emirical erformances, whilst using much fewer GPU-hours than existing automatic model design aroaches, and notably, 1000x less exensive than standard Neural Architecture Search. On Penn Treebank, ENAS discovers a novel architecture that achieves a test erlexity of 56.3, on ar with the existing state-of-the-art among all methods without ost-training rocessing. On CIFAR-10, ENAS finds a novel architecture that achieves 2.89% test error, which is on ar with the 2.65% test error of NASNet (Zoh et al., 2018).
"
186,2018,Non-convex Conditional Gradient Sliding,Oral/Poster,"We investigate a rojection free otimization method, namely non-convex conditional gradient sliding (NCGS) for non-convex otimization roblems on the  batch, stochastic and finite-sum settings.  Conditional gradient sliding (CGS) method, by integrating Nesterov's accelerated gradient method with Frank-Wolfe  (FW) method in a smart way, outerforms  FW  for convex otimization, by reducing the amount of gradient comutations. However, the study of CGS in the non-convex setting is limited. In this aer, we roose the non-convex conditional gradient sliding (NCGS) methods and analyze their convergence roerties. We also leverage the idea of variance reduction from the recent rogress in convex otimization to obtain  a new algorithm termed  {\em variance reduced NCGS} (NCGS-VR), and obtain faster convergence rate than the batch NCGS in the finite-sum setting. We show that NCGS algorithms outerform their   Frank-Wolfe counterarts both in theory and in ractice, for all three settings, namely the batch, stochastic  and finite-sum setting. This significantly imroves our understanding of otimizing non-convex functions with comlicated feasible sets (where rojection is rohibitively exensive).
"
187,2018,Stochastic Variance-Reduced Cubic Regularized Newton Method,Oral/Poster,"We roose a stochastic variance-reduced cubic regularized Newton method (SVRC) for non-convex otimization. At the core of our algorithm is a novel semi-stochastic gradient along with a semi-stochastic Hessian, which are secifically designed for cubic regularization method. We show that our algorithm is guaranteed to converge to an $(\esilon,\sqrt{\esilon})$-aroximate local minimum within $\tilde{O}(n^{45}\esilon^{32})$ second-order oracle calls, which outerforms the state-of-the-art cubic regularization algorithms including subsamled cubic regularization. Our work also sheds light on the alication of variance reduction technique to high-order non-convex otimization methods. Thorough exeriments on various non-convex otimization roblems suort our theory."
188,2018,On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization,Oral/Poster,"Conventional wisdom in dee learning states that increasing deth imroves exressiveness but comlicates otimization. This aer suggests that, sometimes, increasing deth can seed u otimization. The effect of deth on otimization is decouled from exressiveness by focusing on settings where additional layers amount to overarameterization -- linear neural networks, a well-studied model. Theoretical analysis, as well as exeriments, show that here deth acts as a reconditioner which may accelerate convergence. Even on simle convex roblems such as linear regression with $\ell_$ loss, $2$, gradient descent can benefit from transitioning to a non-convex overarameterized objective, more than it would from some common acceleration schemes. We also rove that it is mathematically imossible to obtain the acceleration effect of overarametrization via gradients of any regularizer."
189,2018,The Dynamics of Learning: A Random Matrix Approach,Oral/Poster,"Understanding the learning dynamics of neural networks is one of the key issues for the imrovement of otimization algorithms as well as for the theoretical comrehension of why dee neural nets work so well today. In this aer, we introduce a random matrix-based framework to analyze the learning dynamics of a single-layer linear network on a binary classification roblem, for data of simultaneously large dimension and size, trained by gradient descent. Our results rovide rich insights into common questions in neural nets, such as overfitting, early stoing and the initialization of training, thereby oening the door for future studies of more elaborate structures and models aearing in today's neural networks.
"
190,2018,Learning K-way D-dimensional Discrete Codes for Compact Embedding Representations,Oral/Poster,"Conventional embedding methods directly associate each symbol with a continuous embedding vector, which is equivalent to alying a linear transformation based on a ``one-hot'' encoding of the discrete symbols. Desite its simlicity, such aroach yields the number of arameters that grows linearly with the vocabulary size and can lead to overfitting. In this work, we roose a much more comact K-way D-dimensional discrete encoding scheme to relace the ``one-hot"" encoding. In the roosed ``KD encoding'', each symbol is reresented by a $D$-dimensional code with a cardinality of $K$, and the final symbol embedding vector is generated by comosing the code embedding vectors. To end-to-end learn semantically meaningful codes, we derive a relaxed discrete otimization aroach based on stochastic gradient descent, which can be generally alied to any differentiable comutational grah with an embedding layer. In our exeriments with various alications from natural language rocessing to grah convolutional networks, the total size of the embedding layer can be reduced u to 98% while achieving similar or better erformance."
191,2018,Discovering Interpretable Representations for Both Deep Generative and Discriminative Models,Oral/Poster,"Interretability of reresentations in both dee generative and discriminative models is highly desirable. Current methods jointly otimize an objective combining accuracy and interretability. However, this may reduce accuracy, and is not alicable to already trained models. We roose two interretability frameworks. First, we rovide an interretable lens for an existing model. We use a generative model which takes as inut the reresentation in an existing (generative or discriminative) model, weakly suervised by limited side information. Alying a flexible and invertible transformation to the inut leads to an interretable reresentation with no loss in accuracy. We extend the aroach using an active learning strategy to choose the most useful side information to obtain, allowing a human to guide what ""interretable"" means. Our second framework relies on joint otimization for a reresentation which is both maximally informative about the side information and maximally comressive about the non-interretable data factors. This leads to a novel ersective on the relationshi between comression and regularization. We also roose a new interretability evaluation metric based on our framework. Emirically, we achieve state-of-the-art results on three datasets using the two roosed algorithms.
"
192,2018,Continuous-Time Flows for Efficient Inference and Density Estimation,Oral/Poster,"Two fundamental roblems in unsuervised learning are efficient inference for latent-variable models and robust density estimation based on large amounts of unlabeled data. Algorithms for the two tasks, such as normalizing flows and generative adversarial networks (GANs), are often develoed indeendently. In this aer, we roose the concet of {\em continuous-time flows} (CTFs), a family of diffusion-based methods that are able to asymtotically aroach a target distribution. Distinct from normalizing flows and GANs, CTFs can be adoted to achieve the above two goals in one framework, with theoretical guarantees. Our framework includes distilling knowledge from a CTF for efficient inference, and learning an exlicit  energy-based distribution with CTFs for density estimation. Both tasks rely on a new technique for distribution matching within amortized learning. Exeriments on various tasks demonstrate romising erformance of the roosed CTF framework, comared to related techniques.
"
193,2018,Tighter Variational Bounds are Not Necessarily Better,Oral/Poster,"We rovide theoretical and emirical evidence that using tighter evidence lower bounds (ELBOs) can be detrimental to the rocess of learning an inference network by reducing the signal-to-noise ratio of the gradient estimator. Our results call into question common imlicit assumtions that tighter ELBOs are better variational objectives for simultaneous model learning and inference amortization schemes. Based on our insights, we introduce three new algorithms: the artially imortance weighted auto-encoder (PIWAE), the multily imortance weighted auto-encoder (MIWAE), and the combination imortance weighted autoencoder (CIWAE), each of which includes the standard imortance weighted auto-encoder (IWAE) as a secial case. We show that each can deliver imrovements over IWAE, even when erformance is measured by the IWAE target itself. Furthermore, our results suggest that PIWAE may be able to deliver simultaneous imrovements in the training of both the inference and generative networks.
"
194,2018,PredRNN++: Towards A Resolution of the Deep-in-Time Dilemma in Spatiotemporal Predictive Learning,Oral/Poster,"We resent PredRNN++, a recurrent network for satiotemoral redictive learning. In ursuit of a great modeling caability for short-term video dynamics, we make our network deeer in time by leveraging a new recurrent structure named Causal LSTM with cascaded dual memories. To alleviate the gradient roagation difficulties in dee redictive models, we roose a Gradient Highway Unit, which rovides alternative quick routes for the gradient flows from oututs back to long-range revious inuts. The gradient highway units work seamlessly with the causal LSTMs, enabling our model to cature the short-term and the long-term video deendencies adatively. Our model achieves state-of-the-art rediction results on both synthetic and real video datasets, showing its ower in modeling entangled motions.
"
195,2018,RadialGAN: Leveraging multiple datasets to improve target-specific predictive models using Generative Adversarial Networks,Oral/Poster,"Training comlex machine learning models for rediction often requires a large amount of data that is not always readily available. Leveraging these external datasets from related but different sources is therefore an imortant task if good redictive models are to be built for deloyment in settings where data can be rare. In this aer we roose a novel aroach to the roblem in which we use multile GAN architectures to learn to translate from one dataset to another, thereby allowing us to effectively enlarge the target dataset, and therefore learn better redictive models than if we simly used the target dataset. We show the utility of such an aroach, demonstrating that our method imroves the rediction erformance on the target domain over using just the target dataset and also show that our framework outerforms several other benchmarks on a collection of real-world medical datasets.
"
196,2018,Differentiable Compositional Kernel Learning for Gaussian Processes,Oral/Poster,"The generalization roerties of Gaussian rocesses deend heavily on the choice of kernel, and this choice remains a dark art. We resent the Neural Kernel Network (NKN), a flexible family of kernels reresented by a neural network. The NKN’s architecture is based on the comosition rules for kernels, so that each unit of the network corresonds to a valid kernel. It can comactly aroximate comositional kernel structures such as those used by the Automatic Statistician (Lloyd et al., 2014), but because the architecture is differentiable, it is end-to-end trainable with gradient- based otimization. We show that the NKN is universal for the class of stationary kernels. Emirically we demonstrate NKN’s attern discovery and extraolation abilities on several tasks that deend crucially on identifying the underlying structure, including time series and texture extraolation, as well as Bayesian otimization.
"
197,2018,Markov Modulated Gaussian Cox Processes for Semi-Stationary Intensity Modeling of Events Data,Oral/Poster,"The Cox rocess is a flexible event model that can account for uncertainty of the intensity function in the Poisson rocess. However, revious aroaches make strong assumtions in terms of time stationarity, otentially failing to generalize when the data do not conform to the assumed stationarity conditions. In this aer we bring u two most oular Cox models reresenting two extremes, and roose a novel semi-stationary Cox rocess model that can take benefits from both models. Our model has a set of Gaussian rocess latent functions governed by a latent stationary Markov rocess where we rovide analytic derivations for the variational inference. Emirical evaluations on several synthetic and real-world events data including the football shot attemts and daily earthquakes, demonstrate that the roosed model is romising, can yield imroved generalization erformance over existing aroaches.
"
198,2018,Improved Regret Bounds for Thompson Sampling in Linear Quadratic Control Problems,Oral/Poster,"Thomson samling (\ts) is an effective aroach to trade off exloration and exloration in reinforcement learning. Desite its emirical success and recent advances, its theoretical analysis is often limited to the Bayesian setting, finite state-action saces, or finite-horizon roblems. In this aer, we study an instance of \ts in the challenging setting of the infinite-horizon linear quadratic (LQ) control, which models roblems with continuous state-action variables, linear dynamics, and quadratic cost. In articular, we analyze the regret in the frequentist sense (i.e., for a fixed unknown environment) in one-dimensional systems. We derive the first $O(\sqrt{T})$ frequentist regret bound for this roblem, thus significantly imroving the $O(T^{23})$ bound of~\citet{abeille2017thomson} and matching the frequentist erformance derived by~\citet{abbasi2011regret} for an otimistic aroach and the Bayesian result of~\citet{ouyang2017learning-based}.  We obtain this result by develoing a novel bound on the regret due to olicy switches, which holds for LQ systems of any dimensionality and it allows udating the arameters and the olicy at each ste, thus overcoming revious limitations due to lazy udates. Finally, we reort numerical simulations suorting the conjecture that our result extends to multi-dimensional systems."
199,2018,Design of Experiments for Model Discrimination Hybridising Analytical and Data-Driven Approaches,Oral/Poster,"Healthcare comanies must submit harmaceutical drugs or medical device to regulatory bodies before marketing new technology. Regulatory bodies frequently require transarent and interretable comutational modelling to justify a new healthcare technology, but researchers may have several cometing models for a biological system and too little data to discriminate between the models. In design of exeriments for model discrimination, where the goal is to design maximally informative hysical exeriments in order to discriminate between rival redictive models, research has focused either on analytical aroaches, which cannot manage all functions, or on data-driven aroaches, which may have comutational difficulties or lack interretable marginal redictive distributions. We develo a methodology for introducing Gaussian rocess surrogates in lieu of the original mechanistic models. This allows us to extend existing design and model discrimination methods develoed for analytical models to cases of non-analytical models.
"
200,2018,Anonymous Walk Embeddings,Oral/Poster,"The task of reresenting entire grahs has seen a surge of rominent results, mainly due to learning convolutional neural networks (CNNs) on grah-structured data. While CNNs demonstrate state-of-the-art erformance in grah classification task, such methods are suervised and therefore steer away from the original roblem of network reresentation in task-agnostic manner. Here, we coherently roose an aroach for embedding entire grahs and show that our feature reresentations with SVM classifier increase classification accuracy of CNN algorithms and traditional grah kernels. For this we describe a recently discovered grah object, \textit{anonymous walk}, on which we design task-indeendent algorithms for learning grah reresentations in exlicit and distributed way. Overall, our work reresents a new scalable unsuervised learning of state-of-the-art reresentations of entire grahs.
"
201,2018,Improving Optimization in Models With Continuous Symmetry Breaking,Oral/Poster,"Many loss functions in reresentation learning are invariant under a continuous symmetry transformation. For examle, the loss function of word embeddings (Mikolov et al., 2013) remains unchanged if we simultaneously rotate all word and context embedding vectors. We show that reresentation learning models for time series ossess an aroximate continuous symmetry that leads to slow convergence of gradient descent. We roose a new otimization algorithm that seeds u convergence using ideas from gauge theory in hysics. Our algorithm leads to orders of magnitude faster convergence and to more interretable reresentations, as we show for dynamic extensions of matrix factorization and word embedding models. We further resent an examle alication of our roosed algorithm that translates modern words into their historic equivalents.
"
202,2018,Conditional Noise-Contrastive Estimation of Unnormalised Models,Oral/Poster,"Many arametric statistical models are not roerly normalised and only secified u to an intractable artition function, which renders arameter estimation difficult. Examles of unnormalised models are Gibbs distributions, Markov random fields, and neural network models in unsuervised dee learning. In revious work, the estimation rincile called noise-contrastive estimation (NCE) was introduced where unnormalised models are estimated by learning to distinguish between data and auxiliary noise. An oen question is how to best choose the auxiliary noise distribution. We here roose a new method that addresses this issue. The roosed method shares with NCE the idea of formulating density estimation as a suervised learning roblem but in contrast to NCE, the roosed method leverages the observed data when generating noise samles. The noise can thus be generated in a semi-automated manner. We first resent the underlying theory of the new method, show that score matching emerges as a limiting case, validate the method on continuous and discrete valued synthetic data, and show that we can exect an imroved erformance comared to NCE when the data lie in a lower-dimensional manifold. Then we demonstrate its alicability in unsuervised dee learning by estimating a four-layer neural image model.
"
203,2018,Canonical Tensor Decomposition for Knowledge Base Completion,Oral/Poster,"The roblem of Knowledge Base Comletion can be framed as a 3rd-order binary tensor comletion roblem. In this light, the Canonical Tensor Decomosition (CP) seems like a natural solution; however, current imlementations of CP on standard Knowledge Base Comletion benchmarks are lagging behind their cometitors. In this work, we attemt to understand the limits of CP for knowledge base comletion. First, we motivate and test a novel regularizer, based on tensor nuclear -norms. Then, we resent a reformulation of the roblem that makes it invariant to arbitrary choices in the inclusion of redicates or their recirocals in the dataset. These two methods combined allow us to beat the current state of the art on several datasets with a CP decomosition, and obtain even better results using the more advanced ComlEx model.
"
204,2018,The Power of Interpolation:  Understanding the Effectiveness of SGD in Modern Over-parametrized Learning,Oral/Poster,"In this aer we aim to formally exlain the henomenon of fast convergence of Stochastic Gradient Descent (SGD) observed in modern machine learning. The key observation is that most modern learning architectures are over-arametrized and are trained to interolate the data by driving the emirical loss (classification and regression) close to zero. While it is still unclear why these interolated solutions erform well on test data, we show that these regimes allow for fast convergence of SGD, comarable in number of iterations to full gradient descent. For convex loss functions we obtain an exonential  convergence bound for {\it mini-batch} SGD  arallel to that for full gradient descent. We show that there is a critical batch size $m^*$ such that: (a) SGD iteration with mini-batch size $m\leq m^*$ is  nearly equivalent to $m$ iterations of mini-batch size $1$ (\emh{linear scaling regime}). (b) SGD iteration with mini-batch $m m^*$ is nearly equivalent to a full gradient descent iteration (\emh{saturation regime}). Moreover, for the quadratic loss, we derive exlicit exressions for the otimal mini-batch and ste size and exlicitly characterize the two regimes above. The critical mini-batch size can be viewed as the limit for effective mini-batch arallelization. It is also nearly indeendent of the data size, imlying $O(n)$ acceleration over GD er unit of comutation. We give exerimental evidence on real data which closely follows our theoretical analyses. Finally, we show how our results fit in the recent develoments in training dee neural networks and discuss connections to adative rates for SGD and variance reduction."
205,2018,A Simple Stochastic Variance Reduced Algorithm with Fast Convergence Rates,Oral/Poster,"Recent years have witnessed exciting rogress in the study of stochastic variance reduced gradient methods (e.g., SVRG, SAGA), their accelerated variants (e.g, Katyusha) and their extensions in many different settings (e.g., online, sarse, asynchronous, distributed). Among them, accelerated methods enjoy imroved convergence rates but have comlex couling structures, which makes them hard to be extended to more settings (e.g., sarse and asynchronous) due to the existence of erturbation. In this aer, we introduce a simle stochastic variance reduced algorithm (MiG), which enjoys the best-known convergence rates for both strongly convex and non-strongly convex roblems. Moreover, we also resent its efficient sarse and asynchronous variants, and theoretically analyze its convergence rates in these settings. Finally, extensive exeriments for various machine learning roblems such as logistic regression are given to illustrate the ractical imrovement in both serial and asynchronous settings.
"
206,2018,Escaping Saddles with Stochastic Gradients,Oral/Poster,"We analyze the variance of stochastic gradients along negative curvature directions in certain non-convex machine learning models and show that stochastic gradients indeed exhibit a strong comonent along these directions. Furthermore, we show that - contrary to the case of isotroic noise - this variance is roortional to the magnitude of the corresonding eigenvalues and not decreasing in the dimensionality. Based uon this bservation we roose a new assumtion under which we show that the injection of exlicit, isotroic noise usually alied to make gradient descent escae saddle oints can successfully be relaced by a simle SGD ste. Additionally - and under the same condition - we derive the first convergence rate for lain SGD to a second-order stationary oint in a number of iterations that is indeendent of the roblem dimension.
"
207,2018,$D^2$: Decentralized Training over Decentralized Data,Oral/Poster,"While training a machine learning  model using multile workers, each of     which collects data from its own data source, it would be useful     when the data collected from different workers are {\em unique} and {\em       different}. Ironically, recent analysis of decentralized arallel   stochastic gradient descent (D-PSGD) relies on the assumtion that the data   hosted on different workers are {\em not too different}. In this aer, we ask   the question: {\em Can we design a decentralized arallel stochastic gradient     descent algorithm that is less sensitive to the data variance across     workers?} In this aer, we resent D$^2$, a novel decentralized arallel stochastic   gradient descent algorithm designed for large data variance \xr{among workers}   (imrecisely, ``decentralized'' data). The core of D$^2$ is a variance   reduction extension of D-PSGD. It imroves the   convergence rate from $O\left({\sigma \over \sqrt{nT}} +     {(n\zeta^2)^{\frac{1}{3}} \over T^{23}}\right)$ to $O\left({\sigma \over       \sqrt{nT}}\right)$ where $\zeta^{2}$ denotes the variance among data on     different workers. As a result, D$^2$ is robust to data variance among   workers. We emirically evaluated D$^2$ on image classification     tasks, where each worker has access to only the data of a     limited set of labels, and find that D$^2$ significantly outerforms   D-PSGD."
208,2018,Machine Theory of Mind,Oral/Poster,"Theory of mind (ToM) broadly refers to humans' ability to reresent the mental states of others, including their desires, beliefs, and intentions. We design a Theory of Mind neural network – a ToMnet – which uses meta-learning to build such models of the agents it encounters. The ToMnet learns a strong rior model for agents’ future behaviour, and, using only a small number of behavioural observations, can bootstra to richer redictions about agents’ characteristics and mental states. We aly the ToMnet to agents behaving in simle gridworld environments, showing that it learns to model random, algorithmic, and dee RL agents from varied oulations, and that it asses classic ToM tasks such as the ""Sally-Anne"" test of recognising that others can hold false beliefs about the world.
"
209,2018,"Been There, Done That: Meta-Learning with Episodic Recall",Oral/Poster,"Meta-learning agents excel at raidly learning new tasks from oen-ended task distributions; yet, they forget what they learn about each task as soon as the next begins. When tasks reoccur – as they do in natural environments – meta-learning agents must exlore again instead of immediately exloiting reviously discovered solutions. We roose a formalism for generating oen-ended yet reetitious environments, then develo a meta-learning architecture for solving these environments. This architecture melds the standard LSTM working memory with a differentiable neural eisodic memory. We exlore the caabilities of agents with this eisodic LSTM in five meta-learning environments with reoccurring tasks, ranging from bandits to navigation and stochastic sequential decision roblems.
"
210,2018,Faster Derivative-Free Stochastic Algorithm for Shared Memory Machines,Oral/Poster,"Asynchronous arallel stochastic gradient otimization has been laying a ivotal role to solve large-scale machine learning roblems in big data alications. Zeroth-order (derivative-free) methods estimate the gradient only by two function evaluations, thus have been alied to solve the roblems where the exlicit gradient calculations are comutationally exensive or infeasible. Recently, the first asynchronous arallel stochastic zeroth-order algorithm (AsySZO) was roosed. However, its convergence rate is O(1SQRT{T}) for the smooth, ossibly non-convex learning roblems, which is significantly slower than O(1T) the best convergence rate of (asynchronous) stochastic gradient algorithm. To fill this ga, in this aer, we first oint out the fundamental reason leading to the slow convergence rate of AsySZO, and then roose a new asynchronous stochastic zerothorder algorithm (AsySZO+). We rovide a faster convergence rate O(1bT) (b is the mini-batch size) for AsySZO+ by the rigorous theoretical analysis, which is a significant imrovement over O(1SQRT{T}). The exerimental results on the alication of ensemble learning confirm that our AsySZO+ has a faster convergence rate than the existing (asynchronous) stochastic zeroth-order algorithms.
"
211,2018,Coded Sparse Matrix Multiplication,Oral/Poster,"In a large-scale and distributed matrix multilication roblem $C=A^{\intercal}B$, where $C\in\mathbb{R}^{r\times t}$,  the coded comutation lays an imortant role to effectively deal with ``stragglers'' (distributed comutations that may get delayed due to few slow or faulty rocessors). However, existing coded schemes could destroy the significant sarsity that exists in large-scale machine learning roblems, and could result in much higher comutation overhead, i.e., $O(rt)$ decoding time. In this aer, we develo a new coded comutation strategy, we call \emh{sarse code}, which achieves near \emh{otimal recovery threshold}, \emh{low comutation overhead}, and \emh{linear decoding time} $O(nnz(C))$.  We imlement our scheme and demonstrate the advantage of the aroach over both uncoded and current fastest coded strategies."
212,2018,Augment and Reduce: Stochastic Inference for Large Categorical Distributions,Oral/Poster,"Categorical distributions are ubiquitous in machine learning, e.g., in classification, language models, and recommendation systems. However, when the number of ossible outcomes is very large, using categorical distributions becomes comutationally exensive, as the comlexity scales linearly with the number of outcomes. To address this roblem, we roose augment and reduce (A&am;R), a method to alleviate the comutational comlexity. A&am;R uses two ideas: latent variable augmentation and stochastic variational inference. It maximizes a lower bound on the marginal likelihood of the data. Unlike existing methods which are secific to softmax, A&am;R is more general and is amenable to other categorical models, such as multinomial robit. On several large-scale classification roblems, we show that A&am;R rovides a tighter bound on the marginal likelihood and has better redictive erformance than existing aroaches.
"
213,2018,Efficient Gradient-Free Variational Inference using Policy Search,Oral/Poster,"Inference from comlex distributions is a common roblem in machine learning needed for many Bayesian methods. We roose an efficient, gradient-free method for learning general GMM aroximations of multimodal distributions based on recent insights from stochastic search methods. Our method establishes information-geometric trust regions to ensure efficient exloration of the samling sace and stability of the GMM udates, allowing for efficient estimation of multi-variate Gaussian variational distributions. For GMMs, we aly a variational lower bound to decomose the learning objective into sub-roblems given by learning the individual mixture comonents and the coefficients. The number of mixture comonents is adated online in order to allow for arbitrary exact aroximations. We demonstrate on several domains that we can learn significantly better aroximations than cometing variational inference methods and that the quality of samles drawn from our aroximations is on ar with samles created by state-of-the-art MCMC samlers that require significantly more comutational resources.
"
214,2018,Fixing a Broken ELBO,Oral/Poster,"Recent work in unsuervised reresentation learning has focused on learning dee directed latentvariable models. Fitting these models by maximizing the marginal likelihood or evidence is tyically intractable, thus a common aroximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or aroximate) does not necessarily result in a good latent reresentation, as we demonstrate both theoretically and emirically. In articular, we derive variational lower and uer bounds on the mutual information between the inut and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between comression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical ELBO, but different quantitative and qualitative characteristics. Our framework also suggests a simle new method to ensure that latent variable models with owerful stochastic decoders do not ignore their latent code.
"
215,2018,Variational Inference and Model Selection with Generalized Evidence Bounds,Oral/Poster,"Recent advances on the scalability and flexibility of variational inference have made it successful at unravelling hidden atterns in comlex data. In this work we roose a new variational bound formulation, yielding an estimator that extends beyond the conventional variational bound. It naturally subsumes the imortance-weighted and Renyi bounds as secial cases, and it is rovably sharer than these counterarts. We also resent an imroved estimator for variational learning, and advocate a novel high signal-to-variance ratio udate rule for the variational arameters. We discuss model-selection issues associated with existing evidence-lower-bound-based variational inference rocedures, and show how to leverage the flexibility of our new formulation to address them. Emirical evidence is rovided to validate our claims.
"
216,2018,The Generalization Error of Dictionary Learning with Moreau Envelopes,Oral/Poster,"This is a theoretical  study on the samle  comlexity of dictionary learning with a general tye of reconstruction loss. The goal is to estimate a $m \times d$ matrix $D$ of unit-norm columns when the only available information is a set of training samles. Points $x$ in $\mathbb{R}^m$ are subsequently aroximated by the linear combination $Da$ after solving the roblem $\min_{a \in \mathbb{R}^d}~ \Phi(x - Da) + g(a)$; function $g:\mathbb{R}^d \to [0,+\infty)$ is either an indicator function or a sarsity romoting regularizer. Here is considered the case where $ \Phi(x) = \inf_{z \in \mathbb{R}^m}~{ ||x-z||_2^2 + h(||z||_2)}$ and $h$ is an even and univariate function on the real line. Connections are drawn between $\Phi$ and the Moreau enveloe of $h$. A new samle comlexity result concerning the $k$-sarse dictionary roblem removes the surious condition on the coherence of $D$ aearing in revious works. Finally, comments are made on the aroximation error of certain families of losses. The derived generalization bounds are of order $\mathcal{O}(\sqrt{\log n n})$ and valid without any further restrictions on the set of dictionaries with unit-norm columns."
217,2018,Network Global Testing by Counting Graphlets,Oral/Poster,"Consider a large social network with ossibly severe degree heterogeneity and mixed-membershis. We are interested in testing whether the network has only one community or there are more than one communities. The roblem is known to be non-trivial, artially due to the resence of severe degree heterogeneity.  We construct a class of test statistics using the numbers of short aths and short cycles,  and the key to our aroach is a general framework for canceling the effects of degree heterogeneity. The tests comare favorably with existing methods. We suort our methods with careful analysis and numerical study with simulated data and a real data examle.
"
218,2018,Large-Scale Sparse Inverse Covariance Estimation via Thresholding and Max-Det Matrix Completion,Oral/Poster,"The sarse inverse covariance estimation roblem is commonly solved using an $\ell_{1}$-regularized Gaussian maximum likelihood estimator known as “grahical lasso”, but its comutational cost becomes rohibitive for large data sets. A recently line of results showed–under mild assumtions–that the grahical lasso estimator can be retrieved by soft-thresholding the samle covariance matrix and solving a maximum determinant matrix comletion (MDMC) roblem. This aer roves an extension of this result, and describes a Newton-CG algorithm to efficiently solve the MDMC roblem. Assuming that the thresholded samle covariance matrix is sarse with a sarse Cholesky factorization, we rove that the algorithm converges to an $\esilon$-accurate solution in $O(n\log(1\esilon))$ time and $O(n)$ memory. The algorithm is highly efficient in ractice: we solve the associated MDMC roblems with as many as 200,000 variables to 7-9 digits of accuracy in less than an hour on a standard lato comuter running MATLAB."
219,2018,Robust and Scalable Models of Microbiome Dynamics,Oral/Poster,"Microbes are everywhere, including in and on our bodies, and have been shown to lay key roles in a variety of revalent human diseases. Consequently, there has been intense interest in the design of bacteriotheraies or ""bugs as drugs,"" which are communities of bacteria administered to atients for secific theraeutic alications. Central to the design of such theraeutics is an understanding of the causal microbial interaction network and the oulation dynamics of the organisms. In this work we resent a Bayesian nonarametric model and associated efficient inference algorithm that addresses the key concetual and ractical challenges of learning microbial dynamics from time series microbe abundance data. These challenges include high-dimensional (300+ strains of bacteria in the gut) but temorally sarse and non-uniformly samled data; high measurement noise; and, nonlinear and hysically non-negative dynamics. Our contributions include a new tye of dynamical systems model for microbial dynamics based on what we term interaction modules, or learned clusters of latent variables with redundant interaction structure (reducing the exected number of interaction coefficients from O(n^2) to O((log n)^2)); a fully Bayesian formulation of the stochastic dynamical systems model that roagates measurement and latent state uncertainty throughout the model; and introduction of a temorally varying auxiliary variable technique to enable efficient inference by relaxing the hard non-negativity constraint on states. We aly our method to simulated and real data, and demonstrate the utility of our technique for system identification from limited data and gaining new biological insights into bacteriotheray design.
"
220,2018,Explicit Inductive Bias for Transfer Learning with Convolutional Networks,Oral/Poster,"In inductive transfer learning, fine-tuning re-trained convolutional networks substantially outerforms training from scratch. When using fine-tuning, the underlying assumtion is that the re-trained model extracts generic features, which are at least artially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task. However, besides the initialization with the re-trained model and the early stoing, there is no mechanism in fine-tuning for retaining the features learned on the source task. In this aer, we investigate several regularization schemes that exlicitly romote the similarity of the final solution with the initial model. We show the benefit of having an exlicit inductive bias towards the initial model, and we eventually recommend a simle $L^2$ enalty with the re-trained model being a reference as the baseline of enalty for transfer learning tasks."
221,2018,GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks,Oral/Poster,"Dee multitask networks, in which one neural network roduces multile redictive oututs, can offer better seed and erformance than their single-task counterarts but are challenging to train roerly. We resent a gradient normalization (GradNorm) algorithm that automatically balances training in dee multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm imroves accuracy and reduces overfitting across multile tasks when comared to single-task networks, static baselines, and other adative multitask loss balancing techniques. GradNorm also matches or surasses the erformance of exhaustive grid search methods, desite only involving a single asymmetry hyerarameter $\alha$. Thus, what was once a tedious search rocess that incurred exonentially more comute for each task added can now be accomlished within a few training runs, irresective of the number of tasks. Ultimately, we will demonstrate that gradient maniulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the otential of multitask learning."
222,2018,Optimizing the Latent Space of Generative Networks,Oral/Poster,"Generative Adversarial Networks (GANs) have achieved remarkable results in the task of generating realistic natural images. In most successful alications, GAN models share two common asects: solving a challenging saddle oint otimization roblem, interreted as an adversarial game between a generator and a discriminator functions; and arameterizing the generator and the discriminator as dee convolutional neural networks. The goal of this aer is to disentangle the contribution of these two factors to the success of GANs. In articular, we introduce Generative Latent Otimization (GLO), a framework to train dee convolutional generators using simle reconstruction losses. Throughout a variety of exeriments, we show that GLO enjoys many of the desirable roerties of GANs: synthesizing visually-aealing samles, interolating meaningfully between samles, and erforming linear arithmetic with noise vectors; all of this without the adversarial otimization scheme.
"
223,2018,Theoretical Analysis of Image-to-Image Translation with Adversarial Learning,Oral/Poster,"Recently, a unified model for image-to-image translation tasks within adversarial learning framework has aroused widesread research interests in comuter vision ractitioners. Their reorted emirical success however lacks solid theoretical interretations for its inherent mechanism. In this aer, we reformulate their model from a brand-new geometrical ersective and have eventually reached a full interretation on some interesting but unclear emirical henomenons from their exeriments. Furthermore, by extending the definition of generalization for generative adversarial nets to a broader sense, we have derived a condition to control the generalization caability of their model. According to our derived condition, several ractical suggestions have also been roosed on model design and dataset construction as a guidance for further emirical researches.
"
224,2018,Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,Oral/Poster,"Model-free dee reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods tyically suffer from two major challenges: very high samle comlexity and brittle convergence roerties, which necessitate meticulous hyerarameter tuning. Both of these challenges severely limit the alicability of such methods to comlex, real-world domains. In this aer, we roose soft actor-critic, an off-olicy actor-critic dee RL algorithm based on the maximum entroy reinforcement learning framework. In this framework, the actor aims to maximize exected reward while also maximizing entroy. That is, to succeed at the task while acting as randomly as ossible. Prior dee RL methods based on this framework have been formulated as Q-learning methods. By combining off-olicy udates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art erformance on a range of continuous control benchmark tasks, outerforming rior on-olicy and off-olicy methods. Furthermore, we demonstrate that, in contrast to other off-olicy algorithms, our aroach is very stable, achieving very similar erformance across different random seeds.
"
225,2018,PIPPS: Flexible Model-Based Policy Search Robust to the Curse of Chaos,Oral/Poster,"Previously, the exloding gradient roblem has been exlained to be   central in dee learning and model-based reinforcement learning,   because it causes numerical issues and instability in   otimization. Our exeriments in model-based reinforcement learning   imly that the roblem is not just a numerical issue, but it may be   caused by a fundamental chaos-like nature of long chains of   nonlinear comutations. Not only do the magnitudes of the gradients   become large, the direction of the gradients becomes essentially   random. We show that rearameterization gradients suffer from the   roblem, while likelihood ratio gradients are robust. Using our   insights, we develo a model-based olicy search framework,   Probabilistic Inference for Particle-Based Policy Search (PIPPS),   which is easily extensible, and allows for almost arbitrary models   and olicies, while simultaneously matching the erformance of   revious data-efficient learning algorithms. Finally, we invent   the total roagation algorithm, which efficiently   comutes a union over all athwise derivative deths during a single   backwards ass, automatically giving greater weight to estimators    with lower variance, sometimes   imroving over rearameterization gradients by $10^6$ times."
226,2018,Probabilistic Recurrent State-Space Models,Oral/Poster,"State-sace models (SSMs) are a highly exressive model class for learning atterns in time series data and for system identification. Deterministic versions of SSMs (e.g., LSTMs) roved extremely successful in modeling comlex time series data. Fully robabilistic SSMs, however, are often found hard to train, even for smaller roblems. We roose a novel model formulation and a scalable training algorithm based on doubly stochastic variational inference and Gaussian rocesses. This combination allows efficient incororation of latent state temoral correlations, which we found to be key to robust training. The effectiveness of the roosed PR-SSM is evaluated on a set of real-world benchmark datasets in comarison to state-of-the-art robabilistic model learning methods. Scalability and robustness are demonstrated on a high dimensional roblem.
"
227,2018,Structured Variationally Auto-encoded Optimization,Oral/Poster,"We tackle the roblem of otimizing a black-box objective function defined over a highly-structured inut sace. This roblem is ubiquitous in science and engineering. In machine learning, inferring the structure of a neural network or the Automatic Statistician (AS), where the otimal kernel combination for a Gaussian rocess is selected, are two imortant examles. We use the \as as a case study to describe our aroach, that can be easily generalized to other domains. We roose an Structure Generating Variational Auto-encoder (SG-VAE) to embed the original sace of kernel combinations into some low-dimensional continuous manifold where Bayesian otimization (BO) ideas are used. This is ossible when structural knowledge of the roblem is available, which can be given via a simulator or any other form of generating otentially good solutions. The right exloration-exloitation balance is imosed by roagating into the search the uncertainty of the latent sace of the SG-VAE,  that is comuted using variational inference. The key asect of our aroach is that the SG-VAE can be used to bias the search towards relevant regions, making it suitable for transfer learning tasks. Several exeriments in various alication domains are used to illustrate the utility and generality of the aroach described in this work.
"
228,2018,A Robust Approach to Sequential Information Theoretic Planning,Oral/Poster,"In many sequential lanning alications a natural aroach to generating high quality lans is to maximize an information reward such as mutual information (MI).  Unfortunately, MI lacks a closed form in all but trivial models, and so must be estimated.  In alications where the cost of lan execution is exensive, one desires lanning estimates which admit theoretical guarantees. Through the use of robust M-estimators we obtain bounds on absolute deviation of estimated MI.  Moreover, we roose a sequential algorithm which integrates inference and lanning by maximally reusing articles in each stage.  We validate the utility of using robust estimators in the sequential aroach on a Gaussian Markov Random Field wherein information measures have a closed form. Lastly, we demonstrate the benefits of our integrated aroach in the context of sequential exeriment design for inferring causal regulatory networks from gene exression levels.  Our method shows imrovements over a recent method which selects intervention exeriments based on the same MI objective.
"
229,2018,Error Estimation for Randomized Least-Squares Algorithms via the Bootstrap,Oral/Poster,"Over the course of the ast decade, a variety of randomized algorithms have been roosed for comuting aroximate least-squares (LS) solutions in large-scale settings. A longstanding ractical issue is that, for any given inut, the user rarely knows the actual error of an aroximate solution (relative to the exact solution). Likewise, it is difficult for the user to know recisely how much comutation is needed to achieve the desired error tolerance. Consequently, the user often aeals to worst-case error bounds that tend to offer only qualitative guidance. As a more ractical alternative, we roose a bootstra method to comute a osteriori error estimates for randomized LS algorithms. These estimates ermit the user to numerically assess the error of a given solution, and to redict how much work is needed to imrove a ""reliminary"" solution. In addition, we rovide theoretical consistency results for the method, which are the first such results in this context (to the best of our knowledge). From a ractical standoint, the method also has considerable flexibility, insofar as it can be alied to several oular sketching algorithms, as well as a variety of error metrics. Moreover, the extra ste of error estimation does not add much cost to an underlying sketching algorithm. Finally, we demonstrate the effectiveness of the method with emirical results.
"
230,2018,Distributed Asynchronous Optimization with Unbounded Delays: How Slow Can You Go?,Oral/Poster,"One of the most widely used otimization methods for large-scale machine learning roblems is distributed asynchronous stochastic gradient descent (DASGD). However, a key issue that arises here is that of delayed gradients: when a “worker” node asynchronously contributes a gradient udate to the “master”, the global model arameter may have changed, rendering this information stale. In massively arallel comuting grids, these delays can quickly add u if the comutational throughut of a node is saturated, so the convergence of DASGD is uncertain under these conditions. Nevertheless, by using a judiciously chosen quasilinear ste-size sequence, we show that it is ossible to amortize these delays and achieve global convergence with robability 1, even when the delays grow at a olynomial rate. In this way, our results hel reaffirm the successful alication of DASGD to large-scale otimization roblems.
"
231,2018,Error Compensated Quantized SGD and its Applications to Large-scale Distributed Optimization,Oral/Poster,"Large-scale distributed otimization is of great imortance in various alications. For data-arallel based distributed learning, the inter-node gradient communication often becomes the erformance bottleneck. In this aer, we roose the error comensated quantized stochastic gradient descent algorithm to imrove the training efficiency. Local gradients are quantized to reduce the communication overhead, and accumulated quantization error is utilized to seed u the convergence. Furthermore, we resent theoretical analysis on the convergence behaviour, and demonstrate its advantage over cometitors. Extensive exeriments indicate that our algorithm can comress gradients by a factor of u to two magnitudes without erformance degradation.
"
232,2018,Low-Rank Riemannian Optimization on Positive Semidefinite Stochastic Matrices with Applications to Graph Clustering,Oral/Poster,"This aer develos a Riemannian otimization framework for solving otimization roblems on the set of symmetric ositive semidefinite stochastic matrices. The aer first reformulates the roblem by factorizing the otimization variable as $\mathbf{X}=\mathbf{Y}\mathbf{Y}^T$ and deriving conditions on $$, i.e., the number of columns of $\mathbf{Y}$, under which the factorization yields a satisfactory solution. The rearameterization of the roblem allows its formulation as an otimization over either an embedded or quotient Riemannian manifold whose geometries are investigated. In articular, the aer exlicitly derives the tangent sace, Riemannian gradients and retraction oerator that allow the design of efficient otimization methods on the roosed manifolds. The numerical results reveal that, when the otimal solution has a known low-rank, the resulting algorithms resent a clear comlexity advantage when comared with state-of-the-art Euclidean and Riemannian aroaches for grah clustering alications."
233,2018,"Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients",Oral/Poster,"The ADAM otimizer is exceedingly oular in the dee learning community. Often it works very well, sometimes it doesn’t. Why? We interret ADAM as a combination of two asects: for each weight, the udate direction is determined by the sign of stochastic gradients, whereas the udate magnitude is determined by an estimate of their relative variance. We disentangle these two asects and analyze them in isolation, gaining insight into the mechanisms underlying ADAM. This analysis also extends recent results on adverse effects of ADAM on generalization, isolating the sign asect as the roblematic one. Transferring the variance adatation to SGD gives rise to a novel method, comleting the ractitioner’s toolbox for roblems where ADAM fails.
"
234,2018,Discovering and Removing Exogenous State Variables and Rewards for Reinforcement Learning,Oral/Poster,"Exogenous state variables and rewards can slow down reinforcement learning by injecting uncontrolled variation into the reward signal. We formalize exogenous state variables and rewards and identify conditions under which an MDP with exogenous state can be decomosed into an exogenous Markov Reward Process involving only the exogenous state+reward and an endogenous Markov Decision Process defined with resect to only the endogenous rewards. We also derive a variance-covariance condition under which Monte Carlo olicy evaluation on the endogenous MDP is accelerated comared to using the full MDP. Similar seedus are likely to carry over to all RL algorithms. We develo two algorithms for discovering the exogenous variables and test them on several MDPs. Results show that the algorithms are ractical and can significantly seed u reinforcement learning.
"
235,2018,Differentially Private Database Release via Kernel Mean Embeddings,Oral/Poster,"We lay theoretical foundations for new database release mechanisms that allow third-arties to construct consistent estimators of oulation statistics, while ensuring that the rivacy of each individual contributing to the database is rotected. The roosed framework rests on two main ideas. First, releasing (an estimate of) the kernel mean embedding of the data generating random variable instead of the database itself still allows third-arties to construct consistent estimators of a wide class of oulation statistics. Second, the algorithm can satisfy the definition of differential rivacy by basing the released kernel mean embedding on entirely synthetic data oints, while controlling accuracy through the metric available in a Reroducing Kernel Hilbert Sace. We describe two instantiations of the roosed framework, suitable under different scenarios, and rove theoretical results guaranteeing differential rivacy of the resulting algorithms and the consistency of estimators constructed from their oututs.
"
236,2018,Extracting Automata from Recurrent Neural Networks Using Queries and Counterexamples,Oral/Poster,"We resent a novel algorithm that uses exact learning and abstraction to extract a deterministic finite automaton describing the state dynamics of a given trained RNN.  We do this using Angluin's \lstar algorithm as a learner and the trained RNN as an oracle.  Our technique efficiently extracts accurate automata from trained RNNs,  even when the state vectors are large and require fine differentiation.
"
237,2018,Neural Dynamic Programming for Musical Self Similarity,Oral/Poster,"We resent a neural sequence model designed secifically for symbolic music. The model is based on a learned edit distance mechanism which generalises a classic recursion from comuter science, leading to a neural dynamic rogram. Reeated motifs are detected by learning the transformations between them. We reresent the arising comutational deendencies using a novel data structure, the edit tree; this ersective suggests natural aroximations which afford the scaling u of our otherwise cubic time algorithm. We demonstrate our model on real and synthetic data; in all cases it out-erforms a strong stacked long short-term memory benchmark.
"
238,2018,Learning long term dependencies via Fourier recurrent units,Oral/Poster,"It is a known fact that training recurrent neural networks for tasks  that have long term deendencies is challenging. One of the main reasons is the vanishing or exloding gradient roblem, which revents gradient information from  roagating to early layers. In this aer we roose a simle recurrent architecture, the Fourier Recurrent Unit (FRU), that stabilizes the gradients that arise in its training while giving us stronger exressive ower. Secifically, FRU summarizes  the hidden states $h^{(t)}$ along the temoral dimension with Fourier basis functions. This allows gradients to easily reach any layer due to FRU's residual learning structure and the global suort of  trigonometric functions. We show that FRU has gradient lower and uer bounds  indeendent of temoral dimension. We also show the strong exressivity of  sarse Fourier basis, from which FRU obtains its strong exressive ower. Our  exerimental study also demonstrates that with fewer arameters the roosed architecture outerforms other recurrent architectures on many tasks."
239,2018,Autoregressive Convolutional Neural Networks for Asynchronous Time Series,Oral/Poster,"We roose Significance-Offset Convolutional Neural Network, a dee convolutional network architecture for regression of multivariate asynchronous time series. The model is insired by standard autoregressive (AR) models and gating mechanisms used in recurrent neural networks. It involves an AR-like weighting system, where the final redictor is obtained as a weighted sum of adjusted regressors, while the weights are data-deendent functions learnt through a convolutional network. The architecture was designed for alications on asynchronous time series and is evaluated on such datasets: a hedge fund rorietary dataset of over 2 million quotes for a credit derivative index, an artificially generated noisy autoregressive series and UCI household electricity consumtion dataset. The roosed architecture achieves romising results as comared to convolutional and recurrent neural networks.
"
240,2018,Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation,Oral/Poster,"Modern reinforcement learning algorithms reach suer-human erformance on many board and video games, but they are samle inefficient, i.e. they tyically require significantly more laying exerience than humans to reach an equal erformance level. To imrove samle efficiency, an agent may build a model of the environment and use lanning methods to udate its olicy. In this article we introduce Variational State Tabulation (VaST), which mas an environment with a high-dimensional state sace (e.g. the sace of visual inuts) to an abstract tabular model. Prioritized sweeing with small backus, a highly efficient lanning method, can then be used to udate state-action values. We show how VaST can raidly learn to maximize reward in tasks like 3D navigation and efficiently adat to sudden changes in rewards or transition robabilities.
"
241,2018,Regret Minimization for Partially Observable Deep Reinforcement Learning,Oral/Poster,"Dee reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image ixels. However, algorithms that estimate state and state-action value functions tyically assume a fully observed state and must comensate for artial observations by using finite length observation histories or recurrent networks. In this work, we roose a new dee reinforcement learning algorithm based on counterfactual regret minimization that iteratively udates an aroximation to an advantage-like function and is robust to artially observed state. We demonstrate that this new algorithm can substantially outerform strong baseline methods on several artially observed reinforcement learning tasks: learning first-erson 3D navigation in Doom and Minecraft, and acting in the resence of artially observed objects in Doom and Pong.
"
242,2018,Goodness-of-fit Testing for Discrete Distributions via Stein Discrepancy,Oral/Poster,"Recent work has combined Stein's method with reroducing kernel Hilbert sace theory to develo nonarametric goodness-of-fit tests for un-normalized robability distributions. However, the currently available tests aly exclusively to distributions with smooth density functions. In this work, we introduce a kernelized Stein discreancy measure for discrete saces, and develo a nonarametric goodness-of-fit test for discrete distributions with intractable normalization constants. Furthermore, we roose a general characterization of Stein oerators that encomasses both discrete and continuous distributions, roviding a recie for constructing new Stein oerators. We aly the roosed goodness-of-fit test to three statistical models involving discrete distributions, and our exeriments show that the roosed test tyically outerforms a two-samle test based on the maximum mean discreancy.
"
243,2018,Unbiased Objective Estimation in Predictive Optimization,Oral/Poster,"For data-driven decision-making, one romising aroach, called redictive otimization, is to solve maximization roblems i n which the objective function to be maximized is estimated from data. Predictive otimization, however, suffers from the roblem of a calculated otimal solution’s being evaluated too otimistically, i.e., the value of the objective function is overestimated. This aer investigates such otimistic bias and resents two methods for correcting it. The first, which is analogous to cross-validation, successfully corrects the otimistic bias but results in underestimation of the true value. Our second method emloys resamling techniques to avoid both overestimation and underestimation. We show that the second method, referred to as the arameter erturbation method, achieves asymtotically unbiased estimation. Emirical results for both artificial and real-world datasets demonstrate that our roosed aroach successfully corrects the otimistic bias.
"
244,2018,Ultra Large-Scale Feature Selection using Count-Sketches,Oral/Poster,"Feature selection is an imortant challenge in machine learning. It lays a crucial role in the exlainability of machine-driven decisions that are raidly ermeating throughout modern society. Unfortunately, the exlosion in the size and dimensionality of real-world datasets oses a severe challenge to standard feature selection algorithms. Today, it is not uncommon for datasets to have billions of dimensions. At such scale, even storing the feature vector is imossible, causing most existing feature selection methods to fail. Workarounds like feature hashing, a standard aroach to large-scale machine learning, hels with the comutational feasibility, but at the cost of losing the interretability of features. In this aer, we resent MISSION, a novel framework for ultra large-scale feature selection that erforms stochastic gradient descent while maintaining an efficient reresentation of the features in memory using a Count-Sketch data structure. MISSION retains the simlicity of feature hashing without sacrificing the interretability of the features while using only O(log^2()) working memory. We demonstrate that MISSION accurately and efficiently erforms feature selection on real-world, large-scale datasets with billions of dimensions.
"
245,2018,"Matrix Norms in Data Streams: Faster, Multi-Pass and Row-Order",Oral/Poster,"A central roblem in mining massive data streams is characterizing which functions of an underlying frequency vector can be aroximated efficiently.  Given the revalence of large scale linear algebra roblems in machine learning, recently there has been considerable effort in extending this data stream roblem to that of estimating functions of a matrix.  This setting generalizes classical roblems to the analogous ones for matrices.  For examle, instead of estimating frequent-item counts,  we now wish to estimate ``frequent-direction'' counts.  A related examle is to estimate norms, which now corresond to estimating a vector norm on the singular values of the matrix.  Desite recent efforts, the current understanding for such matrix roblems is considerably weaker than that for vector roblems.   We study a number of asects of estimating matrix norms in a stream that have not reviously been considered:  (1) multi-ass algorithms, (2) algorithms that see the underlying matrix one row at a time, and (3) time-efficient algorithms.  Our multi-ass and row-order algorithms use less memory  than what is rovably required in the single-ass and entrywise-udate models,  and thus give searations between these models (in terms of memory). Moreover, all of our algorithms are considerably faster than revious ones.  We also rove a number of lower bounds, and obtain for instance,  a near-comlete characterization of the memory required of row-order algorithms for estimating Schatten $$-norms of sarse matrices. We comlement our results with numerical exeriments."
246,2018,Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?,Oral/Poster,"Dee reinforcement learning has achieved many recent successes, but our understanding of its strengths and limitations is hamered by the lack of rich environments in which we can fully characterize otimal behavior, and corresondingly diagnose individual actions against such a characterization. Here we consider a family of combinatorial games, arising from work of Erdos, Selfridge, and Sencer, and we roose their use as environments for evaluating and comaring different aroaches to reinforcement learning. These games have a number of aealing features: they are challenging for current learning aroaches, but they form (i) a low-dimensional, simly arametrized environment where (ii) there is a linear closed form solution for otimal behavior from any state, and (iii) the difficulty of the game can be tuned by changing environment arameters in an interretable way. We use these Erdos-Selfridge-Sencer games not only to comare different algorithms, but test for generalization, make comarisons to suervised learning, analyse multiagent lay, and even develo a self lay algorithm.
"
247,2018,The Mirage of Action-Dependent Baselines in Reinforcement Learning,Oral/Poster,"Policy gradient methods are a widely used class of model-free reinforcement learning algorithms where a state-deendent baseline is used to reduce gradient estimator variance. Several recent aers extend the baseline to deend on both the state and action and suggest that this significantly reduces variance and imroves samle efficiency without introducing bias into the gradient estimates. To better understand this develoment, we decomose the variance of the olicy gradient estimator and numerically show that learned state-action-deendent baselines do not in fact reduce variance over a state-deendent baseline in commonly tested benchmark domains. We confirm this unexected result by reviewing the oen-source code accomanying these rior aers, and show that subtle imlementation decisions cause deviations from the methods resented in the aers and exlain the source of the reviously observed emirical gains. Furthermore, the variance decomosition highlights areas for imrovement, which we demonstrate by illustrating a simle change to the tyical value function arameterization that can significantly imrove erformance.
"
248,2018,Composite Marginal Likelihood Methods for Random Utility Models,Oral/Poster,"We roose a novel and flexible rank-breaking-then-comosite-marginal-likelihood (RBCML) framework for learning random utility models (RUMs), which include the Plackett-Luce model. We characterize conditions for the objective function of RBCML to be strictly log-concave by roving that strict log-concavity is reserved under convolution and marginalization. We characterize necessary and sufficient conditions for RBCML to satisfy consistency and asymtotic normality. Exeriments on synthetic data show that RBCML for Gaussian RUMs achieves better statistical efficiency and comutation efficiency than the state-of-the-art algorithm and our RBCML for the Plackett-Luce model rovides flexible tradeoffs between running time and statistical efficiency.
"
249,2018,Ranking Distributions based on Noisy Sorting,Oral/Poster,"We roose a new statistical model for ranking data, i.e., a new family of robability distributions on ermutations. Our model is insired by the idea of a data-generating rocess in the form of a noisy sorting rocedure, in which deterministic comarisons between airs of items are relaced by Bernoulli trials. The robability of roducing a certain ranking as a result then essentially deends on the Bernoulli arameters, which can be interreted as airwise references. We show that our model can be written in closed form if insertion or quick sort are used as sorting algorithms, and roose a maximum likelihood aroach for arameter estimation. We also introduce a generalization of the model, in which the constraints on airwise references are relaxed, and for which maximum likelihood estimation can be carried out based on a variation of the generalized iterative scaling algorithm. Exerimentally, we show that the models erform very well in terms of goodness of fit, comared to existing models for ranking data.
"
250,2018,DICOD: Distributed Convolutional Coordinate Descent for Convolutional Sparse Coding,Oral/Poster,"In this aer, we introduce DICOD, a convolutional sarse coding algorithm which builds shift invariant reresentations for long signals. This algorithm is designed to run in a distributed setting, with local message assing, making it communication efficient. It is based on coordinate descent and uses locally greedy udates which accelerate the resolution comared to greedy coordinate selection. We rove the convergence of this algorithm and highlight its comutational seed-u which is suer-linear in the number of cores used. We also rovide emirical evidence for the acceleration roerties of our algorithm comared to state-of-the-art methods.
"
251,2018,Exploring Hidden Dimensions in Accelerating Convolutional Neural Networks,Oral/Poster,"The ast few years have witnessed growth in the comutational requirements for training dee convolutional neural networks. Current aroaches arallelize training onto multile devices by alying a single arallelization strategy (e.g., data or model arallelism) to all layers in a network. Although easy to reason about, these aroaches result in subotimal runtime erformance in large-scale distributed training, since different layers in a network may refer different arallelization strategies. In this aer, we roose layer-wise arallelism that allows each layer in a network to use an individual arallelization strategy. We jointly otimize how each layer is arallelized by solving a grah search roblem. Our evaluation shows that layer-wise arallelism outerforms state-of-the-art aroaches by increasing training throughut, reducing communication costs, achieving better scalability to multile GPUs, while maintaining original network accuracy.
"
252,2018,Deep Models of Interactions Across Sets,Oral/Poster,"We use dee learning to model interactions across two or more sets of objects, such as user–movie ratings or rotein–drug bindings. The canonical reresentation of such interactions is a matrix (or tensor) with an exchangeability roerty: the encoding’s meaning is not changed by ermuting rows or columns. We argue that models should hence be Permutation Equivariant (PE): constrained to make the same redictions across such ermutations. We resent a arameter-sharing scheme and rove that it is maximally exressive under the PE constraint. This scheme yields three benefits. First, we demonstrate erformance cometitive with the state of the art on multile matrix comletion benchmarks. Second, our models require a number of arameters indeendent of the numbers of objects and thus scale well to large datasets. Third, models can be queried about new objects that were not available at training time, but for which interactions have since been observed. We observed surrisingly good generalization erformance on this matrix extraolation task, both within domains (e.g., new users and new movies drawn from the same distribution used for training) and even across domains (e.g., redicting music ratings after training on movie ratings).
"
253,2018,ContextNet: Deep learning for Star Galaxy Classification,Oral/Poster,"We resent a framework to comose artificial neural networks in cases where the data cannot be treated as indeendent events. Our articular motivation is star galaxy classification for ground based otical surveys. Due to a turbulent atmoshere and imerfect instruments, a single image of an astronomical object is not enough to definitively classify it as a star or galaxy. Instead the context of the surrounding objects imaged at the same time need to be considered in order to make an otimal classification. The model we resent is divided into three distinct ANNs: one designed to cature local features about each object, the second to comare these features across all objects in an image, and the third to make a final rediction for each object based on the local and comared features. By exloiting the ability to relicate the weights of an ANN, the model can handle an arbitrary and variable number of individual objects embedded in a larger exosure. We train and test our model on simulations of a large u and coming ground based survey, the Large Synotic Survey Telescoe (LSST). We comare to the state of the art aroach, showing imroved overall erformance as well as better erformance for a secific class of objects that is imortant for the LSST.
"
254,2018,First Order Generative Adversarial Networks,Oral/Poster,"GANs excel at learning high dimensional distributions, but they can udate generator arameters in directions that do not corresond to the steeest descent direction of the objective. Prominent examles of roblematic udate directions include those used in both Goodfellow's original GAN and the WGAN-GP. To formally describe an otimal udate direction, we introduce a theoretical framework which allows the derivation of requirements on both the divergence and corresonding method for determining an udate direction, with these requirements guaranteeing unbiased mini-batch udates in the direction of steeest descent. We roose a novel divergence which aroximates the Wasserstein distance while regularizing the critic's first order information. Together with an accomanying udate direction, this divergence fulfills the requirements for unbiased steeest descent udates. We verify our method, the First Order GAN, with image generation on CelebA, LSUN and CIFAR-10 and set a new state of the art on the One Billion Word language generation task.
"
255,2018,Max-Mahalanobis Linear Discriminant Analysis Networks,Oral/Poster,"A dee neural network (DNN) consists of a nonlinear transformation from an inut to a feature reresentation, followed by a common softmax linear classifier. Though many efforts have been devoted to designing a roer architecture for nonlinear transformation, little investigation has been done on the classifier art. In this aer, we show that a roerly designed classifier can imrove robustness to adversarial attacks and lead to better rediction results. Secifically, we define a Max-Mahalanobis distribution (MMD) and theoretically show that if the inut distributes as a MMD, the linear discriminant analysis (LDA) classifier will have the best robustness to adversarial examles. We further roose a novel Max-Mahalanobis linear discriminant analysis (MM-LDA) network, which exlicitly mas a comlicated data distribution in the inut sace to a MMD in the latent feature sace and then alies LDA to make redictions. Our results demonstrate that the MM-LDA networks are significantly more robust to adversarial attacks, and have better erformance in class-biased classification.
"
256,2018,Learning Maximum-A-Posteriori Perturbation Models for Structured Prediction in Polynomial Time,Oral/Poster,"MAP erturbation models have emerged as a owerful framework for inference in structured rediction. Such models rovide a way to efficiently samle from the Gibbs distribution and facilitate redictions that are robust to random noise. In this aer, we roose a rovably olynomial time randomized algorithm for learning the arameters of erturbed MAP redictors. Our aroach is based on minimizing a novel Rademacher-based generalization bound on the exected loss of a erturbed MAP redictor, which can be comuted in olynomial time. We obtain conditions under which our randomized learning algorithm can guarantee generalization to unseen examles.
"
257,2018,Structured Output Learning with Abstention: Application to Accurate Opinion Prediction,Oral/Poster,"Motivated by Suervised Oinion Analysis, we roose a novel framework devoted to Structured Outut Learning with Abstention (SOLA). The structure rediction model is able to abstain from redicting some labels in the structured outut at a cost chosen by the user in a flexible way. For that urose, we decomose the roblem into the learning of a air of redictors, one devoted to structured abstention and the other, to structured outut rediction. To comare fully labeled training data with redictions otentially containing abstentions, we define a wide class of asymmetric abstention-aware losses. Learning is achieved by surrogate regression in an aroriate feature sace while rediction with abstention is erformed by solving a new re-image roblem. Thus, SOLA extends recent ideas about Structured Outut Prediction via surrogate roblems and calibration theory and enjoys statistical guarantees on the resulting excess risk. Instantiated on a hierarchical abstention-aware loss, SOLA is shown to be relevant for fine-grained oinion mining and gives state-of-the-art results on this task. Moreover, the abstention-aware reresentations can be used to cometitively redict user-review ratings  based on a sentence-level oinion redictor.
"
258,2018,SBEED: Convergent Reinforcement Learning with Nonlinear Function Approximation,Oral/Poster,"When function aroximation is used, solving the Bellman otimality equation with stability guarantees has remained a major oen roblem in reinforcement learning for decades.  The fundamental difficulty is that the Bellman oerator may become an exansion in general, resulting in oscillating and even divergent behavior of oular algorithms like Q-learning.  In this aer, we revisit the Bellman equation, and reformulate it into a novel rimal-dual otimization roblem using Nesterov's smoothing technique and the Legendre-Fenchel transformation.  We then develo a new algorithm, called Smoothed Bellman Error Embedding, to solve this otimization roblem where any differentiable function class may be used.  We rovide what we believe to be the first convergence guarantee for general nonlinear function aroximation, and analyze the algorithm's samle comlexity. Emirically, our algorithm comares favorably to state-of-the-art baselines in several benchmark control roblems.
"
259,2018,Smoothed Action Value Functions for Learning Gaussian Policies,Oral/Poster,"State-action value functions (i.e., Q-values) are ubiquitous in reinforcement learning (RL), giving rise to oular algorithms such as SARSA and Q-learning. We roose a new notion of action value defined by a Gaussian smoothed version of the exected Q-value. We show that such smoothed Q-values still satisfy a Bellman equation, making them learnable from exerience samled from an environment. Moreover, the gradients of exected reward with resect to the mean and covariance of a arameterized Gaussian olicy can be recovered from the gradient and Hessian of the smoothed Q-value function. Based on these relationshis we develo new algorithms for training a Gaussian olicy directly from a learned smoothed Q-value aroximator. The aroach is additionally amenable to roximal otimization by augmenting the objective with a enalty on KL-divergence from a revious olicy. We find that the ability to learn both a mean and covariance during training leads to significantly imroved results on standard continuous control benchmarks.
"
260,2018,Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron,Oral/Poster,"We resent an extension to the Tacotron seech synthesis architecture that learns a latent embedding sace of rosody, derived from a reference acoustic reresentation containing the desired rosody. We show that conditioning Tacotron on this learned embedding sace results in synthesized audio that matches the rosody of the reference signal with fine time detail even when the reference and synthesis seakers are different.  Additionally, we show that a reference rosody embedding can be used to synthesize text that is different from that of the reference utterance. We define several quantitative and subjective metrics for evaluating rosody transfer, and reort results with accomanying audio samles from single-seaker and 44-seaker Tacotron models on a rosody transfer task.
"
261,2018,"Style Tokens: Unsupervised Style Modeling, Control and Transfer in End-to-End Speech Synthesis",Oral/Poster,"In this work, we roose codeglobal style tokens'' (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end seech synthesis system. The embeddings are trained with no exlicit labels, yet learn to model a large range of acoustic exressiveness. GSTs lead to a rich set of significant results. The soft interretablecodelabels'' they generate can be used to control synthesis in novel ways, such as varying seed and seaking style -- indeendently of the text content. They can also be used for style transfer, relicating the seaking style of a single audio cli across an entire long-form text corus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and seaker identity, roviding a ath towards highly scalable but robust seech synthesis.
"
262,2018,AutoPrognosis: Automated Clinical Prognostic Modeling via Bayesian Optimization with Structured Kernel Learning,Oral/Poster,"Clinical rognostic models derived from largescale healthcare data can inform critical diagnostic and theraeutic decisions. To enable off-theshelf usage of machine learning (ML) in rognostic research, we develoed AUTOPROGNOSIS: a system for automating the design of redictive modeling ielines tailored for clinical rognosis. AUTOPROGNOSIS otimizes ensembles of ieline configurations efficiently using a novel batched Bayesian otimization (BO) algorithm that learns a low-dimensional decomosition of the ielines’ high-dimensional hyerarameter sace in concurrence with the BO rocedure. This is achieved by modeling the ielines’ erformances as a black-box function with a Gaussian rocess rior, and modeling the “similarities” between the ielines’ baseline algorithms via a sarse additive kernel with a Dirichlet rior. Meta-learning is used to warmstart BO with external data from “similar” atient cohorts by calibrating the riors using an algorithm that mimics the emirical Bayes method. The system automatically exlains its redictions by resenting the clinicians with logical association rules that link atients’ features to redicted risk strata. We demonstrate the utility of AUTOPROGNOSIS using 10 major atient cohorts reresenting various asects of cardiovascular atient care.
"
263,2018,TAPAS: Tricks to Accelerate (encrypted) Prediction As a Service,Oral/Poster,"Machine learning methods are widely used for a variety of rediction roblems. Prediction as a service is a aradigm in which service roviders with technological exertise and comutational resources may erform redictions for clients. However, data rivacy severely restricts the alicability of such services, unless measures to kee client data rivate (even from the service rovider) are designed. Equally imortant is to minimize the nature of comutation and amount of communication required between client and server. Fully homomorhic encrytion offers a way out, whereby clients may encryt their data, and on which the server may erform arithmetic comutations. The one drawback of using fully homomorhic encrytion is the amount of time required to evaluate large machine learning models on encryted data. We combine several ideas from the machine learning literature, articularly work on quantization and sarsification of neural networks, together with algorithmic tools to seed-u and arallelize comutation using encryted data.
"
264,2018,End-to-End Learning for the Deep Multivariate Probit Model,Oral/Poster,"The multivariate robit model (MVP) is a oular classic model for studying binary resonses of multile entities.  Nevertheless, the comutational challenge of learning the MVP model, given that its likelihood involves integrating over a multidimensional constrained sace of latent variables, significantly limits its alication in ractice. We roose a flexible dee generalization of the classic MVP, the Dee Multivariate Probit Model (DMVP), which is an end-to-end learning scheme that uses an efficient arallel samling rocess of the multivariate robit model to exloit  GPU-boosted dee neural networks. We resent both theoretical and emirical analysis of the convergence behavior of DMVP's samling rocess with resect to the resolution of the correlation structure. We rovide convergence guarantees for DMVP and our emirical analysis demonstrates the advantages of DMVP's samling comared with standard MCMC-based methods. We also show that when alied to multi-entity modelling roblems, which are natural DMVP alications, DMVP trains faster than classical MVP , by at least an order of magnitude, catures rich correlations among entities, and further imroves the joint likelihood of entities comared with several cometitive models.
"
265,2018,Differentiable Dynamic Programming for Structured Prediction and Attention,Oral/Poster,"Dynamic rogramming (DP) solves a variety of structured combinatorial roblems by iteratively breaking them down into smaller subroblems. In site of their versatility, many DP algorithms are non-differentiable, which hamers their use as a layer in neural networks trained by backroagation. To address this issue, we roose to smooth the max oerator in the dynamic rogramming recursion, using a strongly convex regularizer. This allows to relax both the otimal value and solution of the original combinatorial roblem, and turns a broad class of DP algorithms into differentiable oerators. Theoretically, we rovide a new robabilistic ersective on backroagating through these DP oerators, and relate them to inference in grahical models. We derive two articular instantiations of our framework, a smoothed Viterbi algorithm for sequence rediction and a smoothed DTW algorithm for time-series alignment. We showcase these instantiations on structured rediction (audio-to-score alignment, NER) and on structured and sarse attention for translation.
"
266,2018,Optimal Distributed Learning with Multi-pass Stochastic Gradient Methods,Oral/Poster,"We study generalization roerties of distributed algorithms in the setting of nonarametric regression over a reroducing kernel Hilbert sace (RKHS). We investigate distributed stochastic gradient methods (SGM), with mini-batches and multi-asses over the data. We show that otimal generalization error bounds can be retained for distributed SGM  rovided that the artition level is not too large.  Our results are suerior to the state-of-the-art theory, covering the cases that the regression function may not be in the hyothesis saces. Particularly, our results show that distributed SGM has a smaller theoretical comutational comlexity, comared with distributed kernel ridge regression (KRR) and classic SGM.
"
267,2018,Byzantine-Robust Distributed Learning: Towards Optimal Statistical Rates,Oral/Poster,"In this aer, we develo distributed otimization algorithms that are rovably robust against Byzantine failures---arbitrary and otentially adversarial behavior, in distributed comuting systems, with a focus on achieving otimal statistical erformance. A main result of this work is a shar analysis of two robust distributed gradient descent algorithms based on median and trimmed mean oerations, resectively. We rove statistical error rates for all of strongly convex, non-strongly convex, and smooth non-convex oulation loss functions. In articular, these algorithms are shown to achieve order-otimal statistical error rates for strongly convex losses. To achieve better communication efficiency, we further roose a median-based distributed algorithm that is rovably robust, and uses only one communication round. For strongly convex quadratic loss, we show that this algorithm achieves the same otimal error rate as the robust distributed gradient descent algorithms.
"
268,2018,SQL-Rank: A Listwise Approach to Collaborative Ranking,Oral/Poster,"In this aer, we roose a listwise aroach for constructing user-secific rankings in recommendation systems in a collaborative fashion. We contrast the listwise aroach to revious ointwise and airwise aroaches, which are based on treating either each rating or each airwise comarison as an indeendent instance resectively. By extending the work of ListNet (Cao et al., 2007), we cast listwise collaborative ranking as maximum likelihood under a ermutation model which alies robability mass to ermutations based on a low rank latent score matrix. We resent a novel algorithm called SQL-Rank, which can accommodate ties and missing data and can run in linear time. We develo a theoretical framework for analyzing listwise ranking methods based on a novel reresentation theory for the ermutation model. Alying this framework to collaborative ranking, we derive asymtotic statistical rates as the number of users and items grow together. We conclude by demonstrating that our SQL-Rank method often outerforms current state-of-the-art algorithms for imlicit feedback such as Weighted-MF and BPR and achieve favorable results when comared to exlicit feedback algorithms such as matrix factorization and collaborative ranking.
"
269,2018,Extreme Learning to Rank via Low Rank Assumption,Oral/Poster,"We consider the setting where we wish to erform ranking for hundreds of thousands of users which is common in recommender systems and web search ranking. Learning a single ranking function is unlikely to cature the variability across all users while learning a ranking function for each erson is time-consuming and requires large amounts of data from each user. To address this situation, we roose a Factorization RankSVM algorithm which learns a series of k basic ranking functions and then constructs for each user a local ranking function that is a combination of them. We develo a fast algorithm to reduce the time comlexity of gradient descent solver by exloiting the low-rank structure, and the resulting algorithm is much faster than existing methods. Furthermore, we rove that the generalization error of the roosed method can be significantly better than training individual RankSVMs. Finally, we resent some interesting atterns in the rincial ranking functions learned by our algorithms.
"
270,2018,Adversarial Attack on Graph Structured Data,Oral/Poster,"Dee learning on grah structures has shown exciting results in various alications. However, few attentions have been aid to the robustness of such models, in contrast to numerous research work for image or text adversarial attack and defense. In this aer, we focus on the adversarial attacks that fool dee learning models by modifying the combinatorial structure of data. We first roose a reinforcement learning based attack method that learns the generalizable attack olicy, while only requiring rediction labels from the target classifier. We further roose attack methods based on genetic algorithms and gradient descent in the scenario where additional rediction confidence or gradients are available.  We use both synthetic and real-world data to show that, a family of Grah Neural Network models are vulnerable to these attacks, in both grah-level and node-level classification tasks. We also show such attacks can be used to diagnose the learned classifiers.
"
271,2018,Reinforcing Adversarial Robustness using Model Confidence Induced by Adversarial Training,Oral/Poster,"In this aer we study leveraging \emh{confidence information} induced by adversarial training to reinforce adversarial robustness of a given adversarially trained model. A natural measure of confidence is $\|F(\bfx)\|_\infty$ (i.e. how confident $F$ is about its rediction?). We start by analyzing an adversarial training formulation roosed by Madry et al.. We demonstrate that, under a variety of instantiations, an only somewhat good solution to their objective induces confidence to be a discriminator, which can distinguish between right and wrong model redictions in a neighborhood of a oint samled from the underlying distribution. Based on this, we roose Highly Confident Near Neighbor ($\HCNN$), a framework that combines confidence information and nearest neighbor search, to reinforce adversarial robustness of a base model. We give algorithms in this framework and erform a detailed emirical study. We reort encouraging exerimental results that suort our analysis, and also discuss roblems we observed with existing adversarial training."
272,2018,Closed-form Marginal Likelihood in Gamma-Poisson Matrix Factorization,Oral/Poster,"We resent novel understandings of the Gamma-Poisson (GaP) model, a robabilistic matrix factorization model for count data. We show that GaP can be rewritten free of the scoreactivation matrix. This gives us new insights about the estimation of the toicdictionary matrix by maximum marginal likelihood estimation. In articular, this exlains the robustness of this estimator to over-secified values of the factorization rank, esecially its ability to automatically rune irrelevant dictionary columns, as emirically observed in revious work. The marginalization of the activation matrix leads in turn to a new Monte Carlo Exectation-Maximization algorithm with favorable roerties.
"
273,2018,Learning Binary Latent Variable Models: A Tensor Eigenpair Approach,Oral/Poster,"Latent variable models with hidden binary units aear in various alications. Learning such models, in articular in the resence of noise, is a challenging comutational roblem. In this aer we roose a novel sectral aroach to this roblem, based on the eigenvectors of both the second order moment matrix and third order moment tensor of the observed data. We rove that under mild non-degeneracy conditions, our method consistently estimates the model arameters at the otimal arametric rate. Our tensor-based method generalizes revious orthogonal tensor decomosition aroaches, where the hidden units were assumed to be either statistically indeendent or mutually exclusive. We illustrate the consistency of our method on simulated data and demonstrate its usefulness in learning a common model for oulation mixtures in genetics.
"
274,2018,Thompson Sampling for Combinatorial Semi-Bandits,Oral/Poster,"We study the alication of the Thomson samling (TS) methodology to the stochastic combinatorial multi-armed bandit (CMAB) framework. We analyze the standard TS algorithm for the general CMAB, and obtain the first distribution-deendent regret bound of $O(m\log T  \Delta_{\min}) $ for TS under general CMAB, where $m$ is the number of arms, $T$ is the time horizon, and $\Delta_{\min}$ is the minimum ga between the exected reward of the otimal solution and any non-otimal solution. We also show that one cannot use an aroximate oracle in TS algorithm for even MAB roblems. Then we exand the analysis to matroid bandit, a secial case of CMAB and for which we could remove the indeendence assumtion across arms and achieve a better regret bound. Finally, we use some exeriments to show the comarison of regrets of CUCB and CTS algorithms."
275,2018,Let’s be Honest: An Optimal No-Regret Framework for Zero-Sum Games,Oral/Poster,"We revisit the roblem of solving two-layer zero-sum games in the decentralized setting. We roose a simle algorithmic framework that simultaneously achieves the best rates for honest regret as well as adversarial regret, and in addition resolves the oen roblem of removing the logarithmic terms in convergence to the value of the game. We achieve this goal in three stes. First, we rovide a novel analysis of the otimistic mirror descent (OMD), showing that it can be modified to guarantee fast convergence for both honest regret and value of the game, when the layers are laying collaboratively. Second, we roose a new algorithm, dubbed as robust otimistic mirror descent (ROMD), which attains otimal adversarial regret without knowing the time horizon beforehand. Finally, we roose a simle signaling scheme, which enables us to bridge OMD and ROMD to achieve the best of both worlds. Numerical examles are resented to suort our theoretical claims and show that our non-adative ROMD algorithm can be cometitive to OMD with adative ste-size selection.
"
276,2018,Deep Asymmetric Multi-task Feature Learning,Oral/Poster,"We roose Dee Asymmetric Multitask Feature Learning (Dee-AMTFL) which can learn dee reresentations shared across multile tasks while effectively reventing negative transfer that may haen in the feature sharing rocess. Secifically, we introduce an asymmetric autoencoder term that allows reliable redictors for the easy tasks to have high contribution to the feature learning while suressing the influences of unreliable redictors for more difficult tasks. This allows the learning of less noisy reresentations, and enables unreliable redictors to exloit knowledge from the reliable redictors via the shared latent features. Such asymmetric knowledge transfer through shared features is also more scalable and efficient than inter-task asymmetric transfer. We validate our Dee-AMTFL model on multile benchmark datasets for multitask learning and image classification, on which it significantly outerforms existing symmetric and asymmetric multitask learning models, by effectively reventing negative transfer in dee feature learning.
"
277,2018,Learn from Your Neighbor: Learning Multi-modal Mappings from Sparse Annotations,Oral/Poster,"Many structured rediction roblems (articularly in vision and language domains) are ambiguous, with multile oututs being ‘correct’ for an inut – e.g. there are many ways of describing an image, multile ways of translating a sentence; however, exhaustively annotating the alicability of all ossible oututs is intractable due to exonentially large outut saces (e.g. all English sentences). In ractice, these roblems are cast as multi-class rediction, with the likelihood of only a sarse set of annotations being maximized – unfortunately enalizing for lacing beliefs on lausible but unannotated oututs. We make and test the following hyothesis – for a given inut, the annotations of its neighbors may serve as an additional suervisory signal. Secifically, we roose an objective that transfers suervision from neighboring examles. We first study the roerties of our develoed method in a controlled toy setu before reorting results on multi-label classification and two image-grounded sequence modeling tasks – cationing and question generation. We evaluate using standard task-secific metrics and measures of outut diversity, finding consistent imrovements over standard maximum likelihood training and other baselines.
"
278,2018,Stein Variational Message Passing for Continuous Graphical Models,Oral/Poster,"We roose a novel distributed inference algorithm for continuous grahical models,  by extending Stein variational gradient descent (SVGD) to leverage the Markov deendency structure of the distribution of interest. Our aroach combines SVGD with a set of structured local kernel functions defined on the Markov blanket of each node, which alleviates the curse of high dimensionality and simultaneously yields a distributed algorithm for decentralized inference tasks. We justify our method with theoretical analysis and show that the use of local kernels can be viewed as a new tye of localized aroximation that matches the target distribution on the conditional distributions of each node over its Markov blanket. Our emirical results show that our method outerforms a variety of baselines including standard MCMC and article message assing methods.
"
279,2018,Discrete-Continuous Mixtures in Probabilistic Programming: Generalized Semantics and Inference Algorithms,Oral/Poster,"Desite the recent successes of robabilistic rogramming languages (PPLs) in AI alications, PPLs offer only limited suort for random variables whose distributions combine discrete and continuous elements. We develo the notion of measure-theoretic Bayesian networks (MTBNs) and use it to rovide more general semantics for PPLs with arbitrarily many random variables defined over arbitrary measure saces. We develo two new general samling algorithms that are rovably correct under the MTBN framework: the lexicograhic likelihood weighting (LLW) for general MTBNs and the lexicograhic article filter (LPF), a secialized algorithm for state-sace models. We further integrate MTBNs into a widely used PPL system, BLOG, and verify the effectiveness of the new inference algorithms through reresentative examles.
"
280,2018,Towards Binary-Valued Gates for Robust LSTM Training,Oral/Poster,"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. It aims to use gates to control information flow (e.g., whether to ski some information or not) in the recurrent comutations, although its ractical imlementation based on soft gates only artially achieves this goal. In this aer, we roose a new way for LSTM training, which ushes the outut values of the gates towards 0 or 1. By doing so, we can better control the information flow: the gates are mostly oen or closed, instead of in a middle state, which makes the results more interretable. Emirical studies show that (1) Although it seems that we restrict the model caacity, there is no erformance dro: we achieve better or comarable erformances due to its better generalization ability; (2) The oututs of gates are not sensitive to their inuts: we can easily comress the LSTM unit in multile ways, e.g., low-rank aroximation and low-recision aroximation. The comressed models are even better than the baseline models without comression.
"
281,2018,Fitting New Speakers Based on a Short Untranscribed Sample,Oral/Poster,"Learning-based Text To Seech systems have the otential to generalize from one seaker to the next and thus require a relatively short samle of any new voice. However, this romise is currently largely unrealized. We resent a method that is designed to cature a new seaker from a short untranscribed audio samle. This is done by emloying an additional network that given an audio samle, laces the seaker in the embedding sace. This network is trained as art of the seech synthesis system using various consistency losses. Our results demonstrate a greatly  imroved erformance on both the dataset seakers, and, more imortantly, when fitting new voices, even from very short samles.
"
282,2018,Stochastic Variance-Reduced Policy Gradient,Oral/Poster,"In this aer, we roose a novel reinforcement-learning algorithm consisting in a stochastic variance-reduced version of olicy gradient for solving Markov Decision Processes (MDPs).         Stochastic variance-reduced gradient (SVRG) methods have roven to be very successful in suervised learning.         However, their adatation to olicy gradient is not straightforward and needs to account for I) a non-concave objective function; II) aroximations in the full gradient comutation; and III) a non-stationary samling rocess.         The result is SVRPG, a stochastic variance-reduced olicy gradient algorithm that leverages on imortance weights to reserve the unbiasedness of the gradient estimate.         Under standard assumtions on the MDP, we rovide convergence guarantees for SVRPG with a convergence rate that is linear under increasing batch sizes.         Finally, we suggest ractical variants of SVRPG, and we emirically evaluate them on continuous MDPs.
"
283,2018,Convergent Tree Backup and Retrace with Function Approximation,Oral/Poster,"Off-olicy learning is key to scaling u reinforcement learning as it allows to learn about a target olicy from the exerience generated by a different behavior olicy. Unfortunately, it has been challenging to combine off-olicy learning with function aroximation and multi-ste bootstraing in a way that leads to both stable and efficient algorithms. In this work, we show that the Tree Backu and Retrace algorithms are unstable with linear function aroximation, both in theory and in ractice with secific examles. Based on our analysis, we then derive stable and efficient gradient-based algorithms using a quadratic convex-concave saddle-oint formulation. By exloiting the roblem structure roer to these algorithms, we are able to rovide convergence guarantees and finite-samle bounds. The alicability of our new analysis also goes beyond Tree Backu and Retrace and allows us to rovide new convergence rates for the GTD and GTD2 algorithms without having recourse to rojections or Polyak averaging.
"
284,2018,Alternating Randomized Block Coordinate Descent,Oral/Poster,"Block-coordinate descent algorithms and alternating minimization methods are fundamental otimization algorithms and an imortant rimitive in large-scale otimization and machine learning. While various block-coordinate-descent-tye methods have been studied extensively, only alternating minimization -- which alies to the setting of only two blocks -- is known to have convergence time that scales indeendently of the least smooth block. A natural question is then: is the setting of two blocks secial? We show that the answer is ``no'' as long as the least smooth block can be otimized exactly -- an assumtion that is also needed in the setting of alternating minimization. We do so by introducing a novel algorithm AR-BCD, whose convergence time scales indeendently of the least smooth (ossibly non-smooth) block. The basic algorithm generalizes both alternating minimization and randomized block coordinate (gradient) descent, and we also rovide its accelerated version -- AAR-BCD.
"
285,2018,Shampoo: Preconditioned Stochastic Tensor Optimization,Oral/Poster,"Preconditioned gradient methods are among the most general and owerful tools in otimization. However, reconditioning requires storing and maniulating rohibitively large matrices. We describe and analyze a new structure-aware reconditioning algorithm, called Shamoo, for stochastic otimization over tensor saces. Shamoo maintains a set of reconditioning matrices, each of which oerates on a single dimension, contracting over the remaining dimensions. We establish convergence guarantees in the stochastic convex setting, the roof of which builds uon matrix trace inequalities.  Our exeriments with state-of-the-art dee learning models show that Shamoo is caable of converging considerably faster than commonly used otimizers. Surrisingly, although it involves a more comlex udate rule,  Shamoo's runtime er ste is comarable in ractice to that of simle gradient methods such as SGD, AdaGrad, and Adam.
"
286,2018,Stochastic Wasserstein Barycenters,Oral/Poster,"We resent a stochastic algorithm to comute the barycenter of a set of robability distributions under the Wasserstein metric from otimal transort. Unlike revious aroaches, our method extends to continuous inut distributions and allows the suort of the barycenter to be adjusted in each iteration. We tackle the roblem without regularization, allowing us to recover a shar outut whose suort is contained within the suort of the true barycenter. We give examles where our algorithm recovers a more meaningful barycenter than revious work. Our method is versatile and can be extended to alications such as generating suer samles from a given distribution and recovering blue noise aroximations.
"
287,2018,Accelerating Natural Gradient with Higher-Order Invariance,Oral/Poster,"An aealing roerty of the natural gradient is that it is invariant to arbitrary differentiable rearameterizations of the model. However, this invariance roerty requires infinitesimal stes and is lost in ractical imlementations with small but finite ste sizes. In this aer, we study invariance roerties from a combined ersective of Riemannian geometry and numerical differential equation solving. We define the order of invariance of a numerical method to be its convergence order to an invariant solution. We roose to use higher-order integrators and geodesic corrections to obtain more invariant otimization trajectories. We rove the numerical convergence roerties of geodesic corrected udates and show that they can be as comutational efficient as lain natural gradient. Exerimentally, we demonstrate that invariance leads to faster otimization and our techniques imrove on traditional natural gradient in dee neural network training and natural olicy gradient for reinforcement learning.
"
288,2018,Learning unknown ODE models with Gaussian processes,Oral/Poster,"In conventional ODE modelling coefficients of an equation driving the system state forward in time are estimated. However, for many comlex systems it is ractically imossible to determine the equations or interactions governing the underlying dynamics. In these settings, arametric ODE model cannot be formulated. Here, we overcome this issue by introducing a novel aradigm of nonarametric ODE modelling that can learn the underlying dynamics of arbitrary continuous-time systems without rior knowledge. We roose to learn non-linear, unknown differential functions from state observations using Gaussian rocess vector fields within the exact ODE formalism. We demonstrate the model's caabilities to infer dynamics from sarse data and to simulate the system forward into future.
"
289,2018,Constraining the Dynamics of Deep Probabilistic Models,Oral/Poster,"We introduce a novel generative formulation of dee robabilistic models imlementing ""soft"" constraints on their function dynamics. In articular, we develo a flexible methodological framework where the modeled functions and derivatives of a given order are subject to inequality or equality constraints. We then characterize the osterior distribution over model and constraint arameters through stochastic variational inference. As a result, the roosed aroach allows for accurate and scalable uncertainty quantification on the redictions and on all arameters. We demonstrate the alication of equality constraints in the challenging roblem of arameter inference in ordinary differential equation models, while we showcase the alication of inequality constraints on the roblem of monotonic regression of count data. The roosed aroach is extensively tested in several exerimental settings, leading to highly cometitive results in challenging modeling alications, while offering high exressiveness, flexibility and scalability.
"
290,2018,Fast Decoding in Sequence Models Using Discrete Latent Variables,Oral/Poster,"Autoregressive sequence models based on dee neural networks, such as RNNs, Wavenet and Transformer are the state-of-the-art on many tasks. However, they lack arallelism and are thus slow for long sequences. RNNs lack arallelism both during training and decoding, while architectures like WaveNet and Transformer are much more arallel during training, but still lack arallelism during decoding.  We resent a method to extend sequence models using  discrete latent variables that makes decoding much more arallel. The main idea behind this aroach is to first autoencode the target sequence into a shorter discrete latent sequence, which is generated autoregressively, and finally decode the full sequence from this shorter latent sequence in a arallel manner. To this end, we introduce a new method for constructing discrete latent variables and comare it with reviously introduced methods. Finally, we verify that our model works on the task of neural machine translation, where our models are an order of magnitude faster than comarable autoregressive models and, while lower in BLEU than urely autoregressive models, better than reviously roosed non-autogregressive translation.
"
291,2018,High Performance Zero-Memory Overhead Direct Convolutions,Oral/Poster,"The comutation of convolution layers in dee neural networks tyically rely on high erformance routines that  trade sace for time by using additional memory (either for acking uroses or required as art of the algorithm) to imrove erformance.  The roblems with such an aroach are two-fold. First, these routines incur additional memory overhead which reduces the overall size of the network that can fit on embedded devices with limited memory caacity. Second,  these high erformance routines were not otimized for erforming convolution,  which means that the erformance obtained is  usually less than conventionally exected. In this aer, we demonstrate that direct convolution, when imlemented correctly, eliminates all memory overhead, and yields erformance that is between 10% to 400% times better than existing high erformance imlementations of convolution layers on conventional and embedded CPU architectures. We also show that a high erformance direct convolution exhibits better scaling erformance, i.e. suffers less erformance dro, when increasing the number of threads.
"
292,2018,Approximate Leave-One-Out for Fast Parameter Tuning in High Dimensions,Oral/Poster,"We study the arameter tuning roblem for the enalized regression model. Finding the otimal choice of the regularization arameter is a challenging roblem in high-dimensional regimes where both the number of observations n and the number of arameters  are large. We roose two frameworks to obtain a comutationally efficient aroximation ALO of the leave-one-out cross validation (LOOCV) risk for nonsmooth losses and regularizers. Our two frameworks are based on the rimal and dual formulations of the enalized regression model. We rove the equivalence of the two aroaches under smoothness conditions. This equivalence enables us to justify the accuracy of both methods under such conditions. We use our aroaches to obtain a risk estimate for several standard roblems, including generalized LASSO, nuclear norm regularization and suort vector machines. We exerimentally demonstrate the effectiveness of our results for non-differentiable cases.
"
293,2018,Improved large-scale graph learning through ridge spectral sparsification,Oral/Poster,"The reresentation and learning benefits of methods based on grah Lalacians, such as Lalacian smoothing or harmonic function solution for semi-suervised learning (SSL),  are emirically and theoretically well suorted. Nonetheless, the exact versions of these methods scale oorly with the number of nodes $n$ of the grah. In this aer, we combine a sectral sarsification routine with Lalacian learning. Given a grah $G$ as inut, our algorithm comutes a sarsifier in a distributed way in $O(n\log^3(n))$ time, $O(m\log^3(n))$ work and $O(n\log(n))$ memory, using only $\log(n)$ rounds of communication. Furthermore, motivated by the regularization often emloyed in learning algorithms, we show that constructing sarsifiers that reserve the sectrum of the Lalacian only u to the regularization level may drastically reduce the size of the final grah. By constructing a sectrally-similar grah, we are able to bound the error induced by the sarsification for a variety of downstream tasks (e.g., SSL). We emirically validate the theoretical guarantees on Amazon co-urchase grah and comare to the state-of-the-art heuristics."
294,2018,Distilling the Posterior in Bayesian Neural Networks,Oral/Poster,"In many alications of dee learning, it is crucial to cature model and rediction uncertainty. Unlike classic neural networks (NN), Bayesian neural networks (BNN) allow us to reason about uncertainty in a more rinciled way.  Stochastic Gradient Langevin Dynamics (SGLD) enables learning a BNN with only simle modifications to the standard otimization framework (SGD). Instead of obtaining a single oint-estimate of the model, the result of SGLD is samles from the BNN osterior. However, SGLD and its extensions require storage of the entire history of model arameters, a otentially rohibitive cost (esecially for large neural networks). We roose a framework, Adversarial Posterior Distillation, to distill the SGLD samles using Generative Adversarial Networks (GAN). At test-time, samles are generated by the GAN. We show that this distillation framework incurs no loss in erformance on recent BNN alications including anomaly detection, active learning, and defense against attacks. By construction, our framework not only distills the Bayesian redictive distribution, but the osterior itself.  This allows users to comute quantity such as the aroximate model variance, which is useful in the downstream tasks.
"
295,2018,Scalable approximate Bayesian inference for particle tracking data,Oral/Poster,"Many imortant datasets in hysics, chemistry, and biology consist of noisy sequences of images of multile moving overlaing articles. In many cases, the observed articles are indistinguishable, leading to unavoidable uncertainty about nearby articles’ identities. Exact Bayesian inference is intractable in this setting, and revious aroximate Bayesian methods scale oorly. Non-Bayesian aroaches that outut a single “best” estimate of the article tracks (thus discarding imortant uncertainty information) are therefore dominant in ractice. Here we roose a flexible and scalable amortized aroach for Bayesian inference on this task. We introduce a novel neural network method to aroximate the (intractable) filter-backward-samle-forward algorithm for Bayesian inference in this setting. By varying the simulated training data for the network, we can erform inference on a wide variety of data tyes. This aroach is therefore highly flexible and imroves on the state of the art in terms of accuracy; rovides uncertainty estimates about the article locations and identities; and has a test run-time that scales linearly as a function of the data length and number of articles, thus enabling Bayesian inference in arbitrarily large article tracking datasets.
"
296,2018,Weakly Consistent Optimal Pricing Algorithms in Repeated Posted-Price Auctions with Strategic Buyer,Oral/Poster,We study revenue otimization learning algorithms for reeated osted-rice auctions where a seller interacts with a single strategic buyer that holds a fixed rivate valuation for a good and seeks to maximize his cumulative discounted surlus. We roose a novel algorithm that never decreases offered rices and has  a tight strategic regret bound of $\Theta(\log\log T)$. This result closes the oen research question on the existence of a no-regret horizon-indeendent weakly consistent ricing. We also show that the roerty of non-decreasing rices is nearly necessary for a weakly consistent algorithm to be a no-regret one.
297,2018,Practical Contextual Bandits with Regression Oracles,Oral/Poster,"A major challenge in contextual bandits is to design general-urose algorithms that are both ractically useful and theoretically well-founded. We resent a new technique that has the emirical and comutational advantages of realizability-based aroaches combined with the flexibility of agnostic methods. Our algorithms leverage the availability of a regression oracle for the value-function class, a more realistic and reasonable oracle than the classification oracles over olicies tyically assumed by agnostic methods. Our aroach generalizes both UCB and LinUCB to far more exressive ossible model classes and achieves low regret under certain distributional assumtions. In an extensive emirical evaluation, we find that our aroach tyically matches or outerforms both realizability-based and agnostic baselines.
"
298,2018,Stochastic Variance-Reduced Hamilton Monte Carlo Methods,Oral/Poster,"We roose a fast stochastic Hamilton Monte Carlo (HMC) method, for samling from a smooth and strongly log-concave distribution. At the core of our roosed method is a variance reduction technique insired by the recent advance in stochastic otimization. We show that, to achieve $\esilon$ accuracy in 2-Wasserstein distance, our algorithm achieves $\tilde O\big(n+\kaa^{2}d^{12}\esilon+\kaa^{43}d^{13}n^{23}\esilon^{23}\big)$ gradient comlexity (i.e., number of comonent gradient evaluations), which outerforms the state-of-the-art HMC and stochastic gradient HMC methods in a wide regime. We also extend our algorithm for samling from smooth and general log-concave distributions, and rove the corresonding gradient comlexity as well. Exeriments on both synthetic and real data demonstrate the suerior erformance of our algorithm."
299,2018,Asynchronous Stochastic Quasi-Newton MCMC for Non-Convex Optimization,Oral/Poster,"Recent studies have illustrated that stochastic gradient Markov Chain Monte Carlo techniques have a strong otential in non-convex otimization, where local and global convergence guarantees can be shown under certain conditions. By building u on this recent theory, in this study, we develo an asynchronous-arallel stochastic L-BFGS algorithm for non-convex otimization. The roosed algorithm is suitable for both distributed and shared-memory settings. We rovide formal theoretical analysis and show that the roosed method achieves an ergodic convergence rate of ${\cal O}(1\sqrt{N})$ ($N$ being the total number of iterations) and it can achieve a linear seedu under certain conditions. We erform several exeriments on both synthetic and real datasets. The results suort our theory and show that the roosed algorithm rovides a significant seedu over the recently roosed synchronous distributed L-BFGS algorithm."
300,2018,GAIN: Missing Data Imputation using Generative Adversarial Nets,Oral/Poster,"We roose a novel method for imuting missing data by adating the well-known Generative Adversarial Nets (GAN) framework. Accordingly, we call our method Generative Adversarial Imutation Nets (GAIN). The generator (G) observes some comonents of a real data vector, imutes the missing comonents conditioned on what is actually observed, and oututs a comleted vector. The discriminator (D) then takes a comleted vector and attemts to determine which comonents were actually observed and which were imuted. To ensure that D forces G to learn the desired distribution, we rovide D with some additional information in the form of a hint vector. The hint reveals to D artial information about the missingness of the original samle, which is used by D to focus its attention on the imutation quality of articular comonents. This hint ensures that G does in fact learn to generate according to the true data distribution. We tested our method on various datasets and found that GAIN significantly outerforms state-of-the-art imutation methods.
"
301,2018,Synthesizing Programs for Images using Reinforced Adversarial Learning,Oral/Poster,"Advances in dee generative networks have led to imressive results in recent years. Nevertheless, such models can often waste their caacity on the minutiae of datasets, resumably due to weak inductive biases in their decoders. This is where grahics engines may come in handy since they abstract away low-level details and reresent images as high-level rograms. Current methods that combine dee learning and renderers are limited by hand-crafted likelihood or distance functions, a need for large amounts of suervision, or difficulties in scaling their inference algorithms to richer datasets. To mitigate these issues, we resent SPIRAL, an adversarially trained agent that generates a rogram which is executed by a grahics engine to interret and samle images. The goal of this agent is to fool a discriminator network that distinguishes between real and rendered data, trained with a distributed reinforcement learning setu without any suervision. A surrising finding is that using the discriminator's outut as a reward signal is the key to allow the agent to make meaningful rogress at matching the desired outut rendering. To the best of our knowledge, this is the first demonstration of an end-to-end, unsuervised and adversarial inverse grahics agent on challenging real world (MNIST, Omniglot, CelebA) and synthetic 3D datasets. A video of the agent can be found at htts:youtu.beiSyvwAwa7vk.
"
302,2018,Geometry Score: A Method For Comparing Generative Adversarial Networks,Oral/Poster,"One of the biggest challenges in the research of generative adversarial networks (GANs) is assessing the quality of generated samles and detecting various levels of mode collase. In this work, we construct a novel measure of erformance of a GAN by comaring geometrical roerties of the underlying data manifold and the generated one, which rovides both qualitative and quantitative means for evaluation. Our algorithm can be alied to datasets of an arbitrary nature and is not limited to visual data. We test the obtained metric on various real-life models and datasets and demonstrate that our method rovides new insights into roerties of GANs.
"
303,2018,Addressing Function Approximation Error in Actor-Critic Methods,Oral/Poster,"In value-based reinforcement learning methods such as dee Q-learning, function aroximation errors are known to lead  to overestimated value estimates and subotimal olicies. We show that this roblem ersists in an actor-critic setting and roose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a air of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying olicy udates to reduce er-udate error and further imrove erformance. We evaluate our method on the suite of OenAI gym tasks, outerforming the state of the art in every environment tested.
"
304,2018,Fast Bellman Updates for Robust MDPs,Oral/Poster,"We describe two efficient, and exact, algorithms for comuting Bellman udates in robust Markov decision rocesses (MDPs). The first algorithm uses a homotoy continuation method to comute udates for L1-constrained s,a-rectangular ambiguity sets. It runs in quasi-linear time for lain L1-norms and also generalizes to weighted L1-norms. The second algorithm uses bisection to comute udates for robust MDPs with s-rectangular ambiguity sets. This algorithm, when combined with the homotoy method, also has a quasi-linear runtime. Unlike revious methods, our algorithms comute the rimal solution in addition to the otimal objective value, which makes them useful in olicy iteration methods. Our exerimental results indicate that the roosed methods are over 1,000 times faster than Gurobi, a state-of-the-art commercial otimization ackage, for small instances, and the erformance ga grows considerably with roblem size.
"
305,2018,Configurable Markov Decision Processes,Oral/Poster,"In many real-world roblems, there is the ossibility to configure, to a limited extent, some environmental arameters to imrove the erformance of a learning agent. In this aer, we roose a novel framework, Configurable Markov Decision Processes (Conf-MDPs), to model this new tye of interaction with the environment. Furthermore, we rovide a new learning algorithm, Safe Policy-Model Iteration (SPMI), to jointly and adatively otimize the olicy and the environment configuration. After having introduced our aroach and derived some theoretical results, we resent the exerimental evaluation in two exlicative roblems to show the benefits of the environment configurability on the erformance of the learned olicy.
"
306,2018,Prediction Rule Reshaping,Oral/Poster,"Two methods are roosed for high-dimensional shae-constrained regression and classification. These methods reshae re-trained rediction rules to satisfy shae constraints like monotonicity and convexity. The first method can be alied to any re-trained rediction rule, while the second method deals secifically with random forests. In both cases, efficient algorithms are develoed for comuting the estimators, and exeriments are erformed to demonstrate their erformance on four datasets. We find that reshaing methods enforce shae constraints without comromising redictive accuracy.
"
307,2018,Dimensionality-Driven Learning with Noisy Labels,Oral/Poster,"Datasets with significant roortions of noisy (incorrect) class labels resent challenges for training accurate Dee Neural Networks (DNNs). We roose a new ersective for understanding DNN generalization for such datasets, by investigating the dimensionality of the dee reresentation subsace of training samles. We show that from a dimensionality ersective, DNNs exhibit quite distinctive learning styles when trained with clean labels versus when trained with a roortion of noisy labels. Based on this finding, we develo a new dimensionality-driven learning strategy, which monitors the dimensionality of subsaces during training and adats the loss function accordingly. We emirically demonstrate that our aroach is highly tolerant to significant roortions of noisy labels, and can effectively learn low-dimensional local subsaces that cature the data distribution.
"
308,2018,Learning Memory Access Patterns,Oral/Poster,"The exlosion in workload comlexity and the recent slow-down in Moore's law scaling call for new aroaches towards efficient comuting. Researchers are now beginning to use recent advances in machine learning in software otimizations; augmenting or relacing traditional heuristics and data structures. However, the sace of machine learning for comuter hardware architecture is only lightly exlored. In this aer, we demonstrate the otential of dee learning to address the von Neumann bottleneck of memory erformance. We focus on the critical roblem of learning memory access atterns, with the goal of constructing accurate and efficient memory refetchers. We relate contemorary refetching strategies to n-gram models in natural language rocessing, and show how recurrent neural networks can serve as a dro-in relacement. On a suite of challenging benchmark datasets, we find that neural networks consistently demonstrate suerior erformance in terms of recision and recall. This work reresents the first ste towards ractical neural-network based refetching, and oens a wide range of exciting directions for machine learning in comuter architecture research.
"
309,2018,Geodesic Convolutional Shape Optimization,Oral/Poster,"Aerodynamic shae otimization has many industrial alications. Existing methods, however, are so comutationally demanding that tyical engineering ractices are to either simly try a limited number of hand-designed shaes or restrict oneself to shaes that can be arameterized using only few degrees of freedom. In this work, we introduce a new way to otimize comlex shaes fast and accurately. To this end, we train Geodesic Convolutional Neural Networks to emulate a fluidynamics simulator. The key to making this aroach ractical is remeshing the original shae using a oly-cube ma, which makes it ossible to erform the comutations on GPUs instead of CPUs.  The neural net is then used to formulate an objective function that is differentiable with resect to the shae arameters, which can then be otimized using a gradient-based technique.  This outerforms state-of-the-art methods by 5 to 20\% for standard roblems and, even more imortantly, our aroach alies to cases that revious methods cannot handle.
"
310,2018,Visualizing and Understanding Atari Agents,Oral/Poster,"While dee reinforcement learning (dee RL) agents are effective at maximizing rewards, it is often unclear what strategies they use to do so. In this aer, we take a ste toward exlaining dee RL agents through a case study using Atari 2600 environments. In articular, we focus on using saliency mas to understand how an agent learns and executes a olicy. We introduce a method for generating useful saliency mas and use it to show 1) what strong agents attend to, 2) whether agents are making decisions for the right or wrong reasons, and 3) how agents evolve during learning. We also test our method on non-exert human subjects and find that it imroves their ability to reason about these agents. Overall, our results show that saliency information can rovide significant insight into an RL agent's decisions and learning behavior.
"
311,2018,"An Efficient, Generalized Bellman Update For Cooperative Inverse Reinforcement Learning",Oral/Poster,"Our goal is for AI systems to correctly identify and act according to their human user's objectives. Cooerative Inverse Reinforcement Learning (CIRL) formalizes this value alignment roblem as a two-layer game between a human and robot, in which only the human knows the arameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action sace size exonential in the size of the reward arameter sace. In this work, we exloit a secific roerty of CIRL: the human is a full information agent. This enables us to derive an otimality-reserving modification to the standard Bellman udate, which reduces the comlexity of the roblem by an exonential factor. Additionally, we show that our modified Bellman udate allows us to relax CIRL's assumtion of human rationality. We aly this udate to a variety of POMDP solvers, including exact methods, oint-based methods, and Monte Carlo Tree Search methods. We find that it enables us to scale CIRL to non-trivial roblems, with larger reward arameter saces, and larger action saces for both robot and human. In solutions to these larger roblems, the human exhibits edagogical (teaching) behavior, while the robot interrets it as such and attains higher value for the human.
"
312,2018,Is Generator Conditioning Causally Related to GAN Performance?,Oral/Poster,"Recent work suggests that controlling the entire distribution of Jacobian singular values is an imortant design consideration in dee learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks. We find that this Jacobian generally becomes ill-conditioned at the beginning of training. Moreover, we find that the average (across the latent sace) conditioning of the generator is highly redictive of two other ad-hoc metrics for measuring the “quality” of trained GANs: the Incetion Score and the Frechet Incetion Distance. We then test the hyothesis that this relationshi is causal by roosing a “regularization” technique (called Jacobian Claming) that softly enalizes the condition number of the generator Jacobian. Jacobian Claming imroves the mean score for nearly all datasets on which we tested it. It also greatly reduces inter-run variance of the aforementioned scores, addressing (at least artially) one of the main criticisms of GANs.
"
313,2018,K-Beam Minimax: Efficient Optimization for Deep Adversarial Learning,Oral/Poster,"Minimax otimization lays a key role in adversarial training of machine learning algorithms, such as learning generative models, domain adatation, rivacy reservation, and robust learning. In this aer, we demonstrate the failure of alternating gradient descent in minimax otimization roblems due to the discontinuity of solutions of the inner maximization.  To address this, we roose a new $\esilon$-subgradient descent algorithm that addresses this roblem by simultaneously tracking $K$ candidate solutions. Practically, the algorithm can find solutions that revious saddle-oint algorithms cannot find, with only a sublinear increase of comlexity in $K$.  We analyze the conditions under which the algorithm converges to the true solution in detail.  A significant imrovement in stability and convergence seed of the algorithm is observed in simle reresentative roblems, GAN training, and domain-adatation roblems."
314,2018,Inductive Two-Layer Modeling with Parametric Bregman Transfer,Oral/Poster,"Latent rediction models, exemlified by multi-layer networks, emloy hidden variables that automate abstract feature discovery. They tyically ose nonconvex otimization roblems and effective semi-definite rogramming (SDP) relaxations have been develoed to enable global solutions (Aslan et al., 2014).However, these models rely on nonarametric training of layer-wise kernel reresentations, and are therefore restricted to transductive learning which slows down test rediction. In this aer, we develo a new inductive learning framework for arametric transfer functions using matching losses. The result for ReLU utilizes comletely ositive matrices, and the inductive learner not only delivers suerior accuracy but also offers an order of magnitude seedu over SDP with constant aroximation guarantees.
"
315,2018,Does Distributionally Robust Supervised Learning Give Robust Classifiers?,Oral/Poster,"Distributionally Robust Suervised Learning (DRSL) is necessary for building reliable machine learning systems. When machine learning is deloyed in the real world, its erformance can be significantly degraded because test data may follow a different distribution from training data. DRSL with f-divergences exlicitly considers the worst-case distribution shift by minimizing the adversarially reweighted training loss. In this aer, we analyze this DRSL, focusing on the classification scenario. Since the DRSL is exlicitly formulated for a distribution shift scenario, we naturally exect it to give a robust classifier that can aggressively handle shifted distributions. However, surrisingly, we rove that the DRSL just ends u giving a classifier that exactly fits the given training distribution, which is too essimistic. This essimism comes from two sources: the articular losses used in classification and the fact that the variety of distributions to which the DRSL tries to be robust is too wide. Motivated by our analysis, we roose simle DRSL that overcomes this essimism and emirically demonstrate its effectiveness.
"
316,2018,Understanding Generalization and Optimization Performance of Deep CNNs,Oral/Poster,"This work aims to rovide understandings on the remarkable success of dee convolutional neural networks (CNNs) by theoretically analyzing their generalization erformance and establishing otimization guarantees for gradient descent based training algorithms. Secifically, for a CNN model consisting of $l$ convolutional layers and one fully connected layer, we rove that its generalization error is bounded by $\mathcal{O}(\sqrt{\dt\widetilde{\varrho}n})$ where $\theta$ denotes  freedom degree of the network arameters and $\widetilde{\varrho}=\mathcal{O}(\log(\rod_{i=1}^{l}\rwi{i} (\ki{i}-\si{i}+1))+\log(\rf))$ encasulates architecture arameters including the  kernel size $\ki{i}$, stride $\si{i}$, ooling size $$ and arameter magnitude $\rwi{i}$. To our best knowledge, this is the first generalization bound that only deends on $\mathcal{O}(\log(\rod_{i=1}^{l+1}\rwi{i}))$, tighter than existing ones that all involve an exonential term like $\mathcal{O}(\rod_{i=1}^{l+1}\rwi{i})$. Besides, we rove that for an arbitrary gradient descent algorithm, the comuted aroximate stationary oint by minimizing emirical risk is also an aroximate stationary oint to the oulation risk. This well exlains why gradient descent training algorithms usually erform sufficiently well in ractice. Furthermore, we rove the one-to-one corresondence and convergence guarantees for the non-degenerate stationary oints between the emirical and oulation risks. It imlies that the comuted local minimum for the emirical risk is also close to a local minimum for the oulation risk,  thus ensuring that the otimized CNN model well generalizes to new data."
317,2018,The Multilinear Structure of ReLU Networks,Oral/Poster,"We study the loss surface of neural networks equied with a hinge loss criterion and ReLU or leaky ReLU nonlinearities. Any such network defines a iecewise multilinear form in arameter sace.  By aealing to harmonic analysis we show that all local minima of such network are non-differentiable, excet for those minima that occur in a region of arameter sace where the loss surface is erfectly flat. Non-differentiable minima are therefore not technicalities or athologies; they are heart of the roblem when investigating the loss of ReLU networks. As a consequence, we must emloy techniques from nonsmooth analysis to study these loss surfaces. We show how to aly these techniques in some illustrative cases.
"
318,2018,Parallel and Streaming Algorithms for K-Core Decomposition,Oral/Poster,"The k-core decomosition is a fundamental rimitive in many machine learning and data mining alications. We resent the first distributed and the first streaming algorithms to comute and maintain an aroximate k-core decomosition with rovable guarantees. Our algorithms achieve rigorous bounds on sace comlexity while bounding the number of asses or number of rounds of comutation. We do so by resenting a new owerful sketching technique for k-core decomosition, and then by showing it can be comuted efficiently in both streaming and MaReduce models. Finally, we confirm the effectiveness of our sketching technique emirically on a number of ublicly available grahs.
"
319,2018,Fast Approximate Spectral Clustering for Dynamic Networks,Oral/Poster,"Sectral clustering is a widely studied roblem, yet its comlexity is rohibitive for dynamic grahs of even modest size. We claim that it is ossible to reuse information of ast cluster assignments to exedite comutation. Our aroach builds on a recent idea of sidesteing the main bottleneck of sectral clustering, i.e., comuting the grah eigenvectors, by a olynomial-based randomized sketching technique. We show that the roosed algorithm achieves clustering assignments with quality aroximating that of sectral clustering and that it can yield significant comlexity benefits when the grah dynamics are aroriately bounded. In our exeriments, our method clusters 30k node grahs 3.9$\times$ faster in average and deviates from the correct assignment by less than 0.1\%."
320,2018,Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of Spurious Local Minima,Oral/Poster,"We consider the roblem of learning an one-hidden-layer neural network with non-overlaing convolutional layer and ReLU activation function, i.e., $f(Z; w, a) = \sum_j a_j\sigma(w^\to Z_j)$, in which both the convolutional weights $w$ and the outut weights $a$ are arameters to be learned.  We rove that with Gaussian inut $\vZ$, there is a surious local minimizer. Surrisingly, in the resence of the surious local minimizer, starting from randomly initialized weights, gradient descent with weight normalization can still be roven to recover the true arameters with constant robability (which can be boosted to robability $1$ with multile restarts). We also show that with constant robability, the same rocedure could also converge to the surious local minimum, showing that the local minimum lays a non-trivial role in the dynamics of gradient descent.  Furthermore, a quantitative analysis shows that the gradient descent dynamics has two hases: it starts off slow, but converges much faster after several iterations."
321,2018,Neural Networks Should Be Wide Enough to Learn Disconnected Decision Regions,Oral/Poster,"In the recent literature the imortant role of deth in dee learning has been emhasized. In this aer we argue that sufficient width of a feedforward network is equally imortant by answering the simle question under which conditions the decision regions of a neural network are connected. It turns out that for a class of activation functions including leaky ReLU, neural networks having a yramidal structure, that is no layer has more hidden units than the inut dimension, roduce necessarily connected decision regions. This imlies that a sufficiently wide hidden layer is necessary to guarantee that the network can roduce disconnected decision regions. We discuss the imlications of this result for the construction of neural networks, in articular the relation to the roblem of adversarial maniulation of classifiers.
"
322,2018,Greed is Still Good: Maximizing Monotone Submodular+Supermodular (BP) Functions,Oral/Poster,"We analyze the erformance of the greedy algorithm, and also a   discrete semi-gradient based algorithm, for maximizing the sum of a   suBmodular and suPermodular (BP) function (both of which are   non-negative monotone non-decreasing) under two tyes of   constraints, either a cardinality constraint or $\geq 1$ matroid   indeendence constraints.  These roblems occur naturally in several   real-world alications in data science, machine learning, and   artificial intelligence.  The roblems are ordinarily inaroximable   to any factor.  Using the curvature $\curv_f$ of the   submodular term, and introducing $\curv^g$ for the suermodular term   (a natural dual curvature for suermodular functions), however, both   of which are comutable in linear time, we show that BP maximization   can be efficiently aroximated by both the greedy and the   semi-gradient based algorithm.  The algorithms yield multilicative   guarantees of   $\frac{1}{\curv_f}\left[1-e^{-(1-\curv^g)\curv_f}\right]$ and   $\frac{1-\curv^g}{(1-\curv^g)\curv_f + }$ for the two tyes of   constraints resectively. For ure monotone suermodular constrained   maximization, these yield $1-\curvg$ and $(1-\curvg)$ for the two   tyes of constraints resectively.  We also analyze the hardness of   BP maximization and show that our guarantees match hardness by a   constant factor and by $O(\ln())$ resectively. Comutational   exeriments are also rovided suorting our analysis."
323,2018,Black-box Adversarial Attacks with Limited Queries and Information,Oral/Poster,"Current neural network-based classifiers are suscetible to adversarial examles even in the black-box setting, where the attacker only has query access to the model. In ractice, the threat model for real-world systems is often more restrictive than the tyical black-box model where the adversary can observe the full outut of the network on arbitrarily many chosen inuts. We define three realistic threat models that more accurately characterize many real-world classifiers: the query-limited setting, the artial-information setting, and the label-only setting. We develo new attacks that fool classifiers under these more restrictive threat models, where revious methods would be imractical or ineffective. We demonstrate that our methods are effective against an ImageNet classifier under our roosed threat models. We also demonstrate a targeted black-box attack against a commercial classifier, overcoming the challenges of limited query access, artial information, and other ractical issues to break the Google Cloud Vision API.
"
324,2018,Using Inherent Structures to design Lean 2-layer RBMs,Oral/Poster,"Understanding the reresentational ower of  Restricted Boltzmann Machines (RBMs) with multile layers is  an ill-understood roblem and is an area of active research. Motivated from the aroach of \emh{Inherent Structure formalism} (Stillinger &am; Weber, 1982), extensively used in analysing Sin Glasses, we roose a novel measure called \emh{Inherent Structure Caacity} (ISC), which characterizes the reresentation caacity of a fixed architecture RBM by the exected number of modes of distributions emanating from the RBM with arameters drawn from a rior distribution. Though ISC is intractable, we show that for a single layer RBM architecture ISC aroaches a finite constant as number of hidden units are increased and to further imrove the ISC, one needs to add a second layer. Furthermore, we introduce \emh{Lean} RBMs, which are multi-layer RBMs where each layer can have at-most O(n) units with the number of visible units being n.  We show that for every single layer RBM with Omega(n^{2+r}), r &gt;= 0,   hidden units there exists  a two-layered \emh{lean} RBM with Theta(n^2) arameters with the same ISC, establishing  that 2 layer RBMs can achieve the same reresentational ower as single-layer RBMs but using far fewer number of arameters.  To the best of our knowledge, this is the first result which quantitatively establishes the need for layering.
"
325,2018,Not to Cry Wolf: Distantly Supervised Multitask Learning in Critical Care,Oral/Poster,"Patients in the intensive care unit (ICU) require constant and close suervision. To assist clinical staff in this task, hositals use monitoring systems that trigger audiovisual alarms if their algorithms indicate that a atient's condition may be worsening. However, current monitoring systems are extremely sensitive to movement artefacts and technical errors. As a result, they tyically trigger hundreds to thousands of false alarms er atient er day - drowning the imortant alarms in noise and adding to the exhaustion of clinical staff. In this setting, data is abundantly available, but obtaining trustworthy annotations by exerts is laborious and exensive. We frame the roblem of false alarm reduction from multivariate time series as a machine-learning task and address it with a novel multitask network architecture that utilises distant suervision through multile related auxiliary tasks in order to reduce the number of exensive labels required for training. We show that our aroach leads to significant imrovements over several state-of-the-art baselines on real-world ICU data and rovide new insights on the imortance of task selection and architectural choices in distantly suervised multitask learning.
"
326,2018,Composable Planning with Attributes,Oral/Poster,"The tasks that an agent will need to solve often are not known during training. However, if the agent knows which roerties of the environment are imortant then, after learning how its actions affect those roerties, it may be able to use this knowledge to solve comlex tasks without training secifically for them. Towards this end, we consider a setu in which an environment is augmented with a set of user defined attributes that arameterize the features of interest. We roose a method that learns a olicy for transitioning between ``nearby'' sets of attributes, and maintains a grah of ossible transitions. Given a task at test time that can be exressed in terms of a target set of attributes, and a current state, our model infers the attributes of the current state and searches over aths through attribute sace to get a high level lan, and then uses its low level olicy to execute the lan.  We show in 3D block stacking, grid-world games, and StarCraft that our model is able to generalize to longer, more comlex tasks at test time by comosing simler learned olicies.
"
327,2018,Measuring abstract reasoning in neural networks,Oral/Poster,"Whether neural networks can learn abstract reasoning or whether they merely rely on suerficial statistics is a toic of recent debate. Here, we roose a dataset and challenge designed to robe abstract reasoning, insired by a well-known human IQ test. To succeed at this challenge, models must coe with various generalisation 'regimes' in which the training data and test questions differ in clearly-defined ways. We show that oular models such as ResNets erform oorly, even when the training and test sets differ only minimally, and we resent a novel architecture, with structure designed to encourage reasoning, that does significantly better. When we vary the way in which the test questions and training data differ, we find that our model is notably roficient at certain forms of generalisation, but notably weak at others. We further show that the model's ability to generalise imroves markedly if it is trained to redict symbolic exlanations for its answers. Altogether, we introduce and exlore ways to both measure and induce stronger abstract reasoning in neural networks. Our freely-available dataset should motivate further rogress in this direction.
"
328,2018,Projection-Free Online Optimization with Stochastic Gradient: From Convexity to Submodularity,Oral/Poster,"Online otimization has been a successful framework for solving large-scale roblems under comutational constraints and artial information. Current methods for online convex otimization require either a rojection or exact gradient comutation at each ste, both of which can be rohibitively exensive for large-scale alications. At the same time, there is a growing trend of non-convex otimization in machine learning community and a need for online methods. Continuous DR-submodular functions, which exhibit a natural diminishing returns condition, have recently been roosed as a broad class of non-convex functions which may be efficiently otimized. Although online methods have been introduced, they suffer from similar roblems. In this work, we roose Meta-Frank-Wolfe, the first online rojection-free algorithm that uses stochastic gradient estimates. The algorithm relies on a careful samling of gradients in each round and achieves the otimal $O( \sqrt{T})$ adversarial regret bounds for convex and continuous submodular otimization. We also roose One-Shot Frank-Wolfe, a simler algorithm which requires only a single stochastic gradient estimate in each round and achieves an $O(T^{23})$ stochastic regret bound for convex and continuous submodular otimization. We aly our methods to develo a novel ""lifting"" framework for the online discrete submodular maximization and also see that they outerform current state-of-the-art techniques on various exeriments."
329,2018,Self-Bounded Prediction Suffix Tree via Approximate String Matching,Oral/Poster,"Prediction suffix trees (PST) rovide an effective tool for sequence modelling and rediction. Current rediction techniques for PSTs rely on exact matching between the suffix of the current sequence and the reviously observed sequence. We resent a rovably correct algorithm for learning a PST with aroximate suffix matching by relaxing the exact matching condition. We then resent a self-bounded enhancement of our algorithm where the deth of suffix tree grows automatically in resonse to the model erformance on a training sequence. Through exeriments on synthetic datasets as well as three real-world datasets, we show that the aroximate matching PST results in better redictive erformance than the other variants of PST.
"
330,2018,MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels,Oral/Poster,"Recent dee networks are caable of memorizing the entire data even when the labels are comletely random. To overcome the overfitting on corruted labels, we roose a novel technique of learning another neural network, called MentorNet, to suervise the training of the base dee networks, namely, StudentNet. During training, MentorNet rovides a curriculum (samle weighting scheme) for StudentNet to focus on the samle the label of which is robably correct. Unlike the existing curriculum that is usually redefined by human exerts, MentorNet learns a data-driven curriculum dynamically with StudentNet. Exerimental results demonstrate that our aroach can significantly imrove the generalization erformance of dee networks trained on corruted training data. Notably, to the best of our knowledge, we achieve the best-ublished result on WebVision, a large benchmark containing 2.2 million images of real-world noisy labels.
"
331,2018,Curriculum Learning by Transfer Learning: Theory and Experiments with Deep Networks,Oral/Poster,"We rovide theoretical investigation of curriculum learning in the context of stochastic gradient descent when otimizing the convex linear regression loss. We rove that the rate of convergence of an ideal curriculum learning method is monotonically increasing with the difficulty of the examles. Moreover, among all equally difficult oints, convergence is faster when using oints which incur higher loss with resect to the current hyothesis. We then analyze curriculum learning in the context of training a CNN. We describe a method which infers the curriculum by way of transfer learning from another network, re-trained on a different task. While this aroach can only aroximate the ideal curriculum, we observe emirically similar behavior to the one redicted by the theory, namely, a significant boost in convergence seed at the beginning of training. When the task is made more difficult, imrovement in generalization erformance is also observed. Finally, curriculum learning exhibits robustness against unfavorable conditions such as excessive regularization.
"
332,2018,Composite Functional Gradient Learning of Generative  Adversarial Models,Oral/Poster,"This aer first resents a theory for generative adversarial methods that does not rely on the traditional minimax formulation. It shows that with a strong discriminator, a good generator can be learned so that the KL divergence between the distributions of real data and generated data imroves after each functional gradient ste until it converges to zero. Based on the theory, we roose a new stable generative adversarial method. A theoretical insight into the original GAN from this new viewoint is also rovided. The exeriments on image generation show the effectiveness of our new method.
"
333,2018,LaVAN: Localized and Visible Adversarial Noise,Oral/Poster,"Most works on adversarial examles for dee-learning based image classifiers use noise that, while small, covers the entire image. We exlore the case where the noise is allowed to be visible but confined to a small, localized atch of the image, without covering any of the main object(s) in the image. We show that it  is ossible to generate localized adversarial noises that cover only 2% of the ixels in the  image, none of them over the main object, and that are transferable across images and locations, and successfully fool a state-of-the-art Incetion v3 model with very high success rates.
"
334,2018,Approximation Guarantees for Adaptive Sampling,Oral/Poster,"In this aer we analyze an adative samling aroach for submodular maximization. Adative samling is a technique that has recently been shown to achieve a constant factor aroximation guarantee for submodular maximization under a cardinality constraint with exonentially fewer adative rounds than any reviously studied constant factor aroximation algorithm for this roblem. Adativity quantifies the number of sequential rounds that an algorithm makes when function evaluations can be executed in arallel and is the arallel running time of an algorithm, u to low order terms. Adative samling achieves its exonential seedu at the exense of aroximation. In theory, it is guaranteed to roduce a solution that is a 13 aroximation to the otimum. Nevertheless, exeriments show that adative samling techniques achieve far better values in ractice.  In this aer we rovide theoretical justification for this henomenon. In articular, we show that under very mild conditions of curvature of a function, adative samling techniques achieve an aroximation arbitrarily close to 12 while maintaining their low adativity. Furthermore, we show that the aroximation ratio aroaches 1 in direct relationshi to a homogeneity roerty of the submodular function. In addition, we conduct exeriments on real data sets in which the curvature and homogeneity roerties can be easily maniulated and demonstrate the relationshi between aroximation and curvature, as well as the effectiveness of adative samling in ractice.
"
335,2018,Constrained Interacting Submodular Groupings,Oral/Poster,"We introduce the roblem of grouing a finite ground set into blocks   where each block is a subset of the ground set and where: (i) the   blocks are individually highly valued by a submodular function (both   robustly and in the average case) while satisfying block-secific   matroid constraints; and (ii) block scores interact where blocks are   jointly scored highly, thus making the blocks mutually   non-redundant.  Submodular functions are good models of information   and diversity; thus, the above can be seen as grouing the ground   set into matroid constrained blocks that are both intra- and   inter-diverse. Potential alications include forming ensembles of   classificationregression models, artitioning data for arallel   rocessing, and summarization.  In the non-robust case, we reduce   the roblem to non-monotone submodular maximization subject to   multile matroid constraints. In the mixed robustaverage case, we   offer a bi-criterion guarantee for a olynomial time deterministic   algorithm and a robabilistic guarantee for randomized algorithm, as   long as the involved submodular functions (including the inter-block   interaction terms) are monotone. We close with a case study in which   we use these algorithms to find high quality diverse ensembles of   classifiers, showing good results.
"
336,2018,Residual Unfairness in Fair Machine Learning from Prejudiced Data,Oral/Poster,"Recent work in fairness in machine learning has roosed adjusting for fairness by equalizing accuracy metrics across grous and has also studied how datasets affected by historical rejudices may lead to unfair decision olicies. We connect these lines of work and study the residual unfairness that arises when a fairness-adjusted redictor is not actually fair on the target oulation due to systematic censoring of training data by existing biased olicies. This scenario is articularly common in the same alications where fairness is a concern. We characterize theoretically the imact of such censoring on standard fairness metrics for binary classifiers and rovide criteria for when residual unfairness may or may not aear. We rove that, under certain conditions, fairness-adjusted classifiers will in fact induce residual unfairness that eretuates the same injustices, against the same grous, that biased the data to begin with, thus showing that even state-of-the-art fair machine learning can have a ""bias in, bias out"" roerty. When certain benchmark data is available, we show how samle reweighting can estimate and adjust fairness metrics while accounting for censoring. We use this to study the case of Sto, Question, and Frisk (SQF) and demonstrate that attemting to adjust for fairness eretuates the same injustices that the olicy is infamous for.
"
337,2018,Adversarial Regression with Multiple Learners,Oral/Poster,"Desite the considerable success enjoyed by machine learning techniques in ractice, numerous studies demonstrated that many aroaches are vulnerable to attacks. An imortant class of such attacks involves adversaries changing features at test time to cause incorrect redictions. Previous investigations of this roblem it a single learner against an adversary. However, in many situations an adversary’s decision is aimed at a collection of learners, rather than secifically targeted at each indeendently. We study the roblem of adversarial linear regression with multile learners. We aroximate the resulting game by exhibiting an uer bound on learner loss functions, and show that the resulting game has a unique symmetric equilibrium. We resent an algorithm for comuting this equilibrium, and show through extensive exeriments that equilibrium models are significantly more robust than conventional regularized linear regression.
"
338,2018,Representation Tradeoffs for Hyperbolic Embeddings,Oral/Poster,"Hyerbolic embeddings offer excellent quality with few dimensions when embedding hierarchical data structures. We give a combinatorial construction that embeds trees into hyerbolic sace with arbitrarily low distortion without otimization. On WordNet, this algorithm obtains a mean-average-recision of 0.989 with only two dimensions, outerforming existing work by 0.11 oints. We rovide bounds characterizing the recision-dimensionality tradeoff inherent in any hyerbolic embedding. To embed general metric saces, we roose a hyerbolic generalization of multidimensional scaling (h-MDS). We show how to erform exact recovery of hyerbolic oints from distances, rovide a erturbation analysis, and give a recovery result that enables us to reduce dimensionality. Finally, we extract lessons from the algorithms and theory above to design a scalable PyTorch-based imlementation that can handle incomlete information.
"
339,2018,Improving Sign Random Projections With Additional Information,Oral/Poster,"Sign random rojections (SRP) is a technique which allows the user to quickly estimate the angular similarity and inner roducts between data. We roose using additional information to imrove these estimates which is easy to imlement and cost efficient. We rove that the variance of our estimator is lower than the variance of SRP. Our roosed method can also be used together with other modifications of SRP, such as Suer-Bit LSH (SBLSH). We demonstrate the effectiveness of our method on the MNIST test dataset and the Gisette dataset. We discuss how our roosed method can be extended to random rojections or even other hashing algorithms.
"
340,2018,"Bandits with Delayed, Aggregated Anonymous Feedback",Oral/Poster,"We study a variant of the stochastic $K$-armed bandit roblem, which we call ""bandits with delayed, aggregated anonymous feedback''. In this roblem, when the layer ulls an arm, a reward is generated, however it is not immediately observed. Instead, at the end of each round the layer observes only the sum of a number of reviously generated rewards which haen to arrive in the given round. The rewards are stochastically delayed and due to the aggregated nature of the observations, the information of which arm led to a articular reward is lost. The question is what is the cost of the information loss due to this delayed, aggregated anonymous feedback? Previous works have studied bandits with stochastic, non-anonymous delays and found that the regret increases only by an additive factor relating to the exected delay. In this aer, we show that this additive regret increase can be maintained in the harder delayed, aggregated anonymous feedback setting when the exected delay (or a bound on it) is known. We rovide an algorithm that matches the worst case regret of the non-anonymous roblem exactly when the delays are bounded, and u to logarithmic factors or an additive variance term for unbounded delays."
341,2018,Make the Minority Great Again: First-Order Regret Bound for Contextual Bandits,Oral/Poster,"Regret bounds in online learning comare the layer's erformance to $L*$, the otimal erformance in hindsight with a fixed strategy. Tyically such bounds scale with the square root of the time horizon $T$. The more refined concet of first-order regret bound relaces this with a scaling $\sqrt{L*}$, which may be much smaller than $\sqrt{T}$.  It is well known that minor variants of standard algorithms satisfy first-order regret bounds in the full information and multi-armed bandit settings. In a COLT 2017 oen roblem, Agarwal, Krishnamurthy, Langford, Luo, and Schaire raised the issue that existing techniques do not seem sufficient to obtain first-order regret bounds for the contextual bandit roblem. In the resent aer, we resolve this oen roblem by resenting a new strategy based on augmenting the olicy sace."
342,2018,Learning Policy Representations in Multiagent Systems,Oral/Poster,"Modeling agent behavior is central to understanding the emergence of comlex henomena in multiagent systems. Prior work in agent modeling has largely been task-secific and driven by hand-engineering domain-secific rior knowledge. We roose a general learning framework for modeling agent behavior in any multiagent system using only a handful of interaction data. Our framework casts agent modeling as a reresentation learning roblem. Consequently, we construct a novel objective insired by imitation learning and agent identification and design an algorithm for unsuervised learning of reresentations of agent olicies. We demonstrate emirically the utility of the roosed framework in (i) a challenging high-dimensional cometitive environment for continuous control and (ii) a cooerative environment for communication, on suervised redictive tasks, unsuervised clustering, and olicy otimization using dee reinforcement learning.
"
343,2018,Learning to Coordinate with Coordination Graphs in Repeated Single-Stage Multi-Agent Decision Problems,Oral/Poster,"Learning to coordinate between multile agents is an imortant roblem in many reinforcement learning roblems. Key to learning to coordinate is exloiting loose coulings, i.e., conditional indeendences between agents. In this aer we study learning in reeated fully cooerative games, multi-agent multi-armed bandits (MAMABs), in which the exected rewards can be exressed as a coordination grah. We roose multi-agent uer confidence exloration (MAUCE), a new algorithm for MAMABs that exloits loose coulings, which enables us to rove a regret bound that is logarithmic in the number of arm ulls and only linear in the number of agents. We emirically comare MAUCE to sarse cooerative Q-learning, and a state-of-the-art combinatorial bandit aroach, and show that it erforms much better on a variety of settings, including learning control olicies for wind farms.
"
344,2018,Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations,Oral/Poster,"Dee neural networks have become the state-of-the-art models in numerous machine learning tasks. However, general guidance to network architecture design is still missing. In our work, we bridge dee neural network design with numerical differential equations. We show that many effective networks, such as ResNet, PolyNet, FractalNet and RevNet, can be interreted as different numerical discretizations of differential equations. This finding brings us a brand new ersective on the design of effective dee architectures. We can take advantage of the rich knowledge in numerical analysis to guide us in designing new and otentially more effective dee networks. As an examle, we roose a linear multi-ste architecture (LM-architecture) which is insired by the linear multi-ste method solving ordinary differential equations. The LM-architecture is an effective structure that can be used on any ResNet-like networks. In articular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the networks obtained by alying the LM-architecture on ResNet and ResNeXt resectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on both CIFAR and ImageNet with comarable numbers of trainable arameters. In articular, on both CIFAR and ImageNet, LM-ResNetLM-ResNeXt can significantly comress (&gt;50%) the original networks while maintaining a similar erformance. This can be exlained mathematically using the concet of modified equation from numerical analysis. Last but not least, we also establish a connection between stochastic control and noise injection in the training rocess which hels to imrove generalization of the networks. Furthermore, by relating stochastic training strategy with stochastic dynamic system, we can easily aly stochastic training to the networks with the LM-architecture. As an examle, we introduced stochastic deth to LM-ResNet and achieve significant imrovement over the original LM-ResNet on CIFAR10.
"
345,2018,Compressing Neural Networks using the Variational Information Bottelneck,Oral/Poster,"Neural networks can be comressed to reduce memory and comutational requirements, or to increase accuracy by facilitating the use of a larger base architecture.  In this aer we focus on runing individual neurons, which can simultaneously trim model size, FLOPs, and run-time memory.  To imrove uon the erformance of existing comression algorithms we utilize the information bottleneck rincile instantiated via a tractable variational bound.  Minimization of this information theoretic bound reduces the redundancy between adjacent layers by aggregating useful information into a subset of neurons that can be reserved.  In contrast, the activations of disosable neurons are shut off via an attractive form of sarse regularization that emerges naturally from this framework, roviding tangible advantages over traditional sarsity enalties without contributing additional tuning arameters to the energy landscae.  We demonstrate state-of-the-art comression rates across an array of datasets and network architectures.
"
346,2018,Scalable Bilinear Pi Learning Using State and Action Features,Oral/Poster,"Aroximate linear rogramming (ALP) reresents one of the major algorithmic families to solve large-scale Markov decision rocesses (MDP).  In this work, we study a rimal-dual formulation of the ALP, and develo a scalable, model-free algorithm called bilinear $\i$ learning for reinforcement learning when a samling oracle is rovided.  This algorithm enjoys a number of advantages.  First, it adots linear and bilinear models to reresent the high-dimensional value function and state-action distributions, resectively, using given state and action features.  Its run-time comlexity deends on the number of features, not the size of the underlying MDPs.  Second, it oerates in a fully online fashion without having to store any samle, thus having minimal memory footrint.  Third, we rove that it is samle-efficient, solving for the otimal olicy to high recision with a samle comlexity linear in the dimension of the arameter sace."
347,2018,Time Limits in Reinforcement Learning,Oral/Poster,"In reinforcement learning, it is common to let an agent interact for a fixed amount of time with its environment before resetting it and reeating the rocess in a series of eisodes. The task that the agent has to learn can either be to maximize its erformance over (i) that fixed eriod, or (ii) an indefinite eriod where time limits are only used during training to diversify exerience. In this aer, we rovide a formal account for how time limits could effectively be handled in each of the two cases and exlain why not doing so can cause state-aliasing and invalidation of exerience relay, leading to subotimal olicies and training instability. In case (i), we argue that the terminations due to time limits are in fact art of the environment, and thus a notion of the remaining time should be included as art of the agent's inut to avoid violation of the Markov roerty. In case (ii), the time limits are not art of the environment and are only used to facilitate learning. We argue that this insight should be incororated by bootstraing from the value of the state at the end of each artial eisode. For both cases, we illustrate emirically the significance of our considerations in imroving the erformance and stability of existing reinforcement learning algorithms, showing state-of-the-art results on several control tasks.
"
348,2018,Semi-Supervised Learning on Data Streams via Temporal Label Propagation,Oral/Poster,"We consider the roblem of labeling oints on a fast-moving data stream when only a small number of labeled examles are available. In our setting, incoming oints must be rocessed efficiently and the stream is too large to store in its entirety. We resent a semi-suervised learning algorithm for this task. The algorithm maintains a small synosis of the stream which can be quickly udated as new oints arrive, and labels every incoming oint by rovably learning from the full history of the stream. Exeriments on real datasets validate that the algorithm can quickly and accurately classify oints on a stream with a small quantity of labeled examles.
"
349,2018,Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval and Matrix Completion,Oral/Poster,"Recent years have seen a flurry of activities in designing rovably efficient nonconvex otimization rocedures for solving statistical estimation roblems. For various roblems like hase retrieval or low-rank matrix comletion, state-of-the-art nonconvex rocedures  require roer regularization (e.g.~trimming, regularized cost, rojection) in order to guarantee fast convergence. When it comes to vanilla rocedures such as gradient descent, however, rior theory either recommends highly conservative learning rates to avoid overshooting, or comletely lacks erformance guarantees. This aer uncovers a striking henomenon in several nonconvex roblems: even in the absence of exlicit regularization, gradient descent follows a trajectory staying within a basin that enjoys nice geometry, consisting of oints incoherent with the samling mechanism. This ``imlicit regularization'' feature allows gradient descent to roceed in a far more aggressive fashion without overshooting, which in turn results in substantial comutational savings. Focusing on two statistical estimation roblems, i.e.~solving random quadratic systems of equations and low-rank matrix comletion, we establish that gradient descent achieves near-otimal statistical and comutational guarantees without exlicit regularization. As a byroduct, for noisy matrix comletion, we demonstrate that gradient descent enables otimal control of both entrywise and sectral-norm errors.
"
350,2018,A Fast and Scalable Joint Estimator for Integrating Additional Knowledge in Learning Multiple Related Sparse Gaussian Graphical Models,Oral/Poster,"We consider the roblem of including additional knowledge in estimating sarse Gaussian grahical models (sGGMs) from aggregated samles, arising often in bioinformatics and neuroimaging alications. Previous joint sGGM estimators either fail to use existing knowledge or cannot scale-u to many tasks (large $K$) under a high-dimensional (large $$) situation.  In this aer, we roose a  novel \underline{J}oint \underline{E}lementary \underline{E}stimator incororating additional \underline{K}nowledge (JEEK) to infer multile related sarse Gaussian Grahical models from large-scale heterogeneous data. Using domain knowledge as weights, we design a novel hybrid norm as the minimization objective to enforce the suerosition of two weighted sarsity constraints, one on the shared interactions and the other on the task-secific structural atterns. This enables JEEK to elegantly consider various forms of existing knowledge based on the domain at hand and avoid the need to design knowledge-secific otimization. JEEK is solved through a fast and entry-wise arallelizable solution that largely imroves the comutational efficiency of the state-of-the-art  $O(^5K^4)$ to $O(^2K^4)$. We conduct a rigorous statistical analysis showing that JEEK achieves the same convergence rate $O(\log(K)n_{tot})$ as the state-of-the-art estimators that are much harder to comute. Emirically, on multile synthetic datasets and one real-world data from neuroscience, JEEP outerforms the seed of the state-of-arts significantly while achieving the same level of rediction accuracy."
351,2018,Bucket Renormalization for Approximate Inference,Oral/Poster,"Probabilistic grahical models are a key tool in machine learning alications.  Comuting the artition function, i.e., normalizing constant, is a fundamental task of statistical inference but is generally comutationally intractable, leading to extensive study of aroximation methods. Iterative variational methods are a oular and successful family of aroaches. However, even state of the art variational methods can return oor results or fail to converge on difficult instances. In this aer, we instead consider comuting the artition function via sequential summation over variables. We develo robust aroximate algorithms by combining ideas from mini-bucket elimination with tensor network and renormalization grou methods from statistical hysics. The resulting “convergence-free” methods show good emirical erformance on both synthetic and real-world benchmark models, even for difficult instances.
"
352,2018,Kernel Recursive ABC: Point Estimation with Intractable Likelihood,Oral/Poster,"We roose a novel aroach to arameter estimation for simulator-based statistical models with intractable likelihood. Our roosed method involves recursive alication of kernel ABC and kernel herding to the same observed data. We rovide a theoretical exlanation regarding why the aroach works, showing (for the oulation setting) that, under a certain assumtion, oint estimates obtained with this method converge to the true arameter, as recursion roceeds. We have conducted a variety of numerical exeriments, including arameter estimation for a real-world edestrian flow simulator, and show that in most cases our method outerforms existing aroaches.
"
353,2018,Modeling Others using Oneself in Multi-Agent Reinforcement Learning,Oral/Poster,"We consider the multi-agent reinforcement learning setting with imerfect information. The reward function deends on the hidden goals of both agents, so the agents must infer the other layers’ goals from their observed behavior in order to maximize their returns. We roose a new aroach for learning in these domains: Self Other-Modeling (SOM), in which an agent uses its own olicy to redict the other agent’s actions and udate its belief of their hidden goal in an online manner. We evaluate this aroach on three different tasks and show that the agents are able to learn better olicies using their estimate of the other layers’ goals, in both cooerative and cometitive settings.
"
354,2018,Tropical Geometry of Deep Neural Networks,Oral/Poster,"We establish, for the first time, exlicit connections between feedforward neural networks  with ReLU activation and troical geometry --- we show that the family of such neural networks is equivalent to the family of troical rational mas.  Among other things, we deduce that feedforward ReLU neural networks with one hidden layer can be characterized by zonotoes, which serve as building blocks for deeer networks; we relate decision boundaries of such neural networks to troical hyersurfaces, a major object of study in troical geometry; and we rove that linear regions of such neural networks corresond to vertices of olytoes associated with troical rational functions.  An insight from our troical formulation is that a deeer network is exonentially more exressive than a shallow network.
"
355,2018,Learning Dynamics of Linear Denoising Autoencoders,Oral/Poster,"Denoising autoencoders (DAEs) have roven useful for unsuervised reresentation learning, but a thorough theoretical understanding is still lacking of how the inut noise influences learning. Here we develo theory for how noise influences learning in DAEs. By focusing on linear DAEs, we are able to derive analytic exressions that exactly describe their learning dynamics. We verify our theoretical redictions with simulations as well as exeriments on MNIST and CIFAR-10. The theory illustrates how, when tuned correctly, noise allows DAEs to ignore low variance directions in the inuts while learning to reconstruct them. Furthermore, in a comarison of the learning dynamics of DAEs to standard regularised autoencoders, we show that noise has a similar regularisation effect to weight decay, but with faster training dynamics. We also show that our theoretical redictions aroximate learning dynamics on real-world data and qualitatively match observed dynamics in nonlinear DAEs.
"
356,2018,Nonparametric variable importance using an augmented neural network with multi-task learning,Oral/Poster,"In redictive modeling alications, it is often of interest to determine the relative contribution of subsets of features in exlaining the variability of an outcome. It is useful to consider this variable imortance as a function of the unknown, underlying data-generating mechanism rather than the secific redictive algorithm used to fit the data. In this aer, we connect these ideas in nonarametric variable imortance to machine learning, and rovide a method for efficient estimation of variable imortance when building a redictive model using a neural network. We show how a single augmented neural network with multi-task learning simultaneously estimates the imortance of many feature subsets, imroving on revious rocedures for estimating imortance. We demonstrate on simulated data that our method is both accurate and comutationally efficient, and aly our method to both a study of heart disease and for redicting mortality in ICU atients.
"
357,2018,Training Neural Machines with Trace-Based Supervision,Oral/Poster,"We investigate the effectiveness of trace-based suervision methods for training existing neural abstract machines. To define the class of neural machines amenable to trace-based suervision, we introduce the concet of a differential neural comutational machine (dNCM) and show that several existing architectures (NTMs, NRAMs) can be described as dNCMs. We erformed a detailed exerimental evaluation with NTM and NRAM machines, showing that additional suervision on the interretable ortions of these architectures leads to better convergence and generalization caabilities of the learning hase than standard training, in both noise-free and noisy scenarios.
"
358,2018,Open Category Detection with PAC Guarantees,Oral/Poster,"Oen category detection is the roblem of detecting ""alien"" test instances that belong to categories or classes that were not resent in the training data. In many alications, reliably detecting such aliens is central to ensuring the safety and accuracy of test set redictions. Unfortunately, there are no algorithms that rovide theoretical guarantees on their ability to detect aliens under general assumtions. Further, while there are algorithms for oen category detection, there are few emirical results that directly reort alien detection rates. Thus, there are significant theoretical and emirical gas in our understanding of oen category detection. In this aer, we take a ste toward addressing this ga by studying a simle, but ractically-relevant variant of oen category detection. In our setting, we are rovided with a ""clean"" training set that contains only the target categories of interest and an unlabeled ""contaminated'' training set that contains a fraction alha of alien examles. Under the assumtion that we know an uer bound on alha we develo an algorithm with PAC-style guarantees on the alien detection rate, while aiming to minimize false alarms. Emirical results on synthetic and standard benchmark datasets demonstrate the regimes in which the algorithm can be effective and rovide a baseline for further advancements.
"
359,2018,SAFFRON: an Adaptive Algorithm for Online Control of the False Discovery Rate,Oral/Poster,"In the online false discovery rate (FDR) roblem, one observes a ossibly infinite sequence of $$-values $P_1,P_2,\dots$, each testing a different null hyothesis, and an algorithm must ick a sequence of rejection thresholds $\alha_1,\alha_2,\dots$ in an online fashion, effectively rejecting the $k$-th null hyothesis whenever $P_k \leq \alha_k$. Imortantly, $\alha_k$ must be a function of the ast, and cannot deend on $P_k$ or any of the later unseen $$-values, and must be chosen to guarantee that for any time $t$, the FDR u to time $t$ is less than some re-determined quantity $\alha \in (0,1)$. In this work, we resent a owerful new framework for online FDR control that we refer to as ``SAFFRON''. Like older alha-investing algorithms, SAFFRON starts off with an error budget (called alha-wealth) that it intelligently allocates to different tests over time, earning back some alha-wealth whenever it makes a new discovery. However, unlike older methods, SAFFRON's threshold sequence is based on a novel estimate of the alha fraction that it allocates to true null hyotheses. In the offline setting, algorithms that emloy an estimate of the roortion of true nulls are called ``adative'', hence SAFFRON can be seen as an online analogue of the offline Storey-BH adative rocedure. Just as Storey-BH is tyically more owerful than the Benjamini-Hochberg (BH) rocedure under indeendence, we demonstrate that SAFFRON is also more owerful than its non-adative counterarts such as LORD."
360,2018,Learning Localized Spatio-Temporal Models From Streaming Data,Oral/Poster,"We address the roblem of redicting satio-temoral rocesses with temoral atterns that vary across satial regions, when data is obtained as a stream. That is, when the training dataset is augmented sequentially. Secifically, we develo a localized satio-temoral covariance model of the rocess that can cature satially varying temoral eriodicities in the data. We then aly a covariance-fitting methodology to learn the model arameters which yields a redictor that can be udated sequentially with each new data oint. The roosed method is evaluated using both synthetic and real climate data which demonstrate its ability to accurately redict data missing in satial regions over time.
"
361,2018,Feasible Arm Identification,Oral/Poster,"We introduce the feasible arm identification roblem, a ure exloration multi-armed bandit roblem where the agent is given a set of $D$-dimensional arms and a olyhedron $P = \{x : A x \leq b \} \subset R^D$. Pulling an arm gives a random vector and the goal is to determine, using a fixed budget of $T$ ulls, which of the arms have means belonging to $P$. We roose three algorithms MD-UCBE, MD-SAR, and MD-APT and rovide a unified analysis establishing uer bounds for each of them. We also establish a lower bound that matches u to constants the uer bounds of MD-UCBE and MD-APT. Finally, we demonstrate the effectiveness of our algorithms on synthetic and real-world datasets."
362,2018,"Fast Maximization of Non-Submodular, Monotonic Functions on the Integer Lattice",Oral/Poster,"The otimization of submodular functions on the integer lattice has received much attention recently, but the objective functions of many alications are non-submodular. We rovide two aroximation algorithms for maximizing a non-submodular function on the integer lattice subject to a cardinality constraint; these are the first algorithms for this urose that have olynomial query comlexity. We roose a general framework for influence maximization on the integer lattice that generalizes rior works on this toic, and we demonstrate the efficiency of our algorithms in this context.
"
363,2018,Decentralized Submodular Maximization: Bridging Discrete and Continuous Settings,Oral/Poster,"In this aer, we showcase the interlay between discrete and continuous otimization in network-structured settings. We roose the first  fully decentralized otimization method for a wide class of non-convex objective functions that ossess a diminishing returns roerty. More secifically, given an arbitrary connected network and a global continuous submodular function, formed by a sum of local functions, we develo Decentralized Continuous Greedy (DCG), a message assing algorithm that converges to the tight $(1-1e)$ aroximation factor of the otimum global solution using only local comutation and communication. We also rovide strong convergence bounds as a function of network size and sectral characteristics of the underlying toology. Interestingly, DCG readily rovides a simle recie for decentralized discrete submodular maximization through the means of continuous relaxations. Formally, we demonstrate that by lifting the local discrete functions to continuous domains and using DCG as an interface we can develo a consensus algorithm that also achieves the tight $(1-1e)$ aroximation guarantee of the global discrete solution once a roer rounding scheme is alied."
364,2018,Towards Fast Computation of Certified Robustness for ReLU Networks,Oral/Poster,"Verifying the robustness roerty of a general Rectified Linear Unit (ReLU) network is an NP-comlete roblem. Although finding the exact minimum adversarial distortion is hard, giving a certified lower bound of the minimum distortion is ossible. Current available methods of comuting such a bound are either time-consuming or deliver low quality bounds that are too loose to be useful. In this aer, we exloit the secial structure of ReLU networks and rovide two comutationally efficient algorithms (Fast-Lin, Fast-Li) that are able to certify non-trivial lower bounds of minimum adversarial distortions. Exeriments show that (1) our methods deliver bounds close to (the ga is 2-3X) exact minimum distortions found by Relulex in small networks while our algorithms are more than 10,000 times faster; (2) our methods deliver similar quality of bounds (the ga is within 35\% and usually around 10\%; sometimes our bounds are even better) for larger networks comared to the methods based on solving linear rogramming roblems but our algorithms are 33-14,000 times faster; (3) our method is caable of solving large MNIST and CIFAR networks u to 7 layers with more than 10,000 neurons within tens of seconds on a single CPU core. In addition, we show that there is no olynomial time algorithm that can aroximately find the minimum $\ell_1$ adversarial distortion of a ReLU network with a $0.99\ln n$ aroximation ratio unless NP=P, where $n$ is the number of neurons in the network."
365,2018,A Two-Step Computation of the Exact GAN Wasserstein Distance,Oral/Poster,"In this aer, we roose a two-ste method to comute the Wasserstein distance in Wasserstein Generative Adversarial Networks (WGANs): 1) The convex art of our objective can be solved by linear rogramming; 2) The non-convex residual can be aroximated by a dee neural network. We theoretically rove that the roosed formulation is equivalent to the discrete Monge-Kantorovich dual formulation. Furthermore, we give the aroximation error bound of the Wasserstein distance and the error bound of generalizing the Wasserstein distance from discrete to continuous distributions. Our aroach otimizes the exact Wasserstein distance, obviating the need for weight cliing reviously used in WGANs. Results on synthetic data show that the our method comutes the Wasserstein distance more accurately. Qualitative and quantitative results on MNIST, LSUN and CIFAR-10 datasets show that the roosed method is more efficient than state-of-the-art WGAN methods, and still roduces images of comarable quality.
"
366,2018,Spatio-temporal Bayesian On-line Changepoint Detection with Model Selection,Oral/Poster,"Bayesian On-line Changeoint Detection is extended to on-line model selection and non-stationary satio-temoral rocesses. We roose satially structured Vector Autoregressions (VARs) for modelling the rocess between changeoints (CPs) and give an uer bound on the aroximation error of such models. The resulting algorithm erforms rediction, model selection and CP detection on-line. Its time comlexity is linear and its sace comlexity constant, and thus it is two orders of magnitudes faster than its closest cometitor.  In addition, it outerforms the state of the art for multivariate data.
"
367,2018,Fast Stochastic AUC Maximization with $O(1/n)$-Convergence Rate,Oral/Poster,"In this aer, we consider statistical learning with AUC (area under ROC curve) maximization in the classical stochastic setting where one random data drawn from an  unknown distribution is revealed at each iteration for udating the model. Although consistent convex surrogate losses for AUC maximization have been roosed to make the roblem tractable, it remains an challenging roblem to design fast otimization algorithms in the classical stochastic setting due to that the convex surrogate loss deends on random airs of examles from ositive and negative classes. Building on a saddle oint formulation for a consistent square loss,  this aer rooses a novel stochastic algorithm to imrove the standard $O(1\sqrt{n})$ convergence rate to $\widetilde O(1n)$ convergence rate without strong convexity assumtion or any favorable statistical assumtions (e.g., low noise), where $n$ is the number of random samles. To the best of our knowledge, this is the first stochastic algorithm for AUC maximization with a statistical convergence rate as fast as $O(1n)$ u to a logarithmic factor. Extensive exeriments on eight large-scale benchmark data sets demonstrate the suerior erformance of the roosed algorithm comaring with existing stochastic or online algorithms for AUC maximization."
368,2018,Accurate Uncertainties for Deep Learning Using Calibrated Regression,Oral/Poster,"Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods rovide a general framework to quantify uncertainty. However, because of model missecification and the use of aroximate inference, Bayesian uncertainty estimates are often inaccurate — for examle, a 90% credible interval may not contain the true outcome 90% of the time. Here, we roose a simle rocedure for calibrating any regression algorithm; when alied to Bayesian and robabilistic models, it is guaranteed to roduce calibrated uncertainty estimates given enough data. Our rocedure is insired by Platt scaling and extends revious work on classification. We evaluate this aroach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently oututs well-calibrated credible intervals while imroving erformance on time series forecasting and model-based reinforcement learning tasks.
"
369,2018,Neural Autoregressive Flows,Oral/Poster,"Normalizing flows and autoregressive models have been successfully combined to roduce state-of-the-art results in density estimation, via Masked Autoregressive Flows (MAF) (Paamakarios et al., 2017), and to accelerate state-of-the-art WaveNet-based seech synthesis to 20x faster than real-time (Oord et al., 2017), via Inverse Autoregressive Flows (IAF) (Kingma et al., 2016). We unify and generalize these aroaches, relacing the (conditionally) affine univariate transformations of MAFIAF with a more general class of invertible univariate transformations exressed as monotonic neural networks. We demonstrate that the roosed neural autoregressive flows (NAF) are universal aroximators for continuous robability distributions, and their greater exressivity allows them to better cature multimodal target distributions. Exerimentally, NAF yields state-of-the-art erformance on a suite of density estimation tasks and outerforms IAF in variational autoencoders trained on binarized MNIST.
"
370,2018,Probabilistic Boolean Tensor Decomposition,Oral/Poster,"Boolean tensor decomosition aroximates data of multi-way binary relationshis as roduct of interretable low-rank binary factors, following the rules Boolean algebra. Here, we resent its first robabilistic treatment. We facilitate scalable samling-based osterior inference by exloitation of the combinatorial structure of the factor conditionals. Maximum a osteriori estimates consistently outerform existing non-robabilistic aroaches. We show that our erformance gains can artially be exlained by convergence to solutions that occuy relatively large regions of the arameter sace, as well as by imlicit model averaging. Moreover, the Bayesian treatment facilitates model selection with much greater accuracy than the reviously suggested minimum descrition length based aroach. We investigate three real-world data sets. First, temoral interaction networks and behavioural data of university students demonstrate the inference of instructive latent atterns. Next, we decomose a tensor with more than 10 Billion data oints, indicating relations of gene exression in cancer atients. Not only does this demonstrate scalability, it also rovides an entirely novel ersective on relational roerties of continuous data and, in the resent examle, on the molecular heterogeneity of cancer. Our imlementation is available on GitHub: htts:github.comTammoRLogicalFactorisationMachines
"
371,2018,A Primal-Dual Analysis of Global Optimality in Nonconvex Low-Rank Matrix Recovery,Oral/Poster,"We roose a rimal-dual based framework for analyzing the  global otimality of nonconvex low-rank matrix recovery. Our analysis are based on the restricted strongly convex and smooth conditions, which can be verified for a broad family of loss functions. In addition, our analytic framework can directly handle the widely-used incoherence constraints through the lens of duality. We illustrate the alicability of the roosed framework to matrix comletion and one-bit matrix comletion, and rove that all these roblems have no surious local minima. Our results not only imrove the samle comlexity required for characterizing the global otimality of matrix comletion, but also resolve an oen roblem in \citet{ge2017no} regarding one-bit matrix comletion. Numerical exeriments show that rimal-dual based algorithm can successfully recover the global otimum for various low-rank roblems.
"
372,2018,A Delay-tolerant Proximal-Gradient Algorithm for Distributed Learning,Oral/Poster,"Distributed learning aims at comuting high-quality models by training over scattered data. This covers a diversity of scenarios, including comuter clusters or mobile agents. One of the main challenges is then to deal with heterogeneous machines and unreliable communications. In this setting, we roose and analyze a flexible asynchronous otimization algorithm for solving nonsmooth learning roblems. Unlike most existing methods, our algorithm is adjustable to various levels of communication costs, machines comutational owers, and data distribution evenness. We rove that the algorithm converges linearly with a fixed learning rate that does not deend on communication delays nor on the number of machines. Although long delays in communication may slow down erformance, no delay can break convergence.
"
373,2018,Randomized Block Cubic Newton Method,Oral/Poster,"We study the roblem of minimizing the sum of three convex functions: a differentiable, twice-differentiable and a non-smooth term in a high dimensional setting. To this effect we roose and analyze a randomized block cubic Newton (RBCN) method, which in each iteration builds a model of the objective function formed as the sum of the natural models of its three comonents: a linear model with a quadratic regularizer for the differentiable term, a quadratic model with a cubic regularizer for the twice differentiable term, and erfect (roximal)  model for the nonsmooth term. Our method in each iteration minimizes the model over a random subset of blocks of the search variable. RBCN is the first algorithm with these roerties, generalizing several existing methods, matching the best known bounds in all secial cases. We establish ${\cal O}(1\esilon)$, ${\cal O}(1\sqrt{\esilon})$ and ${\cal O}(\log (1\esilon))$ rates under different assumtions on the comonent functions. Lastly, we show numerically that our method outerforms the state-of-the-art on a variety of machine learning roblems, including cubically regularized least-squares, logistic regression with constraints, and Poisson regression."
374,2018,Massively Parallel Algorithms and Hardness for Single-Linkage Clustering under $\ell_p$ Distances,Oral/Poster,"We resent first massively arallel (MPC) algorithms and hardness of aroximation results for comuting  Single-Linkage Clustering of n inut d-dimensional vectors under Hamming, $\ell_1, \ell_2$ and $\ell_\infty$ distances. All our algorithms run in O(log n) rounds of MPC for any fixed d and achieve (1+\esilon)-aroximation for all distances (excet Hamming for which we show an exact algorithm). We also show constant-factor inaroximability results for o(\log n)-round algorithms under standard MPC hardness assumtions (for sufficiently large dimension deending on the distance used). Efficiency of imlementation of our algorithms in Aache Sark is demonstrated through exeriments on the largest available vector datasets from the UCI machine learning reository exhibiting seedus of several orders of magnitude."
375,2018,Local Density Estimation in High Dimensions,Oral/Poster,"An imortant question that arises in the study of high dimensional vector reresentations learned from data is: given a set D of vectors and a query q, estimate the number of oints within a secified distance threshold of q. Our algorithm uses locality sensitive hashing to rerocess the data to accurately and efficiently estimate the answers to such questions via an unbiased estimator that uses imortance samling. A key innovation is the ability to maintain a small number of hash tables via rerocessing data structures and algorithms that samle from multile buckets in each hash table. We give bounds on the sace requirements and query comlexity of our scheme, and demonstrate the effectiveness of our algorithm by exeriments on a standard word embedding dataset.
"
376,2018,To Understand Deep Learning We Need to Understand Kernel Learning,Oral/Poster,"Generalization erformance of classifiers in dee learning has recently become a subject of intense study. Dee models, which are tyically heavily over-arametrized, tend to fit the training data exactly.  Desite this codeoverfitting"", they erform well on test data, a  henomenon  not yet fully understood. The first oint of our aer is that strong erformance of overfitted classifiers is not a unique feature of dee learning. Using six real-world  and two synthetic datasets, we establish exerimentally that kernel machines trained to have zero classification error  or  near zero regression error (interolation) erform very well on test data. We roceed to give a lower bound on the norm of zero loss solutions for smooth kernels, showing that they increase nearly exonentially with  data size.  None of the existing bounds roduce non-trivial results for interolating solutions. We also show exerimentally that (non-smooth) Lalacian kernels easily fit random labels, a finding that arallels results recently reorted for ReLU neural networks. In contrast, fitting noisy data requires many more eochs for smooth Gaussian kernels. Similar  erformance of overfitted Lalacian and Gaussian classifiers on test, suggests that  generalization is tied to the roerties of the kernel function  rather than the  otimization rocess. Some  key henomena of  dee learning are manifested similarly in  kernel methods in the moderncodeoverfitted"" regime. The combination of the exerimental and theoretical results resented in this aer indicates a need for  new theoretical ideas for understanding roerties of classical kernel methods. We argue that rogress on understanding  dee learning will be difficult until more  tractable ``shallow'' kernel methods are better understood.
"
377,2018,Learning in Reproducing Kernel Kreı̆n Spaces,Oral/Poster,"We formulate a novel regularized risk minimization roblem for learning in reroducing kernel Kreı̆n saces and show that the strong reresenter theorem alies to it. As a result of the latter, the learning roblem can be exressed as the minimization of a quadratic form over a hyershere of constant radius. We resent an algorithm that can find a globally otimal solution to this non-convex otimization roblem in time cubic in the number of instances. Moreover, we derive the gradient of the solution with resect to its hyerarameters and, in this way, rovide means for efficient hyerarameter tuning. The aroach comes with a generalization bound exressed in terms of the Rademacher comlexity of the corresonding hyothesis sace. The major advantage over standard kernel methods is the ability to learn with various domain secific similarity measures for which ositive definiteness does not hold or is difficult to establish. The aroach is evaluated emirically using indefinite kernels defined on structured as well as vectorial data. The emirical results demonstrate a suerior erformance of our aroach over the state-of-the-art baselines.
"
378,2018,Functional Gradient Boosting based on Residual Network Perception,Oral/Poster,"Residual Networks (ResNets) have become state-of-the-art models in dee learning and several theoretical studies have been devoted to understanding why ResNet works so well. One attractive viewoint on ResNet is that it is otimizing the risk in a functional sace by consisting of an ensemble of effective features.  In this aer, we adot this viewoint to construct a new gradient boosting method, which is known to be very owerful in data analysis. To do so, we formalize the boosting ersective of ResNet mathematically using the notion of functional gradients and roose a new method called ResFGB for classification tasks by leveraging ResNet ercetion. Two tyes of generalization guarantees are rovided from the otimization ersective: one is the margin bound and the other is the exected risk bound by the samle-slitting technique. Exerimental results show suerior erformance of the roosed method over state-of-the-art methods such as LightGBM.
"
379,2018,"Binary Classification with Karmic, Threshold-Quasi-Concave Metrics",Oral/Poster,"Comlex erformance measures, beyond the oular measure of accuracy, are increasingly being used in the context of binary classification. These comlex erformance measures are tyically not even decomosable, that is, the loss evaluated on a batch of samles cannot tyically be exressed as a sum or average of losses evaluated at individual samles, which in turn requires new theoretical and methodological develoments beyond standard treatments of suervised learning. In this aer, we advance this understanding of binary classification for comlex erformance measures by identifying two key roerties: a so-called Karmic roerty, and a more technical threshold-quasi-concavity roerty, which we show is milder than existing structural assumtions imosed on erformance measures. Under these roerties, we show that the Bayes otimal classifier is a threshold function of the conditional robability of ositive class. We then leverage this result to come u with a comutationally ractical lug-in classifier, via a novel threshold estimator, and further, rovide a novel statistical analysis of classification error with resect to comlex erformance measures.
"
380,2018,Characterizing Implicit Bias in Terms of Optimization Geometry,Oral/Poster,"We study the bias of generic otimization methods, including Mirror Descent, Natural Gradient Descent and Steeest Descent with resect to different otentials and norms, when otimizing underdetermined linear models or searable linear classification roblems.  We ask the question of whether the global minimum (among the many ossible global minima) reached by otimization can be characterized in terms of the otential or norm, and indecently of hyer-arameter choices, such as stesize and momentum.
"
381,2018,prDeep: Robust Phase Retrieval with a Flexible Deep Network,Oral/Poster,"Phase retrieval algorithms have become an imortant comonent in many modern comutational imaging systems.  For instance, in the context of tychograhy and seckle correlation imaging, they enable imaging ast the diffraction limit and through scattering media, resectively. Unfortunately, traditional hase retrieval algorithms struggle in the resence of noise. Progress has been made recently on more robust algorithms using signal riors, but at the exense of limiting the range of suorted measurement models (e.g., to Gaussian or coded diffraction atterns).  In this work we leverage the regularization-by-denoising framework and a convolutional neural network denoiser to create {\em rDee}, a new hase retrieval algorithm that is both robust and broadly alicable.   We test and validate rDee in simulation to demonstrate that it is robust to noise and can handle a variety of system models.
"
382,2018,Adversarial Time-to-Event Modeling,Oral/Poster,"Modern health data science alications leverage abundant molecular and electronic health data, roviding oortunities for machine learning to build statistical models to suort clinical ractice. Time-to-event analysis, also called survival analysis, stands as one of the most reresentative examles of such statistical models. We resent a dee-network-based aroach that leverages adversarial learning to address a key challenge in modern time-to-event modeling: nonarametric estimation of event-time distributions. We also introduce a rinciled cost function to exloit information from censored events (events that occur subsequent to the observation window). Unlike most time-to-event models, we focus on the estimation of time-to-event distributions, rather than time ordering. We validate our model on both benchmark and real datasets, demonstrating that the roosed formulation yields significant erformance gains relative to a arametric alternative, which we also roose.
"
383,2018,MAGAN: Aligning Biological Manifolds,Oral/Poster,"It is increasingly common in many tyes of natural and hysical systems (esecially biological systems) to have different tyes of measurements erformed on the same underlying system. In such settings, it is imortant to align the manifolds arising from each measurement in order to integrate such data and gain an imroved icture of the system; we tackle this roblem using generative adversarial networks (GANs). Recent attemts to use GANs to find corresondences between sets of samles do not exlicitly erform roer alignment of manifolds. We resent the new Manifold Aligning GAN (MAGAN) that aligns two manifolds such that related oints in each measurement sace are aligned. We demonstrate alications of MAGAN in single-cell biology in integrating two different measurement tyes together: cells from the same tissue are measured with both genomic (single-cell RNA-sequencing) and roteomic (mass cytometry) technologies. We show that MAGAN successfully aligns manifolds such that known correlations between measured markers are imroved comared to other recently roosed models.
"
384,2018,Multicalibration: Calibration for the (Computationally-Identifiable) Masses,Oral/Poster,"We develo and study multicalibration as a new measure of fairness in machine learning that aims to mitigate inadvertent or malicious discrimination that is introduced at training time (even from ground truth data).  Multicalibration guarantees meaningful (calibrated) redictions for every suboulation that can be identified within a secified class of comutations.  The secified class can be quite rich; in articular, it can contain many overlaing subgrous of a rotected grou.  We demonstrate that in many settings this strong notion of rotection from discrimination is rovably attainable and aligned with the goal of obtaining accurate redictions. Along the way, we resent algorithms for learning a multicalibrated redictor, study the comutational comlexity of this task, and illustrate tight connections to the agnostic learning model.
"
385,2018,Improving the Privacy and Accuracy of ADMM-Based Distributed Algorithms,Oral/Poster,"Alternating direction method of multilier (ADMM) is a oular method used to design distributed versions of a machine learning algorithm, whereby local comutations are erformed on local data with the outut exchanged among neighbors in an iterative fashion. During this iterative rocess the leakage of data rivacy arises. A differentially rivate ADMM was roosed in rior work (Zhang &am; Zhu, 2017) where only the rivacy loss of a single node during one iteration was bounded, a method that makes it difficult to balance the tradeoff between the utility attained through distributed comutation and rivacy guarantees when considering the total rivacy loss of all nodes over the entire iterative rocess. We roose a erturbation method for ADMM where the erturbed term is correlated with the enalty arameters; this is shown to imrove the utility and rivacy simultaneously. The method is based on a modified ADMM where each node indeendently determines its own enalty arameter in every iteration and decoules it from the dual udating ste size. The condition for convergence of the modified ADMM and the lower bound on the convergence rate are also derived.
"
386,2018,PixelSNAIL: An Improved Autoregressive Generative Model,Oral/Poster,"Autoregressive generative models achieve the best results in density estimation tasks involving high dimensional data, such as images or audio. They ose density estimation as a sequence modeling task, where a recurrent neural network (RNN) models the conditional distribution over the next element conditioned on all revious elements. In this aradigm, the bottleneck is the extent to which the RNN can model long-range deendencies, and the most successful aroaches rely on causal convolutions. Taking insiration from recent work in meta reinforcement learning, where dealing with long-range deendencies is also essential, we introduce a new generative model architecture that combines causal convolutions with self attention. In this aer, we describe the resulting model and resent state-of-the-art log-likelihood results on heavily benchmarked datasets: CIFAR-10, $32 \times 32$ ImageNet and $64 \times 64$ ImageNet. Our imlementation will be made available at \url{htts:github.comneocxiixelsnail-ublic}."
387,2018,Focused Hierarchical RNNs for Conditional Sequence Processing,Oral/Poster,"Recurrent Neural Networks (RNNs) with attention mechanisms have obtained state-of-the-art results for many sequence rocessing tasks. Most of these models use a simle form of encoder with attention that looks over the entire sequence and assigns a weight to each token indeendently. We resent a mechanism for focusing RNN encoders for sequence modelling tasks which allows them to attend to key arts of the inut as needed. We formulate this using a multi-layer conditional %hierarchical  sequence encoder that reads in one token at a time and makes a discrete decision on whether the token is relevant to the context or question being asked. The discrete gating mechanism takes in the context embedding and the current hidden state as inuts and controls information flow into the layer above. We train it using olicy gradient methods. We evaluate this method on several tyes of tasks with different attributes. First, we evaluate the method on synthetic tasks which allow us to evaluate the model for its generalization ability and robe the behavior of the gates in more controlled settings. We then evaluate this aroach on large scale Question  Answering tasks including the challenging MS MARCO and SearchQA tasks. Our models shows consistent imrovements for both tasks over rior work and our baselines. It has also shown to generalize significantly better on synthetic tasks as comared to the baselines.
"
388,2018,Noise2Noise: Learning Image Restoration without Clean Data,Oral/Poster,"We aly basic statistical reasoning to signal reconstruction by machine learning - learning to ma corruted observations to clean signals - with a simle and owerful conclusion: it is ossible to learn to restore images by only looking at corruted examles, at erformance at and sometimes exceeding training using clean data, without exlicit image riors or likelihood models of the corrution. In ractice, we show that a single model learns hotograhic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersamled MRI scans - all corruted by different rocesses - based on noisy data only.
"
389,2018,Learning to Reweight Examples for Robust Deep Learning,Oral/Poster,"Dee neural networks have been shown to be very owerful modeling tools for many suervised learning tasks involving comlex inut atterns.  However, they can also easily overfit to training set biases and label noises.  In addition to various regularizers, examle reweighting algorithms are oular solutions to these roblems, but they  require careful tuning of additional hyerarameters, such as examle mining schedules and regularization hyerarameters. In contrast to ast reweighting methods, which tyically consist of functions of the cost value of each examle, in this work we roose a novel meta-learning algorithm that learns to assign weights to training examles based on their gradient directions. To determine the examle weights, our method erforms a meta gradient descent ste on the current mini-batch examle weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our roosed method can be easily imlemented on any tye of dee network, does not require any additional hyerarameter tuning, and achieves imressive erformance on class imbalance and corruted label roblems where only a small amount of clean validation data is available.
"
390,2018,Policy and Value Transfer in Lifelong Reinforcement Learning,Oral/Poster,"We consider the roblem of how best to use rior exerience to bootstra lifelong learning, where an agent faces a series of task instances drawn from some task distribution. First, we identify the initial olicy that otimizes exected erformance over the distribution of tasks for increasingly comlex classes of olicy and task distributions. We emirically demonstrate the relative erformance of each olicy class’ otimal element in a variety of simle task distributions. We then consider value-function initialization methods that reserve PAC guarantees while simultaneously minimizing the learning required in two learning algorithms, yielding MaxQInit, a ractical new method for value-function-based transfer. We show that MaxQInit erforms well in simle lifelong RL exeriments.
"
391,2018,GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms,Oral/Poster,"In continuous action domains, standard dee reinforcement learning algorithms like DDPG suffer from inefficient exloration when facing sarse or decetive reward roblems. Conversely, evolutionary and develomental methods focusing on exloration like Novelty Search, Quality-Diversity or Goal Exloration Processes exlore more robustly but are less efficient at fine-tuning olicies using gradient-descent. In this aer, we resent the GEP-PG aroach, taking the best of both worlds by sequentially combining a Goal Exloration Process and two variants of DDPG . We study the learning erformance of these comonents and their combination on a low dimensional decetive reward roblem and on the larger Half-Cheetah benchmark. We show that DDPG fails on the former and that GEP-PG imroves over the best DDPG variant in both environments.
"
392,2018,A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music,Oral/Poster,"The Variational Autoencoder (VAE) has roven to be an effective model for roducing semantically meaningful latent reresentations for natural data. However, it has thus far seen limited alication to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we roose the use of a hierarchical decoder, which first oututs embeddings for subsequences of the inut and then uses these embeddings to generate each subsequence indeendently. This structure encourages the model to utilize its latent code, thereby avoiding the ""osterior collase"" roblem which remains an issue for recurrent VAEs. We aly this architecture to modeling sequences of musical notes and find that it exhibits dramatically better samling, interolation, and reconstruction erformance than a ""flat"" baseline model. An imlementation of our ""MusicVAE"" is available online at htts:goo.glmagentamusicvae-code.
"
393,2018,Understanding the Loss Surface of Neural Networks for Binary Classification,Oral/Poster,"It is widely conjectured that training algorithms for neural networks are successful because all local minima lead to similar erformance; for examle, see (LeCun et al., 2015; Choromanska et al., 2015; Dauhin et al., 2014). Performance is tyically measured in terms of two metrics: training erformance and generalization erformance. Here we focus on the training erformance of neural networks for binary classification, and rovide conditions under which the training error is zero at all local minima of aroriately chosen surrogate loss functions. Our conditions are roughly in the following form: the neurons have to be increasing and strictly convex, the neural network should either be single-layered or is multi-layered with a shortcut-like connection, and the surrogate loss function should be a smooth version of hinge loss. We also rovide counterexamles to show that, when these conditions are relaxed, the result may not hold.
"
394,2018,Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks,Oral/Poster,"Recurrent neural networks have gained widesread use in modeling sequence data across various domains. While many successful recurrent architectures emloy a notion of gating, the exact mechanism that enables such remarkable erformance is not well understood. We develo a theory for signal roagation in recurrent networks after random initialization using a combination of mean field theory and random matrix theory. To simlify our discussion, we introduce a new RNN cell with a simle gating mechanism that we call the minimalRNN and comare it with vanilla RNNs. Our theory allows us to define a maximum timescale over which RNNs can remember an inut. We show that this theory redicts trainability for both recurrent architectures. We show that gated recurrent networks feature a much broader, more robust, trainable region than vanilla RNNs, which corroborates recent exerimental findings. Finally, we develo a closed-form critical initialization scheme that achieves dynamical isometry in both vanilla RNNs and minimalRNNs. We show that this results in significantly imrovement in training dynamics. Finally, we demonstrate that the minimalRNN achieves comarable erformance to its more comlex counterarts, such as LSTMs or GRUs, on a language modeling task.
"
395,2018,Reviving and Improving Recurrent Back-Propagation,Oral/Poster,"In this aer, we revisit the recurrent back-roagation (RBP) algorithm, discuss the conditions under which it alies as well as how to satisfy them in dee neural networks.  We show that RBP can be unstable and roose two variants based on conjugate gradient on the normal equations (CG-RBP) and Neumann series (Neumann-RBP). We further investigate the relationshi between Neumann-RBP and back roagation through time (BPTT) and its truncated version (TBPTT). Our Neumann-RBP has the same time comlexity as TBPTT but only requires constant memory, whereas TBPTT's memory cost scales linearly with the number of truncation stes. We examine all RBP variants along with BPTT and TBPTT in three different alication domains: associative memory with continuous Hofield networks, document classification in citation networks using grah neural networks and hyerarameter otimization for fully connected networks. All exeriments demonstrate that RBPs, esecially the Neumann-RBP variant, are efficient and effective for otimizing convergent recurrent neural networks.
"
396,2018,Riemannian Stochastic Recursive Gradient Algorithm with Retraction and Vector Transport and Its Convergence Analysis,Oral/Poster,"Stochastic variance reduction algorithms have recently become oular for minimizing the average of a large, but finite number of loss functions on a Riemannian manifold. The resent aer rooses a Riemannian stochastic recursive gradient algorithm (R-SRG), which does not require the inverse of retraction between two distant iterates on the manifold. Convergence analyses of R-SRG are erformed on both retraction-convex and non-convex functions under comutationally efficient retraction and vector transort oerations. The key challenge is analysis of the influence of vector transort along the retraction curve. Numerical evaluations reveal that R-SRG cometes well with state-of-the-art Riemannian batch and stochastic gradient algorithms.
"
397,2018,Learning Compact Neural Networks with Regularization,Oral/Poster,"Proer regularization is critical for seeding u training, imroving generalization erformance, and learning comact models that are cost efficient. We roose and analyze regularized gradient descent algorithms for learning shallow neural networks. Our framework is general and covers weight-sharing (convolutional networks), sarsity (network runing), and low-rank constraints among others. We first introduce covering dimension to quantify the comlexity of the constraint set and rovide insights on the generalization roerties. Then, we show that roosed algorithms become well-behaved and local linear convergence occurs once the amount of data exceeds the covering dimension. Overall, our results demonstrate that near-otimal samle comlexity is sufficient for efficient learning and illustrate how regularization can be beneficial to learn over-arameterized networks.
"
398,2018,Investigating Human Priors for Playing Video Games,Oral/Poster,"What makes humans so good at solving seemingly comlex video games?  Unlike comuters, humans bring in a great deal of rior knowledge about the world, enabling efficient decision making. This aer investigates the role of human riors for solving video games. Given a samle game, we conduct a series of ablation studies to quantify the imortance of various riors on human erformance. We do this by modifying the video game environment to systematically mask different tyes of visual information that could be used by humans as riors. We find that removal of some rior knowledge causes a drastic degradation in the seed with which human layers solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general riors, such as the imortance of objects and visual consistency, are critical for efficient game-lay. Videos and the game maniulations are available at htts:rach0012.github.iohumanRL_website
"
399,2018,Decoupling Gradient-Like Learning Rules from Representations,Oral/Poster,"In machine learning, learning often corresonds to changing the arameters of a arameterized function. A learning rule is an algorithm or mathematical exression that secifies recisely how the arameters should be changed. When creating a machine learning system, we must make two decisions: what reresentation should be used (i.e., what arameterized function should be used) and what learning rule should be used to search through the resulting set of reresentable functions. In this aer we focus on gradient-like learning rules, wherein these two decisions are couled in a subtle (and often unintentional) way. Using most learning rules, these two decisions are couled in a subtle (and often unintentional) way. That is, using the same learning rule with two different reresentations that can reresent the same sets of functions can result in two different outcomes. After arguing that this couling is undesirable, articularly when using neural networks, we resent a method for artially decouling these two decisions for a broad class of gradient-like learning rules that san unsuervised learning, reinforcement learning, and suervised learning.
"
400,2018,Invariance of Weight Distributions in Rectified MLPs,Oral/Poster,"An interesting aroach to analyzing neural networks that has received renewed attention is to examine the equivalent kernel of the neural network. This is based on the fact that a fully connected feedforward network with one hidden layer, a certain weight distribution, an activation function, and an infinite number of neurons can be viewed as a maing into a Hilbert sace. We derive the equivalent kernels of MLPs with ReLU or Leaky ReLU activations for all rotationally-invariant weight distributions, generalizing a revious result that required Gaussian weight distributions. Additionally, the Central Limit Theorem is used to show that for certain activation functions, kernels corresonding to layers with weight distributions having $0$ mean and finite absolute third moment are asymtotically universal, and are well aroximated by the kernel corresonding to layers with sherical Gaussian weights. In dee networks, as deth increases the equivalent kernel aroaches a athological fixed oint, which can be used to argue why training randomly initialized networks can be difficult. Our results also have imlications for weight initialization."
401,2018,Stronger Generalization Bounds for Deep Nets via a Compression Approach,Oral/Poster,"Dee nets generalize well desite having more arameters than the number of training samles. Recent works try to give an exlanation using PAC-Bayes and Margin-based analyses, but do not as yet result in samle comlexity bounds better than naive arameter counting. The current aer shows generalization bounds that are orders of magnitude better in ractice. These rely uon new succinct rearametrizations of the trained net --- a comression that is exlicit and efficient. These yield generalization bounds via a simle comression-based framework introduced here. Our results also rovide some theoretical justification for widesread emirical success in comressing dee nets. Analysis of correctness of our comression relies uon some newly identified noise stability roerties of trained dee nets, which are also exerimentally verified. The study of these roerties and resulting generalization bounds are also extended to convolutional nets, which had eluded  earlier attemts on roving generalization.
"
402,2018,Near Optimal Frequent Directions for Sketching Dense and Sparse Matrices,Oral/Poster,"Given a large matrix $A\in\real^{n\times d}$, we consider the roblem of comuting a sketch matrix $B\in\real^{\ell\times d}$ which is significantly smaller than but still well aroximates $A$. We are interested in minimizing the \emh{covariance error} $\norm{A^TA-B^TB}_2.$ We consider the roblems in the streaming model, where the algorithm can only make one ass over the inut with limited working sace. The oular Frequent Directions algorithm of~\cite{liberty2013simle} and its variants achieve otimal sace-error tradeoff. However, whether the running time can be imroved remains an unanswered question. In this aer, we almost settle the time comlexity of this roblem. In articular, we rovide new sace-otimal algorithms with faster running times.  Moreover, we also show that the running times of our algorithms are near-otimal unless the state-of-the-art running time of matrix multilication can be imroved significantly."
403,2018,Loss Decomposition for Fast Learning in Large Output Spaces,Oral/Poster,"For roblems with large outut saces, evaluation of the loss function and its gradient are exensive, tyically taking linear time in the size of the outut sace. Recently, methods have been develoed to seed u learning via efficient data structures for Nearest-Neighbor Search (NNS) or Maximum Inner-Product Search (MIPS). However, the erformance of such data structures tyically degrades in high dimensions. In this work, we roose a novel technique to reduce the intractable high dimensional search roblem to several much more tractable lower dimensional ones via dual decomosition of the loss function. At the same time, we demonstrate guaranteed convergence to the original loss via a greedy message assing rocedure. In our exeriments on multiclass and multilabel classification with hundreds of thousands of classes, as well as training ski-gram word embeddings with a vocabulary size of half a million, our technique consistently imroves the accuracy of search-based gradient aroximation methods and outerforms samling-based gradient aroximation methods by a large margin.
"
404,2018,Stochastic Proximal Algorithms for AUC Maximization,Oral/Poster,"Stochastic otimization algorithms such as SGDs udate the model sequentially with chea er-iteration costs, making them amenable for large-scale data analysis. However, most of the existing studies focus on the classification accuracy which can not be directly alied to the imortant roblems of maximizing the Area under the ROC curve (AUC) in  imbalanced classification and biartite ranking.  In this aer, we develo a novel stochastic roximal algorithm for AUC maximization which is referred to as SPAM.   Comared with the revious literature, our algorithm SPAM alies to a non-smooth enalty function, and achieves a convergence rate of O(log tt) for strongly convex functions while both sace and er-iteration costs are of one datum.
"
405,2018,Accelerated Spectral Ranking,Oral/Poster,"The roblem of rank aggregation from airwise and multiway comarisons has a wide range of imlications, ranging from recommendation systems to sorts rankings to social choice. Some of the most oular algorithms for this roblem come from the class of sectral ranking algorithms; these include the rank centrality (RC) algorithm for airwise comarisons, which returns consistent estimates under the Bradley-Terry-Luce (BTL) model for airwise comarisons (Negahban et al., 2017), and its generalization, the Luce sectral ranking (LSR) algorithm, which returns consistent estimates under the more general multinomial logit (MNL) model for multiway comarisons (Maystre &am; Grossglauser, 2015). In this aer, we design a rovably faster sectral ranking algorithm, which we call accelerated sectral ranking (ASR), that is also consistent under the MNLBTL models.  Our accelerated algorithm is achieved by designing a random walk that has a faster mixing time than the random walks associated with revious algorithms. In addition to a faster algorithm, our results yield imroved samle comlexity bounds for recovery of the MNLBTL arameters: to the best of our knowledge, we give the first general samle comlexity bounds for recovering the arameters of the MNL model from multiway comarisons under any (connected) comarison grah (and imrove significantly over revious bounds for the BTL model for airwise comarisons). We also give a message-assing interretation of our algorithm, which suggests a decentralized distributed imlementation. Our exeriments on several real-world and synthetic datasets confirm that our new ASR algorithm is indeed orders of magnitude faster than existing algorithms.
"
406,2018,Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning,Oral/Poster,"Bayesian neural networks with latent variables are scalable and flexible robabilistic models: they account for uncertainty in the estimation of the network weights and, by making use of latent variables, can cature comlex noise atterns in the data. Using these models we show how to erform and utilize a decomosition of uncertainty in aleatoric and eistemic comonents for decision making uroses. This allows us to successfully identify informative oints for active learning of functions with heteroscedastic and bimodal noise. Using the decomosition we further define a novel risk-sensitive criterion for reinforcement learning to identify olicies that balance exected cost, model-bias and noise aversion.
"
407,2018,Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam,Oral/Poster,"Uncertainty comutation in dee learning is essential to design robust and reliable systems. Variational inference (VI) is a romising aroach for such comutation, but requires more effort to imlement and execute comared to maximum-likelihood methods. In this aer, we roose new natural-gradient algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms can be imlemented within the Adam otimizer by erturbing the network weights during gradient evaluations, and uncertainty estimates can be chealy obtained by using the vector that adats the learning rate. This requires lower memory, comutation, and imlementation effort than existing VI methods, while obtaining uncertainty estimates of comarable quality. Our emirical results confirm this and further suggest that the weight-erturbation in our algorithm could be useful for exloration in reinforcement learning and stochastic otimization.
"
408,2018,Learning One Convolutional Layer with Overlapping Patches,Oral/Poster,"We give the first rovably efficient algorithm for learning a one hidden layer convolutional network with resect to a general class of (otentially overlaing) atches under mild conditions on the underlying distribution.  We rove that our framework catures commonly used schemes from comuter vision, including one-dimensional and two-dimensional ``atch and stride'' convolutions. Our algorithm-- {\em Convotron}-- is insired by recent work alying isotonic regression to learning neural networks.  Convotron uses a simle, iterative udate rule that is stochastic in nature and tolerant to noise (requires only that the conditional mean function is a one layer convolutional network, as oosed to the realizable setting).  In contrast to gradient descent, Convotron requires no secial initialization or learning-rate tuning to converge to the global otimum. We also oint out that learning one hidden convolutional layer with resect to a Gaussian distribution and just {\em one} disjoint atch $P$ (the other atches may be arbitrary) is {\em easy} in the following sense: Convotron can efficiently recover the hidden weight vector by udating {\em only} in the direction of $P$."
409,2018,A Spline Theory of Deep Learning,Oral/Poster,"We build a rigorous bridge between dee networks (DNs) and aroximation theory via sline functions and oerators. Our key result is that a large class of DNs can be written as a comosition of {\em max-affine sline oerators} (MASOs), which rovide a owerful ortal through which to view and analyze their inner workings. For instance, conditioned on the inut signal, the outut of a MASO DN can be written as a simle affine transformation of the inut. This imlies that a DN constructs a set of signal-deendent, class-secific temlates against which the signal is comared via a simle inner roduct; we exlore the links to the classical theory of otimal classification via matched filters and the effects of data memorization. Going further, we roose a simle enalty term that can be added to the cost function of any DN learning algorithm to force the temlates to be orthogonal with each other; this leads to significantly imroved classification erformance and reduced overfitting with no change to the DN architecture.  The sline artition of the inut signal sace oens u a new geometric avenue to study how DNs organize signals in a hierarchical fashion. As an alication, we develo and validate a new distance metric for signals that quantifies the difference between their artition encodings.
"
410,2018,Structured Variational Learning of Bayesian Neural Networks with Horseshoe Priors,Oral/Poster,"Bayesian Neural Networks (BNNs) have recently received increasing attention for their ability to rovide well-calibrated osterior uncertainties.  However, model selection---even choosing the number of nodes---remains an oen question.  Recent work has roosed the use of a horseshoe rior over node re-activations of a Bayesian neural network, which effectively turns off nodes that do not hel exlain the data.  In this work, we roose several modeling and inference advances that consistently imrove the comactness of the model learned while maintaining redictive erformance, esecially in smaller-samle settings including reinforcement learning.
"
411,2018,Variational Bayesian dropout: pitfalls and fixes,Oral/Poster,"Droout, a~stochastic regularisation technique for training of neural networks, has recently been reinterreted as a~secific tye of aroximate inference algorithm for Bayesian neural networks. The~main contribution of the~reinterretation is in roviding a~theoretical framework useful for analysing and extending the~algorithm. We show that the~roosed framework suffers from several issues; from undefined or athological behaviour of the~true osterior related to use of imroer riors, to an ill-defined variational objective due to singularity of the~aroximating distribution relative to the~true osterior. Our analysis of the~imroer log uniform rior used in variational Gaussian droout suggests the~athologies are generally irredeemable, and that the~algorithm still works only because the~variational formulation annuls some of the~athologies. To address the~singularity issue, we roffer Quasi-KL (QKL) divergence, a~new aroximate inference objective for aroximation of high-dimensional distributions. We show that motivations for variational Bernoulli droout based on discretisation and noise have QKL as a limit. Proerties of QKL are studied both theoretically and on a~simle ractical examle which shows that the~QKL-otimal aroximation of a~full rank Gaussian with a~degenerate one naturally leads to the~Princial Comonent Analysis solution.
"
412,2018,Adversarial Learning with Local Coordinate Coding,Oral/Poster,"Generative adversarial networks (GANs) aim to generate realistic data from some rior distribution (e.g., Gaussian noises). However, such rior distribution is often indeendent of real data and thus may lose semantic information (e.g., geometric structure or content in images) of data. In ractice, the semantic information might be reresented by some latent distribution learned from data, which, however, is hard to be used for samling in GANs. In this aer, rather than samling from the re-defined rior distribution, we roose a Local Coordinate Coding (LCC) based samling method to imrove GANs. We derive a generalization bound for LCC based GANs and rove that a small dimensional inut is sufficient to achieve good generalization. Extensive exeriments on various real-world datasets demonstrate the effectiveness of the roosed method.
"
413,2018,Learning Representations and Generative Models for 3D Point Clouds,Oral/Poster,"Three-dimensional geometric data offer an excellent domain for studying reresentation learning and generative modeling. In this aer, we look at geometric data reresented as oint clouds. We introduce a dee AutoEncoder (AE) network with state-of-the-art reconstruction quality and generalization ability. The learned reresentations outerform existing methods on 3D recognition tasks and enable shae editing via simle algebraic maniulations, such as semantic art editing, shae analogies and shae interolation, as well as shae comletion. We erform a thorough study of different generative models including GANs oerating on the raw oint clouds, significantly imroved GANs trained in the fixed latent sace of our AEs, and Gaussian Mixture Models (GMMs). To quantitatively evaluate generative models we introduce measures of samle fidelity and diversity based on matchings between sets of oint clouds. Interestingly, our evaluation of generalization, fidelity and diversity reveals that GMMs trained in the latent sace of our AEs yield the best results overall.
"
414,2018,Bayesian Uncertainty Estimation for Batch Normalized Deep Networks,Oral/Poster,"We show that training a dee network using batch normalization is equivalent to aroximate inference in Bayesian models. We further demonstrate that this finding allows us to make meaningful estimates of the model uncertainty using conventional architectures, without modifications to the network or the training rocedure. Our aroach is thoroughly validated by measuring the quality of uncertainty in a series of emirical exeriments on different tasks. It outerforms baselines with strong statistical significance, and dislays cometitive erformance with recent Bayesian aroaches.
"
415,2018,Noisy Natural Gradient as Variational Inference,Oral/Poster,"Variational Bayesian neural nets combine the flexibility of dee learning with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff between chea but simle variational families (e.g.~fully factorized) or exensive and comlicated inference rocedures. We show that natural gradient ascent with adative weight noise imlicitly fits a variational osterior to maximize the evidence lower bound (ELBO). This insight allows us to train full-covariance, fully factorized, or matrix-variate Gaussian variational osteriors using noisy versions of natural gradient, Adam, and K-FAC, resectively, making it ossible to scale u to modern-size ConvNets. On standard regression benchmarks, our noisy K-FAC algorithm makes better redictions and matches Hamiltonian Monte Carlo's redictive variances better than existing methods. Its imroved uncertainty estimates lead to more efficient exloration in active learning, and intrinsic motivation for reinforcement learning.
"
416,2018,Deep Variational Reinforcement Learning for POMDPs,Oral/Poster,"Many real-world sequential decision making roblems are artially observable by nature, and the environment model is tyically unknown. Consequently, there is great need for reinforcement learning methods that can tackle such roblems given only a stream of rewards and incomlete and noisy observations. In this aer, we roose dee variational reinforcement learning (DVRL), which introduces an inductive bias that allows an agent to learn a generative model of the environment and erform inference in that model to effectively aggregate the available information. We develo an n-ste aroximation to the evidence lower bound (ELBO), allowing the model to be trained jointly with the olicy. This ensures that the latent state reresentation is suitable for the control task. In exeriments on Mountain Hike and flickering Atari we show that our method outerforms revious aroaches relying on recurrent neural networks to encode the ast.
"
417,2018,Recurrent Predictive State Policy Networks,Oral/Poster,"We introduce Recurrent Predictive State Policy(RPSP)  networks,  a recurrent architecture that brings insights from redictive state reresentations to reinforcement learning in artially ob-servable environments.   Predictive state olicy networks consist of a recursive filter, which kees track of a belief about the state of the environment, and a reactive olicy that directly mas beliefs to actions, to maximize the cumulative reward. The recursive filter leverages redictive state reresentations (PSRs) (Rosencrantz &am; Gordon, 2004; Sun et al., 2016) by modeling redictive state—a rediction of the distribution of future observations conditioned on history and future actions.This reresentation gives rise to a rich class of statistically consistent algorithms (Hefny et al.,2017) to initialize the recursive filter. Predictive stats serves as an equivalent reresentation of a belief state. Therefore, the olicy comonent of the RPSP-network can be urely reactive, simlifying training while still allowing otimal behavior. Moreover, we use the PSR interretation during training as well, by incororating rediction error in the loss function. The entire network (recursive filter and reactive olicy) is still differentiable and can be trained using gradient-based methods. We otimize our olicy using a combination of olicy gradient based on rewards (Williams, 1992)and gradient descent based on rediction error.We show the efficacy of RPSP-networks on a set of robotic control tasks from OenAI Gym. We emirically show that RPSP-networks erform well comared with memory-reserving networks such as GRUs, as well as finite memory models, being the overall best erforming method.
"
418,2018,The Mechanics of n-Player Differentiable Games,Oral/Poster,"The cornerstone underinning dee learning is the guarantee that gradient descent on an objective converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, where there are multile interacting losses. The behavior of gradient-based methods in games is not well understood -- and is becoming increasingly imortant as adversarial and multi-objective architectures roliferate. In this aer, we develo new techniques to understand and control the dynamics in general games. The key result is to decomose the second-order dynamics into two comonents. The first is related to otential games, which reduce to gradient descent on an imlicit function; the second relates to Hamiltonian games, a new class of games that obey a conservation law, akin to conservation laws in classical mechanical systems. The decomosition motivates Symlectic Gradient Adjustment (SGA), a new algorithm for finding stable fixed oints in general games. Basic exeriments show SGA is cometitive with recently roosed algorithms for finding local Nash equilibria in GANs -- whilst at the same time being alicable to -- and having guarantees in -- much more general games.
"
419,2018,Improved Training of Generative Adversarial Networks Using Representative Features,Oral/Poster,"Desite the success of generative adversarial networks (GANs) for image generation, the trade-off between visual quality and image diversity remains a significant issue. This aer achieves both aims simultaneously by imroving the stability of training GANs. The key idea of the roosed aroach is to imlicitly regularize the discriminator using reresentative features. Focusing on the fact that standard GAN minimizes reverse Kullback-Leibler (KL) divergence, we transfer the reresentative feature, which is extracted from the data distribution using a re-trained autoencoder (AE), to the discriminator of standard GANs. Because the AE learns to minimize forward KL divergence, our GAN training with reresentative features is influenced by both reverse and forward KL divergence. Consequently, the roosed aroach is verified to imrove visual quality and diversity of state of the art GANs using extensive evaluations.
"
420,2018,Hierarchical Multi-Label Classification Networks,Oral/Poster,"One of the most challenging machine learning roblems is a articular case of data classification in which classes are hierarchically structured and objects can be assigned to multile aths of the class hierarchy at the same time. This task is known as hierarchical multi-label classification (HMC), with alications in text classification, image annotation, and in bioinformatics roblems such as rotein function rediction. In this aer, we roose novel neural network architectures for HMC called HMCN, caable of simultaneously otimizing local and global loss functions for discovering local hierarchical class-relationshis and global information from the entire class hierarchy while enalizing hierarchical violations. We evaluate its erformance in 21 datasets from four distinct domains, and we comare it against the current HMC state-of-the-art aroaches. Results show that HMCN substantially outerforms all baselines with statistical significance, arising as the novel state-of-the-art for HMC.
"
421,2018,Knowledge Transfer with Jacobian Matching,Oral/Poster,"Classical distillation methods transfer reresentations from a codeteacher'' neural network to acodestudent'' network by matching their outut activations. Recent methods also match the Jacobians, or the gradient of outut activations with the inut. However, this involves making some ad hoc decisions, in articular, the choice of the loss function. In this aer, we first establish an equivalence between Jacobian matching and distillation with inut noise, from which we derive aroriate loss functions for Jacobian matching. We then rely on this analysis to aly Jacobian matching to transfer learning by establishing equivalence of a recent transfer learning rocedure to distillation. We then show exerimentally on standard image datasets that Jacobian-based enalties imrove distillation, robustness to noisy inuts, and transfer learning.
"
422,2018,Towards Black-box Iterative Machine Teaching,Oral/Poster,"In this aer, we make an imortant ste towards the black-box machine teaching by considering the cross-sace machine teaching, where the teacher and the learner use different feature reresentations and the teacher can not fully observe the learner's model. In such scenario, we study how the teacher is still able to teach the learner to achieve faster convergence rate than the traditional assive learning. We roose an active teacher model that can actively query the learner (i.e., make the learner take exams) for estimating the learner's status and rovably guide the learner to achieve faster convergence. The samle comlexities for both teaching and query are rovided. In the exeriments, we comare the roosed active teacher with the omniscient teacher and verify the effectiveness of the active teacher model.
"
423,2018,Improving the Gaussian Mechanism for Differential Privacy: Analytical Calibration and Optimal Denoising,Oral/Poster,"The Gaussian mechanism is an essential building block used in multitude of differentially rivate data analysis algorithms. In this aer we revisit the Gaussian mechanism and show that the original analysis has several imortant limitations. Our analysis reveals that the variance formula for the original mechanism is far from tight in the high rivacy regime ($\varesilon \to 0$) and it cannot be extended to the low rivacy regime ($\varesilon \to \infty$). We address these limitations by develoing an otimal Gaussian mechanism whose variance is calibrated directly using the Gaussian cumulative density function instead of a tail bound aroximation. We also roose to equi the Gaussian mechanism with a ost-rocessing ste based on adative estimation techniques by leveraging that the distribution of the erturbation is known. Our exeriments show that analytical calibration removes at least a third of the variance of the noise comared to the classical Gaussian mechanism, and that denoising dramatically imroves the accuracy of the Gaussian mechanism in the high-dimensional regime."
424,2018,Importance Weighted Transfer of Samples in Reinforcement Learning,Oral/Poster,"We consider the transfer of exerience samles (i.e., tules &lt; s, a, s', r &gt;) in reinforcement learning (RL), collected from a set of source tasks to imrove the learning rocess in a given target task. Most of the related aroaches focus on selecting the most relevant source samles for solving the target task, but then all the transferred samles are used without considering anymore the discreancies between the task models. In this aer, we roose a model-based technique that automatically estimates the relevance (imortance weight) of each source samle for solving the target task. In the roosed aroach, all the samles are transferred and used by a batch RL algorithm to solve the target task, but their contribution to the learning rocess is roortional to their imortance weight. By extending the results for imortance weighting rovided in suervised learning literature, we develo a finite-samle analysis of the roosed batch RL algorithm. Furthermore, we emirically comare the roosed algorithm to state-of-the-art aroaches, showing that it achieves better learning erformance and is very robust to negative transfer, even when some source tasks are significantly different from the target task.
"
425,2018,Beyond the One-Step Greedy Approach in Reinforcement Learning,Oral/Poster,"The famous Policy Iteration algorithm alternates between olicy imrovement and olicy evaluation. Imlementations of this algorithm with several variants of the latter evaluation stage, e.g, n-ste and trace-based returns, have been analyzed in revious works. However, the case of multile-ste lookahead olicy imrovement, desite the recent increase in emirical evidence of its strength, has to our knowledge not been carefully analyzed yet. In this work, we introduce the first such analysis. Namely, we formulate variants of multile-ste olicy imrovement, derive new algorithms using these definitions and rove their convergence. Moreover, we show that recent rominent Reinforcement Learning algorithms are, in fact, instances of our framework. We thus shed light on their emirical success and give a recie for deriving new algorithms for future study.
"
426,2018,"Optimization, fast and slow: optimally switching between local and Bayesian optimization",Oral/Poster,"We develo the first Bayesian Otimization algorithm, BLOSSOM, which selects between multile alternative acquisition functions and traditional local otimization at each ste. This is combined with a novel stoing condition based on exected regret. This airing allows us to obtain the best characteristics of both local and Bayesian otimization, making efficient use of function evaluations while yielding suerior convergence to the global minimum on a selection of otimization roblems, and also halting otimization once a rinciled and intuitive stoing condition has been fulfilled.
"
427,2018,Batch Bayesian Optimization via Multi-objective Acquisition Ensemble for Automated Analog Circuit Design,Oral/Poster,"Bayesian otimization methods are romising for the otimization of black-box functions that are exensive to evaluate. In this aer, a novel batch Bayesian otimization aroach is roosed. The arallelization is realized via a multi-objective ensemble of multile acquisition functions. In each iteration, the multi-objective otimization of the multile acquisition functions is erformed to search for the Pareto front of the acquisition functions. The batch of inuts are then selected from the Pareto front. The Pareto front reresents the best trade-off between the multile acquisition functions. Such a olicy for batch Bayesian otimization can significantly imrove the efficiency of otimization. The roosed method is comared with several state-of-the-art batch Bayesian otimization algorithms using analytical benchmark functions and real-world analog integrated circuits. The exerimental results show that the roosed method is cometitive comared with the state-of-the-art algorithms.
"
428,2018,Graphical Nonconvex Optimization via an Adaptive Convex Relaxation,Oral/Poster,"We consider the roblem of learning high-dimensional Gaussian grahical models.  The grahical lasso is one of the most oular methods for estimating Gaussian grahical models. However, it does not achieve the oracle rate of convergence. In this aer, we roose the grahical nonconvex otimization for otimal estimation in Gaussian grahical models, which is then aroximated by a sequence of convex rograms. Our roosal is comutationally tractable and roduces an estimator that achieves the oracle rate of convergence. The statistical error introduced by the sequential aroximation using a sequence of convex rograms is clearly demonstrated via a contraction roerty.  The roosed methodology is then extended to modeling semiarametric grahical models. We show via numerical studies that the roosed estimator outerforms other oular methods for estimating Gaussian grahical models.
"
429,2018,Approximate message passing for amplitude based optimization,Oral/Poster,"We consider an $\ell_2$-regularized non-convex otimization roblem for recovering signals from their noisy haseless observations. We design and study the erformance of a message assing algorithm that aims to solve this otimization roblem. We consider the asymtotic setting $m,n \rightarrow \infty$, $mn \rightarrow \delta$ and obtain shar erformance bounds, where $m$ is the number of measurements and $n$ is the signal dimension. We show that for comlex signals the algorithm can erform accurate recovery with only $m=\left ( \frac{64}{\i^2}-4\right)n\arox 2.5n$ measurements. Also, we rovide shar analysis on the sensitivity of the algorithm to noise. We highlight the following facts about our message assing algorithm: (i) Adding $\ell_2$ regularization to the non-convex loss function can be beneficial even in the noiseless setting; (ii) sectral initialization has marginal imact on the erformance of the algorithm."
430,2018,Delayed Impact of Fair Machine Learning,Oral/Poster,"Fairness in machine learning has redominantly been studied in static classification settings without concern for how decisions change the underlying oulation over time. Conventional wisdom suggests that fairness criteria romote the long-term well-being of those grous they aim to rotect. We study how static fairness criteria interact with temoral indicators of well-being, such as long-term imrovement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-ste feedback model, common fairness criteria in general do not romote imrovement over time, and may in fact cause harm in cases where an unconstrained objective would not. We comletely characterize the delayed imact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria erform favorably. Our results highlight the imortance of measurement and temoral modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.
"
431,2018,Tempered Adversarial Networks,Oral/Poster,"Generative adversarial networks (GANs) have been shown to roduce realistic samles from high-dimensional distributions, but training them is considered hard. A ossible exlanation for training instabilities is the inherent imbalance between the networks: While the discriminator is trained directly on both real and fake samles, the generator only has control over the fake samles it roduces since the real data distribution is fixed by the choice of a given dataset. We roose a simle modification that gives the generator control over the real samles which leads to a temered learning rocess for both generator and discriminator. The real data distribution asses through a lens before being revealed to the discriminator, balancing the generator and discriminator by gradually revealing more detailed features necessary to roduce high-quality results. The roosed module automatically adjusts the learning rocess to the current strength of the networks, yet is generic and easy to add to any GAN variant. In a number of exeriments, we show that this can imrove quality, stability andor convergence seed across a range of different GAN architectures (DCGAN, LSGAN, WGAN-GP).
"
432,2018,Fast Information-theoretic Bayesian Optimisation,Oral/Poster,"Information-theoretic Bayesian otimisation techniques have demonstrated state-of-the-art erformance in tackling imortant global otimisation roblems. However, current information-theoretic aroaches require many aroximations in imlementation, introduce often-rohibitive comutational overhead and limit the choice of kernels available to model the objective. We develo a fast information-theoretic Bayesian Otimisation method, FITBO, that avoids the need for samling the global minimiser, thus significantly reducing comutational overhead. Moreover, in comarison with existing aroaches, our method faces fewer constraints on kernel choice and enjoys the merits of dealing with the outut sace. We demonstrate emirically that FITBO inherits the erformance associated with information-theoretic Bayesian otimisation, while being even faster than simler Bayesian otimisation aroaches, such as Exected Imrovement.
"
433,2018,Tight Regret Bounds for Bayesian Optimization in One Dimension,Oral/Poster,"We consider the roblem of Bayesian otimization (BO) in one dimension, under a Gaussian rocess rior and Gaussian samling noise.  We rovide a theoretical analysis showing that, under fairly mild technical assumtions on the kernel, the best ossible cumulative regret u to time $T$ behaves as $\Omega(\sqrt{T})$ and $O(\sqrt{T\log T})$. This gives a tight characterization u to a $\sqrt{\log T}$ factor, and includes the first non-trivial lower bound for noisy BO.  Our assumtions are satisfied, for examle, by the squared exonential and Mat\'ern-$\nu$ kernels, with the latter requiring $\nu  2$.  Our results certify the near-otimality of existing bounds (Srinivas {\em et al.}, 2009) for the SE kernel, while roving them to be strictly subotimal for the Mat\'ern kernel with $\nu  2$."
434,2018,Image Transformer,Oral/Poster,"Image generation has been successfully cast as an autoregressive sequence generation or transformation roblem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently roosed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can rocess in ractice, desite maintaining significantly larger recetive fields er layer than tyical convolutional neural networks. While concetually simle, our generative models significantly outerform the current state of the art in image generation on ImageNet, imroving the best ublished negative log-likelihood on ImageNet from 3.83 to 3.77. We also resent results on image suer-resolution with a large magnification ratio, alying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our suer-resolution model fool human observers three times more often than the revious state of the art.
"
435,2018,Kernelized Synaptic Weight Matrices,Oral/Poster,"In this aer we introduce a novel neural network architecture, in which weight matrices are re-arametrized in terms of low-dimensional vectors, interacting through kernel functions. A layer of our network can be interreted as introducing a (otentially infinitely wide) linear layer between inut and outut. We describe the theory underinning this model and validate it with concrete examles, exloring how it can be used to imose structure on neural networks in diverse alications ranging from data visualization to recommender systems. We achieve state-of-the-art erformance in a collaborative filtering task (MovieLens).
"
436,2018,A Distributed Second-Order Algorithm You Can Trust,Oral/Poster,"Due to the raid growth of data and comutational resources, distributed otimization has become an active research area in recent years. While first-order methods seem to dominate the field, second-order methods are nevertheless attractive as they otentially require fewer communication rounds to converge. However, there are significant drawbacks that imede their wide adotion, such as the comutation and the communication of a large Hessian matrix. In this aer we resent a new algorithm for distributed training of generalized linear models that only requires the comutation of diagonal blocks of the Hessian matrix on the individual workers. To deal with this aroximate information we roose an adative aroach that - akin to trust-region methods - dynamically adats the auxiliary model to comensate for modeling errors. We rovide theoretical rates of convergence for a wide class of roblems including $L_1$-regularized objectives. We also demonstrate that our aroach achieves state-of-the-art results on multile large benchmark datasets."
437,2018,On Acceleration with Noise-Corrupted Gradients,Oral/Poster,"Accelerated algorithms have broad alications in large-scale otimization, due to their generality and fast convergence. However, their stability in the ractical setting of noise-corruted gradient oracles is not well-understood. This aer rovides two main technical contributions: (i) a new accelerated method AGDP that generalizes Nesterov's AGD and imroves on the recent method AXGD (Diakonikolas &am; Orecchia, 2018), and (ii) a theoretical study of accelerated algorithms under noisy and inexact gradient oracles, which is suorted by numerical exeriments.  This study leverages the simlicity of AGDP and its analysis to clarify the interaction between noise and acceleration and to suggest modifications to the algorithm that reduce the mean and variance of the error incurred due to the gradient noise.
"
438,2018,Gradient Coding from Cyclic MDS Codes and Expander Graphs,Oral/Poster,"Gradient coding is a technique for straggler mitigation in distributed learning. In this aer we design novel gradient codes using tools from classical coding theory, namely, cyclic MDS codes, which comare favourably with existing solutions, both in the alicable range of arameters and in the comlexity of the involved algorithms. Second, we introduce an aroximate variant of the gradient coding roblem, in which we settle for aroximate gradient comutation instead of the exact one. This aroach enables graceful degradation, i.e., the $\ell_2$ error of the aroximate gradient is a decreasing function of the number of stragglers. Our main result is that the normalized adjacency matrix of an exander grah can yield excellent aroximate gradient codes, and that this aroach allows us to erform significantly less comutation comared to exact gradient coding. We exerimentally test our aroach on Amazon EC2, and show that the generalization error of aroximate gradient coding is very close to the full gradient while requiring significantly less comutation from the workers."
439,2018,Accelerating Greedy Coordinate Descent Methods,Oral/Poster,"We introduce and study two algorithms to accelerate greedy coordinate descent in theory and in ractice: Accelerated Semi-Greedy Coordinate Descent (ASCD) and Accelerated Greedy Coordinate Descent (AGCD). On the theory side, our main results are for ASCD: we show that ASCD achieves $O(1k^2)$ convergence, and it also achieves accelerated linear convergence for strongly convex functions. On the emirical side, while both AGCD and ASCD outerform Accelerated Randomized Coordinate Descent on most instances in our numerical exeriments, we note that AGCD significantly outerforms the other two methods in our exeriments, in site of a lack of theoretical guarantees for this method.  To comlement this emirical finding for AGCD, we resent an exlanation why standard roof techniques for acceleration cannot work for AGCD, and we further introduce a technical condition under which AGCD is guaranteed to have accelerated convergence. Finally, we confirm that this technical condition holds in our numerical exeriments."
440,2018,Finding Influential Training Samples for Gradient Boosted Decision Trees,Oral/Poster,"We address the roblem of finding influential training samles for a articular case of tree ensemble-based models, e.g., Random Forest (RF) or Gradient Boosted Decision Trees (GBDT). A natural way of formalizing this roblem is studying how the model's redictions change uon leave-one-out retraining, leaving out each individual training samle. Recent work has shown that, for arametric models, this analysis can be conducted in a comutationally efficient way. We roose several ways of extending this framework to non-arametric GBDT ensembles under the assumtion that tree structures remain fixed. Furthermore, we introduce a general scheme of obtaining further aroximations to our method that balance the trade-off between erformance and comutational comlexity. We evaluate our aroaches on various exerimental setus and use-case scenarios and demonstrate both the quality of our aroach to finding influential training samles in comarison to the baselines and its comutational efficiency.
"
441,2018,Improving Regression Performance with Distributional Losses,Oral/Poster,"There is growing evidence that converting targets to soft targets in suervised learning can rovide considerable gains in erformance. Much of this work has considered classification, converting hard zero-one values to soft labels---such as by adding label noise, incororating label ambiguity or using distillation. In arallel, there is some evidence from a regression setting in reinforcement learning that learning distributions can imrove erformance. In this work, we investigate the reasons for this imrovement, in a regression setting. We introduce a novel distributional regression loss, and similarly find it significantly imroves rediction accuracy. We investigate several common hyotheses, around reducing overfitting and imroved reresentations. We instead find evidence for an alternative hyothesis: this loss is easier to otimize, with better behaved gradients, resulting in imroved generalization. We rovide theoretical suort for this alternative hyothesis, by characterizing the norm of the gradients of this loss.
"
442,2018,QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning,Oral/Poster,"In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often ossible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exloit centralised learning, but the best strategy for then extracting decentralised olicies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised olicies in a centralised end-to-end fashion. QMIX emloys a network that estimates joint action-values as a comlex non-linear combination of er-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the er-agent values, which allows tractable maximisation of the joint action-value in off-olicy learning, and guarantees consistency between the centralised and decentralised olicies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outerforms existing value-based multi-agent reinforcement learning methods.
"
443,2018,Learning to Act in Decentralized Partially Observable MDPs,Oral/Poster,"We address a long-standing oen roblem of reinforcement learning in decentralized artially observable Markov decision rocesses. Previous attemts focussed on different forms of generalized olicy iteration, which at best led to local otima. In this aer, we restrict attention to lans, which are simler to store and udate than olicies. We derive, under certain conditions, the first near-otimal cooerative multi-agent reinforcement learning algorithm. To achieve significant scalability gains,  we relace the greedy maximization by mixed-integer linear rogramming. Exeriments show our aroach can learn to act near-otimally in many finite domains from the literature.
"
444,2018,Local Convergence Properties of SAGA/Prox-SVRG and Acceleration,Oral/Poster,"In this aer, we resent a local convergence anal- ysis for a class of stochastic otimisation meth- ods: the roximal variance reduced stochastic gradient methods, and mainly focus on SAGA (Defazio et al., 2014) and Prox-SVRG (Xiao &am; Zhang, 2014). Under the assumtion that the non-smooth comonent of the otimisation rob- lem is artly smooth relative to a smooth mani- fold, we resent a unified framework for the local convergence analysis of SAGAProx-SVRG: (i) the sequences generated by the methods are able to identify the smooth manifold in a finite num- ber of iterations; (ii) then the sequence enters a local linear convergence regime. Furthermore, we discuss various ossibilities for accelerating these algorithms, including adating to better lo- cal arameters, and alying higher-order deter- ministicstochastic otimisation methods which can achieve suer-linear convergence. Several concrete examles arising from machine learning are considered to demonstrate the obtained result.
"
445,2018,Stein Points,Oral/Poster,"An imortant task in comutational statistics and machine learning is to aroximate a osterior distribution (x) with an emirical measure suorted on a set of reresentative oints {x_i\}_{i=1}^n. This aer focuses on methods where the selection of oints is essentially deterministic, with an emhasis on achieving accurate aroximation when $n$ is small. To this end, we resent Stein Points. The idea is to exloit either a greedy or a conditional gradient method to iteratively minimise a kernel Stein discreancy between the emirical measure and (x). Our emirical results demonstrate that Stein Points enable accurate aroximation of the osterior at modest comutational cost. In addition, theoretical results are rovided to establish convergence of the method."
446,2018,Large-Scale Cox Process Inference using Variational Fourier Features,Oral/Poster,"Gaussian rocess modulated Poisson rocesses rovide a flexible framework for modeling satiotemoral oint atterns. So far this had been restricted to one dimension, binning to a re-determined grid, or small data sets of u to a few thousand data oints. Here we introduce Cox rocess inference based on Fourier features. This sarse reresentation induces global rather than local constraints on the function sace and is comutationally efficient. This allows us to formulate a grid-free aroximation that scales well with the number of data oints and the size of the domain. We demonstrate that this allows MCMC aroximations to the non-Gaussian osterior. In ractice, we find that Fourier features have more consistent otimization behavior than revious aroaches. Our aroximate Bayesian method can fit over 100 000 events with comlex satiotemoral atterns in three dimensions on a single GPU.
"
447,2018,SADAGRAD: Strongly Adaptive Stochastic Gradient Methods,Oral/Poster,"Although the convergence rates of existing variants of ADAGRAD have a better deendence on the number of iterations under the strong convexity condition, their iteration comlexities have a exlicitly linear deendence on the dimensionality of the roblem. To alleviate this bad deendence, we roose a simle yet novel variant of ADAGRAD for stochastic (weakly) strongly convex otimization. Different from existing variants, the roosed variant (referred to as SADAGRAD) uses an adative restarting scheme in which (i) ADAGRAD serves as a sub-routine and is restarted eriodically; (ii) the number of iterations for restarting ADAGRAD deends on the history of learning that incororates knowledge of the geometry of the data. In addition to the adative roximal functions and adative number of iterations for restarting, we also develo a variant that is adative to the (imlicit) strong convexity from the data, which together makes the roosed algorithm strongly adative. In terms of iteration comlexity, in the worst case SADAGRAD has an O(1\esilon) for finding an \esilon-otimal solution similar to other variants. However, it could enjoy faster convergence and much better deendence on the roblem’s dimensionality when stochastic gradients are sarse. Extensive exeriments on large-scale data sets demonstrate the efficiency of the roosed algorithms in comarison with several variants of ADAGRAD and stochastic gradient method.
"
448,2018,Gradient Primal-Dual Algorithm Converges to Second-Order Stationary Solution for  Nonconvex Distributed Optimization Over Networks,Oral/Poster,"In this work, we study two first-order rimal-dual based algorithms, the Gradient Primal-Dual Algorithm (GPDA) and the Gradient Alternating Direction Method of Multiliers (GADMM),  for  solving a class of linearly constrained non-convex otimization roblems. We show that with random initialization of  the rimal and dual variables,  both algorithms are able to comute second-order stationary solutions (ss2) with robability one. This is the first result showing that rimal-dual algorithm is caable of finding ss2 when only using first-order information;  it also extends the existing results for first-order, but {rimal-only} algorithms.       An imortant imlication of our result is that it also gives rise to the first global convergence result to the ss2, for two classes of  unconstrained distributed non-convex learning roblems over multi-agent networks.
"
449,2018,A Progressive Batching L-BFGS Method for Machine Learning,Oral/Poster,"The standard L-BFGS method relies on gradient aroximations that are not dominated by noise, so that search directions are descent directions, the line search is reliable, and quasi-Newton udating yields useful quadratic models of the objective function. All of this aears to call for a full batch aroach, but since small batch sizes give rise to faster algorithms with better generalization roerties, L-BFGS is currently not considered an algorithm of choice for large-scale machine learning alications. One need not, however, choose between the two extremes reresented by the full batch or highly stochastic regimes, and may instead follow a rogressive batching aroach in which the samle size increases during the course of the otimization. In this aer, we resent a new version of the L-BFGS algorithm that combines three basic comonents - rogressive batching, a stochastic line search, and stable quasi-Newton udating - and that erforms well on training logistic regression and dee neural networks. We rovide suorting convergence theory for the method.
"
450,2018,WSNet: Compact and Efficient Networks Through Weight Sampling,Oral/Poster,"We resent a new aroach and a novel architecture, termed WSNet, for learning comact and efficient dee neural networks. Existing aroaches conventionally learn full model arameters indeendently and then comress them via ad hoc rocessing such as model runing or filter factorization. Alternatively, WSNet rooses learning model arameters by samling from a comact set of learnable arameters, which naturally enforces arameter sharing throughout the learning rocess. We demonstrate that such a novel weight samling aroach (and induced WSNet) romotes both weights and comutation sharing favorably. By emloying this method, we can more efficiently learn much smaller networks with cometitive erformance comared to baseline networks with equal numbers of convolution filters. Secifically, we consider learning comact and efficient 1D convolutional neural networks for audio classification. Extensive exeriments on multile audio classification datasets verify the effectiveness of WSNet. Combined with weight quantization, the resulted models are u to 180x smaller and theoretically u to 16x faster than the well-established baselines, without noticeable erformance dro.
"
451,2018,Entropy-SGD optimizes the prior of a PAC-Bayes bound: Generalization properties of Entropy-SGD and data-dependent priors,Oral/Poster,"We show that Entroy-SGD (Chaudhari et al., 2017), when viewed as a learning algorithm, otimizes a PAC-Bayes bound on the risk of a Gibbs (osterior) classifier, i.e., a randomized classifier obtained by a risk-sensitive erturbation of the weights of a learned classifier. Entroy-SGD works by otimizing the bound’s rior, violating the hyothesis of the PAC-Bayes theorem that the rior is chosen indeendently of the data. Indeed, available imlementations of Entroy-SGD raidly obtain zero training error on random labels and the same holds of the Gibbs osterior. In order to obtain a valid generalization bound, we rely on a result showing that data-deendent riors obtained by stochastic gradient Langevin dynamics (SGLD) yield valid PAC-Bayes bounds rovided the target distribution of SGLD is es-differentially rivate. We observe that test error on MNIST and CIFAR10 falls within the (emirically nonvacuous) risk bounds comuted under the assumtion that SGLD reaches stationarity. In articular, Entroy-SGLD can be configured to yield relatively tight generalization bounds and still fit real labels, although these same settings do not obtain state-of-the-art erformance.
"
452,2018,"High-Quality Prediction Intervals for Deep Learning: A Distribution-Free, Ensembled Approach",Oral/Poster,"This aer considers the generation of rediction intervals (PIs) by neural networks for quantifying uncertainty in regression tasks. It is axiomatic that high-quality PIs should be as narrow as ossible, whilst caturing a secified ortion of data. We derive a loss function directly from this axiom that requires no distributional assumtion. We show how its form derives from a likelihood rincile, that it can be used with gradient descent, and that model uncertainty is accounted for in ensembled form. Benchmark exeriments show the method outerforms current state-of-the-art uncertainty quantification methods, reducing average PI width by over 10%.
"
453,2018,Competitive Caching with Machine Learned Advice,Oral/Poster,"We develo a framework for augmenting online algorithms with a machine learned oracle to achieve cometitive ratios that rovably imrove uon unconditional worst case lower bounds when the oracle has low error. Our aroach treats the oracle as a comlete black box, and is not deendent on its inner workings, or the exact distribution of its errors. We aly this framework to the traditional caching roblem — creating an eviction strategy for a cache of size k. We demonstrate that naively following the oracle’s recommendations may lead to very oor erformance, even when the average error is quite low. Instead we show how to modify the Marker algorithm to take into account the oracle’s redictions, and rove that this combined aroach achieves a cometitive ratio that both (i) decreases as the oracle’s error decreases, and (ii) is always caed by O(log k), which can be achieved without any oracle inut. We comlement our results with an emirical evaluation of our algorithm on real world datasets, and show that it erforms well emirically even using simle off the shelf redictions.
"
454,2018,Approximation Algorithms for Cascading Prediction Models,Oral/Poster,"We resent an aroximation algorithm that takes a ool of re-trained models as inut and roduces from it a cascaded model with similar accuracy but lower average-case cost.  Alied to state-of-the-art ImageNet classification models, this yields u to a 2x reduction in floating oint multilications, and u to a 6x reduction in average-case memory IO.  The auto-generated cascades exhibit intuitive roerties, such as using lower-resolution inut for easier images and requiring higher rediction confidence when using a comutationally cheaer model.
"
455,2018,Orthogonal Machine Learning: Power and Limitations,Oral/Poster,"Double machine learning rovides n^{12}-consistent estimates of arameters of interest even when high-dimensional or nonarametric nuisance arameters are estimated at an n^{-14} rate. The key is to emloy Neyman-orthogonal moment equations which are first-order insensitive to erturbations in the nuisance arameters. We show that the n^{-14} requirement can be imroved to n^{-1(2k+2)} by emloying a k-th order notion of orthogonality that grants robustness to more comlex or higher-dimensional nuisance arameters. In the artially linear regression setting oular in causal inference, we show that we can construct second-order orthogonal moments if and only if the treatment residual is not normally distributed.  Our roof relies on Stein's lemma and may be of indeendent interest.  We conclude by demonstrating the robustness benefits of an exlicit doubly-orthogonal estimation rocedure for treatment effect.
"
456,2018,Causal Bandits with Propagating Inference,Oral/Poster,"Bandit is a framework for designing sequential exeriments, where a learner selects an arm $A \in \mathcal{A}$ and obtains an observation corresonding to $A$ in each exeriment. Theoretically, the tight regret lower-bound for the general bandit is olynomial with resect to the number of arms $|\mathcal{A}|$, and thus, to overcome this bound, the bandit roblem with side-information is often considered. Recently, a bandit framework over a causal grah was introduced, where the structure of the causal grah is available as side-information and the arms are identified with interventions on the causal grah. Existing algorithms for causal bandit overcame the $\Omega(\sqrt{|\mathcal{A}|T})$ simle-regret lower-bound; however, their algorithms work only when the interventions $\mathcal{A}$ are localized around a single node (i.e., an intervention roagates only to its neighbors). We then roose a novel causal bandit algorithm for an arbitrary set of interventions, which can roagate throughout the causal grah. We also show that it achieves $O(\sqrt{ \gamma^*\log(|\mathcal{A}|T)  T})$ regret bound, where $\gamma^*$ is determined by using a causal grah structure. In articular, if the maximum in-degree of the causal grah is a constant, then $\gamma^* = O(N^2)$, where $N$ is the number of nodes."
457,2018,Mix & Match - Agent Curricula for Reinforcement Learning,Oral/Poster,"We introduce Mix and match (M&am;M) -- a training framework designed to facilitate raid and effective learning in RL agents that would be too slow or too challenging to train otherwise.The key innovation is a rocedure that allows us to automatically form a curriculum over agents. Through such a curriculum we can rogressively train more comlex agents by, effectively, bootstraing from solutions found by simler agents.In contradistinction to tyical curriculum learning aroaches, we do not gradually modify the tasks or environments resented, but instead use a rocess to gradually alter how the olicy is reresented internally.We show the broad alicability of our method by demonstrating significant erformance gains in three different exerimental setus: (1) We train an agent able to control more than 700 actions in a challenging 3D first-erson task; using our method to rogress through an action-sace curriculum we achieve both faster training and better final erformance than one obtains using traditional methods.(2) We further show that M&am;M can be used successfully to rogress through a curriculum of architectural variants defining an agents internal state. (3) Finally, we illustrate how a variant of our method can be used to imrove agent erformance in a multitask setting.
"
458,2018,The Uncertainty Bellman Equation and Exploration,Oral/Poster,"We consider the exlorationexloitation roblem in reinforcement learning. For exloitation, it is well known that the Bellman equation connects the value at any time-ste to the exected value at subsequent time-stes. In this aer we consider a similar uncertainty Bellman equation (UBE), which connects the uncertainty at any time-ste to the exected uncertainties at subsequent time-stes, thereby extending the otential exloratory benefit of a olicy beyond individual time-stes. We rove that the unique fixed oint of the UBE yields an uer bound on the variance of the osterior distribution of the Q-values induced by any olicy. This bound can be much tighter than traditional count-based bonuses that comound standard deviation rather than variance. Imortantly, and unlike several existing aroaches to otimism, this method scales naturally to large systems with comlex generalization. Substituting our UBE-exloration strategy for $\esilon$-greedy imroves DQN erformance on 51 out of 57 games in the Atari suite."
459,2018,Hierarchical Imitation and Reinforcement Learning,Oral/Poster,"We study how to effectively leverage exert feedback to learn sequential decision-making olicies. We focus on roblems with sarse rewards and long time horizons, which tyically ose significant challenges in reinforcement learning. We roose an algorithmic framework, called hierarchical guidance, that leverages the hierarchical structure of the underlying roblem to integrate different modes of exert interaction. Our framework can incororate different combinations of imitation learning (IL) and reinforcement learning (RL) at different levels, leading to dramatic reductions in both exert effort and cost of exloration. Using long-horizon benchmarks, including Montezuma's Revenge, we  demonstrate that our aroach can learn significantly faster than hierarchical RL, and be significantly more label-efficient than standard IL. We also theoretically analyze labeling cost for certain instantiations of our framework.
"
460,2018,Policy Optimization with Demonstrations,Oral/Poster,"Exloration remains a significant challenge to reinforcement learning methods, esecially in environments where reward signals are sarse. Recent methods of learning from demonstrations have shown to be romising in overcoming exloration difficulties but tyically require considerable high-quality demonstrations that are difficult to collect. We roose to effectively leverage available demonstrations to guide exloration through enforcing occuancy measure matching between the learned olicy and current demonstrations, and develo a novel Policy Otimization from Demonstration (POfD) method. We show that POfD induces imlicit dynamic reward shaing and brings rovable benefits for olicy imrovement. Furthermore, it can be combined with olicy gradient methods to roduce state-of-the-art results, as demonstrated exerimentally on a range of oular benchmark sarse-reward tasks, even when the demonstrations are few and imerfect.
"
461,2018,Fast Gradient-Based Methods with Exponential Rate: A Hybrid Control Framework,Oral/Poster,"Ordinary differential equations, and in general a dynamical system viewoint, have seen a resurgence of interest in develoing fast otimization methods, mainly thanks to the availability of well-established analysis tools. In this study, we ursue a similar objective and roose a class of hybrid control systems that adots a 2nd-order differential equation as its continuous flow. A distinctive feature of the roosed differential equation in comarison with the existing literature is a state-deendent, time-invariant daming term that acts as a feedback control inut. Given a user-defined scalar $\alha$, it is shown that the roosed control inut steers the state trajectories to the global otimizer of a desired objective function with a guaranteed rate of  convergence $\mathcal{O}(e^{-\alha t})$. Our framework requires that the objective function satisfies the so called Polyak--{\L}ojasiewicz inequality. Furthermore, a discretization method is introduced such that the resulting discrete dynamical system ossesses an exonential rate of convergence."
462,2018,Level-Set Methods for Finite-Sum Constrained Convex Optimization,Oral/Poster,"We consider the constrained otimization where the objective function and the constraints are defined as summation of finitely many loss functions. This model has alications in machine learning such as Neyman-Pearson classification. We consider two level-set methods to solve this class of roblems, an existing inexact Newton method and a new feasible level-set method.  To udate the level arameter towards the otimality, both methods require an oracle that generates uer and lower bounds as well as an affine-minorant of the level function. To construct the desired oracle, we reformulate the level function as the value of a saddle-oint roblem using the conjugate and ersective of the loss functions. Then a stochastic variance-reduced gradient method with a secial Bregman divergence is roosed as the oracle for solving that saddle-oint roblem. The secial divergence ensures the roximal maing in each iteration can be solved in a closed form. The total comlexity of both level-set methods using the roosed oracle are analyzed.
"
463,2018,A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations,Oral/Poster,"Backroagation-based visualizations have been roosed to interret convolutional neural networks (CNNs), however a theory is missing to justify their behaviors: Guided backroagation (GBP) and deconvolutional network (DeconvNet) generate more human-interretable but less class-sensitive visualizations than saliency ma. Motivated by this, we develo a theoretical exlanation revealing that GBP and DeconvNet are essentially doing (artial) image recovery which is unrelated to the network decisions. Secifically, our analysis shows that the backward ReLU introduced by GBP and DeconvNet, and the local connections in CNNs are the two main causes of comelling visualizations. Extensive exeriments are rovided that suort the theoretical analysis.
"
464,2018,A Boo(n) for Evaluating Architecture Performance,Oral/Poster,"We oint out imortant roblems with the common ractice of using the best single model erformance for comaring dee learning architectures, and we roose a method that corrects these flaws. Each time a model is trained, one gets a different result due to random factors in the training rocess, which include random arameter initialization and random data shuffling. Reorting the best single model erformance does not aroriately address this stochasticity. We roose a normalized exected best-out-of-$n$ erformance ($\text{Boo}_n$) as a way to correct these roblems."
465,2018,RLlib: Abstractions for Distributed Reinforcement Learning,Oral/Poster,"Reinforcement learning (RL) algorithms involve the dee nesting of highly irregular comutation atterns, each of which tyically exhibits oortunities for distributed comutation. We argue for distributing RL comonents in a comosable way by adating algorithms for to-down hierarchical control, thereby encasulating arallelism and resource requirements within short-running comute tasks. We demonstrate the benefits of this rincile through RLlib: a library that rovides scalable software rimitives for RL. These rimitives enable a broad range of algorithms to be imlemented with high erformance, scalability, and substantial code reuse. RLlib is available as art of the oen source Ray roject at htt:rllib.io.
"
466,2018,Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator,Oral/Poster,"Direct olicy gradient methods for reinforcement learning and continuous control roblems are a oular aroach for a variety of reasons: 1) they are easy to imlement without exlicit knowledge of the underlying model, 2) they are an ``end-to-end'' aroach, directly otimizing the erformance metric of interest, 3) they inherently allow for richly arameterized olicies. A notable drawback is that even in the most basic continuous control roblem (that of linear quadratic regulators), these methods must solve a non-convex otimization roblem, where little is understood about their efficiency from both comutational and statistical ersectives. In contrast, system identification and model based lanning in otimal control theory have a much more solid theoretical footing, where much is known with regards to their comutational and statistical roerties.  This work bridges this ga showing that (model free) olicy gradient methods globally converge to the otimal solution and are efficient (olynomially so in relevant roblem deendent quantities) with regards to their samle and comutational comlexities.
"
467,2018,The Edge Density Barrier: Computational-Statistical Tradeoffs in Combinatorial Inference,Oral/Poster,"We study the hyothesis testing roblem of inferring the existence of combinatorial structures in undirected grahical models. Although there exist extensive studies on the information-theoretic limits of this roblem, it remains largely unexlored whether such limits can be attained by efficient algorithms. In this aer, we quantify the minimum comutational comlexity required to attain the information-theoretic limits based on an oracle comutational model. We rove that, for testing common combinatorial structures, such as clique, nearest neighbor grah and erfect matching, against an emty grah, or large clique against small clique, the information-theoretic limits are rovably unachievable by tractable algorithms in general. More imortantly, we define structural quantities called the weak and strong edge densities, which offer dee insight into the existence of such comutational-statistical tradeoffs. To the best of our knowledge, our characterization is the first to identify and exlain the fundamental tradeoffs between statistics and comutation for combinatorial inference roblems in undirected grahical models.
"
468,2018,Sound Abstraction and Decomposition of Probabilistic Programs,Oral/Poster,"Probabilistic rogramming languages are a flexible tool for secifying statistical models, but this flexibility comes at the cost of efficient analysis. It is currently difficult to comactly reresent the subtle indeendence roerties of a robabilistic rogram, and exloit indeendence roerties to decomose inference. Classical grahical model abstractions do cature some roerties of the underlying distribution, enabling inference algorithms to oerate at the level of the grah toology. However, we observe that grah-based abstractions are often too coarse to cature interesting roerties of rograms. We roose a form of sound abstraction for robabilistic rograms wherein the abstractions are themselves simlified rograms. We rovide a theoretical foundation for these abstractions, as well as an algorithm to generate them. Exerimentally, we also illustrate the ractical benefits of our framework as a tool to decomose robabilistic rogram inference.
"
469,2018,Parallel WaveNet: Fast High-Fidelity Speech Synthesis,Oral/Poster,"The recently-develoed WaveNet architecture is the current state of the art in realistic seech synthesis, consistently rated as more natural sounding for many different languages than any revious system. However, because WaveNet relies on sequential generation of one audio samle at a time, it is oorly suited to today's massively arallel comuters, and therefore hard to deloy in a real-time roduction setting. This aer introduces Probability Density Distillation, a new method for training a arallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is caable of generating high-fidelity seech samles at more than 20 times faster than real-time, a 1000x seed u relative to the original WaveNet, and caable of serving multile English and Jaanese voices in a roduction setting.
"
470,2018,Modeling Sparse Deviations for Compressed Sensing using Generative Models,Oral/Poster,"In comressed sensing,  a small number of linear measurements can be used to reconstruct an unknown signal. Existing aroaches leverage assumtions on the structure of these signals, such as sarsity or the availability of a generative model. A domain-secific generative model can rovide a stronger rior and thus allow for recovery with far fewer measurements. However, unlike sarsity-based aroaches, existing methods based on generative models guarantee exact recovery only over their suort, which is tyically only a small subset of the sace on which the signals are defined. We roose Sarse-Gen, a framework that allows for sarse deviations from the suort set, thereby achieving the best of both worlds by using a domain secific rior and allowing reconstruction over the full sace of signals. Theoretically, our framework rovides a new class of signals that can be acquired using comressed sensing, reducing classic sarse vector recovery to a secial case and avoiding the  restrictive suort due to a generative model rior. Emirically, we observe consistent imrovements in reconstruction accuracy over cometing aroaches, esecially in the more ractical setting of transfer comressed sensing where a generative model for a data-rich, source domain aids sensing on a data-scarce, target domain.
"
471,2018,Revealing Common Statistical Behaviors in Heterogeneous Populations,Oral/Poster,"In many areas of neuroscience and biological data analysis, it is desired to reveal common atterns among a grou of subjects. Such analyses lay imortant roles e.g., in detecting functional brain networks from fMRI scans and in identifying brain regions which show increased activity in resonse to certain stimuli. Grou level techniques usually assume that all subjects in the grou behave according to a single statistical model, or that deviations from the common model have simle arametric forms. Therefore, comlex subject-secific deviations from the common model severely imair the erformance of such methods. In this aer, we roose nonarametric algorithms for estimating the common covariance matrix and the common density function of several variables in a heterogeneous grou of subjects. Our estimates converge to the true model as the number of subjects tends to infinity, under very mild conditions. We illustrate the effectiveness of our methods through extensive simulations as well as on real-data from fMRI scans and from arterial blood ressure and hotolethysmogram measurements.
"
472,2018,Improved nearest neighbor search using auxiliary information and priority functions,Oral/Poster,"Nearest neighbor search using random rojection trees has recently been shown to achieve suerior erformance,  in terms of better accuracy while retrieving less number of data oints, comared to locality sensitive hashing based methods. However, to achieve accetable nearest neighbor search accuracy for large scale alications, where number of data oints andor number of features can be very large, it requires users to maintain, store and search through large number of such indeendent random rojection trees, which may be undesirable for many ractical alications. To address this issue, in this aer we resent different search strategies to imrove nearest neighbor search erformance of a single random rojection tree. Our aroach exloits roerties of single and multile random rojections, which allows us  to store meaningful auxiliary information at internal nodes of a random rojection tree as well as to design riority functions to guide the search rocess that results  in imroved nearest neighbor search erformance. Emirical results on multile real world datasets show that our roosed method imroves the search accuracy of a single tree comared to baseline methods.
"
473,2018,Trainable Calibration Measures for Neural Networks from Kernel Mean Embeddings,Oral/Poster,"Modern neural networks have recently been found to be oorly calibrated, rimarily in the direction of over-confidence. Methods like entroy enalty and temerature smoothing imrove calibration by claming confidence, but in doing so comromise the many legitimately confident redictions. We roose a more rinciled fix that minimizes an exlicit calibration error during training. We resent MMCE, a RKHS kernel based measure of calibration that is efficiently trainable alongside the negative likelihood loss without careful hyer-arameter tuning. Theoretically too, MMCE is a sound measure of calibration that is minimized at erfect calibration, and whose finite samle estimates are consistent and enjoy fast convergence rates. Extensive exeriments on several network architectures demonstrate that MMCE is a fast, stable, and accurate method to minimize calibration error while maximally reserving the number of high confidence redictions.
"
474,2018,QuantTree: Histograms for Change Detection in Multivariate Data Streams,Oral/Poster,"We address the roblem of detecting distribution changes in multivariate data streams by means of histograms. Histograms are very general and flexible models, which have been relatively ignored in the change-detection literature as they often require a number of bins that grows unfeasibly with the data dimension. We resent \QuantTree, a recursive binary slitting scheme that adatively defines the histogram bins to ease the detection of any distribution change. Our design scheme imlies that i) we can easily control the overall number of bins and ii) the bin robabilities do not deend on the distribution of stationary data. This latter is a very relevant asect in change detection, since thresholds of tests statistics based on these histograms (e.g., the Pearson statistic or the total variation) can be numerically comuted from univariate and synthetically generated data, yet guaranteeing a controlled false ositive rate. Our exeriments show that the roosed histograms are very effective in detecting changes in high dimensional data streams, and that the resulting thresholds can effectively control the false ositive rate, even when the number of training samles is relatively small.
"
475,2018,"An Iterative, Sketching-based Framework for Ridge Regression",Oral/Poster,"Ridge regression is a variant of regularized least squares regression that is articularly suitable in settings where the number of redictor variables greatly exceeds the number of observations. We resent a simle, iterative, sketching-based algorithm for ridge regression that  guarantees high-quality aroximations to the otimal solution vector. Our analysis builds uon two simle structural results that boil down to randomized matrix multilication, a fundamental and well-understood rimitive of randomized linear algebra. An imortant contribution of our work is the analysis of the behavior of subsamled ridge regression roblems when the ridge leverage scores are used: we rove that accurate aroximations can be achieved by a samle whose size deends on the degrees of freedom of the ridge-regression roblem rather than the dimensions of the design matrix. Our exerimental evaluations verify our theoretical results on both real and synthetic data.
"
476,2018,Learning Low-Dimensional Temporal Representations,Oral/Poster,"Low-dimensional discriminative reresentations enhance machine learning methods in both erformance and comlexity, motivating suervised dimensionality reduction (DR) that transforms high-dimensional data to a discriminative subsace. Most DR methods require data to be i.i.d., however, in some domains, data naturally come in sequences, where the observations are temorally correlated. We roose a DR method called LT-LDA to learn low-dimensional temoral reresentations. We construct the searability among sequence classes by lifting the holistic temoral structures, which are established based on temoral alignments and may change in different subsaces. We jointly learn the subsace and the associated alignments by otimizing an objective which favors easily-searable temoral structures, and show that this objective is connected to the inference of alignments, thus allows an iterative solution. We rovide both theoretical insight and emirical evaluation on real-world sequence datasets to show the interest of our method.
"
477,2018,Rapid Adaptation with Conditionally Shifted Neurons,Oral/Poster,"We describe a mechanism by which artificial neural networks can learn raid adatation - the ability to adat on the fly, with little data, to new tasks - that we call conditionally shifted neurons. We aly this mechanism in the framework of metalearning, where the aim is to relicate some of the flexibility of human learning in machines. Conditionally shifted neurons modify their activation values with task-secific shifts retrieved from a memory module, which is oulated raidly based on limited task exerience. On metalearning benchmarks from the vision and language domains, models augmented with conditionally shifted neurons achieve state-of-the-art results.
"
478,2018,PDE-Net: Learning PDEs from Data,Oral/Poster,"Partial differential equations (PDEs) lay a rominent role in many discilines of science and engineering. PDEs are commonly derived based on emirical observations. However, with the raid develoment of sensors, comutational ower, and data storage in the ast decade, huge quantities of data can be easily collected and efficiently stored. Such vast quantity of data offers new oortunities for data-driven discovery of hysical laws. Insired by the latest develoment of neural network designs in dee learning, we roose a new feed-forward dee network, called PDE-Net, to fulfill two objectives at the same time: to accurately redict dynamics of comlex systems and to uncover the underlying hidden PDE models. Comaring with existing aroaches, our aroach has the most flexibility by learning both differential oerators and the nonlinear resonse function of the underlying PDE model. A secial feature of the roosed PDE-Net is that all filters are roerly constrained, which enables us to easily identify the governing PDE models while still maintaining the exressive and redictive ower of the network. These constrains are carefully designed by fully exloiting the relation between the orders of differential oerators and the orders of sum rules of filters (an imortant concet originated from wavelet theory). Numerical exeriments show that the PDE-Net has the otential to uncover the hidden PDE of the observed dynamics, and redict the dynamical behavior for a relatively long time, even in a noisy environment.
"
479,2018,Theoretical Analysis of Sparse Subspace Clustering with Missing Entries,Oral/Poster,"Sarse Subsace Clustering (SSC) is a oular unsuervised machine learning method for clustering data lying close to an unknown union of low-dimensional linear subsaces; a roblem with numerous alications in attern recognition and comuter vision. Even though the behavior of SSC for comlete data is by now well-understood, little is known about its theoretical roerties when alied to data with missing entries. In this aer we give theoretical guarantees for SSC with incomlete data, and rovide theoretical evidence that rojecting the zero-filled data onto the observation attern of the oint being exressed can lead to substantial imrovement in erformance; a henomenon already known exerimentally. The main insight of our analysis is that even though this rojection induces additional missing entries, this is counterbalanced by the fact that the rojected and zero-filled data are in effect incomlete oints associated with the union of the corresonding rojected subsaces, with resect to which the oint being exressed is comlete. The significance of this henomenon otentially extends to the entire class of self-exressive methods.
"
480,2018,Topological mixture estimation,Oral/Poster,"We introduce toological mixture estimation, a comletely nonarametric and comutationally efficient solution to the roblem of estimating a one-dimensional mixture with generic unimodal comonents.  We reeatedly erturb the unimodal decomosition of Baryshnikov and Ghrist to roduce a toologically and information-theoretically otimal unimodal mixture.  We also detail a smoothing rocess that otimally exloits toological ersistence of the unimodal category in a natural way when working directly with samle data.  Finally, we illustrate these techniques through examles.
"
481,2018,On Matching Pursuit and Coordinate Descent,Oral/Poster,"Two oular examles of first-order otimization methods over linear saces are coordinate descent and matching ursuit algorithms, with their randomized variants. While the former targets the otimization by moving along coordinates, the latter considers a generalized notion of directions. Exloiting the connection between the two algorithms, we resent a unified analysis of both, roviding affine invariant sublinear $O(1t)$ rates on smooth objectives and linear convergence on strongly convex objectives. As a byroduct of our affine invariant analysis of matching ursuit, our rates for steeest coordinate descent are the tightest known. Furthermore, we show the first accelerated convergence rate $O(1t^2)$ for matching ursuit and steeest coordinate descent on convex objectives."
482,2018,Frank-Wolfe with Subsampling Oracle,Oral/Poster,"We analyze two novel randomized variants of the Frank-Wolfe (FW) or conditional gradient algorithm. While classical FW algorithms require solving a linear minimization roblem over the domain at each iteration, the roosed method only requires to solve a linear minimization roblem over a small \emh{subset} of the original domain. The first algorithm that we roose is a randomized variant of the original FW algorithm and achieves a $\mathcal{O}(1t)$ sublinear convergence rate as in the deterministic counterart. The second algorithm is a randomized variant of the Away-ste FW algorithm, and again as its deterministic counterart, reaches linear (i.e., exonential) convergence rate making it the first rovably convergent randomized variant of Away-ste FW. In both cases, while subsamling reduces the convergence rate by a constant factor, the linear minimization ste can be a fraction of the cost of that of the deterministic versions, esecially when the data is streamed. We illustrate comutational gains of both algorithms on regression roblems, involving both $\ell_1$ and latent grou lasso enalties."
483,2018,Reinforcement Learning with Function-Valued Action Spaces for Partial Differential Equation Control,Oral/Poster,"Recent work has shown that reinforcement learning (RL) is a romising aroach to control dynamical systems described by artial differential equations (PDE). This aer shows how to use RL to tackle more general PDE control roblems that have continuous high-dimensional action saces with satial relationshi among action dimensions. In articular, we roose the concet of action descritors, which encode regularities among satially-extended action dimensions and enable the agent to control high-dimensional action PDEs. We rovide theoretical evidence suggesting that this aroach can be more samle efficient comared to a conventional aroach that treats each action dimension searately and does not exlicitly exloit the satial regularity of the action sace. The action descritor aroach is then used within the dee deterministic olicy gradient algorithm. Exeriments on two PDE control roblems, with u to 256-dimensional continuous actions, show the advantage of the roosed aroach over the conventional one.
"
484,2018,Fourier Policy Gradients,Oral/Poster,"We roose a new way of deriving olicy gradient udates for reinforcement learning. Our technique, based on Fourier analysis, recasts integrals that arise with exected olicy gradients as convolutions and turns them into multilications. The obtained analytical solutions allow us to cature the low variance benefits of EPG in a broad range of settings. For the critic, we treat trigonometric and radial basis functions, two function families with the universal aroximation roerty. The choice of olicy can be almost arbitrary, including mixtures or hybrid continuous-discrete robability distributions. Moreover, we derive a general family of samle-based estimators for stochastic olicy gradients, which unifies existing results on samle-based aroximation. We believe that this technique has the otential to shae the next generation of olicy gradient aroaches, owered by analytical results.
"
485,2018,Adaptive Three Operator Splitting,Oral/Poster,"We roose and analyze a novel adative ste size variant of the Davis-Yin three oerator slitting, a method that can solve otimization roblems comosed of a sum of a smooth term for which we have access to its gradient and an arbitrary number of otentially non-smooth terms for which we have access to their roximal oerator. The roosed method leverages local information of the objective function, allowing for larger ste sizes while reserving the convergence roerties of the original method. It only requires two extra function evaluations er iteration and does not deend on any ste size hyerarameter besides an initial estimate. We rovide a convergence rate analysis of this method, showing sublinear convergence rate for general convex functions and linear convergence under stronger assumtions, matching the best known rates of its non adative variant. Finally, an emirical comarison with related methods on 6 different roblems illustrates the comutational advantage of the adative ste size strategy.
"
486,2018,A Conditional Gradient Framework for Composite Convex Minimization with Applications to Semidefinite Programming,Oral/Poster,"We roose a conditional gradient framework for a comosite convex minimization temlate with broad alications. Our aroach combines smoothing and homotoy techniques under the CGM framework, and rovably achieves the otimal convergence rate. We demonstrate that the same rate holds if the linear subroblems are solved aroximately with additive or multilicative error. In contrast with the relevant work, we are able to characterize the convergence when the non-smooth term is an indicator function. Secific alications of our framework include the non-smooth minimization, semidefinite rogramming, and minimization with linear inclusion constraints over a comact domain. Numerical evidence demonstrates the benefits of our framework.
"
487,2018,Learning Semantic Representations for Unsupervised Domain Adaptation,Oral/Poster,"It is imortant to transfer the knowledge from label-rich source domain to unlabeled target domain due to the exensive cost of manual labeling efforts. Prior domain adatation methods address this roblem through aligning the global distribution statistics between source domain and target domain, but a drawback of rior methods is that they ignore the semantic information contained in samles, e.g., features of backacks in target domain might be maed near features of cars in source domain. In this aer, we resent moving semantic transfer network, which learn semantic reresentations for unlabeled target samles by aligning labeled source centroid and seudo-labeled target centroid. Features in same class but different domains are exected to be maed nearby, resulting in an imroved target classification accuracy. Moving average centroid alignment is cautiously designed to comensate the insufficient categorical information within each mini batch. Exeriments testify that our model yields state of the art results on standard datasets.
"
488,2018,Learning Adversarially Fair and Transferable Representations,Oral/Poster,"In this aer, we advocate for reresentation learning as the key to mitigating unfair rediction outcomes downstream. Motivated by a scenario where learned reresentations are used by third arties with unknown objectives, we roose and exlore adversarial reresentation learning as a natural method of ensuring those arties act fairly. We connect grou fairness (demograhic arity, equalized odds, and equal oortunity) to different adversarial objectives. Through worst-case theoretical guarantees and exerimental validation, we show that the choice of this objective is crucial to fair rediction. Furthermore, we resent the first in-deth exerimental demonstration of fair transfer learning and demonstrate emirically that our learned reresentations admit fair redictions on new tasks while maintaining utility, an essential goal of fair reresentation learning.
"
489,2018,Spurious Local Minima are Common in Two-Layer ReLU Neural Networks,Oral/Poster,"We consider the otimization roblem associated with training simle ReLU neural networks of the form $\mathbf{x}\masto \sum_{i=1}^{k}\max\{0,\mathbf{w}_i^\to \mathbf{x}\}$ with resect to the squared loss. We rovide a comuter-assisted roof that even if the inut distribution is standard Gaussian, even if the dimension is arbitrarily large, and even if the target values are generated by such a network, with orthonormal arameter vectors, the roblem can still have surious local minima once $6\le k\le 20$. By a concentration of measure argument, this imlies that in high inut dimensions, \emh{nearly all} target networks of the relevant sizes lead to surious local minima. Moreover, we conduct exeriments which show that the robability of hitting such local minima is quite high, and increasing with the network size. On the ositive side, mild over-arameterization aears to drastically reduce such local minima, indicating that an over-arameterization assumtion is necessary to get a ositive result in this setting."
490,2018,Efficient end-to-end learning for quantizable representations,Oral/Poster,"Embedding reresentation learning via neural networks is at the core foundation of modern similarity based search. While much effort has been ut in develoing algorithms for learning binary hamming code reresentations for search efficiency, this still requires a linear scan of the entire dataset er each query and trades off the search accuracy through binarization. To this end, we consider the roblem of directly learning a quantizable embedding reresentation and the sarse binary hash code end-to-end which can be used to construct an efficient hash table not only roviding significant search reduction in the number of data but also achieving the state of the art search accuracy outerforming revious state of the art dee metric learning methods. We also show that finding the otimal sarse binary hash code in a mini-batch can be comuted exactly in olynomial time by solving a minimum cost flow roblem. Our results on Cifar-100 and on ImageNet datasets show the state of the art search accuracy in recision@k and NMI metrics while roviding u to 98X and 478X search seedu resectively over exhaustive linear search. The source code is available at htts:github.commaestrojeongDee-Hash-Table-ICML18.
"
491,2018,Solving Partial Assignment Problems using Random Clique Complexes,Oral/Poster,"We resent an alternate formulation of the artial assignment roblem as matching random clique comlexes, that are higher-order analogues of random grahs, designed to rovide a set of invariants that better detect higher-order structure. The roosed method creates random clique adjacency matrices for each k-skeleton of the random clique comlexes and matches them, taking into account each oint as the affine combination of its geometric neighborhood. We justify our solution theoretically, by analyzing the runtime and storage comlexity of our algorithm along with the asymtotic behavior of the quadratic assignment roblem (QAP) that is associated with the underlying random clique adjacency matrices. Exeriments on both synthetic and real-world datasets, containing severe occlusions and distortions, rovide insight into the accuracy, efficiency, and robustness of our aroach. We outerform diverse matching algorithms by a significant margin.
"
492,2018,Generalized Earley Parser: Bridging Symbolic Grammars and Sequence Data for Future Prediction,Oral/Poster,"Future redictions on sequence data (e.g., videos or audios) require the algorithms to cature non-Markovian and comositional roerties of high-level semantics. Context-free grammars are natural choices to cature such roerties, but traditional grammar arsers (e.g., Earley arser) only take symbolic sentences as inuts. In this aer, we generalize the Earley arser to arse sequence data which is neither segmented nor labeled. This generalized Earley arser integrates a grammar arser with a classifier to find the otimal segmentation and labels, and makes to-down future redictions. Exeriments show that our method significantly outerforms other aroaches for future human activity rediction.
"
493,2018,Convergence guarantees for a class of non-convex and   non-smooth optimization problems,Oral/Poster,"Non-convex otimization roblems arise frequently in machine learning, including feature selection, structured matrix learning, mixture modeling, and neural network training.  We consider the roblem of finding critical oints of a broad class of non-convex roblems with non-smooth comonents. We analyze the behavior of two gradient-based methods---namely a sub-gradient method, and a roximal method.  Our main results are to establish rates of convergence for general roblems, and also exhibit faster rates for sub-analytic functions.  As an alication of our theory, we obtain a simlification of the oular CCCP algorithm, which retains all the desirable convergence roerties of the original method, along with a significantly lower cost er iteration.  We illustrate our methods and theory via alication to the roblems of best subset selection, robust estimation, and shae from shading reconstruction.
"
494,2018,Estimation of Markov Chain via Rank-constrained Likelihood,Oral/Poster,This aer studies the estimation of low-rank Markov chains from emirical trajectories. We roose a non-convex estimator based on rank-constrained likelihood maximization. Statistical uer bounds are rovided for the Kullback-Leiber divergence and the $\ell_2$ risk between the estimator and the true transition matrix. The estimator reveals a comressed state sace of the Markov chain. We also develo a novel DC (difference of convex function) rogramming algorithm to tackle the rank-constrained non-smooth otimization roblem. Convergence results are established.  Exeriments show that the roosed estimator achieves better emirical erformance than other oular aroaches.
495,2018,Efficient First-Order Algorithms for Adaptive Signal Denoising,Oral/Poster,"We consider the roblem of discrete-time signal denoising, focusing on a secific family of non-linear convolution-tye estimators. Each such estimator is associated with a time-invariant filter which is obtained adatively, by solving a certain convex otimization roblem. Adative convolution-tye estimators were demonstrated to have favorable statistical roerties, see (Juditsky &am; Nemirovski, 2009; 2010; Harchaoui et al., 2015b; Ostrovsky et al., 2016). Our first contribution is an efficient imlementation of these estimators via the known first-order roximal algorithms. Our second contribution is a comutational comlexity analysis of the roosed rocedures, which takes into account their statistical nature and the related notion of statistical accuracy. The roosed rocedures and their analysis are illustrated on a simulated data benchmark.
"
496,2018,Continuous and Discrete-time Accelerated Stochastic Mirror Descent for Strongly Convex Functions,Oral/Poster,"We rovide a second-order stochastic differential equation (SDE), which characterizes the continuous-time dynamics of accelerated stochastic mirror descent (ASMD) for strongly convex functions. This SDE lays a central role in designing new discrete-time ASMD algorithms via numerical discretization, and roviding neat analyses of their convergence rates based on Lyaunov functions. Our results suggest that the only existing ASMD algorithm, namely, AC-SA roosed in \citet{ghadimi2012otimal} is one instance of its kind, and we can actually derive new instances of ASMD with fewer tuning arameters. This sheds light on revisiting accelerated stochastic otimization through the lens of SDEs, which can lead to a better understanding of acceleration in stochastic otimization, as well as new simler algorithms. Numerical exeriments on both synthetic and real data suort our theory.
"
497,2018,Noisin: Unbiased Regularization for Recurrent Neural Networks,Oral/Poster,"Recurrent neural networks (RNNs) are owerful models of sequential data. They have been successfully used in domains such as text and seech. However, RNNs are suscetible to overfitting; regularization is imortant. In this aer we develo Noisin, a new method for regularizing RNNs. Noisin injects random noise into the hidden states of the RNN and then maximizes the corresonding marginal likelihood of the data. We show how Noisin alies to any RNN and we study many different tyes of noise. Noisin is unbiased--it reserves the underlying RNN on average. We characterize how Noisin regularizes its RNN both theoretically and emirically. On language modeling benchmarks, Noisin imroves over droout by as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We also comared the state-of-the-art language model of Yang et al. 2017, both with and without Noisin. On the Penn Treebank, the method with Noisin more quickly reaches state-of-the-art erformance.
"
498,2018,Hierarchical Deep Generative Models for Multi-Rate Multivariate Time Series,Oral/Poster,"Multi-Rate Multivariate Time Series (MR-MTS) are the multivariate time series observations which come with various samling rates and encode multile temoral deendencies. State-sace models such as Kalman filters and dee learning models such as dee Markov models are mainly designed for time series data with the same samling rate and cannot cature all the deendencies resent in the MR-MTS data. To address this challenge, we roose the Multi-Rate Hierarchical Dee Markov Model (MR-HDMM), a novel dee generative model which uses the latent hierarchical structure with a learnable switch mechanism to cature the temoral deendencies of MR-MTS. Exerimental results on two real-world datasets demonstrate that our MR-HDMM model outerforms the existing state-of-the-art dee learning and state-sace models on forecasting and interolation tasks. In addition, the latent hierarchies in our model rovide a way to show and interret the multile temoral deendencies.
"
499,2018,Disentangled Sequential Autoencoder,Oral/Poster,"We resent a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio. Our dee generative model learns a latent reresentation of the data which is slit into a static and dynamic art, allowing us to aroximately disentangle latent time-deendent features (dynamics) from features which are reserved over time (content). This architecture gives us artial control over generating content and dynamics by conditioning on either one of these sets of features. In our exeriments on artificially generated cartoon video clis and voice recordings, we show that we can convert the content of a given sequence into another one by such content swaing. For audio, this allows us to convert a male seaker into a female seaker and vice versa, while for video we can searately maniulate shaes and dynamics.  Furthermore, we give emirical evidence for the hyothesis that stochastic RNNs as latent state models are more efficient at comressing and generating long sequences than deterministic ones, which may be relevant for alications in video comression.
"
500,2018,Stochastic Video Generation with a Learned Prior,Oral/Poster,"Generating video frames that accurately redict future world states is challenging. Existing aroaches either fail to cature the full distribution of outcomes, or yield blurry generations, or both. In this aer we introduce a video generation model with a learned rior over stochastic latent variables at each time ste. Video frames are generated by drawing samles from this rior and combining them with a deterministic estimate of the future frame. The aroach is simle and easily trained end-to-end on a variety of datasets. Samle generations are both varied and shar, even many frames into the future, and comare favorably to those from existing aroaches.
"
501,2018,Mutual Information Neural Estimation,Oral/Poster,"We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks.   We resent a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in samle size, trainable through back-ro, and strongly consistent. We resent a handful of alications on which MINE can be used to minimize or maximize mutual information. We aly MINE to imrove adversarially trained generative models. We also use MINE to imlement the Information Bottleneck, alying it to suervised classification; our results demonstrate substantial imrovement in flexibility and erformance in these settings.
"
502,2018,Adversarially Regularized Autoencoders,Oral/Poster,"Dee latent variable models, trained using variational autoencoders or generative adversarial networks, are now a key technique for reresentation learning of continuous structures. However, alying similar methods to discrete structures, such as text sequences or discretized images, has roven to be more challenging. In this work, we roose a more flexible method for training dee latent variable models of discrete structures. Our aroach is based on the recently roosed Wasserstein Autoencoder (WAE) which formalizes adversarial autoencoders as an otimal transort roblem. We first extend this framework to model discrete sequences, and then further exlore different learned riors targeting a controllable reresentation. Unlike many other latent variable generative models for text, this adversarially regularized autoencoder (ARAE) allows us to generate fluent textual oututs as well as erform maniulations in the latent sace to induce change in the outut sace. Finally we show that the latent reresentation can be trained to erform unaligned textual style transfer, giving imrovements both in automatic measures and human evaluation.
"
503,2018,Policy Optimization as Wasserstein Gradient Flows,Oral/Poster,"Policy otimization is a core comonent of reinforcement learning (RL), and most existing RL methods directly otimize arameters of a olicy based on maximizing the exected total reward, or its surrogate. Though often achieving encouraging emirical success, its corresondence to olicy-distribution otimization has been unclear mathematically. We lace olicy otimization into the sace of robability measures, and interret it as Wasserstein gradient flows. On the robability-measure sace, under secified circumstances, olicy otimization becomes convex in terms of distribution otimization. To make otimization feasible, we develo efficient algorithms by numerically solving the corresonding discrete gradient flows. Our technique is alicable to several RL settings, and is related to many state-of-the-art olicy-otimization algorithms. Secifically, we define gradient flows on both the arameter-distribution sace and olicy-distribution sace, leading to what we term indirect-olicy and direct-olicy learning frameworks, resectively. Extensive exeriments verify the effectiveness of our framework, often obtaining better erformance comared to related algorithms.
"
504,2018,Self-Imitation Learning,Oral/Poster,"This aer rooses Self-Imitation Learning (SIL), a simle off-olicy actor-critic algorithm that learns to reroduce the agent's ast good decisions. This algorithm is designed to verify our hyothesis that exloiting ast good exeriences can indirectly drive dee exloration. Our emirical results show that SIL significantly imroves advantage actor-critic (A2C) on several hard exloration Atari games and is cometitive to the state-of-the-art count-based exloration methods. We also show that SIL imroves roximal olicy otimization (PPO) on MuJoCo tasks.
"
505,2018,Spectrally Approximating Large Graphs with Smaller Graphs,Oral/Poster,"How does coarsening affect the sectrum of a general grah? We rovide conditions such that the rincial eigenvalues and eigensaces of a coarsened and original grah Lalacian matrices are close. The achieved aroximation is shown to deend on standard grah-theoretic roerties, such as the degree and eigenvalue distributions, as well as on the ratio between the coarsened and actual grah sizes. Our results carry imlications for learning methods that utilize coarsening. For the articular case of sectral clustering, they imly that coarse eigenvectors can be used to derive good quality assignments even without refinement—this henomenon was reviously observed, but lacked formal justification.
"
506,2018,On the Spectrum of Random Features Maps of High Dimensional Data,Oral/Poster,"Random feature mas are ubiquitous in modern statistical machine learning, where they generalize random rojections by means of owerful, yet often difficult to analyze nonlinear oerators. In this aer we leverage the ""concentration"" henomenon induced by random matrix theory to erform a sectral analysis on the Gram matrix of these random feature mas, here for Gaussian mixture models of simultaneously large dimension and size. Our results are instrumental to a deeer understanding on the interlay of the nonlinearity and the statistics of the data, thereby allowing for a better tuning of random feature-based techniques.
"
507,2018,Learning Registered Point Processes from Idiosyncratic Observations,Oral/Poster,"A arametric oint rocess model is develoed, with modeling based on the assumtion that sequential observations often share latent henomena, while also ossessing idiosyncratic effects.  An alternating otimization method is roosed to learn a coderegistered'' oint rocess that accounts for shared structure, as well ascodewaring'' functions that characterize idiosyncratic asects of each observed sequence.  Under reasonable constraints, in each iteration we udate the samle-secific waring functions by solving a set of constrained nonlinear rogramming roblems in arallel, and udate the model by maximum likelihood estimation.  The justifiability, comlexity and robustness of the roosed method are investigated in detail, and the influence of sequence stitching on the learning results is examined emirically. Exeriments on both synthetic and real-world data demonstrate that the method yields exlainable oint rocess models, achieving encouraging results comared to state-of-the-art methods.
"
508,2018,Deep Bayesian Nonparametric Tracking,Oral/Poster,"Time-series data often exhibit irregular behavior, making them hard to analyze and exlain with a simle dynamic model. For examle, information in social networks may show change-oint-like bursts that then diffuse  with smooth dynamics. Powerful models such as dee neural networks learn smooth functions from data, but are not as well-suited (in off-the-shelf form) for discovering and exlaining sarse, discrete and bursty dynamic atterns. Bayesian models can do this well by encoding the aroriate robabilistic assumtions in the model rior. We roose an integration of Bayesian nonarametric methods within dee neural networks for modeling irregular atterns in time-series data. We use a Bayesian nonarametrics to model change-oint behavior in time, and a dee neural network to model nonlinear latent sace dynamics. We comare with a non-dee linear version of the model also roosed here. Emirical evaluations demonstrates imroved erformance and interretable results when tracking stock rices and Twitter trends.
"
509,2018,Learning and Memorization,Oral/Poster,"In the machine learning research community, it is generally believed that there is a tension between memorization and generalization. In this work we examine to what extent this tension exists by exloring if it is ossible to generalize by memorizing alone. Although direct memorization with a looku table obviously does not generalize, we find that introducing deth in the form of a network of suort-limited looku tables leads to generalization that is significantly above chance and closer to those obtained by standard learning algorithms on several tasks derived from MNIST and CIFAR-10. Furthermore, we demonstrate through a series of emirical results that our aroach allows for a smooth tradeoff between memorization and generalization and exhibits some of the most salient characteristics of neural networks: deth imroves erformance; random data can be memorized and yet there is generalization on real data; and memorizing random data is harder in a certain sense than memorizing real data. The extreme simlicity of the algorithm and otential connections with generalization theory oint to several interesting directions for future research.
"
510,2018,Attention-based Deep Multiple Instance Learning,Oral/Poster,"Multile instance learning (MIL) is a variation of suervised learning where a single class label is assigned to a bag of instances. In this aer, we state the MIL roblem as learning the Bernoulli distribution of the bag label where the bag label robability is fully arameterized by neural networks. Furthermore, we roose a neural network-based ermutation-invariant aggregation oerator that corresonds to the attention mechanism. Notably, an alication of the roosed attention-based oerator rovides insight into the contribution of each instance to the bag label. We show emirically that our aroach achieves comarable erformance to the best MIL methods on benchmark MIL datasets and it outerforms other methods on a MNIST-based MIL dataset and two real-life histoathology datasets without sacrificing interretability.
"
511,2018,Classification from Pairwise Similarity and Unlabeled Data,Oral/Poster,"Suervised learning needs a huge amount of labeled data, which can be a big bottleneck under the situation where there is a rivacy concern or labeling cost is high. To overcome this roblem, we roose a new weakly-suervised learning setting where only similar (S) data airs (two examles belong to the same class) and unlabeled (U) data oints are needed instead of fully labeled data, which is called SU classification. We show that an unbiased estimator of the classification risk can be obtained only from SU data, and the estimation error of its emirical risk minimizer achieves the otimal arametric convergence rate. Finally, we demonstrate the effectiveness of the roosed method through exeriments.
"
512,2018,Analyzing the Robustness of Nearest Neighbors to Adversarial Examples,Oral/Poster,"Motivated by safety-critical alications, test-time attacks on classifiers via adversarial examles has recently received a great deal of attention. However, there is a general lack of understanding on why adversarial examles arise; whether they originate due to inherent roerties of data or due to lack of training samles remains ill-understood. In this work, we introduce a theoretical framework analogous to bias-variance theory for understanding these effects. We use our framework to analyze the robustness of a canonical non-arametric classifier – the k-nearest neighbors. Our analysis shows that its robustness roerties deend critically on the value of k – the classifier may be inherently non-robust for small k, but its robustness aroaches that of the Bayes Otimal classifier for fast-growing k. We roose a novel modified 1-nearest neighbor classifier, and guarantee its robustness in the large samle limit. Our exeriments suggest that this classifier may have good robustness roerties even for reasonable data set sizes.
"
513,2018,On the Implicit Bias of Dropout,Oral/Poster,"Algorithmic aroaches endow dee learning systems with imlicit bias that hels them generalize even in over-arametrized settings. In this aer, we focus on understanding such a bias induced in learning through droout, a oular technique to avoid overfitting in dee learning. For single hidden-layer linear neural networks, we show that droout tends to make the norm of incomingoutgoing weight vectors of all the hidden nodes equal. In addition, we rovide a comlete characterization of the otimization landscae induced by droout.
"
514,2018,Convolutional Imputation of Matrix Networks,Oral/Poster,"A matrix network is a family of matrices, with their relations modeled as a weighted grah. We consider the task of comleting a artially observed matrix network. The observation comes from a novel samling scheme where a fraction of matrices might be comletely unobserved. How can we recover the entire matrix network from incomlete observations? This mathematical roblem arises in many alications including medical imaging and social networks.  To recover the matrix network, we roose a structural assumtion that the matrices are low-rank after the grah Fourier transform on the network. We formulate a convex otimization roblem and rove an exact recovery guarantee for the otimization roblem. Furthermore, we numerically characterize the exact recovery regime for varying rank and samling rate and discover a new hase transition henomenon. Then we give an iterative imutation algorithm to efficiently solve otimization roblem and comlete large scale matrix networks.  We demonstrate the algorithm with a variety of alications such as MRI and Facebook user network.
"
515,2018,Detecting and Correcting for Label Shift with Black Box Predictors,Oral/Poster,"Faced with  distribution shift between training and test set, we wish to detect and quantify the shift, and to correct our classifiers without test set labels. Motivated by medical diagnosis, where diseases (targets), cause symtoms (observations), we focus on label shift, where the label marginal (y) changes but the conditional (x| y) does not. We roose Black Box Shift Estimation (BBSE) to estimate the test distribution (y). BBSE exloits arbitrary black box redictors to reduce dimensionality rior to shift correction. While better redictors give tighter estimates, BBSE works even when redictors are biased, inaccurate, or uncalibrated, so long as their confusion matrices are invertible. We rove BBSE's consistency, bound its error, and introduce a statistical test that uses BBSE to detect shift. We also leverage BBSE to correct classifiers. Exeriments demonstrate accurate estimates and imroved rediction, even on high-dimensional datasets of natural images.
"
516,2018,Orthogonality-Promoting Distance Metric Learning: Convex Relaxation and Theoretical Analysis,Oral/Poster,"Distance metric learning (DML), which learns a distance metric from labeled ""similar"" and ""dissimilar"" data airs, is widely utilized. Recently, several works investigate orthogonality-romoting regularization (OPR), which encourages the rojection vectors in DML to be close to being orthogonal, to achieve three effects: (1) high balancedness -- achieving comarable erformance on both frequent and infrequent classes; (2) high comactness -- using a small number of rojection vectors to achieve a ""good"" metric; (3) good generalizability -- alleviating overfitting to training data. While showing romising results, these aroaches suffer three roblems. First, they involve solving non-convex otimization roblems where achieving the global otimal is NP-hard. Second, it lacks a theoretical understanding why OPR can lead to balancedness. Third, the current generalization error analysis of OPR is not directly on the regularizer. In this aer, we address these three issues by (1) seeking convex relaxations of the original nonconvex roblems so that the global otimal is guaranteed to be achievable; (2) roviding a formal analysis on OPR's caability of romoting balancedness; (3)  roviding a theoretical analysis that directly reveals the relationshi between OPR and generalization erformance. Exeriments on various datasets demonstrate that our convex methods are more effective in romoting balancedness, comactness, and generalization, and are comutationally more efficient, comared with the nonconvex methods.
"
517,2018,Comparison-Based Random Forests,Oral/Poster,"Assume we are given a set of items from a general metric sace, but we neither have access to the reresentation of the data nor to the distances between data oints. Instead, suose that we can actively choose a trilet of items (A, B, C) and ask an oracle whether item A is closer to item B or to item C. In this aer, we roose a novel random forest algorithm for regression and classification that relies only on such trilet comarisons. In the theory art of this aer, we establish sufficient conditions for the consistency of such a forest. In a set of comrehensive exeriments, we then demonstrate that the roosed random forest is efficient both for classification and regression. In articular, it is even cometitive with other methods that have direct access to the metric reresentation of the data.
"
518,2018,A Probabilistic Theory of Supervised Similarity Learning for Pointwise ROC Curve Optimization,Oral/Poster,"The erformance of many machine learning techniques deends on the choice of an aroriate similarity or distance measure on the inut sace. Similarity learning (or metric learning) aims at building such a measure from training data so that observations with the same (res. different) label are as close (res. far) as ossible. In this aer, similarity learning is investigated from the ersective of airwise biartite ranking, where the goal is to rank the elements of a database by decreasing order of the robability that they share the same label with some query data oint, based on the similarity scores. A natural erformance criterion in this setting is ointwise ROC otimization: maximize the true ositive rate under a fixed false ositive rate. We study this novel ersective on similarity learning through a rigorous robabilistic framework. The emirical version of the roblem gives rise to a constrained otimization formulation involving U-statistics, for which we derive universal learning rates as well as faster rates under a noise assumtion on the data distribution. We also address the large-scale setting by analyzing the effect of samling-based aroximations. Our theoretical results are suorted by illustrative numerical exeriments.
"
519,2018,Provable Variable Selection for Streaming Features,Oral/Poster,"In large-scale machine learning alications and high-dimensional statistics, it is ubiquitous to address a considerable number of features among which many are redundant. As a remedy, online feature selection has attracted increasing attention in recent years. It sequentially reveals features and evaluates the imortance of them. Though online feature selection has roven an elegant methodology, it is usually challenging to carry out a rigorous theoretical characterization. In this work, we roose a rovable online feature selection algorithm that utilizes the online leverage score. The selected features are then fed to $k$-means clustering, making the clustering ste memory and comutationally efficient. We rove that with high robability, erforming $k$-means clustering based on the selected feature sace does not deviate far from the otimal clustering using the original data. The emirical results on real-world data sets demonstrate the effectiveness of our algorithm."
520,2018,Out-of-sample extension of graph adjacency spectral embedding,Oral/Poster,"Many oular dimensionality reduction rocedures have out-of-samle extensions, which allow a ractitioner to aly a learned embedding to observations not seen in the initial training samle. In this work, we consider the roblem of obtaining an out-of-samle extension for the adjacency sectral embedding, a rocedure for embedding the vertices of a grah into Euclidean sace. We resent two different aroaches to this roblem, one based on a least-squares objective and the other based on a maximum-likelihood formulation. We show that if the grah of interest is drawn according to a certain latent osition model called a random dot roduct grah, then both of these out-of-samle extensions estimate the true latent osition of the out-of-samle vertex with the same error rate. Further, we rove a central limit theorem for the least-squares-based extension, showing that the estimate is asymtotically normal about the truth in the large-grah limit.
"
521,2018,Gradient Descent for Sparse Rank-One Matrix Completion for Crowd-Sourced Aggregation of Sparsely Interacting Workers,Oral/Poster,"We consider worker skill estimation for the single coin Dawid-Skene crowdsourcing model. In ractice skill-estimation is challenging because worker assignments are sarse and irregular due to the arbitrary, and uncontrolled availability of workers. We formulate skill estimation as a rank-one correlation-matrix comletion roblem, where the observed comonents corresond to observed label correlation between workers. We show that the correlation matrix can be successfully recovered and skills identifiable if and only if the samling matrix (observed comonents) is irreducible and aeriodic. We then roose an efficient gradient descent scheme and show that skill estimates converges to the desired global otima for such samling matrices. Our roof is original and the results are surrising in light of the fact that even the weighted rank-one matrix factorization roblem is NP hard in general. Next we derive samle comlexity bounds for the noisy case in terms of sectral roerties of the signless Lalacian of the samling matrix. Our roosed scheme achieves state-of-art erformance on a number of real-world datasets.
"
522,2018,Fast and Sample Efficient Inductive Matrix Completion via Multi-Phase Procrustes Flow,Oral/Poster,"We revisit the inductive matrix comletion roblem that aims to recover a rank-$r$ matrix with ambient dimension $d$ given $n$ features as the side rior information. The goal is to make use of the known $n$ features to reduce samle and comutational comlexities. We resent and analyze a new gradient-based non-convex otimization algorithm that converges to the true underlying matrix at a linear rate with samle comlexity only linearly deending on $n$ and logarithmically deending on $d$. To the best of our knowledge, all revious algorithms either have a quadratic deendency on the number of features in samle comlexity or a sub-linear comutational convergence rate. In addition, we rovide exeriments on both synthetic and real world data to demonstrate the effectiveness of our roosed algorithm."
523,2018,DCFNet: Deep Neural Network with Decomposed Convolutional Filters,Oral/Poster,"Filters in a Convolutional Neural Network (CNN) contain model arameters learned from enormous amounts of data. In this aer,  we suggest to decomose convolutional filters in CNN as a truncated exansion with re-fixed bases, namely the Decomosed Convolutional Filters network (DCFNet), where the exansion coefficients remain learned from data. Such a structure not only reduces the number of trainable arameters and comutation, but also imoses filter regularity by bases truncation. Through extensive exeriments, we consistently observe that DCFNet maintains accuracy for image classification tasks with a significant reduction of model arameters, articularly with Fourier-Bessel (FB) bases, and even with random bases. Theoretically, we analyze the reresentation stability of DCFNet with resect to inut variations, and rove reresentation stability under generic assumtions on the exansion coefficients. The analysis is consistent with the emirical observations.
"
524,2018,Optimization Landscape and Expressivity of Deep CNNs,Oral/Poster,"We analyze the loss landscae and exressiveness of ractical dee convolutional neural networks (CNNs) with shared weights and max ooling layers. We show that such CNNs roduce linearly indeendent features at a ``wide'' layer which has more neurons than the number of training samles. This condition holds e.g. for the VGG network. Furthermore, we rovide for such wide CNNs necessary and sufficient conditions for global minima with zero training error. For the case where the wide layer is followed by a fully connected layer we show that almost every critical oint of the emirical loss is a global minimum with zero training error. Our analysis suggests that both deth and width are very imortant in dee learning. While deth brings more reresentational ower and allows the network to learn high level features, width smoothes the otimization landscae of the loss function in the sense that a sufficiently wide network  has a well-behaved loss surface with almost no bad local minima.
"
525,2018,Scalable Gaussian Processes with Grid-Structured Eigenfunctions (GP-GRIEF),Oral/Poster,"We introduce a kernel aroximation strategy that enables comutation of the Gaussian rocess log marginal likelihood and all hyerarameter derivatives in O() time. Our GRIEF kernel consists of  eigenfunctions found using a Nyström aroximation from a dense Cartesian roduct grid of inducing oints. By exloiting algebraic roerties of Kronecker and Khatri-Rao tensor roducts, comutational comlexity of the training rocedure can be ractically indeendent of the number of inducing oints. This allows us to use arbitrarily many inducing oints to achieve a globally accurate kernel aroximation, even in high-dimensional roblems. The fast likelihood evaluation enables tye-I or II Bayesian inference on large-scale datasets. We benchmark our algorithms on real-world roblems with u to two-million training oints and 10^33 inducing oints.
"
526,2018,Learning in Integer Latent Variable Models with Nested Automatic Differentiation,Oral/Poster,"We develo nested automatic differentiation (AD) algorithms for exact inference and learning in integer latent variable models. Recently, Winner, Sujono, and Sheldon showed how to reduce marginalization in a class of integer latent variable models to evaluating a robability generating function which contains many levels of nested high-order derivatives. We contribute faster and more stable AD algorithms for this challenging roblem and a novel algorithm to comute exact gradients for learning. These contributions lead to significantly faster and more accurate learning algorithms, and are the first AD algorithms whose running time is olynomial in the number of levels of nesting.
"
527,2018,CyCADA: Cycle-Consistent Adversarial Domain Adaptation,Oral/Poster,"Domain adatation is critical for success in new, unseen environments. Adversarial adatation models have shown tremendous rogress towards adating to new environments by focusing either on discovering domain invariant reresentations or by maing between unaired image domains.  While feature sace methods are difficult to interret and sometimes fail to cature ixel-level and low-level domain shifts, image sace methods sometimes fail to incororate high level semantic knowledge relevant for the end task. We roose a model which adats between domains using both generative image sace alignment and latent reresentation sace alignment.  Our aroach, Cycle-Consistent Adversarial Domain Adatation (CyCADA), guides transfer between domains according to a secific discriminatively trained task and  avoids divergence by enforcing consistency of the relevant semantics before and after adatation. We evaluate our method on a variety of visual recognition and rediction settings, including digit classification and semantic segmentation of road scenes, advancing state-of-the-art erformance for unsuervised adatation from synthetic to real world driving domains.
"
528,2018,Rectify Heterogeneous Models with Semantic Mapping,Oral/Poster,"On the way to the robust learner for real-world alications, there are still great challenges, including considering unknown environments with limited data. Learnware (Zhou; 2016) describes a novel ersective, and claims that learning models should have reusable and evolvable roerties. We roose to Encode Meta InformaTion of features (EMIT), as the model secification for characterizing the changes, which grants the model evolvability to bridge heterogeneous feature saces. Then, re-trained models from related tasks can be Reused by our REctiFy via heterOgeneous Redictor Maing (REFORM}) framework. In summary, the re-trained model is adated to a new environment with different features, through model refining on only a small amount of training data in the current task. Exerimental results over both synthetic and real-world tasks with diverse feature configurations validate the effectiveness and ractical utility of the roosed framework.
"
529,2018,DVAE++: Discrete Variational Autoencoders with Overlapping Transformations,Oral/Poster,"Training of discrete latent variable models remains challenging because assing gradient information through discrete units is difficult. We roose a new class of smoothing transformations based on a mixture of two overlaing distributions, and show that the roosed transformation can be used for training binary latent models with either directed or undirected riors. We derive a new variational bound to efficiently train with Boltzmann machine riors. Using this bound, we develo DVAE++, a generative model with a global discrete rior and a hierarchy of convolutional continuous variables. Exeriments on several benchmarks show that overlaing transformations outerform other recent continuous relaxations of discrete latent variables including Gumbel-Softmax (Maddison et al., 2016; Jang et al., 2016), and discrete variational autoencoders (Rolfe 2016).
"
530,2018,Iterative Amortized Inference,Oral/Poster,"Inference models are a key comonent in scaling variational inference to dee latent variable models, most notably as encoder networks in variational auto-encoders (VAEs). By relacing conventional otimization-based inference with a learned model, inference is amortized over data examles and therefore more comutationally efficient. However, standard inference models are restricted to direct maings from data to aroximate osterior estimates. The failure of these models to reach fully otimized aroximate osterior estimates results in an amortization ga. We aim toward closing this ga by roosing iterative inference models, which learn to erform inference otimization through reeatedly encoding gradients. Our aroach generalizes standard inference models in VAEs and rovides insight into several emirical findings, including to-down inference techniques. We demonstrate the inference otimization caabilities of iterative inference models and show that they outerform standard inference models on several benchmark data sets of images and text.
"
531,2018,Blind Justice: Fairness with Encrypted Sensitive Attributes,Oral/Poster,"Recent work has exlored how to train machine learning models which do not discriminate against any subgrou of the oulation as determined by sensitive attributes such as gender or race. To avoid disarate treatment, sensitive attributes should not be considered. On the other hand, in order to avoid disarate imact, sensitive attributes must be examined, e.g., in order to learn a fair model, or to check if a given model is fair. We introduce methods from secure multi-arty comutation which allow us to avoid both. By encryting sensitive attributes, we show how an outcome-based fair model may be learned, checked, or have its oututs verified and held to account, without users revealing their sensitive attributes.
"
532,2018,Active Learning with Logged Data,Oral/Poster,"We consider active learning with logged data, where labeled examles are drawn conditioned on a redetermined logging olicy, and the goal is to learn a classifier on the entire oulation, not just conditioned on the logging olicy. Prior work addresses this roblem either when only logged data is available, or urely in a controlled random exerimentation setting where the logged data is ignored. In this work, we combine both aroaches to rovide an algorithm that uses logged data to bootstra and inform exerimentation, thus achieving the best of both worlds. Our work is insired by a connection between controlled random exerimentation and active learning, and modifies existing disagreement-based active learning algorithms to exloit logged data.
"
533,2018,A Reductions Approach to Fair Classification,Oral/Poster,"We resent a systematic aroach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our aroach encomasses many other reviously studied definitions as secial cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification roblems, whose solutions yield a randomized classifier with the lowest (emirical) error subject to the desired constraints. We introduce two reductions that work for any reresentation of the cost-sensitive classifier and comare favorably to rior baselines on a variety of data sets, while overcoming several of their disadvantages.
"
534,2018,Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness,Oral/Poster,"The most revalent notions of fairness in machine learning fix a small collection of re-defined grous (such as race or gender), and then ask for aroximate arity of some statistic of the classifier (such as false ositive rate) across these grous.  Constraints of this form are suscetible to fairness gerrymandering, in which a classifier is fair on each individual grou, but badly violates the fairness constraint on structured subgrous, such as certain combinations of rotected attribute values.  We thus consider  fairness across exonentially or infinitely many subgrous, defined by a structured class of functions over the rotected attributes. We first rove that the roblem of auditing subgrou fairness for both equality of false ositive rates and statistical arity is comutationally equivalent to the roblem of weak agnostic learning --- which means it is hard in the worst case, even for simle structured subclasses. However, it also suggests that common heuristics for learning can be alied to successfully solve the auditing roblem in ractice.  We then derive an algorithm that rovably converges in a olynomial number of stes to the best subgrou-fair distribution over classifiers, given access to an oracle which can solve the agnostic learning roblem. The algorithm is based on a formulation of subgrou fairness as a zero-sum game between a Learner (the rimal layer) and an Auditor (the dual layer). We imlement a variant of this algorithm using heuristic oracles, and show that we can effectively both audit and learn fair classifiers on a real dataset.
"
535,2018,Bayesian Model Selection for Change Point Detection and Clustering,Oral/Poster,"We address a generalization of change oint detection with the urose of detecting the change locations and the levels of clusters of a iecewise constant signal. Our aroach is to model it as a nonarametric enalized least square model selection on a family of models indexed over the collection of artitions of the design oints and roose a comutationally efficient algorithm to aroximately solve it. Statistically, minimizing such a enalized criterion yields an aroximation to the maximum a-osteriori robability (MAP) estimator. The criterion is then analyzed and an oracle inequality is derived using a Gaussian concentration inequality. The oracle inequality is used to derive on one hand conditions for consistency and on the other hand an adative uer bound on the exected square risk of the estimator, which statistically motivates our aroximation. Finally, we aly our algorithm to simulated data to exerimentally validate the statistical guarantees and illustrate its behavior.
"
536,2018,A Unified Framework for Structured Low-rank Matrix Learning,Oral/Poster,"We consider the roblem of learning a low-rank matrix, constrained to lie in a linear subsace, and introduce a novel factorization for modeling such matrices. A salient feature of the roosed factorization scheme is it decoules the low-rank and the structural constraints onto searate factors. We formulate the otimization roblem on the  Riemannian sectrahedron manifold, where the Riemannian framework allows to develo comutationally efficient conjugate gradient and trust-region algorithms. Exeriments on roblems such as standardrobustnon-negative matrix comletion, Hankel matrix learning and multi-task learning demonstrate the efficacy of our aroach.
"
537,2018,Firing Bandits: Optimizing Crowdfunding,Oral/Poster,"In this aer, we model the roblem of otimizing crowdfunding latforms, such as the non-rofit Kiva or for-rofit KickStarter, as a variant of the multi-armed bandit roblem. In our setting, Bernoulli arms emit no rewards until their cumulative number of successes over any number of trials exceeds a fixed threshold and then rovides no additional reward for any additional trials - a rocess reminiscent to that of a neuron firing once it reaches the action otential and then saturates. In the sirit of an infinite armed bandit roblem, the layer can add new arms whose exected robability of success is drawn iid from an unknown distribution -- this endless suly of rojects models the harsh reality that the number of rojects seeking funding greatly exceeds the total caital available by lenders. Crowdfunding latforms naturally fall under this setting where the arms are otential rojects, and their robability of success is the robability that a otential funder decides to fund it after reviewing it. The goal is to lay arms (rioritize the dislay of rojects on a webage) to maximize the number of arms that reach the firing threshold (meet their goal amount) using as few total trials (number of imressions) as ossible over all the layed arms. We rovide an algorithm for this setting and rove sublinear regret bounds.
"
538,2018,Multi-Fidelity Black-Box Optimization with Hierarchical Partitions,Oral/Poster,"Motivated by settings such as hyer-arameter tuning and hysical simulations, we consider the roblem of black-box otimization of a function. Multi-fidelity techniques have become oular for alications where exact function evaluations are exensive, but coarse (biased) aroximations are available at much lower cost. A canonical examle is that of hyer-arameter selection in a learning algorithm. The learning algorithm can be trained for fewer iterations -- this results in a lower cost, but its validation error is only coarsely indicative of the same if the algorithm had been trained till comletion. We incororate the multi-fidelity setu into the owerful framework of black-box otimization through hierarchical artitioning. We develo tree-search based multi-fidelity algorithms with theoretical guarantees on simle regret. We finally demonstrate the erformance gains of our algorithms on both real and synthetic datasets.
"
539,2018,Compiling Combinatorial Prediction Games,Oral/Poster,"In online otimization, the goal is to iteratively choose solutions from a decision sace, so as to minimize the average cost over time. As long as this decision sace is described by combinatorial constraints, the roblem is generally intractable. In this aer, we consider the aradigm of comiling the set of combinatorial constraints into a deterministic and Decomosable Negation Normal Form (dDNNF) circuit, for which the tasks of linear otimization and solution samling take linear time. Based on this framework, we rovide efficient characterizations of existing combinatorial rediction strategies, with a articular attention to mirror descent techniques. These strategies are comared on several real-world benchmarks for which the set of Boolean constraints is reliminarily comiled into a dDNNF circuit.
"
540,2018,Rates of Convergence of Spectral Methods for Graphon Estimation,Oral/Poster,"This aer studies the roblem of estimating the grahon function -- a generative mechanism for a class of random grahs that are useful aroximations to real networks. Secifically, a grah of  $n$ vertices is generated such that each air of two vertices  $i$ and $j$ are connected indeendently with robability $\rho_n \times f(x_i,x_j)$, where $x_i$ is the unknown $d$-dimensional  label of vertex $i$, $f$ is an unknown symmetric function, and $\rho_n$, assumed to be $\Omega(\log nn)$, is a scaling arameter characterizing the grah sarsity. The task is to estimate grahon $f$ given the grah. Recent studies have identified the  minimax otimal estimation error rate for $d=1$. However, there exists a wide ga between the known error rates of olynomial-time estimators  and the minimax otimal error rate.  We imrove on the reviously known error rates of olynomial-time estimators,    by analyzing a sectral method, namely universal singular value thresholding (USVT) algorithm. When $f$ belongs to either H\""{o}lder or Sobolev sace with smoothness  index $\alha$, we show the error rates of USVT are at most $(n\rho)^{ -2 \alha  (2\alha+d)}$. These error rates aroach the  minimax otimal error rate $\log (n\rho)(n\rho)$ roved in rior work  for $d=1$, as  $\alha$ increases, i.e., $f$ becomes smoother. Furthermore, when $f$ is analytic with infinitely many times differentiability, we show the error rate of USVT is  at most $\log^d (n\rho)(n\rho)$.  When $f$ is a ste function which corresonds to the stochastic block model with $k$ blocks for some $k$, the error rate of USVT is at most $k(n\rho)$, which is larger than the  minimax otimal error rate by at most a multilicative factor $k\log k$. This coincides with the comutational ga observed in community detection. A key ingredient of our analysis is  to derive the eigenvalue decaying rate of the edge robability matrix using iecewise olynomial aroximations of the grahon function $f$."
541,2018,Characterizing and Learning Equivalence Classes of Causal DAGs under Interventions,Oral/Poster,"We consider the roblem of learning causal DAGs in the setting where both observational and interventional data is available. This setting is common in biology, where gene regulatory networks can be intervened on using chemical reagents or gene deletions. Hauser &am; Buhlmann (2012) reviously characterized the identifiability of causal DAGs under erfect interventions, which eliminate deendencies between targeted variables and their direct causes. In this aer, we extend these identifiability results to general interventions, which may modify the deendencies between targeted variables and their causes without eliminating them. We define and characterize the interventional Markov equivalence class that can be identified from general (not necessarily erfect) intervention exeriments. We also roose the first rovably consistent algorithm for learning DAGs in this setting and evaluate our algorithm on simulated and biological datasets.
"
542,2018,Minimal I-MAP MCMC for Scalable Structure Discovery in Causal DAG Models,Oral/Poster,"Learning a Bayesian network (BN) from data can be useful for decision-making or discovering causal relationshis. However, traditional methods often fail in modern alications, which exhibit a larger number of observed variables than data oints. The resulting uncertainty about the underlying network as well as the desire to incororate rior information recommend a Bayesian aroach to learning the BN, but the highly combinatorial structure of BNs oses a striking challenge for inference. The current state-of-the-art methods such as order MCMC are faster than revious methods but revent the use of many natural structural riors and still have running time exonential in the maximum indegree of the true directed acyclic grah (DAG) of the BN. We here roose an alternative osterior aroximation based on the observation that, if we incororate emirical conditional indeendence tests, we can focus on a high-robability DAG associated with each order of the vertices. We show that our method allows the desired flexibility in rior secification, removes timing deendence on the maximum indegree, and yields rovably good osterior aroximations; in addition, we show that it achieves suerior accuracy, scalability, and samler mixing on several datasets.
"
543,2018,StrassenNets: Deep Learning with a Multiplication Budget,Oral/Poster,"A large fraction of the arithmetic oerations required to evaluate dee neural networks (DNNs) consists of matrix multilications, in both convolution and fully connected layers. We erform end-to-end learning of low-cost aroximations of matrix multilications in DNN layers by casting matrix multilications as 2-layer sum-roduct networks (SPNs) (arithmetic circuits) and learning their (ternary) edge weights from data. The SPNs disentangle multilication and addition oerations and enable us to imose a budget on the number of multilication oerations. Combining our method with knowledge distillation and alying it to image classification DNNs (trained on ImageNet) and language modeling DNNs (using LSTMs), we obtain a first-of-a-kind reduction in number of multilications (over 99.5%) while maintaining the redictive erformance of the full-recision models. Finally, we demonstrate that the roosed framework is able to rediscover Strassen's matrix multilication algorithm, learning to multily $2 \times 2$ matrices using only 7 multilications instead of 8."
544,2018,Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace,Oral/Poster,"Gradient-based meta-learning methods leverage gradient descent to learn the commonalities among various tasks. While revious such methods have been successful in meta-learning tasks, they resort to simle gradient descent during meta-testing. Our rimary contribution is the {\em MT-net}, which enables the meta-learner to learn on each layer's activation sace a subsace that the task-secific learner erforms gradient descent on. Additionally, a task-secific learner of an {\em MT-net} erforms gradient descent with resect to a meta-learned distance metric, which wars the activation sace to be more sensitive to task identity. We demonstrate that the dimension of this learned subsace reflects the comlexity of the task-secific learner's adatation task, and also that our model is less sensitive to the choice of initial learning rates than revious gradient-based meta-learning methods. Our method achieves state-of-the-art or comarable erformance on few-shot classification and regression tasks.
"
545,2018,Candidates vs. Noises Estimation for Large Multi-Class Classification Problem,Oral/Poster,"This aer rooses a method for multi-class classification roblems, where the number of classes K is large. The method, referred to as Candidates vs. Noises Estimation (CANE), selects a small subset of candidate classes and samles the remaining classes. We show that CANE is always consistent and comutationally efficient. Moreover, the resulting estimator has low statistical variance aroaching that of the maximum likelihood estimator, when the observed label belongs to the selected candidates with high robability. In ractice, we use a tree structure with leaves as classes to romote fast beam search for candidate selection. We further aly the CANE method to estimate word robabilities in learning large neural language models. Extensive exerimental results show that CANE achieves better rediction accuracy over the Noise-Contrastive Estimation (NCE), its variants and a number of the state-of-the-art tree classifiers, while it gains significant seedu comared to standard O(K) methods.
"
546,2018,"CRAFTML, an Efficient Clustering-based Random Forest for Extreme Multi-label Learning",Oral/Poster,"Extreme Multi-label Learning (XML) considers large sets of items described by a number of labels that can exceed one million. Tree-based methods, which hierarchically artition the roblem into small scale sub-roblems, are articularly romising in this context to reduce the learningrediction comlexity and to oen the way to arallelization. However, the current best aroaches do not exloit tree randomization which has shown its efficiency in random forests and they resort to comlex artitioning strategies. To overcome these limits, we here introduce a new random forest based algorithm with a very fast artitioning aroach called CRAFTML. Exerimental comarisons on nine datasets from the XML literature show that it outerforms the other tree-based aroaches. Moreover with a arallelized imlementation reduced to five cores, it is cometitive with the best state-of-the-art methods which run on one hundred-core machines.
"
547,2018,Overcoming Catastrophic Forgetting with Hard Attention to the Task,Oral/Poster,"Catastrohic forgetting occurs when a neural network loses the information learned in a revious task after training on subsequent tasks. This roblem remains a hurdle for artificial intelligence systems with sequential learning caabilities. In this aer, we roose a task-based hard attention mechanism that reserves revious tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and revious masks are exloited to condition such learning. We show that the roosed mechanism is effective for reducing catastrohic forgetting, cutting current rates by 45 to 80%. We also show that it is robust to different hyerarameter choices, and that it offers a number of monitoring caabilities. The aroach features the ossibility to control both the stability and comactness of the learned knowledge, which we believe makes it also attractive for online learning or network comression alications.
"
548,2018,Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions,Oral/Poster,"Many existing comression aroaches have been focused and evaluated on convolutional neural networks (CNNs) where fully-connected layers contain the most arameters (e.g., LeNet and AlexNet). However, the current trend of ushing CNNs deeer with convolutions has created a ressing demand to achieve higher comression gains on CNNs where convolutions dominate the arameter amount (e.g., GoogLeNet, ResNet and Wide ResNet). Further, convolutional layers always account for most energy consumtion in run time. To this end, this aer investigates the relatively less-exlored direction of comressing convolutional layers in dee CNNs. We introduce a novel sectrally relaxed k -means regularization, that tends to aroximately make hard assignments of convolutional layer weights to K learned cluster centers during re-training. Comression is then achieved through weight-sharing, by only recording K cluster centers and weight assignment indexes. Our roosed ieline, termed Dee k -Means, has well-aligned goals between re-training and comression stages. We further roose an imroved set of metrics to estimate energy consumtion of CNN hardware imlementations, whose estimation results are verified to be consistent with reviously roosed energy estimation tool extraolated from actual hardware measurements. We have evaluated Dee k -Means in comressing several CNN models in terms of both comression ratio and energy consumtion reduction, observing romising results without incurring accuracy loss.
"
549,2018,Efficient Neural Audio Synthesis,Oral/Poster,"Sequential models achieve state-of-the-art results in audio, visual and textual domains with resect to both estimating the data distribution and generating desired samles. Efficient samling for this class of models at the cost of little to no loss in quality has however remained an elusive roblem. With a focus on text-to-seech synthesis, we describe a set of general techniques for reducing samling time while maintaining high outut quality. We first describe a single-layer recurrent neural network, the WaveRNN, with a  dual softmax layer that matches the quality of the state-of-the-art WaveNet model. The comact form of the network makes it ossible to generate 24 kHz 16-bit audio 4 times faster than real time on a GPU.  Secondly, we aly a weight runing technique to reduce the number of weights in the WaveRNN. We find that, for a constant number of arameters, large sarse networks erform better than small dense networks and this relationshi holds ast sarsity levels of more than 96%. The small number of weights in a Sarse WaveRNN makes it ossible to samle high-fidelity audio on a mobile hone CPU in real time. Finally, we describe a new deendency scheme for samling that lets us trade a constant number of non-local, distant deendencies for the ability to generate samles in batches. The Batch WaveRNN roduces 8 samles er ste without loss of quality and offers orthogonal ways of further increasing samling efficiency.
"
550,2018,Born Again Neural Networks,Oral/Poster,"Knowledge Distillation (KD) consists of  transferring ``knowledge'' from one machine learning model (the teacher) to another (the student). Commonly, the teacher is a high-caacity model with formidable erformance, while the student is more comact. By transferring knowledge, one hoes to benefit from the student's comactness, without sacrificing too much erformance. We study KD from a new ersective: rather than comressing models, we train students arameterized identically to their teachers. Surrisingly, these Born-Again Networks (BANs), outerform their teachers significantly, both on comuter vision and language modeling tasks. Our exeriments with BANs based on DenseNets demonstrate state-of-the-art erformance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional exeriments exlore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential comonents  of KD, demonstrating the effect of the teacher oututs on both redicted and non-redicted classes.
"
551,2018,Adaptive Sampled Softmax with Kernel Based Sampling,Oral/Poster,"Softmax is the most commonly used outut function for multiclass roblems and is widely used in areas such as vision, natural language rocessing, and recommendation. A softmax model has linear costs in the number of classes which makes it too exensive for many real-world roblems. A common aroach to seed u training involves samling only some of the classes at each training ste. It is known that this method is biased and that the bias increases the more the samling distribution deviates from the outut distribution. Nevertheless, almost all recent work uses simle samling distributions that require a large samle size to mitigate the bias. In this work, we roose a new class of kernel based samling methods and develo an efficient samling algorithm. Kernel based samling adats to the model as it is trained, thus resulting in low bias. It can also be easily alied to many models because it relies only on the model's last hidden layer. We emirically study the trade-off of bias, samling distribution and samle size and show that kernel based samling results in low bias with few samles.
"
552,2018,JointGAN: Multi-Domain Joint Distribution Learning with Generative Adversarial Nets,Oral/Poster,"A new generative adversarial network is develoed for joint distribution matching.Distinct from most existing aroaches, that only learn conditional distributions, the roosed model aims to learn a joint distribution of multile random variables (domains). This is achieved by learning to samle from conditional distributions between the domains, while simultaneously learning to samle from the marginals of each individual domain.The roosed framework consists of multile generators and a single softmax-based critic, all jointly trained via adversarial learning.From a simle noise source, the roosed framework allows synthesis of draws from the marginals, conditional draws given observations from a subset of random variables, or comlete draws from the full joint distribution. Most examles considered are for joint analysis of two domains, with examles for three domains also resented.
"
553,2018,Autoregressive Quantile Networks for Generative Modeling,Oral/Poster,"We introduce autoregressive imlicit quantile networks (AIQN), a fundamentally different aroach to generative modeling than those commonly used, that imlicitly catures the distribution using quantile regression. AIQN is able to achieve suerior ercetual quality and imrovements in evaluation metrics, without incurring a loss of samle diversity. The method can be alied to many existing models and architectures. In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Incetion scores, FID, non-cherry-icked samles, and inainting results. We consistently observe that AIQN yields a highly stable algorithm that imroves ercetual quality while maintaining a highly diverse distribution.
"
554,2018,On the Power of Over-parametrization in Neural Networks with Quadratic Activation,Oral/Poster,"We rovide new theoretical insights on why over-arametrization is effective in learning neural networks. For a $k$ hidden node shallow network with quadratic activation and $n$ training  data oints, we show as long as $ k \ge \sqrt{2n}$, over-arametrization enables local search algorithms to find a  \emh{globally} otimal solution for general smooth and convex loss functions. Further, desite that the number of arameters may exceed the samle size, using theory of Rademacher comlexity, we show with weight decay, the solution also generalizes well if the data is samled from a regular distribution such as Gaussian. To rove when $k\ge \sqrt{2n}$, the loss function has benign landscae roerties, we adot an idea from smoothed analysis, which may have other alications in studying loss surfaces of neural networks."
555,2018,On the Limitations of First-Order Approximation in GAN Dynamics,Oral/Poster,"While Generative Adversarial Networks (GANs) have demonstrated romising erformance on multile vision tasks, their learning dynamics are not yet well understood, both in theory and in ractice. To address this issue, we study GAN dynamics in a simle yet rich arametric model that exhibits several of the common roblematic convergence behaviors such as vanishing gradients, mode collase, and diverging or oscillatory behavior. In site of the non-convex nature of our model, we are able to erform a rigorous theoretical analysis of its convergence behavior. Our analysis reveals an interesting dichotomy: a GAN with an otimal discriminator rovably converges, while first order aroximations of the discriminator stes lead to unstable GAN dynamics and mode collase. Our result suggests that using first order discriminator stes (the de-facto standard in most existing GAN setus) might be one of the factors that makes GAN training challenging in ractice.
"
556,2018,Learning to Explore via Meta-Policy Gradient,Oral/Poster,"The erformance of off-olicy learning, including dee Q-learning and dee deterministic olicy gradient (DDPG), critically deends on the choice of the exloration olicy. Existing exloration methods are mostly based on adding noise to the on-going actor olicy and can only exlore \emh{local} regions close to what the actor olicy dictates.  In this work, we develo a simle meta-olicy gradient algorithm that allows us to adatively learn the exloration olicy in DDPG. Our algorithm allows us to train flexible exloration behaviors that are indeendent of the actor olicy, yielding a \emh{global exloration} that significantly seeds u the learning rocess. With an extensive study, we show that our method significantly imroves the samle-efficiency of DDPG on a variety of reinforcement learning continuous control tasks.
"
557,2018,Mean Field Multi-Agent Reinforcement Learning,Oral/Poster,"Existing multi-agent reinforcement learning methods are limited tyically to a small number of agents. When the  agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exonential growth of agent interactions. In this aer, we resent Mean Field Reinforcement Learning where the interactions within the oulation of agents are aroximated by those between a single agent and the average effect from the overall oulation or neighboring agents; the interlay between the two entities is mutually reinforced: the learning of the individual agent's otimal olicy deends on the dynamics of the oulation, while the dynamics of the oulation change according to the collective atterns of the individual olicies. We develo ractical mean field Q-learning and mean field Actor-Critic algorithms and analyze the convergence of the solution to Nash equilibrium. Exeriments on Gaussian squeeze, Ising model, and battle games justify the learning effectiveness of our mean field aroaches. In addition, we reort the first result to solve the Ising model via model-free reinforcement learning methods.
"
558,2018,Online Linear Quadratic Control,Oral/Poster,"We study the roblem of controlling linear time-invariant systems with known noisy dynamics and adversarially chosen quadratic losses.  We resent the first efficient online learning algorithms in this setting that guarantee $O(\sqrt{T})$ regret under mild assumtions, where $T$ is the time horizon.  Our algorithms rely on a novel SDP relaxation for the steady-state distribution of the system.  Crucially, and in contrast to reviously roosed relaxations, the feasible solutions of our SDP all corresond to ``strongly stable'' olicies that mix exonentially fast to a steady state."
559,2018,Online Learning with Abstention,Oral/Poster,"We resent an extensive study of a key roblem in online learning where the learner can ot to abstain from making a rediction, at a certain cost. In the adversarial setting, we show how existing online algorithms and guarantees can be adated to this roblem. In the stochastic setting, we first oint out a bias roblem that limits the straightforward extension of algorithms such as UCB-N to this context. Next, we give a new algorithm, UCB-GT, that exloits historical data and time-varying feedback grahs. We show that this algorithm benefits from more favorable regret guarantees than a natural extension of UCB-N . We further reort the results of a series of exeriments demonstrating that UCB-GT largely outerforms that extension of UCB-N, as well as other standard baselines.
"
560,2018,Celer: a Fast Solver for the Lasso with Dual Extrapolation,Oral/Poster,"Convex sarsity-inducing regularizations are ubiquitous in high-dimensional machine learning, but solving the resulting otimization roblems can be slow. To accelerate solvers, state-of-the-art aroaches consist in reducing the size of the otimization roblem at hand. In the context of regression, this can be achieved either by discarding irrelevant features (screening techniques) or by rioritizing features likely to be included in the suort of the solution (working set techniques). Duality comes into lay at several stes in these techniques. Here, we roose an extraolation technique starting from a sequence of iterates in the dual that leads to the construction of imroved dual oints. This enables a tighter control of otimality as used in stoing criterion, as well as better screening erformance of Ga Safe rules. Finally, we roose a working set strategy based on an aggressive use of Ga Safe screening rules. Thanks to our new dual oint construction, we show significant comutational seedus on multile real-world roblems.
"
561,2018,Cut-Pursuit Algorithm for Regularizing Nonsmooth Functionals with Graph Total Variation,Oral/Poster,"We resent an extension of the cut-ursuit algorithm, introduced by Landrieu and Obozinski (2017), to the grah total-variation regularization of functions with a searable nondifferentiable art. We roose a modified algorithmic scheme as well as adated roofs of convergence. We also resent a heuristic aroach for handling the cases in which the values associated to each vertex of the grah are multidimensional. The erformance of our algorithm, which we demonstrate on difficult, ill-conditioned large-scale inverse and learning roblems, is such that it may in ractice extend the scoe of alication of the total-variation regularization.
"
562,2018,Augmented CycleGAN: Learning Many-to-Many Mappings from Unpaired Data,Oral/Poster,"Learning inter-domain maings from unaired data can imrove erformance in structured rediction tasks, such as image segmentation, by reducing the need for aired data. CycleGAN was recently roosed for this roblem, but critically assumes the underlying inter-domain maing is aroximately deterministic and one-to-one. This assumtion renders the model ineffective for tasks requiring flexible, many-to-many maings. We roose a new model, called Augmented CycleGAN, which learns many-to-many maings between domains. We examine Augmented CycleGAN qualitatively and quantitatively on several image datasets.
"
563,2018,Mixed batches and symmetric discriminators for GAN training,Oral/Poster,"Generative adversarial networks (GANs) are ow- erful generative models based on roviding feed- back to a generative network via a discriminator network. However, the discriminator usually as- sesses individual samles. This revents the dis- criminator from accessing global distributional statistics of generated samles, and often leads to mode droing: the generator models only art of the target distribution. We roose to feed the discriminator with mixed batches of true and fake samles, and train it to redict the ratio of true samles in the batch. The latter score does not deend on the order of samles in a batch. Rather than learning this invariance, we introduce a generic ermutation-invariant discriminator ar- chitecture. This architecture is rovably a uni- versal aroximator of all symmetric functions. Exerimentally, our aroach reduces mode col- lase in GANs on two synthetic datasets, and obtains good results on the CIFAR10 and CelebA datasets, both qualitatively and quantitatively.
"
564,2018,An Algorithmic Framework of Variable Metric Over-Relaxed Hybrid Proximal Extra-Gradient Method,Oral/Poster,"We roose a novel algorithmic framework of Variable Metric Over-Relaxed  Hybrid Proximal Extra-gradient (VMOR-HPE) method with a global convergence guarantee for the maximal monotone oerator inclusion roblem. Its iteration comlexities and local linear convergence rate are rovided, which theoretically demonstrate that a large over-relaxed ste-size contributes to accelerating the roosed VMOR-HPE as a byroduct. Secifically, we find that a large class of rimal and rimal-dual oerator slitting algorithms are all secial cases of VMOR-HPE. Hence, the roosed framework offers a new insight into these oerator slitting algorithms. In addition, we aly VMOR-HPE to the Karush-Kuhn-Tucker (KKT) generalized equation of linear equality constrained multi-block comosite convex otimization, yielding a new algorithm, namely nonsymmetric Proximal Alternating Direction Method of Multiliers with a reconditioned Extra-gradient ste in which the reconditioned metric is generated by a blockwise Barzilai-Borwein line search technique (PADMM-EBB). We also establish iteration comlexities of PADMM-EBB in terms of the KKT residual. Finally, we aly PADMM-EBB to handle the nonnegative dual grah regularized low-rank reresentation roblem. Promising results on synthetic and real datasets corroborate the efficacy of PADMM-EBB.
"
565,2018,Learning Hidden Markov Models from Pairwise Co-occurrences with Application to Topic Modeling,Oral/Poster,"We resent a new algorithm for identifying the transition and emission robabilities of a hidden Markov model (HMM) from the emitted data. Exectation-maximization becomes comutationally rohibitive for long observation records, which are often required for identification. The new algorithm is articularly suitable for cases where the available samle size is large enough to accurately estimate second-order outut robabilities, but not higher-order ones. We show that if one is only able to obtain a reliable estimate of the airwise co-occurrence robabilities of the emissions, it is still ossible to uniquely identify the HMM if the emission robability is \emh{sufficiently scattered}. We aly our method to hidden toic Markov modeling, and demonstrate that we can learn toics with higher quality if documents are modeled as observations of HMMs sharing the same emission (toic) robability, comared to the simle but widely used bag-of-words model.
"
566,2018,DRACO: Byzantine-resilient Distributed Training via Redundant Gradients,Oral/Poster,"Distributed model training is vulnerable to byzantine system failures and adversarial comute nodes, i.e., nodes that use malicious udates to corrut the global model stored at a arameter server (PS). To guarantee some form of robustness, recent work suggests using variants of the geometric median as an aggregation rule, in lace of gradient averaging. Unfortunately, median-based rules can incur a rohibitive comutational overhead in large-scale settings, and their convergence guarantees often require strong assumtions. In this work, we resent DRACO, a scalable framework for robust distributed training that uses ideas from coding theory. In DRACO, each comute node evaluates redundant gradients that are used by the arameter server to eliminate the effects of adversarial udates. DRACO comes with roblem-indeendent robustness guarantees, and the model that it trains is identical to the one trained in the adversary-free setu. We rovide extensive exeriments on real datasets and distributed setus across a variety of large-scale models, where we show that DRACO is several times, to orders of magnitude faster than median-based aroaches.
"
567,2018,Communication-Computation Efficient Gradient Coding,Oral/Poster,"This aer develos coding techniques to reduce the running time of distributed learning tasks. It characterizes the fundamental tradeoff to comute gradients in terms of three arameters: comutation load, straggler tolerance and communication cost. It further gives an exlicit coding scheme that achieves the otimal tradeoff based on recursive olynomial constructions, coding both across data subsets and vector comonents. As a result, the roosed scheme allows to minimize the running time for gradient comutations. Imlementations are made on Amazon EC2 clusters using Python with mi4y ackage. Results show that the roosed scheme maintains the same generalization error while reducing the running time by $32\%$ comared to uncoded schemes and $23\%$ comared to rior coded schemes focusing only on stragglers (Tandon et al., ICML 2017)."
568,2018,"Submodular Hypergraphs: p-Laplacians, Cheeger Inequalities and Spectral Clustering",Oral/Poster,"We introduce submodular hyergrahs, a family of hyergrahs that have different submodular weights associated with different cuts of hyeredges. Submodular hyergrahs arise in cluster- ing alications in which higher-order structures carry relevant information. For such hyergrahs, we define the notion of -Lalacians and derive corresonding nodal domain theorems and k-way Cheeger inequalities. We conclude with the descrition of algorithms for comuting the sectra of 1- and 2-Lalacians that constitute the basis of new sectral hyergrah clustering methods.
"
569,2018,SMAC: Simultaneous Mapping and Clustering Using Spectral Decompositions,Oral/Poster,"We introduce a rinciled aroach for \emh{simultaneous maing and clustering} (SMAC) for establishing consistent mas across heterogeneous object collections (e.g., 2D images or 3D shaes). Our aroach takes as inut a heterogeneous object collection and a set of mas comuted between some airs of objects, and oututs a homogeneous object clustering together with a new set of mas ossessing otimal intra- and inter-cluster consistency.  Our aroach is based on the sectral decomosition of a data matrix storing all airwise mas in its blocks. We additionally rovide tight theoretical guarantees on the exactness of SMAC under established noise models. We also demonstrate the usefulness of the aroach on synthetic and real datasets.
"
570,2018,On Nesting Monte Carlo Estimators,Oral/Poster,"Many roblems in machine learning and statistics involve nested exectations and thus do not ermit conventional Monte Carlo (MC) estimation. For such roblems, one must nest estimators, such that terms in an outer estimator themselves involve calculation of a searate, nested, estimation. We investigate the statistical imlications of nesting MC estimators, including cases of multile levels of nesting, and establish the conditions under which they converge. We derive corresonding rates of convergence and rovide emirical evidence that these rates are observed in ractice. We further establish a number of itfalls that can arise from naive nesting of MC estimators, rovide guidelines about how these can be avoided, and lay out novel methods for reformulating certain classes of nested exectation roblems into single exectations, leading to imroved convergence rates. We demonstrate the alicability of our work by using our results to develo a new estimator for discrete Bayesian exerimental design roblems and derive error bounds for a class of variational objectives.
"
571,2018,Stein Variational Gradient Descent Without Gradient,Oral/Poster,"Stein variational gradient decent (SVGD) has been shown to be a owerful aroximate inference algorithm for comlex distributions. However, the standard SVGD requires calculating the gradient of the target density and cannot be alied when the gradient is unavailable. In this work, we develo a gradient-free variant of SVGD (GF-SVGD), which relaces the true gradient with a surrogate gradient, and corrects the introduced bias by re-weighting the gradients in a roer form.  We show that our GF-SVGD can be viewed as the standard SVGD with a secial choice of kernel, and hence directly inherits all the theoretical roerties of SVGD. We shed insights on the emirical choice of the surrogate gradient and further, roose an annealed GF-SVGD that consistently outerforms a number of recent advanced gradient-free MCMC methods in our emirical studies.
"
572,2018,Detecting non-causal artifacts in multivariate linear regression models,Oral/Poster,"We consider linear models where d otential causes Xem1,...,Xemd are correlated with one target quantity Y and roose a method to infer whether the association is causal or whether  it is an artifact caused by overfitting or hidden common causes. We emloy the idea that in the former case the vector of regression coefficients has `generic' orientation relative to the covariance matrix Sigmaem{XX}  of X. Using an ICA based model for confounding, we show that both confounding and overfitting yield regression vectors  that concentrate mainly in the sace of low eigenvalues of Sigmaem{XX}.
"
573,2018,The Hierarchical Adaptive Forgetting Variational Filter,Oral/Poster,"A common roblem in Machine Learning and statistics consists in detecting whether the current samle in a stream of data belongs to the same distribution as revious ones, is an isolated outlier or inaugurates a new distribution of data. We resent a hierarchical Bayesian algorithm that aims at learning a time-secific aroximate osterior distribution of the arameters describing the distribution of the data observed. We derive the udate equations of the variational arameters of the aroximate osterior at each time ste for models from the exonential family, and show that these udates find interesting corresondents in Reinforcement Learning (RL). In this ersective, our model can be seen as a hierarchical RL algorithm that learns a osterior distribution according to a certain stability confidence that is, in turn, learned according to its own stability confidence. Finally, we show some alications of our generic model, first in a RL context, next with an adative Bayesian Autoregressive model, and finally in the context of Stochastic Gradient Descent otimization.
"
574,2018,Junction Tree Variational Autoencoder for Molecular Graph Generation,Oral/Poster,"We seek to automate the design of molecules based on secific chemical roerties. In comutational terms, this task involves continuous embedding and generation of molecular grahs. Our rimary contribution is the direct realization of molecular grahs, a task reviously aroached by generating linear SMILES strings instead of grahs. Our junction tree variational autoencoder generates molecular grahs in two hases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a grah message assing network. This aroach allows us to incrementally exand molecules while maintaining chemical validity at every ste. We evaluate our model on multile tasks ranging from molecular generation to otimization. Across these tasks, our model outerforms revious state-of-the-art baselines by a significant margin.
"
575,2018,Semi-Amortized Variational Autoencoders,Oral/Poster,"Amortized variational inference (AVI) relaces instance-secific local inference with a global inference network. While AVI has enabled efficient training of dee generative models such as variational autoencoders (VAE), recent emirical work suggests that inference networks can roduce subotimal variational arameters. We roose a hybrid aroach, to use AVI to initialize the variational arameters and run stochastic variational inference (SVI) to refine them. Crucially, the local SVI rocedure is itself differentiable, so the inference network and generative model can be trained end-to-end with gradient-based otimization. This semi-amortized aroach enables the use of rich generative models without exeriencing the osterior-collase henomenon common in training VAEs for roblems like text generation. Exeriments show this aroach outerforms strong autoregressive and variational baselines on standard text and image datasets.
"
576,2018,Adaptive Exploration-Exploitation Tradeoff for Opportunistic Bandits,Oral/Poster,"In this aer, we roose and study oortunistic bandits - a new variant of bandits where the regret of ulling a subotimal arm varies under different environmental conditions, such as network load or roduce rice. When the loadrice is low, so is the costregret of ulling a subotimal arm (e.g., trying a subotimal network configuration). Therefore, intuitively, we could exlore more when the loadrice is low and exloit more when the loadrice is high. Insired by this intuition, we roose  an Adative Uer-Confidence-Bound (AdaUCB) algorithm to adatively balance the exloration-exloitation tradeoff for oortunistic bandits. We rove that AdaUCB achieves O(log T) regret with a smaller coefficient than the traditional UCB algorithm. Furthermore, AdaUCB achieves O(1) regret with resect to T if the exloration cost is zero when the load level is below a certain threshold. Last, based on both synthetic data and real-world traces, exerimental results show that AdaUCB significantly outerforms other bandit algorithms, such as UCB and TS (Thomson Samling), under large loadrice fluctuations.
"
577,2018,Semiparametric Contextual Bandits,Oral/Poster,"This aer studies semiarametric contextual bandits, a generalization of the linear stochastic bandit roblem where the reward for a chosen action is modeled as a linear function of known action features confounded by a non-linear action-indeendent term. We design new algorithms that achieve $\tilde{O}(d\sqrt{T})$ regret over $T$ rounds, when the linear function is $d$-dimensional, which matches the best known bounds for the simler unconfounded case and imroves on a recent result of Greenwald et al. (2017). Via an emirical evaluation, we show that our algorithms outerform rior aroaches when there are non-linear confounding effects on the rewards. Technically, our algorithms use a new reward estimator insired by doubly-robust aroaches and our roofs require new concentration inequalities for self-normalized martingales."
578,2018,Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV),Oral/Poster,"The interretation of dee learning models is a challenge due to their size, comlexity, and often oaque internal state. In addition, many systems, such as image classifiers, oerate on low-level features rather than high-level concets. To address these challenges, we introduce Concet Activation Vectors (CAVs), which rovide an interretation of a neural net's internal state in terms of human-friendly concets. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as art of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concet is imortant to a classification result--for examle, how sensitive a rediction of “zebra” is to the resence of stries. Using the domain of image classification as a testing ground, we describe how CAVs may be used to exlore hyotheses and generate insights for a standard image classification network as well as a medical alication.
"
579,2018,Weightless: Lossy weight encoding for deep neural network compression,Oral/Poster,"The large memory requirements of dee neural networks limit their deloyment and adotion on many devices. Model comression methods effectively reduce the memory requirements of these models, usually through alying transformations such as weight runing or quantization. In this aer, we resent a novel scheme for lossy weight encoding co-designed with weight simlification techniques. The encoding is based on the Bloomier filter, a robabilistic data structure that can save sace at the cost of introducing random errors. Leveraging the ability of neural networks to tolerate these imerfections and by re-training around the errors, the roosed technique, named Weightless, can comress weights by u to 496x without loss of model accuracy. This results in u to a 1.51x imrovement over the state-of-the-art.
"
580,2018,Parallel Bayesian Network Structure Learning,Oral/Poster,"Recent advances in Bayesian Network (BN) structure learning have focused on local-to-global learning, where the grah structure is learned via one local subgrah at a time. As a natural rogression, we investigate arallel learning of BN structures via multile learning agents simultaneously, where each agent learns one local subgrah at a time. We find that arallel learning can reduce the number of subgrahs requiring structure learning by storing reviously queried results and communicating (even artial) results among agents. More secifically, by using novel rules on query subset and suerset inference, many subgrah structures can be inferred without learning. We rovide a sound and comlete arallel structure learning (PSL) algorithm, and demonstrate its imroved efficiency over state-of-the-art single-thread learning algorithms.
"
581,2018,Temporal Poisson Square Root Graphical Models,Oral/Poster,"We roose temoral Poisson square root grahical models (TPSQRs), a generalization of Poisson square root grahical models (PSQRs) secifically designed for modeling longitudinal event data. By estimating the temoral relationshis for all ossible airs of event tyes, TPSQRs can offer a holistic ersective about whether the occurrences of any given event tye could excite or inhibit any other tye. A TPSQR is learned by estimating a collection of interrelated PSQRs that share the same temlate arameterization. These PSQRs are estimated jointly in a seudo-likelihood fashion, where Poisson seudo-likelihood is used to aroximate the original more comutationally intensive seudo-likelihood roblem stemming from PSQRs. Theoretically, we demonstrate that under mild assumtions, the Poisson seudolikelihood aroximation is sarsistent for recovering the underlying PSQR. Emirically, we learn TPSQRs from a real-world large-scale electronic health record (EHR) with millions of drug rescrition and condition diagnosis events, for adverse drug reaction (ADR) detection. Exerimental results demonstrate that the learned TPSQRs can recover ADR signals from the EHR effectively and efficiently.
"
582,2018,Minimax Concave Penalized Multi-Armed Bandit Model with High-Dimensional Covariates,Oral/Poster,"In this aer, we roose a Minimax Concave Penalized Multi-Armed Bandit (MCP-Bandit) algorithm for a decision-maker facing high-dimensional data with latent sarse structure in an online learning and decision-making rocess. We demonstrate that the MCP-Bandit algorithm asymtotically achieves the otimal cumulative regret in samle size T, O(log T), and further attains a tighter bound in both covariates dimension d and the number of significant covariates s, O(s^2 (s + log d).  In addition, we develo a linear aroximation method, the 2-ste Weighted Lasso rocedure, to identify the MCP estimator for the MCP-Bandit algorithm under non-i.i.d. samles. Using this rocedure, the MCP estimator matches the oracle estimator with high robability. Finally, we resent two exeriments to benchmark our roosed the MCP-Bandit algorithm to other bandit algorithms. Both exeriments demonstrate that the MCP-Bandit algorithm erforms favorably over other benchmark algorithms, esecially when there is a high level of data sarsity or when the samle size is not too small.
"
583,2018,Dynamic Regret of Strongly Adaptive Methods,Oral/Poster,"To coe with changing environments, recent develoments in online learning have introduced the concets of adative regret and dynamic regret indeendently. In this aer, we illustrate an intrinsic connection between these two concets by showing that the dynamic regret can be exressed in terms of the adative regret and the functional variation. This observation imlies that strongly adative algorithms can be directly leveraged to minimize the dynamic regret. As a result, we resent a series of strongly adative algorithms that have small dynamic regrets for convex functions, exonentially concave functions, and strongly convex functions, resectively. To the best of our knowledge, this is the first time that exonential concavity is utilized to uer bound the dynamic regret. Moreover, all of those adative algorithms do not need any rior knowledge of the functional variation, which is a significant advantage over revious secialized methods for minimizing dynamic regret.
"
584,2018,Distributed Clustering via LSH Based Data Partitioning,Oral/Poster,"Given the imortance of clustering in the analysisof large scale data, distributed algorithms for formulations such as k-means, k-median, etc. have been extensively studied. A successful aroach here has been the “reduce and merge” aradigm, in which each machine reduces its inut size to Õ(k), and this data reduction continues (ossibly iteratively) until all the data fits on one machine, at which oint the roblem is solved locally. This aroach has the intrinsic bottleneck that each machine must solve a roblem of size ≥ k, and needs to communicate at least Ω(k) oints to the other machines. We roose a novel data artitioning idea to overcome this bottleneck, and in effect, have different machines focus on “finding different clusters”. Under the assumtion that we know the otimum value of the objective u to a oly(n) factor (arbitrary olynomial), we establish worst-case aroximation guarantees for our method. We see that our algorithm results in lower communication as well as a near-otimal number of ‘rounds’ of comutation (in the oular MaReduce framework).
"
585,2018,Learning to Branch,Oral/Poster,"Tree search algorithms, such as branch-and-bound, are the most widely used tools for solving combinatorial roblems. These algorithms recursively artition the search sace to find an otimal solution. To kee the tree small, it is crucial to carefully decide, when exanding a tree node, which variable to branch on at that node to artition the remaining sace. Many artitioning techniques have been roosed, but no theory describes which is otimal. We show how to use machine learning to determine an otimal weighting of any set of artitioning rocedures for the instance distribution at hand using samles. Via theory and exeriments, we show that learning to branch is both ractical and hugely beneficial.
"
586,2018,Minibatch Gibbs Sampling on Large Graphical Models,Oral/Poster,"Gibbs samling is the de facto Markov chain Monte Carlo method used for inference and learning on large scale grahical models. For comlicated factor grahs with lots of factors, the erformance of Gibbs samling can be limited by the comutational cost of executing a single udate ste of the Markov chain. This cost is roortional to the degree of the grah, the number of factors adjacent to each variable. In this aer, we show how this cost can be reduced by using minibatching: subsamling the factors to form an estimate of their sum. We introduce several minibatched variants of Gibbs, show that they can be made unbiased, rove bounds on their convergence rates, and show that under some conditions they can result in asymtotic single-udate-run-time seedus over lain Gibbs samling.
"
587,2018,On the Theory of Variance Reduction for Stochastic Gradient Monte Carlo,Oral/Poster,"We rovide convergence guarantees in Wasserstein distance for a variety of variance-reduction methods: SAGA Langevin diffusion, SVRG Langevin diffusion and control-variate underdamed Langevin diffusion. We analyze these methods under a uniform set of assumtions on the log-osterior distribution, assuming it to be smooth, strongly convex and Hessian Lischitz. This is achieved by a new roof technique combining ideas from finite-sum otimization and the analysis of samling methods. Our shar theoretical bounds allow us to identify regimes of interest where each method erforms better than the others. Our theory is verified with exeriments on real-world and synthetic datasets.
"
588,2018,Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning,Oral/Poster,"In this aer we roose Reward Machines — a tye of finite state machine that suorts the secification of reward functions while exosing reward function structure to the learner and suorting decomosition. We then resent Q-Learning for Reward Machines (QRM), an algorithm which aroriately decomoses the reward machine and uses off-olicy q-learning to simultaneously learn subolicies for the different comonents. QRM is guaranteed to converge to an otimal olicy in the tabular case, in contrast to Hierarchical Reinforcement Learning methods which might converge to subotimal olicies. We demonstrate this behavior exerimentally in two discrete domains. We also show how function aroximation methods like neural networks can be incororated into QRM, and that doing so can find better olicies more quickly than hierarchical methods in a domain with a continuous state sace.
"
589,2018,Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks,Oral/Poster,"Humans can understand and roduce new utterances effortlessly, thanks to their comositional skills. Once a erson learns the meaning of a new verb ""dax,"" he or she can immediately understand the meaning of ""dax twice"" or ""sing and dax."" In this aer, we introduce the SCAN domain, consisting of a set of simle comositional navigation commands aired with the corresonding action sequences. We then test the zero-shot generalization caabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can aly ""mix-and-match"" strategies to solve the task. However, when generalization requires systematic comositional skills (as in the ""dax"" examle above), RNNs fail sectacularly. We conclude with a roof-of-concet exeriment in neural machine translation, suggesting that lack of systematicity might be artially resonsible for neural networks' notorious training data thirst.
"
590,2018,Pathwise Derivatives Beyond the Reparameterization Trick,Oral/Poster,"We observe that gradients comuted via the rearameterization trick are in direct corresondence with solutions of the transort equation in the formalism of otimal transort. We use this ersective to comute (aroximate) athwise gradients for robability distributions not directly amenable to the rearameterization trick: Gamma, Beta, and Dirichlet. We further observe that when the rearameterization trick is alied to the Cholesky-factorized multivariate Normal distribution, the resulting gradients are subotimal in the sense of otimal transort. We derive the otimal gradients and show that they have reduced variance in a Gaussian Process regression task. We demonstrate with a variety of synthetic exeriments and stochastic variational inference tasks that our athwise gradients are cometitive with other methods.
"
591,2018,Message Passing Stein Variational Gradient Descent,Oral/Poster,"Stein variational gradient descent (SVGD) is a recently roosed article-based Bayesian inference method, which has attracted a lot of interest due to its remarkable aroximation ability and article efficiency comared to traditional variational inference and Markov Chain Monte Carlo methods. However, we observed that articles of SVGD tend to collase to modes of the target distribution, and this article degeneracy henomenon becomes more severe with higher dimensions. Our theoretical analysis finds out that there exists a negative correlation between the dimensionality and the reulsive force of SVGD which should be blamed for this henomenon. We roose Message Passing SVGD (MP-SVGD) to solve this roblem. By leveraging the conditional indeendence structure of robabilistic grahical models (PGMs), MP-SVGD converts the original high-dimensional global inference roblem into a set of local ones over the Markov blanket with lower dimensions. Exerimental results show its advantages of reventing vanishing reulsive force in high-dimensional sace over SVGD, and its article efficiency and aroximation flexibility over other inference methods on grahical models.
"
592,2018,State Space Gaussian Processes with Non-Gaussian Likelihood,Oral/Poster,"We rovide a comrehensive overview and tooling for GP modelling with non-Gaussian likelihoods using state sace methods. The state sace formulation allows for solving one-dimensonal GP models in O(n) time and memory comlexity. While existing literature has focused on the connection between GP regression and state sace methods, the comutational rimitives allowing for inference using general likelihoods in combination with the Lalace aroximation (LA), variational Bayes (VB), and assumed density filtering (ADF)  exectation roagation (EP) schemes has been largely overlooked. We resent means of combining the efficient O(n) state sace methodology with existing inference methods. We also furher extend existing methods, and rovide unifying code imlementing all aroaches.
"
593,2018,Constant-Time Predictive Distributions for Gaussian Processes,Oral/Poster,"One of the most comelling features of Gaussian rocess (GP) regression is its ability to rovide well-calibrated osterior distributions. Recent advances in inducing oint methods have sed u GP marginal likelihood and osterior mean comutations, leaving osterior covariance estimation and samling as the remaining comutational bottlenecks. In this aer we address these shortcomings by using the Lanczos algorithm to raidly aroximate the redictive covariance matrix. Our aroach, which we refer to as LOVE (LanczOs Variance Estimates), substantially imroves time and sace comlexity. In our exeriments, LOVE comutes covariances u to 2,000 times faster and draws samles 18,000 times faster than existing methods, all without sacrificing accuracy.
"
594,2018,Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks,Oral/Poster,"We analyze algorithms for aroximating a function $f(x) = \Phi x$ maing $\Re^d$ to $\Re^d$ using dee linear neural networks, i.e.\ that learn a function $h$ arameterized by matrices $\Theta_1,...,\Theta_L$ and defined by $h(x) = \Theta_L \Theta_{L-1} ... \Theta_1 x$.  We focus on algorithms that learn through gradient descent on the oulation quadratic loss in the case that the distribution over the inuts is isotroic.   We rovide olynomial bounds on the number of iterations for gradient descent to aroximate the least squares matrix $\Phi$, in the case where the initial hyothesis $\Theta_1 = ... = \Theta_L = I$  has excess loss bounded by a small enough constant.  On the other hand, we show that gradient descent fails to converge for $\Phi$ whose distance from the identity is a larger constant, and  we show that some forms of regularization toward the identity in each layer do not hel.   If $\Phi$ is symmetric ositive definite, we show that an algorithm that initializes $\Theta_i = I$ learns an $\esilon$-aroximation of $f$  using a number of udates olynomial in $L$, the condition number of $\Phi$, and $\log(d\esilon)$.  In contrast, we show that if the least squares matrix $\Phi$ is symmetric and has a negative eigenvalue, then all members of a class of algorithms that erform gradient descent with identity initialization, and otionally regularize toward the identity in each layer, fail to converge.   We analyze an algorithm for the case that $\Phi$ satisfies $u^{\to} \Phi u  0$ for all $u$, but may not be symmetric.  This algorithm uses two regularizers: one that maintains the invariant $u^{\to} \Theta_L \Theta_{L-1} ... \Theta_1 u  0$ for all $u$, and another that ``balances'' $\Theta_1, ..., \Theta_L$  so that they have the same singular values."
595,2018,On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups,Oral/Poster,"Convolutional neural networks have been extremely successful in the image recognition domain because they ensure equivariance with resect to translations. There have been many recent attemts to generalize this framework to other domains, including grahs and data lying on manifolds. In this aer we give a rigorous, theoretical treatment of convolution and equivariance in neural networks with resect to not just translations, but the action of any comact grou. Our main result is to rove that (given some natural constraints) convolutional structure is not just a sufficient, but also a necessary condition for equivariance to the action of a comact grou. Our exosition makes use of concets from reresentation theory and noncommutative harmonic analysis and derives new generalized convolution formulae.
"
596,2018,Racing Thompson: an Efficient Algorithm for Thompson Sampling with Non-conjugate Priors,Oral/Poster,"Thomson samling has imressive emirical erformance for many multi-armed bandit roblems. But current algorithms for Thomson samling only work for the case of conjugate riors since they require to erform online Bayesian osterior inference, which is a difficult task when the rior is not conjugate. In this aer, we roose a novel algorithm for Thomson samling which only requires to draw samles from a tractable roosal distribution. So our algorithm is efficient even when the rior is non-conjugate. To do this, we reformulate Thomson samling as an otimization rolem via the Gumbel-Max trick. After that we construct a set of random variables and our goal is to identify the one with highest mean which is an instance of best arm identification roblems. Finally, we solve it with techniques in best arm identification. Exeriments show that our algorithm works well in ractice.
"
597,2018,Probably Approximately Metric-Fair Learning,Oral/Poster,"The seminal work of Dwork {\em et al.} [ITCS 2012] introduced a metric-based notion of individual fairness: given a task-secific similarity metric, their notion required that every air of similar individuals should be treated similarly. In the context of machine learning, however, individual fairness does not generalize from a training set to the underlying oulation. We show that this can lead to comutational intractability even for simle fair-learning tasks. With this motivation in mind, we introduce and study a relaxed notion of {\em aroximate metric-fairness}: for a random air of individuals samled from the oulation, with all but a small robability of error, if they are similar then they should be treated similarly.  We formalize the goal of achieving aroximate metric-fairness simultaneously with best-ossible accuracy as Probably Aroximately Correct and Fair (PACF) Learning. We show that aroximate metric-fairness {\em does} generalize, and leverage these generalization guarantees to construct olynomial-time PACF learning algorithms for the classes of linear and logistic redictors.
"
598,2018,Neural Program Synthesis from Diverse Demonstration Videos,Oral/Poster,"Interreting decision making logic in demonstration videos is key to collaborating with and mimicking humans. To emower machines with this ability, we roose a neural rogram synthesizer that is able to exlicitly synthesize underlying rograms from behaviorally diverse and visually comlicated demonstration videos. We introduce a summarizer module as art of our model to imrove the network’s ability to integrate multile demonstrations varying in behavior. We also emloy a multi-task objective to encourage the model to learn meaningful intermediate reresentations for end-to-end training. We show that our model is able to reliably synthesize underlying rograms as well as cature diverse behaviors exhibited in demonstrations. The code is available at htts:shaohua0116.github.iodemo2rogram.
"
599,2018,Video Prediction with Appearance and Motion Conditions,Oral/Poster,"Video rediction aims to generate realistic future frames by learning dynamic visual atterns. One fundamental challenge is to deal with future uncertainty: How should a model behave when there are multile correct, equally robable future? We roose an Aearance-Motion Conditional GAN to address this challenge. We rovide aearance and motion information as conditions that secify how the future may look like, reducing the level of uncertainty. Our model consists of a generator, two discriminators taking charge of aearance and motion athways, and a ercetual ranking module that encourages videos of similar conditions to look similar. To train our model, we develo a novel conditioning scheme that consists of different combinations of aearance and motion conditions. We evaluate our model using facial exression and human action datasets and reort favorable results comared to existing methods.
"
600,2018,CRVI: Convex Relaxation for Variational Inference,Oral/Poster,"We resent a new technique for solving non-convex variational inference otimization roblems. Variational inference is a widely used method for osterior aroximation in which the inference roblem is transformed into an otimization roblem. For most models, this otimization is highly non-convex and so hard to solve. In this aer, we introduce a new aroach to solving the variational inference otimization based on convex relaxation and semidefinite rogramming. Our theoretical results guarantee very tight relaxation bounds that get nearer to the global otimal solution than traditional coordinate ascent. We evaluate the erformance of our aroach on regression and sarse coding.
"
601,2018,Bayesian Coreset Construction via Greedy Iterative Geodesic Ascent,Oral/Poster,"Coherent uncertainty quantification is a key strength of Bayesian methods. But modern algorithms for aroximate Bayesian osterior inference often sacrifice accurate osterior uncertainty estimation in the ursuit of scalability.  This work shows that revious Bayesian coreset construction algorithms---which build a small, weighted subset of the data that aroximates the full dataset---are no excetion. We demonstrate that these algorithms scale the coreset log-likelihood subotimally, resulting in underestimated osterior uncertainty.  To address this shortcoming, we develo greedy iterative geodesic ascent (GIGA), a novel algorithm for Bayesian coreset construction that scales the coreset log-likelihood otimally.  GIGA rovides geometric decay in osterior aroximation error as a function of coreset size, and maintains the fast running time of its redecessors.  The aer concludes with validation of GIGA on both synthetic and real datasets, demonstrating that it reduces osterior aroximation error by orders of magnitude comared with revious coreset constructions.
"
602,2018,Transformation Autoregressive Networks,Oral/Poster,"The fundamental task of general density estimation $(x)$ has been of keen interest to machine learning. In this work, we attemt to systematically characterize methods for density estimation.  Broadly seaking, most of the existing methods can be categorized into either using: a) autoregressive models to estimate the conditional factors of the chain rule, $(x_{i}\, |\, x_{i-1}, \ldots)$; or b) non-linear transformations of variables of a simle base distribution. To better study the characteristics of these categories we roose multile methods for each category. For examle we roose RNN based transformations to model non-Markovian transformation of variables. Further, through a comrehensive study over both real world and synthetic data, we show for that jointly leveraging transformations of variables and autoregressive conditional models, results in a considerable imrovement in erformance. We illustrate the use of our models in outlier detection and image modeling. Finally we introduce a novel data driven framework for learning a family of distributions."
603,2018,Learning equations for extrapolation and control,Oral/Poster,"We resent an aroach to identify concise equations from data using a shallow neural network aroach. In contrast to ordinary black-box regression, this aroach allows understanding functional relations and generalizing them from observed data to unseen arts of the arameter sace. We show how to extend the class of learnable equations for a recently roosed equation learning network to include divisions, and we imrove the learning and model selection strategy to be useful for challenging real-world data. For systems governed by analytical exressions, our method can in many cases identify the true underlying equation and extraolate to unseen domains. We demonstrate its effectiveness by exeriments on a cart-endulum system, where only 2 random rollouts are required to learn the forward dynamics and successfully achieve the swing-u task.
"
604,2018,Analyzing Uncertainty in Neural Machine Translation,Oral/Poster,"Machine translation is a oular test bed for research in neural sequence-to-sequence models but desite much recent research, there is still a lack of understanding of these models. Practitioners reort erformance degradation with large beams, the under-estimation of rare words and a lack of diversity in the final translations. Our study relates some of these issues to the inherent uncertainty of the task, due to the existence of multile valid translations for a single source sentence, and to the extrinsic uncertainty caused by noisy training data. We roose tools and metrics to assess how uncertainty in the data is catured by the model distribution and how it affects search strategies that generate translations. Our results show that search works remarkably well but that the models tend to sread too much robability mass over the hyothesis sace. Next, we roose tools to assess model calibration and show how to easily fix some shortcomings of current models. We release both code and multile human reference translations for two oular benchmarks.
"
605,2018,Hierarchical Text Generation and Planning for Strategic Dialogue,Oral/Poster,"End-to-end models for goal-orientated dialogue are challenging to train, because linguistic and strategic asects are entangled in latent state vectors.  We introduce an aroach to learning reresentations of messages in dialogues by maximizing the likelihood of subsequent sentences and actions, which decoules the semantics of the dialogue utterance from its linguistic realization. We then use these latent sentence reresentations for hierarchical language generation, lanning and reinforcement learning. Exeriments show that our aroach  increases the end-task reward achieved by the model, imroves the effectiveness of long-term lanning using rollouts, and allows self-lay reinforcement learning to imrove decision making without diverging from human language. Our hierarchical latent-variable model outerforms revious work both linguistically and strategically.
"
606,2018,Budgeted Experiment Design for Causal Structure Learning,Oral/Poster,"We study the roblem of causal structure learning when the exerimenter is limited to erform at most $k$ non-adative exeriments of size $1$. We formulate the roblem of finding the best intervention target set as an otimization roblem, which aims to maximize the average number of edges whose directions are resolved. We rove that the corresonding objective function is submodular and a greedy algorithm suffices to achieve $(1-\frac{1}{e})$-aroximation of the otimal value. We further resent an accelerated variant of the greedy algorithm, which can lead to orders of magnitude erformance seedu. We validate our roosed aroach on synthetic and real grahs. The results show that comared to the urely observational setting, our algorithm orients the majority of the edges through a considerably small number of interventions."
607,2018,Accurate Inference for Adaptive Linear Models,Oral/Poster,"Estimators comuted from adatively collected data do not behave like their non-adative brethren. Rather, the sequential deendence of the collection olicy can lead to severe distributional biases that ersist even in the infinite data limit. We develo a general method  -- \emh{$\vect{W}$-decorrelation} -- for transforming the bias of adative linear regression estimators into variance. The method uses only coarse-grained information about the data collection olicy and does not need access to roensity scores or exact knowledge of the olicy. We bound the finite-samle bias and variance of the $\vect{W}$-estimator and develo asymtotically correct confidence intervals based on a novel martingale central limit theorem.  We then demonstrate the emirical benefits of the generic $\vect{W}$-decorrelation rocedure in two different adative data settings: the multi-armed bandit and the autoregressive time series."
608,2018,Path-Level Network Transformation for Efficient Architecture Search,Oral/Poster,"We introduce a new function-reserving transformation for efficient neural architecture search. This network transformation allows reusing reviously trained networks and existing successful architectures that imroves samle efficiency. We aim to address the limitation of current network transformation oerations that can only erform layer-level architecture modifications, such as adding (runing) filters or inserting (removing) a layer, which fails to change the toology of connection aths. Our roosed ath-level transformation oerations enable the meta-controller to modify the ath toology of the given network while keeing the merits of reusing weights, and thus allow efficiently designing effective structures with comlex ath toologies like Incetion models. We further roose a bidirectional tree-structured reinforcement learning meta-controller to exlore a simle yet highly exressive tree-structured architecture sace that can be viewed as a generalization of multi-branch architectures. We exerimented on the image classification datasets with limited comutational resources (about 200 GPU-hours), where we observed imroved arameter efficiency and better test results (97.70% test accuracy on CIFAR-10 with 14.3M arameters and 74.6% to-1 accuracy on ImageNet in the mobile setting), demonstrating the effectiveness and transferability of our designed architectures.
"
609,2018,Progress & Compress: A scalable framework for continual learning,Oral/Poster,"We introduce a concetually simle and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of arameters and is designed to reserve erformance on reviously encountered tasks while accelerating learning rogress on subsequent roblems. This is achieved by training a network with two comonents: A knowledge base, caable of solving reviously encountered roblems, which is connected to an active column that is emloyed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to rotect any reviously acquired skills. This cycle of active learning (rogression) followed by consolidation (comression) requires no architecture growth, no access to or storing of revious data or tasks, and no task-secific arameters. We demonstrate the rogress &am; comress aroach on sequential classification of handwritten alhabets as well as two reinforcement learning domains: Atari games and 3D maze navigation.
"
610,2018,Learning Longer-term Dependencies in RNNs with Auxiliary Losses,Oral/Poster,"Desite recent advances in training recurrent neural networks (RNNs), caturing long-term deendencies in sequences remains a fundamental challenge. Most aroaches use backroagation through time (BPTT), which is difficult to scale to very long sequences. This aer rooses a simle method that imroves the ability to cature long term deendencies in RNNs by adding an unsuervised auxiliary loss to the original objective. This auxiliary loss forces RNNs to either reconstruct revious events or redict next events in a sequence, making truncated backroagation feasible for long sequences and also imroving full BPTT. We evaluate our method on a variety of settings, including ixel-by-ixel image classification with sequence lengths u to 16000, and a real document classification benchmark. Our results highlight good erformance and resource efficiency of this aroach over cometitive baselines, including other recurrent models and a comarable sized Transformer. Further analyses reveal beneficial effects of the auxiliary loss on otimization and regularization, as well as extreme cases where there is little to no backroagation.
"
611,2018,Understanding and Simplifying One-Shot Architecture Search,Oral/Poster,"There is growing interest in automating neural network architecture design. Existing architecture search methods can be comutationally exensive, requiring thousands of different architectures to be trained from scratch. Recent work has exlored \emh{weight sharing} across  models to amortize the cost of training. Although revious methods reduced the cost of architecture search by orders of magnitude, they remain comlex, requiring hyernetworks or reinforcement learning controllers. We aim to understand weight sharing for one-shot architecture search. With careful exerimental analysis, we show that it is ossible to efficiently identify romising architectures from a comlex search sace without either hyernetworks or RL.
"
612,2018,Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents,Oral/Poster,"We consider the fully decentralized multi-agent reinforcement learning (MARL) roblem, where the agents are connected via a time-varying and ossibly sarse communication network. Secifically, we assume that the reward functions of the agents might corresond to different tasks, and are only known to the corresonding agent. Moreover, each agent makes individual decisions based on both the information observed locally and the messages received from its neighbors over the network. To maximize the globally averaged return over the network, we roose two fully decentralized actor-critic algorithms, which are alicable to large-scale MARL roblems in an online fashion. Convergence guarantees are rovided when the value functions are aroximated within the class of linear functions. Our work aears to be the first theoretical study of fully decentralized MARL algorithms for networked agents that use function aroximation.
"
613,2018,State Abstractions for Lifelong Reinforcement Learning,Oral/Poster,"In lifelong reinforcement learning, agents must effectively transfer knowledge across tasks while simultaneously addressing exloration, credit assignment, and generalization. State abstraction can hel overcome these hurdles by comressing the reresentation used by an agent, thereby reducing the comutational and statistical burdens of learning. To this end, we here develo theory to comute and use state abstractions in lifelong reinforcement learning. We introduce two new classes of abstractions: (1) transitive state abstractions, whose otimal form can be comuted efficiently, and (2) PAC state abstractions, which are guaranteed to hold with resect to a distribution of tasks. We show that the joint family of transitive PAC abstractions can be acquired efficiently, reserve near otimal-behavior, and exerimentally reduce samle comlexity in simle domains, thereby yielding a family of desirable abstractions for use in lifelong reinforcement learning. Along with these ositive results, we show that there are athological cases where state abstractions can negatively imact erformance.
"
614,2018,Bounding and Counting Linear Regions of Deep Neural Networks,Oral/Poster,"We investigate the comlexity of dee neural networks (DNN) that reresent iecewise linear (PWL) functions. In articular, we study the number of linear regions, i.e. ieces, that a PWL function reresented by a DNN can attain, both theoretically and emirically. We resent (i) tighter uer and lower bounds for the maximum number of linear regions on rectifier networks, which are exact for inuts of dimension one; (ii) a first uer bound for multi-layer maxout networks; and (iii) a first method to erform exact enumeration or counting of the number of regions by modeling the DNN with a mixed-integer linear formulation.  These bounds come from leveraging the dimension of the sace defining each linear region. The results also indicate that a dee rectifier network can only have more linear regions than every shallow counterart with same number of neurons if that number exceeds the dimension of the inut.
"
615,2018,Clipped Action Policy Gradient,Oral/Poster,"Many continuous control tasks have bounded action saces. When olicy gradient methods are alied to such tasks, out-of-bound actions need to be clied before execution, while olicies are usually otimized as if the actions are not clied. We roose a olicy gradient estimator that exloits the knowledge of actions being clied to reduce the variance in estimation. We rove that our estimator, named clied action olicy gradient (CAPG), is unbiased and achieves lower variance than the conventional estimator that ignores action bounds. Exerimental results demonstrate that CAPG generally outerforms the conventional estimator, indicating that it is a better olicy gradient estimator for continuous control tasks. The source code is available at htts:github.comfnet-researchcag.
"
616,2018,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,Oral/Poster,"In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of arameters. A key challenge is to handle the increased amount of data and extended training time. We have develoed a new distributed agent IMPALA (Imortance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughut by combining decouled acting and learning with a novel off-olicy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeeMind Lab environment (Beattie et al., 2016)) and Atari57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better erformance than revious agents with less data, and crucially exhibits ositive transfer between tasks as a result of its multi-task aroach.
"
617,2018,Inter and Intra Topic Structure Learning with Word Embeddings,Oral/Poster,"One imortant task of toic modeling for text analysis is interretability. By discovering structured toics one is able to yield imroved interretability as well as modeling accuracy. In this aer, we roose a novel toic model with a dee structure that exlores both inter-toic and intra-toic structures informed by word embeddings. Secifically, our model discovers inter toic structures in the form of toic hierarchies and discovers intra toic structures in the form of sub-toics, each of which is informed by word embeddings and catures a fine-grained thematic asect of a normal toic. Extensive exeriments demonstrate that our model achieves the state-of-the-art erformance in terms of erlexity, document classification, and toic quality. Moreover, with toic hierarchies and sub-toics, the toics discovered in our model are more interretable, roviding an illuminating means to understand text data.
"
618,2018,oi-VAE: Output Interpretable VAEs for Nonlinear Group Factor Analysis,Oral/Poster,"Dee generative models have recently yielded encouraging results in roducing subjectively realistic samles of comlex data. Far less attention has been aid to making these generative models interretable. In many scenarios, ranging from scientific alications to finance, the observed variables have a natural grouing.  It is often of interest to understand systems of interaction amongst these grous, and latent factor models (LFMs) are an attractive aroach.  However, traditional LFMs are limited by assuming a linear correlation structure.  We resent an outut interretable VAE (oi-VAE) for groued data that models comlex, nonlinear latent-to-observed relationshis.  We combine a structured VAE comrised of grou-secific generators with a sarsity-inducing rior.  We demonstrate that oi-VAE yields meaningful notions of interretability in the analysis of motion cature and MEG data.  We further show that in these situations, the regularization inherent to oi-VAE can actually lead to imroved generalization and learned generative rocesses.
"
619,2018,The Hidden Vulnerability of Distributed Learning in Byzantium,Oral/Poster,"While machine learning is going through an era of celebrated success, concerns have been raised about the vulnerability of its backbone: stochastic gradient descent (SGD). Recent aroaches have been roosed to ensure the robustness of distributed SGD against adversarial (Byzantine) workers sending \emh{oisoned} gradients during the training hase. Some of these aroaches have been roven \emh{Byzantine--resilient}: they ensure the \emh{convergence} of SGD desite the resence of a minority of adversarial workers. We show in this aer that \emh{convergence is not enough}. In high dimension $d \gg 1$, an adver\-sary can build on the loss function's non--convexity to make SGD converge to \emh{ineffective} models. More recisely, we bring to light that existing Byzantine--resilient schemes leave a \emh{margin of oisoning} of $\bigOmega\left(f(d)\right)$, where $f(d)$ increases at least like $\sqrt[]{d~}$. Based on this \emh{leeway}, we build a simle attack, and exerimentally show its strong to utmost effectivity on CIFAR--10 and MNIST. We introduce \emh{Bulyan}, and rove it significantly reduces the attackers leeway to a narrow $\bigO\,( \sfrac{1}{\sqrt{d~}})$ bound. We emirically show that Bulyan does not suffer the fragility of existing aggregation rules and, at a reasonable cost in terms of required batch size, achieves convergence \emh{as if} only non--Byzantine gradients had been used to udate the model."
620,2018,Asynchronous Byzantine Machine Learning (the case of SGD),Oral/Poster,"Asynchronous distributed machine learning solutions have roven very effective so far, but always assuming erfectly functioning workers. In ractice, some of the workers can however exhibit Byzantine behavior, caused by hardware failures, software bugs, corrut data, or even malicious attacks. We introduce Kardam, the first distributed asynchronous stochastic gradient descent (SGD) algorithm that coes with Byzantine workers. Kardam consists of two comlementary comonents: a filtering and a damening comonent. The first is scalar-based and ensures resilience against 13 Byzantine workers. Essentially, this filter leverages the Lischitzness of cost functions and acts as a self-stabilizer against Byzantine workers that would attemt to corrut the rogress of SGD. The damening comonent bounds the convergence rate by adjusting to stale information through a generic gradient weighting scheme. We rove that Kardam guarantees almost sure convergence in the resence of asynchrony and Byzantine behavior, and we derive its convergence rate. We evaluate Kardam on the CIFAR100 and EMNIST datasets and measure its overhead with resect to non Byzantine-resilient solutions. We emirically show that Kardam does not introduce additional noise to the learning rocedure but does induce a slowdown (the cost of Byzantine resilience) that we both theoretically and emirically show to be less than fn, where f is the number of Byzantine failures tolerated and n the total number of workers. Interestingly, we also emirically observe that the damening comonent is interesting in its own right for it enables to build an SGD algorithm that outerforms alternative staleness-aware asynchronous cometitors in environments with honest workers.
"
